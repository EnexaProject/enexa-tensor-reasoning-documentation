
@article{gels_multidimensional_2019,
	title = {Multidimensional {Approximation} of {Nonlinear} {Dynamical} {Systems}},
	volume = {14},
	issn = {1555-1415},
	doi = {10.1115/1.4043148},
	abstract = {A key task in the field of modeling and analyzing nonlinear dynamical systems is the recovery of unknown governing equations from measurement data only. There is a wide range of application areas for this important instance of system identification, ranging from industrial engineering and acoustic signal processing to stock market models. In order to find appropriate representations of underlying dynamical systems, various data-driven methods have been proposed by different communities. However, if the given data sets are high-dimensional, then these methods typically suffer from the curse of dimensionality. To significantly reduce the computational costs and storage consumption, we propose the method multidimensional approximation of nonlinear dynamical systems (MANDy) which combines data-driven methods with tensor network decompositions. The efficiency of the introduced approach will be illustrated with the aid of several high-dimensional nonlinear dynamical systems.},
	number = {6},
	urldate = {2019-05-01},
	journal = {Journal of Computational and Nonlinear Dynamics},
	author = {Gelß, Patrick and Klus, Stefan and Eisert, Jens and Schütte, Christof},
	month = apr,
	year = {2019},
	pages = {061006--061006--12},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/9DAMBNLE/Gelß et al. - 2019 - Multidimensional Approximation of Nonlinear Dynami.pdf:application/pdf},
}

@book{hackbusch_tensor_2012,
	address = {Berlin Heidelberg},
	series = {Springer {Series} in {Computational} {Mathematics}},
	title = {Tensor {Spaces} and {Numerical} {Tensor} {Calculus}},
	isbn = {978-3-642-28026-9},
	abstract = {Special numerical techniques are already needed to deal with nxn matrices for large n.Tensor data are of size nxnx...xn=n{\textasciicircum}d, where n{\textasciicircum}d exceeds the computer memory by far. They appear for problems of high spatial dimensions. Since standard methods fail, a particular tensor calculus is needed to treat such problems. The monograph describes the methods how tensors can be practically treated and how numerical operations can be performed. Applications are problems from quantum chemistry, approximation of multivariate functions, solution of pde, e.g., with stochastic coefficients, etc. ​},
	language = {en},
	urldate = {2020-01-30},
	publisher = {Springer-Verlag},
	author = {Hackbusch, Wolfgang},
	year = {2012},
	doi = {10.1007/978-3-642-28027-6},
	file = {Snapshot:/Users/alexgoessmann/Zotero/storage/PC9F8YHM/9783642280269.html:text/html},
}

@book{wainwright_high-dimensional_2019,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Statistics}: {A} {Non}-{Asymptotic} {Viewpoint}},
	isbn = {978-1-108-49802-9},
	shorttitle = {High-{Dimensional} {Statistics}},
	abstract = {Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.},
	urldate = {2020-11-23},
	publisher = {Cambridge University Press},
	author = {Wainwright, Martin J.},
	year = {2019},
	doi = {10.1017/9781108627771},
	file = {Snapshot:/Users/alexgoessmann/Zotero/storage/65TBDVYU/8A91ECEEC38F46DAB53E9FF8757C7A4E.html:text/html},
}

@article{hackbusch_new_2009,
	title = {A {New} {Scheme} for the {Tensor} {Representation}},
	volume = {15},
	issn = {1531-5851},
	abstract = {The paper presents a new scheme for the representation of tensors which is well-suited for high-order tensors. The construction is based on a hierarchy of tensor product subspaces spanned by orthonormal bases. The underlying binary tree structure makes it possible to apply standard Linear Algebra tools for performing arithmetical operations and for the computation of data-sparse approximations. In particular, a truncation algorithm can be implemented which is based on the standard matrix singular value decomposition (SVD) method.},
	language = {en},
	number = {5},
	urldate = {2021-06-18},
	journal = {Journal of Fourier Analysis and Applications},
	author = {Hackbusch, W. and Kühn, S.},
	month = oct,
	year = {2009},
	pages = {706--722},
	file = {Springer Full Text PDF:/Users/alexgoessmann/Zotero/storage/SJUUNNLH/Hackbusch und Kühn - 2009 - A New Scheme for the Tensor Representation.pdf:application/pdf},
}

@book{wainwright_graphical_2008,
	title = {Graphical {Models}, {Exponential} {Families}, and {Variational} {Inference}},
	isbn = {978-1-60198-184-4},
	abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances-including the key problems of computing marginals and modes of probability distributions-are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, Graphical Models, Exponential Families and Variational Inference develops general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. It describes how a wide variety of algorithms- among them sum-product, cluster variational methods, expectation-propagation, mean field methods, and max-product-can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.},
	language = {en},
	publisher = {Now Publishers Inc},
	author = {Wainwright, Martin J. and Jordan, Michael Irwin},
	year = {2008},
}

@article{glasser_expressive_2019,
	title = {Expressive power of tensor-network factorizations for probabilistic modeling},
	volume = {32},
	language = {en},
	urldate = {2021-07-06},
	journal = {Advances in Neural Information Processing Systems},
	author = {Glasser, Ivan and Sweke, Ryan and Pancotti, Nicola and Eisert, Jens and Cirac, Ignacio},
	year = {2019},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/HE7UYQQ8/Glasser et al. - 2019 - Expressive power of tensor-network factorizations .pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/F9Z9J49G/b86e8d03fe992d1b0e19656875ee557c-Abstract.html:text/html},
}

@article{robeva_duality_2019,
	title = {Duality of graphical models and tensor networks},
	volume = {8},
	issn = {2049-8772},
	doi = {10.1093/imaiai/iay009},
	abstract = {In this article we show the duality between tensor networks and undirected graphical models with discrete variables. We study tensor networks on hypergraphs, which we call tensor hypernetworks. We show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph. We translate various notions under duality. For example, marginalization in a graphical model is dual to contraction in the tensor network. Algorithms also translate under duality. We show that belief propagation corresponds to a known algorithm for tensor network contraction. This article is a reminder that the research areas of graphical models and tensor networks can benefit from interaction.},
	number = {2},
	urldate = {2021-07-06},
	journal = {Information and Inference: A Journal of the IMA},
	author = {Robeva, Elina and Seigal, Anna},
	month = jun,
	year = {2019},
	pages = {273--288},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/EFRFIZTN/Robeva und Seigal - 2019 - Duality of graphical models and tensor networks.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/5X57ZXMD/5041985.html:text/html},
}

@inproceedings{goesmann_tensor_2020,
	title = {Tensor network approaches for data-driven identiﬁcation of non-linear dynamical laws},
	abstract = {To date, scalable methods for data-driven identiﬁcation of non-linear governing equations do not exploit or offer insight into the fundamental underlying physical structure. In this work, we show that various physical constraints can be captured via tensor network based parameterizations for the governing equation, which naturally ensures scalability. In addition to providing analytic results motivating the use of such models for realistic physical systems, we demonstrate that efﬁcient rank-adaptive optimization algorithms can be used to learn optimal tensor network models without requiring a priori knowledge of the exact tensor ranks.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} - {First} {Workshop} on {Quantum} {Tensor} {Networks} in {Machine} {Learning}},
	author = {Goeßmann, Alex and Roth, Ingo and Kutyniok, Gitta and Götte, Michael and Sweke, Ryan and Eisert, Jens},
	year = {2020},
	pages = {21},
	file = {Goeßmann et al. - Tensor network approaches for data-driven identiﬁc.pdf:/Users/alexgoessmann/Zotero/storage/GVJXJ77H/Goeßmann et al. - Tensor network approaches for data-driven identiﬁc.pdf:application/pdf},
}

@phdthesis{goesmann_uniform_2021,
	address = {Berlin},
	type = {{PhD} {Thesis}},
	title = {Uniform {Concentration} of {Tensor} and {Neural} {Networks}: {An} {Approach} towards {Recovery} {Guarantees}},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	url = {https://depositonce.tu-berlin.de/handle/11303/15990},
	abstract = {This thesis contributes to the uniform concentration approach towards guaranteeing the generalization of learned models. We show probabilistic bounds on various uniform concentration events and demonstrate their utility in recovery guarantees. The thesis is organized in three parts.
In the first part, we develop a unified theoretical framework for the concentration of random variables and the uniform concentration of stochastic processes. We introduce functionals of stochastic processes and apply them in bounds on the supremum. Then we develop methods to transfer uniform concentration events into success guarantees for empirical risk minimization problems.
The second part of this thesis investigates classes of structured random distributions. More precisely, we derive bounds on the uniform concentration of contracted random tensors, which are decomposable into tensor network formats. In particular, we show exact moment bounds on contracted Gaussian tensor networks, which are tensor networks consistent of independent standard Gaussian random cores. By applying comparison theorems for Gaussian variables, the upper moment bounds are extended to more generic Orlicz tensor networks, which are characterized by weaker assumptions made on the random cores. Furthermore, we derive bounds on the concentration of Haar tensor networks, which random cores follow the Haar distribution of Stiefel manifolds. For all examples we continue to provide probabilistic bounds on uniform concentration events, which imply recovery guarantees for tensor regression problems. We further apply our findings in deriving success guarantees for efficient algorithms solving tensor regression problems.
In the third part, we transfer our findings to bounds on the uniform concentration of neural networks following two approaches. First, we derive concentration bounds for shallow ReLU networks with respect to standard Gaussian distributions, where we introduce parameter embeddings that capture the concentration structure. Second, we bound the Rademacher complexity of deep neural networks, which are activated by a contraction, by Rademacher complexities of linear functions. This enables the proof of recovery guarantees for neural networks, which are trained on structured data.},
	language = {en},
	urldate = {2022-01-13},
	school = {Technische Universität Berlin},
	author = {Goeßmann, Alex Christoph},
	year = {2021},
	note = {Accepted: 2021-12-30T15:00:58Z},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/8QLGC9FD/Goeßmann - 2021 - Uniform concentration of tensor and neural network.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/ZKMYB6H4/15990.html:text/html},
}

@article{richardson_markov_2006,
	title = {Markov logic networks},
	volume = {62},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-006-5833-1},
	doi = {10.1007/s10994-006-5833-1},
	abstract = {We propose a simple approach to combining ﬁrst-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a ﬁrst-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it speciﬁes a ground Markov network containing one feature for each possible grounding of a ﬁrst-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efﬁciently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.},
	language = {en},
	number = {1-2},
	urldate = {2023-01-14},
	journal = {Machine Learning},
	author = {Richardson, Matthew and Domingos, Pedro},
	month = feb,
	year = {2006},
	pages = {107--136},
	file = {Richardson und Domingos - 2006 - Markov logic networks.pdf:/Users/alexgoessmann/Zotero/storage/AJNEGGHC/Richardson und Domingos - 2006 - Markov logic networks.pdf:application/pdf},
}

@inproceedings{galarraga_amie_2013,
	address = {Rio de Janeiro Brazil},
	title = {{AMIE}: association rule mining under incomplete evidence in ontological knowledge bases},
	isbn = {978-1-4503-2035-1},
	shorttitle = {{AMIE}},
	url = {https://dl.acm.org/doi/10.1145/2488388.2488425},
	doi = {10.1145/2488388.2488425},
	abstract = {Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive Logic Programming (ILP) can be used to mine logical rules from the KB. These rules can help deduce and add missing knowledge to the KB. While ILP is a mature ﬁeld, mining logical rules from KBs is diﬀerent in two aspects: First, current rule mining systems are easily overwhelmed by the amount of data (state-of-the art systems cannot even run on today’s KBs). Second, ILP usually requires counterexamples. KBs, however, implement the open world assumption (OWA), meaning that absent data cannot be used as counterexamples. In this paper, we develop a rule mining model that is explicitly tailored to support the OWA scenario. It is inspired by association rule mining and introduces a novel measure for conﬁdence. Our extensive experiments show that our approach outperforms state-of-the-art approaches in terms of precision and coverage. Furthermore, our system, AMIE, mines rules orders of magnitude faster than state-of-the-art approaches.},
	language = {en},
	urldate = {2023-11-06},
	booktitle = {Proceedings of the 22nd international conference on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Galárraga, Luis Antonio and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian},
	month = may,
	year = {2013},
	pages = {413--422},
	file = {Galárraga et al. - 2013 - AMIE association rule mining under incomplete evidence in ontological knowledge bases.pdf:/Users/alexgoessmann/Zotero/storage/5J7XU7WH/Galárraga et al. - 2013 - AMIE association rule mining under incomplete evidence in ontological knowledge bases.pdf:application/pdf},
}

@article{lehmann_class_2011,
	title = {Class expression learning for ontology engineering},
	volume = {9},
	issn = {1570-8268},
	url = {https://www.sciencedirect.com/science/article/pii/S1570826811000023},
	doi = {10.1016/j.websem.2011.01.001},
	abstract = {While the number of knowledge bases in the Semantic Web increases, the maintenance and creation of ontology schemata still remain a challenge. In particular creating class expressions constitutes one of the more demanding aspects of ontology engineering. In this article we describe how to adapt a semi-automatic method for learning OWL class expressions to the ontology engineering use case. Specifically, we describe how to extend an existing learning algorithm for the class learning problem. We perform rigorous performance optimization of the underlying algorithms for providing instant suggestions to the user. We also present two plugins, which use the algorithm, for the popular Protégé and OntoWiki ontology editors and provide a preliminary evaluation on real ontologies.},
	number = {1},
	urldate = {2023-11-06},
	journal = {Journal of Web Semantics},
	author = {Lehmann, Jens and Auer, Sören and Bühmann, Lorenz and Tramp, Sebastian},
	month = mar,
	year = {2011},
	keywords = {Heuristics, OWL, Concept learning, Ontology editor plugins, Ontology engineering, Supervised machine learning},
	pages = {71--81},
	file = {ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/FJ2IMSV5/S1570826811000023.html:text/html},
}

@article{demir_drill-_2021,
	title = {{DRILL}- {Deep} {Reinforcement} {Learning} for {Refinement} {Operators} in {ALC}},
	volume = {abs/2106.15373},
	url = {https://ris.uni-paderborn.de/record/25217},
	language = {eng},
	urldate = {2023-11-06},
	journal = {CoRR},
	author = {Demir, Caglar and Ngonga Ngomo, Axel-Cyrille},
	year = {2021},
	file = {Snapshot:/Users/alexgoessmann/Zotero/storage/J97XWWW6/25217.html:text/html},
}

@incollection{pesquita_neural_2023,
	address = {Cham},
	title = {Neural {Class} {Expression} {Synthesis}},
	volume = {13870},
	isbn = {978-3-031-33454-2 978-3-031-33455-9},
	url = {https://link.springer.com/10.1007/978-3-031-33455-9_13},
	language = {en},
	urldate = {2023-11-06},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer Nature Switzerland},
	author = {Kouagou, N’Dah Jean and Heindorf, Stefan and Demir, Caglar and Ngonga Ngomo, Axel-Cyrille},
	editor = {Pesquita, Catia and Jimenez-Ruiz, Ernesto and McCusker, Jamie and Faria, Daniel and Dragoni, Mauro and Dimou, Anastasia and Troncy, Raphael and Hertling, Sven},
	year = {2023},
	doi = {10.1007/978-3-031-33455-9_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {209--226},
	file = {Kouagou et al. - 2023 - Neural Class Expression Synthesis.pdf:/Users/alexgoessmann/Zotero/storage/2JW9DENY/Kouagou et al. - 2023 - Neural Class Expression Synthesis.pdf:application/pdf},
}

@misc{kouagou_neural_2022,
	title = {Neural {Class} {Expression} {Synthesis}},
	url = {http://arxiv.org/abs/2111.08486},
	abstract = {Most existing approaches for class expression learning in description logics are search algorithms. As the search space of these approaches is infinite, they often fail to scale to large learning problems. Our main intuition is that class expression learning can be regarded as a translation problem. Based thereupon, we propose a new family of class expression learning approaches which we dub neural class expression synthesis. Instances of this new family circumvent the high search costs entailed by current algorithms by translating training examples into class expressions in a fashion akin to machine translation solutions. Consequently, they are not subject to the runtime limitations of search-based approaches post training. We study three instances of this novel family of approaches to synthesize class expressions from sets of positive and negative examples. An evaluation of our approach on four benchmark datasets suggests that it can effectively synthesize high-quality class expressions with respect to the input examples in approximately one second on average. Moreover, a comparison to other state-of-the-art approaches suggests that we achieve better F-measures on large datasets. For reproducibility purposes, we provide our implementation as well as pretrained models in our public GitHub repository at https://github.com/fosterreproducibleresearch/NCES.},
	language = {en},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Kouagou, N'Dah Jean and Heindorf, Stefan and Demir, Caglar and Ngomo, Axel-Cyrille Ngonga},
	month = dec,
	year = {2022},
	note = {arXiv:2111.08486 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Kouagou et al. - 2022 - Neural Class Expression Synthesis.pdf:/Users/alexgoessmann/Zotero/storage/67GHBI5I/Kouagou et al. - 2022 - Neural Class Expression Synthesis.pdf:application/pdf},
}

@article{muggleton_inductive_1994,
	title = {Inductive logic programming: {Theory} and methods},
	volume = {19},
	shorttitle = {Inductive logic programming},
	url = {https://www.sciencedirect.com/science/article/pii/0743106694900353},
	urldate = {2023-11-06},
	journal = {The Journal of Logic Programming},
	author = {Muggleton, Stephen and De Raedt, Luc},
	year = {1994},
	note = {Publisher: Elsevier},
	pages = {629--679},
}

@incollection{li_linear_2017,
	address = {Cham},
	title = {Linear {Algebraic} {Characterization} of {Logic} {Programs}},
	volume = {10412},
	isbn = {978-3-319-63557-6 978-3-319-63558-3},
	url = {http://link.springer.com/10.1007/978-3-319-63558-3_44},
	abstract = {This paper introduces a novel approach for computing logic programming semantics based on multilinear algebra. First, a propositional Herbrand base is represented in a vector space and if-then rules in a program are encoded in a matrix. Then we provide methods of computing the least model of a Horn logic program, minimal models of a disjunctive logic program, and stable models of a normal logic program by algebraic manipulation of higher-order tensors. The result of this paper exploits a new connection between linear algebraic computation and symbolic computation, which has potential to realize logical inference in huge scale of knowledge bases.},
	language = {en},
	urldate = {2023-11-06},
	booktitle = {Knowledge {Science}, {Engineering} and {Management}},
	publisher = {Springer International Publishing},
	author = {Sakama, Chiaki and Inoue, Katsumi and Sato, Taisuke},
	editor = {Li, Gang and Ge, Yong and Zhang, Zili and Jin, Zhi and Blumenstein, Michael},
	year = {2017},
	doi = {10.1007/978-3-319-63558-3_44},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {520--533},
	file = {Sakama et al. - 2017 - Linear Algebraic Characterization of Logic Programs.pdf:/Users/alexgoessmann/Zotero/storage/5FT67GGU/Sakama et al. - 2017 - Linear Algebraic Characterization of Logic Programs.pdf:application/pdf},
}

@article{sato_linear_2017,
	title = {A linear algebraic approach to datalog evaluation},
	volume = {17},
	issn = {1471-0684, 1475-3081},
	url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/linear-algebraic-approach-to-datalog-evaluation/CED3EEB903D9D8A16843CFCA5AC4D577},
	doi = {10.1017/S1471068417000023},
	abstract = {We propose a fundamentally new approach to Datalog evaluation. Given a linear Datalog program DB written using N constants and binary predicates, we first translate if-and-only-if completions of clauses in DB into a set E
               
                  q
               (DB) of matrix equations with a non-linear operation, where relations in M
               DB, the least Herbrand model of DB, are encoded as adjacency matrices. We then translate E
               
                  q
               (DB) into another, but purely linear matrix equations Ẽ
               
                  q
               (DB). It is proved that the least solution of Ẽ
               
                  q
               (DB) in the sense of matrix ordering is converted to the least solution of E
               
                  q
               (DB) and the latter gives M
               DB as a set of adjacency matrices. Hence, computing the least solution of Ẽ
               
                  q
               (DB) is equivalent to computing M
               DB specified by DB. For a class of tail recursive programs and for some other types of programs, our approach achieves O(N
               3) time complexity irrespective of the number of variables in a clause since only matrix operations costing O(N
               3) or less are used. We conducted two experiments that compute the least Herbrand models of linear Datalog programs. The first experiment computes transitive closure of artificial data and real network data taken from the Koblenz Network Collection. The second one compared the proposed approach with the state-of-the-art symbolic systems including two Prolog systems and two ASP systems, in terms of computation time for a transitive closure program and the same generation program. In the experiment, it is observed that our linear algebraic approach runs 101 {\textasciitilde} 104 times faster than the symbolic systems when data is not sparse. Our approach is inspired by the emergence of big knowledge graphs and expected to contribute to the realization of rich and scalable logical inference for knowledge graphs.},
	language = {en},
	number = {3},
	urldate = {2023-11-06},
	journal = {Theory and Practice of Logic Programming},
	author = {Sato, Taisuke},
	month = may,
	year = {2017},
	note = {Publisher: Cambridge University Press},
	keywords = {Datalog, least model, matrix, vector space},
	pages = {244--265},
	file = {Eingereichte Version:/Users/alexgoessmann/Zotero/storage/C2AIFVTN/Sato - 2017 - A linear algebraic approach to datalog evaluation.pdf:application/pdf},
}

@article{nickel_review_2016,
	title = {A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}},
	volume = {104},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/7358050/},
	doi = {10.1109/JPROC.2015.2483592},
	abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The ﬁrst is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google’s Knowledge Vault project as an example of such combination.},
	language = {en},
	number = {1},
	urldate = {2023-11-06},
	journal = {Proceedings of the IEEE},
	author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
	month = jan,
	year = {2016},
	pages = {11--33},
	file = {Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowledge Graphs.pdf:/Users/alexgoessmann/Zotero/storage/SYKDZ9EC/Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowledge Graphs.pdf:application/pdf},
}

@article{trouillon_complex_nodate,
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	language = {en},
	author = {Trouillon, Theo and Welbl, Johannes and Riedel, Sebastian},
	file = {Trouillon et al. - Complex Embeddings for Simple Link Prediction.pdf:/Users/alexgoessmann/Zotero/storage/LYGPCF95/Trouillon et al. - Complex Embeddings for Simple Link Prediction.pdf:application/pdf},
}

@misc{trouillon_complex_2017,
	title = {Complex and {Holographic} {Embeddings} of {Knowledge} {Graphs}: {A} {Comparison}},
	shorttitle = {Complex and {Holographic} {Embeddings} of {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/1707.01475},
	abstract = {Embeddings of knowledge graphs have received significant attention due to their excellent performance for tasks like link prediction and entity resolution. In this short paper, we are providing a comparison of two state-of-the-art knowledge graph embeddings for which their equivalence has recently been established, i.e., ComplEx and HolE [Nickel, Rosasco, and Poggio, 2016; Trouillon et al., 2016; Hayashi and Shimbo, 2017]. First, we briefly review both models and discuss how their scoring functions are equivalent. We then analyze the discrepancy of results reported in the original articles, and show experimentally that they are likely due to the use of different loss functions. In further experiments, we evaluate the ability of both models to embed symmetric and antisymmetric patterns. Finally, we discuss advantages and disadvantages of both models and under which conditions one would be preferable to the other.},
	language = {en},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Trouillon, Théo and Nickel, Maximilian},
	month = jul,
	year = {2017},
	note = {arXiv:1707.01475 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Trouillon und Nickel - 2017 - Complex and Holographic Embeddings of Knowledge Graphs A Comparison.pdf:/Users/alexgoessmann/Zotero/storage/RR52API2/Trouillon und Nickel - 2017 - Complex and Holographic Embeddings of Knowledge Graphs A Comparison.pdf:application/pdf},
}

@article{trouillon_knowledge_2017,
	title = {Knowledge {Graph} {Completion} via {Complex} {Tensor} {Factorization}},
	language = {en},
	author = {Trouillon, Theo},
	year = {2017},
	file = {Trouillon - Knowledge Graph Completion via Complex Tensor Factorization.pdf:/Users/alexgoessmann/Zotero/storage/65IT5FVF/Trouillon - Knowledge Graph Completion via Complex Tensor Factorization.pdf:application/pdf},
}

@inproceedings{yang_embedding_2015,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a uniﬁed learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq {\textasciicircum} CityInCountrypb, cq ùñ N ationalitypa, cq. We ﬁnd that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-ofthe-art conﬁdence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	language = {en},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	note = {arXiv:1412.6575 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Yang et al. - 2015 - Embedding Entities and Relations for Learning and Inference in Knowledge Bases.pdf:/Users/alexgoessmann/Zotero/storage/WWTTIZ78/Yang et al. - 2015 - Embedding Entities and Relations for Learning and Inference in Knowledge Bases.pdf:application/pdf},
}

@inproceedings{bigerl_tentris_2020,
	title = {Tentris -{A} {Tensor}-{Based} {Triple} {Store}},
	abstract = {The number and size of RDF knowledge graphs grows continuously. Efficient storage solutions for these graphs are indispensable for their use in real applications. We present such a storage solution dubbed TENTRIS. Our solution represents RDF knowledge graphs as sparse order-3 tensors using a novel data structure, which we dub hypertrie. It then uses tensor algebra to carry out SPARQL queries by mapping SPARQL operations to Einstein summation. By being able to compute Einstein summations efficiently, TENTRIS outperforms the commercial and open-source RDF storage solutions evaluated in our experiments by at least 1.8 times with respect to the average number of queries it can serve per second on three datasets of up to 1 billion triples. Our code, evaluation setup, results, supplementary material and the datasets are provided at https://tentris.dice-research.org/iswc2020.},
	author = {Bigerl, Alexander and Conrads, Felix and Behning, Charlotte and Sherif, Mohamed and Saleem, Muhammad and Ngonga Ngomo, Axel-Cyrille},
	month = sep,
	year = {2020},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/6EWR5B3H/Bigerl et al. - 2020 - Tentris -A Tensor-Based Triple Store.pdf:application/pdf},
}

@misc{lam_graphcast_2023,
	title = {{GraphCast}: {Learning} skillful medium-range global weather forecasting},
	shorttitle = {{GraphCast}},
	url = {http://arxiv.org/abs/2212.12794},
	abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but cannot directly use historical weather data to improve the underlying model. We introduce a machine learning-based method called “GraphCast”, which can be trained directly from reanalysis data. It predicts hundreds of weather variables, over 10 days at 0.25° resolution globally, in under one minute. We show that GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclones, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting, and helps realize the promise of machine learning for modeling complex dynamical systems.},
	language = {en},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
	month = aug,
	year = {2023},
	note = {arXiv:2212.12794 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {Lam et al. - 2023 - GraphCast Learning skillful medium-range global weather forecasting.pdf:/Users/alexgoessmann/Zotero/storage/LRRDNJMD/Lam et al. - 2023 - GraphCast Learning skillful medium-range global weather forecasting.pdf:application/pdf},
}

@article{badreddine_logic_2022,
	title = {Logic {Tensor} {Networks}},
	volume = {303},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221002009},
	doi = {10.1016/j.artint.2021.103649},
	abstract = {Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.},
	urldate = {2023-11-20},
	journal = {Artificial Intelligence},
	author = {Badreddine, Samy and d'Avila Garcez, Artur and Serafini, Luciano and Spranger, Michael},
	month = feb,
	year = {2022},
	keywords = {Deep learning and reasoning, Many-valued logics, Neurosymbolic AI},
	pages = {103649},
	file = {Akzeptierte Version:/Users/alexgoessmann/Zotero/storage/XKSPFEM9/Badreddine et al. - 2022 - Logic Tensor Networks.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/MB3PW4KU/S0004370221002009.html:text/html},
}

@article{westphal_sml-bench_2019,
	title = {{SML}-{Bench} – {A} benchmarking framework for structured machine learning},
	volume = {10},
	issn = {22104968, 15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-180308},
	doi = {10.3233/SW-180308},
	abstract = {The availability of structured data has increased significantly over the past decade and several approaches to learn from structured data have been proposed. These logic-based, inductive learning methods are often conceptually similar, which would allow a comparison among them even if they stem from different research communities. However, so far no efforts were made to define an environment for running learning tasks on a variety of tools, covering multiple knowledge representation languages. With SML-Bench, we propose a benchmarking framework to run inductive learning tools from the ILP and semantic web communities on a selection of learning problems. In this paper, we present the foundations of SML-Bench, discuss the systematic selection of benchmarking datasets and learning problems, and showcase an actual benchmark run on the currently supported tools.},
	language = {en},
	number = {2},
	urldate = {2023-12-20},
	journal = {Semantic Web},
	author = {Westphal, Patrick and Bühmann, Lorenz and Bin, Simon and Jabeen, Hajira and Lehmann, Jens},
	editor = {Ngonga Ngomo, Axel-Cyrille and Fundulaki, Irini and Krithara, Anastasia and Ngonga Ngomo, Axel-Cyrille and Fundulaki, Irini and Krithara, Anastasia},
	month = jan,
	year = {2019},
	pages = {231--245},
	file = {Westphal et al. - 2019 - SML-Bench – A benchmarking framework for structured machine learning.pdf:/Users/alexgoessmann/Zotero/storage/5L642YFC/Westphal et al. - 2019 - SML-Bench – A benchmarking framework for structured machine learning.pdf:application/pdf},
}

@article{marra_statistical_2024,
	title = {From statistical relational to neurosymbolic artificial intelligence: {A} survey},
	volume = {328},
	issn = {0004-3702},
	shorttitle = {From statistical relational to neurosymbolic artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370223002084},
	doi = {10.1016/j.artint.2023.104062},
	abstract = {This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.},
	urldate = {2024-02-20},
	journal = {Artificial Intelligence},
	author = {Marra, Giuseppe and Dumančić, Sebastijan and Manhaeve, Robin and De Raedt, Luc},
	month = mar,
	year = {2024},
	keywords = {Neurosymbolic AI, Learning and reasoning, Probabilistic logics, Statistical relational AI},
	pages = {104062},
	file = {ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/NRNUENXF/S0004370223002084.html:text/html},
}

@article{cohen_tensorlog_2020,
	title = {{TensorLog}: {A} {Probabilistic} {Database} {Implemented} {Using} {Deep}-{Learning} {Infrastructure}},
	volume = {67},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {{TensorLog}},
	url = {https://jair.org/index.php/jair/article/view/11944},
	doi = {10.1613/jair.1.11944},
	abstract = {We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.},
	language = {en},
	urldate = {2024-02-20},
	journal = {Journal of Artificial Intelligence Research},
	author = {Cohen, William and Yang, Fan and Mazaitis, Kathryn Rivard},
	month = feb,
	year = {2020},
	pages = {285--325},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/QK9JGLW2/Cohen et al. - 2020 - TensorLog A Probabilistic Database Implemented Using Deep-Learning Infrastructure.pdf:application/pdf},
}

@article{hochreiter_toward_2022,
	title = {Toward a broad {AI}},
	volume = {65},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3512715},
	doi = {10.1145/3512715},
	language = {en},
	number = {4},
	urldate = {2024-02-22},
	journal = {Communications of the ACM},
	author = {Hochreiter, Sepp},
	month = apr,
	year = {2022},
	pages = {56--57},
	file = {Hochreiter - 2022 - Toward a broad AI.pdf:/Users/alexgoessmann/Zotero/storage/8ZDZBDY8/Hochreiter - 2022 - Toward a broad AI.pdf:application/pdf},
}

@article{pinkas_reasoning_1995,
	title = {Reasoning, nonmonotonicity and learning in connectionist networks that capture propositional knowledge},
	volume = {77},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/000437029400032V},
	doi = {10.1016/0004-3702(94)00032-V},
	abstract = {The paper presents a connectionist framework that is capable of representing and learning propositional knowledge. An extended version of propositional calculus is developed and is demonstrated to be useful for nonmonotonic reasoning, dealing with conflicting beliefs and for coping with inconsistency generated by unreliable knowledge sources. Formulas of the extended calculus are proved to be equivalent in a very strong sense to symmetric networks (like Hopfield networks and Boltzmann machines), and efficient algorithms are given for translating back and forth between the two forms of knowledge representation. A fast learning procedure is presented that allows symmetric networks to learn representations of unknown logic formulas by looking at examples. A connectionist inference engine is then sketched whose knowledge is either compiled from a symbolic representation or learned inductively from training examples. Experiments with large scale randomly generated formulas suggest that the parallel local search that is executed by the networks is extremely fast on average. Finally, it is shown that the extended logic can be used as a high-level specification language for connectionist networks, into which several recent symbolic systems may be mapped. The paper demonstrates how a rigorous bridge can be constructed that ties together the (sometimes opposing) connectionist and symbolic approaches.},
	number = {2},
	urldate = {2024-02-23},
	journal = {Artificial Intelligence},
	author = {Pinkas, Gadi},
	month = sep,
	year = {1995},
	pages = {203--247},
	file = {ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/H4I7IQX5/000437029400032V.html:text/html},
}

@inproceedings{marra_neural_2021,
	title = {Neural markov logic networks},
	url = {https://proceedings.mlr.press/v161/marra21a.html},
	abstract = {We introduce neural Markov logic networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov logic networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Similarly to many neural symbolic methods, NMLNs can exploit embeddings of constants but, unlike them, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion, triple classification and on generation of molecular (graph) data.},
	language = {en},
	urldate = {2024-02-23},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Marra, Giuseppe and Kuželka, Ondřej},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {908--917},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/4AZBL7F4/Marra und Kuželka - 2021 - Neural markov logic networks.pdf:application/pdf;Supplementary PDF:/Users/alexgoessmann/Zotero/storage/2AZDGJFS/Marra und Kuželka - 2021 - Neural markov logic networks.pdf:application/pdf},
}

@misc{grelier_learning_2019,
	title = {Learning with tree-based tensor formats},
	url = {http://arxiv.org/abs/1811.04455},
	doi = {10.48550/arXiv.1811.04455},
	abstract = {This paper is concerned with the approximation of high-dimensional functions in a statistical learning setting, by empirical risk minimization over model classes of functions in tree-based tensor format. These are particular classes of rank-structured functions that can be seen as deep neural networks with a sparse architecture related to the tree and multilinear activation functions. For learning in a given model class, we exploit the fact that tree-based tensor formats are multilinear models and recast the problem of risk minimization over a nonlinear set into a succession of learning problems with linear models. Suitable changes of representation yield numerically stable learning problems and allow to exploit sparsity. For high-dimensional problems or when only a small data set is available, the selection of a good model class is a critical issue. For a given tree, the selection of the tuple of tree-based ranks that minimize the risk is a combinatorial problem. Here, we propose a rank adaptation strategy which provides in practice a good convergence of the risk as a function of the model class complexity. Finding a good tree is also a combinatorial problem, which can be related to the choice of a particular sparse architecture for deep neural networks. Here, we propose a stochastic algorithm for minimizing the complexity of the representation of a given function over a class of trees with a given arity, allowing changes in the topology of the tree. This tree optimization algorithm is then included in a learning scheme that successively adapts the tree and the corresponding tree-based ranks. Contrary to classical learning algorithms for nonlinear model classes, the proposed algorithms are numerically stable, reliable, and require only a low level expertise of the user.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Grelier, Erwan and Nouy, Anthony and Chevreuil, Mathilde},
	month = jan,
	year = {2019},
	note = {arXiv:1811.04455 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/alexgoessmann/Zotero/storage/I22PJXGL/Grelier et al. - 2019 - Learning with tree-based tensor formats.pdf:application/pdf;arXiv.org Snapshot:/Users/alexgoessmann/Zotero/storage/BVVYAZPX/1811.html:text/html},
}

@article{sarker_neuro-symbolic_2022,
	title = {Neuro-symbolic artificial intelligence: {Current} trends},
	volume = {34},
	issn = {18758452, 09217126},
	shorttitle = {Neuro-symbolic artificial intelligence},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AIC-210084},
	doi = {10.3233/AIC-210084},
	abstract = {Neuro-Symbolic Artificial Intelligence – the combination of symbolic methods with methods that are based on artificial neural networks – has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.},
	number = {3},
	urldate = {2024-04-17},
	journal = {AI Communications},
	author = {Sarker, Md Kamruzzaman and Zhou, Lu and Eberhart, Aaron and Hitzler, Pascal},
	month = mar,
	year = {2022},
	pages = {197--209},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/DT6NNMMP/Sarker et al. - 2022 - Neuro-symbolic artificial intelligence Current trends.pdf:application/pdf},
}

@inproceedings{ankan_pgmpy_2015,
	address = {Austin, Texas},
	title = {pgmpy: {Probabilistic} {Graphical} {Models} using {Python}},
	shorttitle = {pgmpy},
	url = {https://conference.scipy.org/proceedings/scipy2015/ankur_ankan.html},
	doi = {10.25080/Majora-7b98e3ed-001},
	abstract = {Probabilistic Graphical Models (PGM) is a technique of compactly representing a joint distribution by exploiting dependencies between the random variables. It also allows us to do inference on joint distributions in a computationally cheaper way than the traditional methods. PGMs are widely used in the ﬁeld of speech recognition, information extraction, image segmentation, modelling gene regulatory networks.},
	language = {en},
	urldate = {2024-07-02},
	author = {Ankan, Ankur and Panda, Abinash},
	year = {2015},
	pages = {6--11},
	file = {Ankan und Panda - 2015 - pgmpy Probabilistic Graphical Models using Python.pdf:/Users/alexgoessmann/Zotero/storage/XFTQUPAG/Ankan und Panda - 2015 - pgmpy Probabilistic Graphical Models using Python.pdf:application/pdf},
}

@incollection{karalis_native_2023,
	title = {Native {Execution} of {GraphQL} {Queries} over {RDF} {Graphs} {Using} {Multi}-{Way} {Joins}},
	url = {https://ebooks.iospress.nl/volumearticle/64014},
	urldate = {2024-07-02},
	booktitle = {Knowledge {Graphs}: {Semantics}, {Machine} {Learning}, and {Languages}},
	publisher = {IOS Press},
	author = {Karalis, Nikolaos and Bigerl, Alexander and Ngonga Ngomo, Axel-Cyrille},
	year = {2023},
	pages = {77--93},
	file = {Available Version (via Google Scholar):/Users/alexgoessmann/Zotero/storage/EBL5MCE9/Karalis et al. - 2023 - Native Execution of GraphQL Queries over RDF Graphs Using Multi-Way Joins.pdf:application/pdf},
}

@inproceedings{karalis_efficient_2024,
	address = {Cham},
	title = {Efficient {Evaluation} of {Conjunctive} {Regular} {Path} {Queries} {Using} {Multi}-way {Joins}},
	isbn = {978-3-031-60626-7},
	doi = {10.1007/978-3-031-60626-7_12},
	abstract = {Recent analyses of real-world queries show that a prominent type of queries is that of conjunctive regular path queries. Despite the increasing popularity of this type of queries, only limited efforts have been invested in their efficient evaluation. Motivated by recent results on the efficiency of worst-case optimal multi-way join algorithms for the evaluation of conjunctive queries, we present a novel multi-way join algorithm for the efficient evaluation of conjunctive regular path queries. The hallmark of our algorithm is the evaluation of the regular path queries found in conjunctive regular path queries using multi-way joins. This enables the exploitation of regular path queries in the planning steps of the proposed algorithm, which is crucial for the algorithm’s efficiency, as shown by the results of our detailed evaluation using the Wikidata-based benchmark WDBench. The results of this evaluation also show that our approach achieves a value of query mixes per hour that is 4.3 higher than the state of the art and that it outperforms all of the competing graph storage solutions in almost 70\% of the benchmark’s queries.},
	language = {en},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer Nature Switzerland},
	author = {Karalis, Nikolaos and Bigerl, Alexander and Heidrich, Liss and Sherif, Mohamed Ahmed and Ngonga Ngomo, Axel-Cyrille},
	editor = {Meroño Peñuela, Albert and Dimou, Anastasia and Troncy, Raphaël and Hartig, Olaf and Acosta, Maribel and Alam, Mehwish and Paulheim, Heiko and Lisena, Pasquale},
	year = {2024},
	pages = {218--235},
}

@inproceedings{kok_learning_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {Learning the structure of {Markov} logic networks},
	isbn = {978-1-59593-180-1},
	url = {https://doi.org/10.1145/1102351.1102407},
	doi = {10.1145/1102351.1102407},
	abstract = {Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. In this paper we develop an algorithm for learning the structure of MLNs from relational databases, combining ideas from inductive logic programming (ILP) and feature induction in Markov networks. The algorithm performs a beam or shortest-first search of the space of clauses, guided by a weighted pseudo-likelihood measure. This requires computing the optimal weights for each candidate structure, but we show how this can be done efficiently. The algorithm can be used to learn an MLN from scratch, or to refine an existing knowledge base. We have applied it in two real-world domains, and found that it outperforms using off-the-shelf ILP systems to learn the MLN structure, as well as pure ILP, purely probabilistic and purely knowledge-based approaches.},
	urldate = {2024-08-07},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Kok, Stanley and Domingos, Pedro},
	month = aug,
	year = {2005},
	pages = {441--448},
	file = {Volltext:/Users/alexgoessmann/Zotero/storage/WZQP9USC/Kok und Domingos - 2005 - Learning the structure of Markov logic networks.pdf:application/pdf},
}

@phdthesis{nyga_interpretation_2017,
	title = {Interpretation of {Natural}-language {Robot} {Instructions}: {Probabilistic} {Knowledge} {Representation}, {Learning}, and {Reasoning}},
	copyright = {Bitte wählen Sie eine Lizenz aus: (Unsere Empfehlung: CC-BY)},
	shorttitle = {Interpretation of {Natural}-language {Robot} {Instructions}},
	url = {https://media.suub.uni-bremen.de/handle/elib/1215},
	abstract = {A robot that can be simply told in natural language what to do -- this has been one of the ultimate long-standing goals in both Artificial Intelligence and Robotics research. In near-future applications, robotic assistants and companions will have to understand and perform commands such as set the table for dinner'', make pancakes for breakfast'', or cut the pizza into 8 pieces.'' Although such instructions are only vaguely formulated, complex sequences of sophisticated and accurate manipulation activities need to be carried out in order to accomplish the respective tasks. The acquisition of knowledge about how to perform these activities from huge collections of natural-language instructions from the Internet has garnered a lot of attention within the last decade. However, natural language is typically massively unspecific, incomplete, ambiguous and vague and thus requires powerful means for interpretation. This work presents PRAC -- Probabilistic Action Cores -- an interpreter for natural-language instructions which is able to resolve vagueness and ambiguity in natural language and infer missing information pieces that are required to render an instruction executable by a robot. To this end, PRAC formulates the problem of instruction interpretation as a reasoning problem in first-order probabilistic knowledge bases. In particular, the system uses Markov logic networks as a carrier formalism for encoding uncertain knowledge. A novel framework for reasoning about unmodeled symbolic concepts is introduced, which incorporates ontological knowledge from taxonomies and exploits semantically similar relational structures in a domain of discourse. The resulting reasoning framework thus enables more compact representations of knowledge and exhibits strong generalization performance when being learnt from very sparse data. Furthermore, a novel approach for completing directives is presented, which applies semantic analogical reasoning to transfer knowledge collected from thousands of natural-language instruction sheets to new situations. In addition, a cohesive processing pipeline is described that transforms vague and incomplete task formulations into sequences of formally specified robot plans. The system is connected to a plan executive that is able to execute the computed plans in a simulator. Experiments conducted in a publicly accessible, browser-based web interface showcase that PRAC is capable of closing the loop from natural-language instructions to their execution by a robot.},
	language = {en},
	urldate = {2024-08-07},
	author = {Nyga, Daniel},
	month = may,
	year = {2017},
	note = {Accepted: 2020-03-09T14:46:38Z
Publisher: Universität Bremen},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/Q2WMY8FH/Nyga - 2017 - Interpretation of Natural-language Robot Instructions Probabilistic Knowledge Representation, Learn.pdf:application/pdf},
}

@inproceedings{tsilionis_tensor-based_2024,
	address = {Jeju, South Korea},
	title = {A {Tensor}-{Based} {Formalization} of the {Event} {Calculus}},
	isbn = {978-1-956792-04-1},
	url = {https://www.ijcai.org/proceedings/2024/397},
	doi = {10.24963/ijcai.2024/397},
	abstract = {We present a formalization of the Event Calculus (EC) in tensor spaces. The motivation for a tensorbased predicate calculus comes from the area of composite event recognition (CER). As a CER engine, we adopt a logic programming implementation of EC with optimizations for continuous narrative assimilation on data streams. We show how to evaluate EC rules algebraically and solve a linear equation to compute the corresponding models. We demonstrate the scalability of our approach with the use of large datasets from a real-world application domain, and show it outperforms significantly symbolic EC, in terms of processing time.},
	language = {en},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the {Thirty}-{ThirdInternational} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Tsilionis, Efthimis and Artikis, Alexander and Paliouras, Georgios},
	month = aug,
	year = {2024},
	pages = {3584--3592},
	file = {PDF:/Users/alexgoessmann/Zotero/storage/3J9ZWKGW/Tsilionis et al. - 2024 - A Tensor-Based Formalization of the Event Calculus.pdf:application/pdf},
}

@inproceedings{tran_fast_2020,
	address = {New York, NY, USA},
	series = {{WWW} '20},
	title = {Fast {Computation} of {Explanations} for {Inconsistency} in {Large}-{Scale} {Knowledge} {Graphs}},
	isbn = {978-1-4503-7023-3},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380014},
	doi = {10.1145/3366423.3380014},
	abstract = {Knowledge graphs (KGs) are essential resources for many applications including Web search and question answering. As KGs are often automatically constructed, they may contain incorrect facts. Detecting them is a crucial, yet extremely expensive task. Prominent solutions detect and explain inconsistency in KGs with respect to accompanying ontologies that describe the KG domain of interest. Compared to machine learning methods they are more reliable and human-interpretable but scale poorly on large KGs. In this paper, we present a novel approach to dramatically speed up the process of detecting and explaining inconsistency in large KGs by exploiting KG abstractions that capture prominent data patterns. Though much smaller, KG abstractions preserve inconsistency and their explanations. Our experiments with large KGs (e.g., DBpedia and Yago) demonstrate the feasibility of our approach and show that it significantly outperforms the popular baseline.},
	urldate = {2024-09-24},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {Association for Computing Machinery},
	author = {Tran, Trung-Kien and Gad-Elrab, Mohamed H. and Stepanova, Daria and Kharlamov, Evgeny and Strötgen, Jannik},
	month = apr,
	year = {2020},
	pages = {2613--2619},
	file = {Full Text PDF:/Users/alexgoessmann/Zotero/storage/PTM44K7G/Tran et al. - 2020 - Fast Computation of Explanations for Inconsistency in Large-Scale Knowledge Graphs.pdf:application/pdf},
}

@misc{teyou_embedding_2024,
	title = {Embedding {Knowledge} {Graphs} in {Degenerate} {Clifford} {Algebras}},
	url = {http://arxiv.org/abs/2402.04870},
	abstract = {Clifford algebras are a natural extension of division algebras, including real numbers, complex numbers, quaternions, and octonions. Previous research in knowledge graph embeddings has focused exclusively on Clifford algebras of a specific type, which do not include nilpotent base vectors—elements that square to zero. In this work, we introduce a novel approach by incorporating nilpotent base vectors with a nilpotency index of two, leading to a more general form of Clifford algebras named degenerate Clifford algebras. This generalization to degenerate Clifford algebras does allow for covering dual numbers and as such include translations and rotations models under the same generalization paradigm for the first time. We develop two models to determine the parameters that define the algebra: one using a greedy search and another predicting the parameters based on neural network embeddings of the input knowledge graph. Our evaluation on seven benchmark datasets demonstrates that this incorporation of nilpotent vectors enhances the quality of embeddings. Additionally, our method outperforms state-ofthe-art approaches in terms of generalization, particularly regarding the mean reciprocal rank achieved on validation data. Finally, we show that even a simple greedy search can effectively discover optimal or near-optimal parameters for the algebra.},
	language = {en},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Teyou, Louis Mozart Kamdem and Demir, Caglar and Ngomo, Axel-Cyrille Ngonga},
	month = sep,
	year = {2024},
	note = {arXiv:2402.04870 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/Users/alexgoessmann/Zotero/storage/W3MTLCH9/Teyou et al. - 2024 - Embedding Knowledge Graphs in Degenerate Clifford Algebras.pdf:application/pdf},
}

@book{getoor_introduction_2019,
	title = {Introduction to {Statistical} {Relational} {Learning}},
	isbn = {978-0-262-53868-8},
	abstract = {Advanced statistical modeling and knowledge representation techniques for a newly emerging area of machine learning and probabilistic reasoning; includes introductory material, tutorials for different proposed approaches, and applications.Handling inherent uncertainty and exploiting compositional structure are fundamental to understanding and designing large-scale systems. Statistical relational learning builds on ideas from probability theory and statistics to address uncertainty while incorporating tools from logic, databases and programming languages to represent structure. In Introduction to Statistical Relational Learning, leading researchers in this emerging area of machine learning describe current formalisms, models, and algorithms that enable effective and robust reasoning about richly structured systems and data. The early chapters provide tutorials for material used in later chapters, offering introductions to representation, inference and learning in graphical models, and logic. The book then describes object-oriented approaches, including probabilistic relational models, relational Markov networks, and probabilistic entity-relationship models as well as logic-based formalisms including Bayesian logic programs, Markov logic, and stochastic logic programs. Later chapters discuss such topics as probabilistic models with unknown objects, relational dependency networks, reinforcement learning in relational domains, and information extraction. By presenting a variety of approaches, the book highlights commonalities and clarifies important differences among proposed approaches and, along the way, identifies important representational and algorithmic issues. Numerous applications are provided throughout.},
	language = {Englisch},
	publisher = {MIT Press},
	author = {Getoor, Lise and Taskar, Ben},
	month = sep,
	year = {2019},
}

@book{koller_probabilistic_2009,
	address = {Cambridge, Mass.},
	edition = {1. edition},
	title = {Probabilistic {Graphical} {Models}: {Principles} and {Techniques}},
	isbn = {978-0-262-01319-2},
	shorttitle = {Probabilistic {Graphical} {Models}},
	abstract = {A general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions.Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.},
	language = {English},
	publisher = {The MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	month = jul,
	year = {2009},
}

@book{hogan_knowledge_2021,
	address = {Cham},
	edition = {1st edition},
	title = {Knowledge {Graphs}},
	isbn = {978-3-031-00790-3},
	abstract = {This book provides a comprehensive and accessible introduction to knowledge graphs, which have recently garnered notable attention from both industry and academia. Knowledge graphs are founded on the principle of applying a graph-based abstraction to data, and are now broadly deployed in scenarios that require integrating and extracting value from multiple, diverse sources of data at large scale. The book defines knowledge graphs and provides a high-level overview of how they are used. It presents and contrasts popular graph models that are commonly used to represent data as graphs, and the languages by which they can be queried before describing how the resulting data graph can be enhanced with notions of schema, identity, and context. The book discusses how ontologies and rules can be used to encode knowledge as well as how inductive techniques—based on statistics, graph analytics, machine learning, etc.—can be used to encode and extract knowledge. It covers techniques for the creation, enrichment, assessment, and refinement of knowledge graphs and surveys recent open and enterprise knowledge graphs and the industries or applications within which they have been most widely adopted. The book closes by discussing the current limitations and future directions along which knowledge graphs are likely to evolve. This book is aimed at students, researchers, and practitioners who wish to learn more about knowledge graphs and how they facilitate extracting value from diverse data at large scale. To make the book accessible for newcomers, running examples and graphical notation are used throughout. Formal definitions and extensive references are also provided for those who opt to delve more deeply into specific topics.},
	language = {English},
	publisher = {Springer},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d’Amato, Claudia and Melo, Gerard de and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jose Emilio Labra and Navigli, Roberto and Neumaier, Sebastian},
	month = nov,
	year = {2021},
}
