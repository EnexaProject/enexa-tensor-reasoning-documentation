\chapter{\chatextmessagePassing}\label{cha:messagePassing}

%% Polytopes - MLN
%The polytopes of mean parameters to \HybridLogicNetworks{} and proposal distributions are an interesting connection between the fields of combinatorical optimization and the study of expressivity of tensor networks.


%% Minimal Connectivity: Local consitency - Hierarchical Tucker -> TO MP CHAPTER
%This is of special interest, when the computation cores of a \HybridLogicNetwork{} are minimally connected, the mean parameters are captured by local consistencies.
%Similar investigations have been made in the field of tensor networks, where minimal connected tensor networks are referred to by Hierarchical Tucker formats (HT).
%Minimal connection is exploited in the tensor network community to show numerical properties of the format, such as closedness and existence of best approximators.

In this chapter we introduce local contraction passed along tensor clusters to calculate global contractions exactly or approximatively.
These message passing schemes provide tradeoffs between efficiency increases and exactness of the global contraction.

We use the $\cpformat$ decompositions to investigate the asymptotic behavior of the message passing algorithms.

\red{The application of message passing schemata to calculate contractions are motivated by commutations of contractions.
We first show this property and then provide message passing schemata.
}

\sect{Commutation of Contractions}

We show in the next theorem, that a contractions can be performed by contracting a subnetwork first and then further contracting the result with the rest.

\begin{theorem}[Commutativity of Contractions]\label{the:splittingContractions}
	Let $\tnetof{\graph}$ be a tensor network on a hypergraph $\graph=(\nodes,\edges)$.
	Let us now split the $\graph$ into two graphs $\graph_1=(\nodesone,\edges_1)$ and $\graph_2=(\nodestwo,\edges_2)$, such that $\edges_1\dot{\cup}\edges_2=\edges$, $\nodesone\cup\nodestwo=\nodes$ and all nodes in $\nodestwo$ are contained in an hyperedge of $\edges_2$.
	We then have for any $\secnodes\subset\nodes$
	\begin{align*}
		\contractionof{\tnetof{\graph}}{\catvariableof{\secnodes}}
		= \contractionof{
			\tnetofat{\graph_1}{\catvariableof{\nodesone}}
			\cup \{\contractionof{\tnetof{\graph_2}}{\catvariableof{\nodestwo\cap(\nodesone\cup\secnodes)}}\}
		}{\catvariableof{\secnodes}}   \, .
\end{align*}
\end{theorem}
\begin{proof}
	For any index $\catindexof{\secnodes}$ we show that
	\begin{align*}
		\contractionof{\tnetof{\graph}}{\indexedcatvariableof{\secnodes}}
		=\contractionof{\tnetof{\graph_1} \cup \{
			\contractionof{\tnetof{\graph_2}}{\catvariableof{\nodestwo\cap(\nodesone\cup\secnodes)}}
		\}}{\indexedcatvariableof{\secnodes}}   \, .
	\end{align*}
	By definition we have
	\begin{align*}
		\contractionof{\tnetof{\graph}}{\indexedcatvariableof{\secnodes}}
		& = \sum_{\catindexof{\nodes/\secnodes}} \prod_{\edge\in\edges} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \\
		& = \sum_{\catindexof{\nodes/\secnodes}}
		 	\left( \prod_{\edge\in\edges_1} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \right)
		 	\cdot \left( \prod_{\edge\in\edges_2} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}}  \right) \\
		& =  \sum_{\catindexof{\nodesone/\secnodes}} \sum_{\catindexof{\nodestwo/(\secnodes\cup\nodesone)}}
			\left( \prod_{\edge\in\edges_1} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \right)
		 	\cdot \left( \prod_{\edge\in\edges_2} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}}  \right) \\
		& =  \sum_{\catindexof{\nodesone/\secnodes}}
			\left( \prod_{\edge\in\edges_1} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \right)
		 	\cdot \left( \sum_{\catindexof{\nodestwo/(\secnodes\cup\nodesone)}}  \prod_{\edge\in\edges_2} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}}  \right) \, .
	\end{align*}
	When contracting the variables $\catvariableof{\nodestwo/(\secnodes\cup\nodesone)}$ on $\tnetof{\graph_2}$, the variables $\catvariableof{\nodestwo\cap(\secnodes\cup\nodesone)}$ are left open.
	We therefore have for any $\catindexof{\nodestwo\cap(\secnodes\cup\nodesone)}$
	\begin{align*}
		\contractionof{\tnetof{\graph_2}}{\indexedcatvariableof{\nodestwo\cap(\secnodes\cup\nodesone)}} =
		 \left( \sum_{\catindexof{\nodestwo/(\secnodes\cup\nodesone)}}  \prod_{\edge\in\edges_2} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}}  \right) \, .
	\end{align*}
	It follows with the above, that
	\begin{align*}
		\contractionof{\tnetof{\graph}}{\indexedcatvariableof{\secnodes}}
		& =  \sum_{\catindexof{\nodesone/\secnodes}}  \left( \prod_{\edge\in\edges_1} \hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \right) \cdot \contractionof{\tnetof{\graph_2}}{\indexedcatvariableof{\nodestwo\cap(\secnodes\cup\nodesone)}}Â \\
		& = \contractionof{\tnetof{\graph_1} \cup \{
			\contractionof{\tnetof{\graph_2}}{\catvariableof{\nodestwo\cap(\nodesone\cup\secnodes)}}
		\}}{\indexedcatvariableof{\secnodes}}   \, . \qedhere
	\end{align*}
\end{proof}


% Message interpretation
We can interpret the inner contraction $\contractionof{\tnetof{\graph_2}}{\catvariableof{\nodestwo\cap(\nodesone\cup\secnodes)}}$ as a message, sent from $\graph_2$ to $\graph_1$.
Based on this intuition, we will define message passing schemes in the next section.


\sect{Exact Contractions}

%\red{This is the junction tree algorithm!}

We apply \theref{the:splittingContractions} to split a contraction into subcontractions, which are consecutively performed.

% Message Passing
Contractions can be performed partially, and the result passed to the rest of the network as a message.

\subsect{Construction of Cluster Graphs}

Let us first introduce with the cluster graph a mechanism to coarse grain the hypergraph capturing a tensor network.

% Cluster Graphs
\begin{definition}[Cluster Graph]
	Given a tensor network $\extnet$ a cluster partition is a partition of the tensor network into $n$ clusters, by a function
		\[ \alpha : \edges \rightarrow [n] \, . \]
	The clusters are with tensors decorated edge sets $\enc = \{\edge \, : \, \alpha(\edge) = i\}$ with variables $\nodes_i = \bigcup_{\edge \in \enc} \edge$.

	We say, that the cluster graph satisfies the running intersection property, when for any clusters $C_i$ and $C_j$ and any $\node\in\nodes_i\cup\nodes_j$ there is a path between $C_i$ and $C_j$ with $\node\in\nodes_k$ for any cluster $C_k$ along the path.
	%The clusters form a graph where edges between $\enc$ and $C_j$ exist, when the node sets $\nodes_i$ and $\nodes_j$ are not disjoint.
	%In this case, we define separation sets $S_{i,j}=\enc\cup C_j$
\end{definition}

Given a cluster graph to a tensor network, we can execute any global contraction by a contraction of local contraction to each cluster.

\begin{theorem}\label{the:contractionClusterSplit}
	Given a tensor network $\extnet$ and a cluster graph.
	We then define for each cluster the node set
	\begin{align*}
		\tilde{\nodes}_i = \bigcup_{j\neq i} \nodes_j
	\end{align*}
	and have
		\[ \contractionof{\extnet}{\catvariableof{\secnodes}} = 
		\contractionof{
			\{ \contractionof{\tnetof{\enc}}{\catvariableof{\nodes_i \cap (\tilde{\nodes}_i\cup\secnodes)}}  : i \in [n]\}
		}{\catvariableof{\secnodes}}  \, . \]
\end{theorem}
\begin{proof}
	By \theref{the:splittingContractions} applied for each cluster seen as a subgraph.
\end{proof}



\subsect{Message Passing to calculate contractions}

% Cluster Graphs
Having a hypergraph $\graph$, we iteratively apply \theref{the:splittingContractions} and call the $\graph_2$ a cluster.
When iterating until $\graph$ is empty, we get a cluster graph, where all tensors are assigned to a cluster.

% Cluster Trees -> Clique Trees in Koller Book
When the cluster are a polytree, that is a union of disjoint trees, we define messages between neighbored clusters $\enc$ and $\secenc$ with $\secenc\prec\enc$ by the contractions
\begin{align*}
	\mesfromtoat{\secclusterenumerator}{\clusterenumerator}{\catvariableof{\nodesof{\clusterenumerator}\cap\nodesof{\secclusterenumerator}}}
	= \contractionof{\{
	\mesfromtoat{\thirdclusterenumerator}{\secclusterenumerator}{\catvariableof{\nodesof{\thirdclusterenumerator}\cap\nodesof{\secclusterenumerator}}}
	\, : \, \thirdenc\prec\secenc\} \cup \tnetof{\secenc}}{\catvariableof{\nodesof{\clusterenumerator}\cap\nodesof{\secclusterenumerator}}} \, .
\end{align*}



We note, that the messages are well defined by these recursive equations, exactly when the cluster graph is a polytree.
%Since messages are recursively defined, we need the tree structure to ensure well-definedness.

% Tree advantage
When the cluster graph is a tree, we can choose a root cluster and order the clusters by the topological order $\prec$.


\begin{lemma}\label{lem:clusterContractionMessage}
	When the cluster graph is a tree satisfying the running intersection property, we have for neighbored clusters $\enc$ and $\secenc$ with $\secenc\prec\enc$
	\begin{align*}
		\mesfromtoat{\clusterenumerator}{\tilde{\clusterenumerator}}{\catvariableof{\nodesof{\clusterenumerator}\cap\nodesof{\tilde{\clusterenumerator}}}}
		= \contractionof{\{\tnetof{\secenc}\,:\,\secenc\prec\enc\}}{\catvariableof{\nodesof{\clusterenumerator}\cap\nodesof{\tilde{\clusterenumerator}}}}   \, .
	\end{align*}
\end{lemma}
\begin{proof}
	By induction over the cardinality $n$ of the preceding clusters.
	\textbf{$n=1$}: For a single preceding cluster the statement holds trivial, since the preceding cluster is the cluster itself.

	\textbf{$n\rightarrow n+1$}: Let us now assume, that the statement holds for up to $n$ preceding clusters, and let there be $n+1$ preceding clusters.
	We build another cluster graph for the cores different from $\enc$, by assigning each cluster $\thirdenc$ to the neighbor $\secenc$ where $\secclusterenumerator\in\neighborsof{\clusterenumerator}$, for which
	\begin{align*}
		\thirdenc\prec\secenc \, .
	\end{align*}
	We use \theref{the:contractionClusterSplit} on this constructed cluster graph and get
	\begin{align*}
		\contractionof{\{\tnetofat{\clusterof{\thirdclusterenumerator}}{\nodevariablesof{\thirdclusterenumerator}} \, : \, \thirdclusterenumerator\neq\clusterenumerator\}}{\nodevariablesof{\clusterenumerator}}
		= \contractionof{
			\Big\{ \contractionof{
				\{\tnetofat{\clusterof{\thirdclusterenumerator}}{\nodevariablesof{\thirdclusterenumerator}} \, : \, \thirdclusterenumerator\prec\secclusterenumerator\}
			}{\catvariableof{\secnodesof{\secclusterenumerator}}}
			\, : \, \secclusterenumerator\in\neighborsof{\clusterenumerator} \Big\}
			}{\nodevariablesof{\clusterenumerator}}
	\end{align*}
	Here by $\secnodesof{\secclusterenumerator}$ we denote the intersection of
	\begin{align*}
		\secnodesof{\secclusterenumerator} = \left(\bigcup_{\thirdclusterenumerator\prec\secclusterenumerator} \nodesof{\thirdclusterenumerator}\right) \cap \left(\bigcup_{\thirdclusterenumerator\nprec\secclusterenumerator} \nodesof{\thirdclusterenumerator}\right)
	\end{align*}
	By the running intersection property, we have $\nodesof{\secclusterenumerator}\cap\nodesof{\clusterenumerator}=\secnodesof{\secclusterenumerator}$.

	We further have for any $\secclusterenumerator\in\neighborsof{\clusterenumerator}$ that
	\begin{align*}
		\cardof{\{\thirdclusterenumerator\prec\secclusterenumerator\}} \leq n \, .
	\end{align*}
	We can therefore apply the assumption of the induction and get
	\begin{align*}
 		\contractionof{\{\tnetofat{\clusterof{\thirdclusterenumerator}}{\nodevariablesof{\thirdclusterenumerator}} \, : \, \thirdclusterenumerator\prec\secclusterenumerator\}
			}{\catvariableof{\secnodesof{\secclusterenumerator}}}
		= \mesfromtoat{\secclusterenumerator}{\clusterenumerator}{\catvariableof{\nodesof{\secclusterenumerator}\cap\nodesof{\clusterenumerator}}}
	\end{align*}

	With the above, we arrive at 
	\begin{align*}
		\mesfromtoat{\clusterenumerator}{\tilde{\clusterenumerator}}{\catvariableof{\nodesof{\clusterenumerator}\cap\nodesof{\tilde{\clusterenumerator}}}}
		= \contractionof{\{\tnetof{\secenc}\,:\,\secenc\prec\enc\}}{\catvariableof{\nodesof{\clusterenumerator}\cap\nodesof{\tilde{\clusterenumerator}}}}   \, . & \qedhere
	\end{align*}
%	\begin{align*}
%		\contractionof{\extnet}{\catvariableof{\nodesof{\clusterenumerator}}} =
%		\contractionof{\bigcup_{\secclusterenumerator\in\neighborsof{\clusterenumerator}}
%		\{\contractionof{\tnetof{\clusterof{\thirdclusterenumerator}}}{\catvariableof{\nodesof{\thirdclusterenumerator}\cap\nodesof{\secclusterenumerator}}} \, : \, \thirdenc\prec\secenc \}
%		\cup \{\tnetof{\enc}\}}{\catvariableof{\nodes_i}} \, .
%	\end{align*}
%	Now, we use the assumption of the induction to get
%	\begin{align*}
%		\contractionof{
%			\{\contractionof{\tnetof{\clusterof{\thirdclusterenumerator}}}{\catvariableof{\nodesof{\thirdclusterenumerator} \cap (\tilde{\nodes}_i\cup\secnodes)}}   \, : \, \cluster_l \prec \cluster_j \}
%		}{\catvariableof{\nodesof{\thirdclusterenumerator}\cap\nodesof{\secclusterenumerator}}}
%		= \mesfromtoat{\thirdclusterenumerator}{\clusterenumerator}{\catvariableof{\nodesof{\thirdclusterenumerator}\cap\nodesof{\secclusterenumerator}}}
%	\end{align*}
%	Note that we used the running intersection property ensuring that whenever a variable of the cluster appears in a previous cluster, the variable is passed in the message.
\end{proof}


\begin{theorem}
	When the cluster graph is a tree satisfying the running intersection property, then we have for each cluster $\enc$ with neighbors $\neighborsof{\clusterenumerator}$
%	Then for each clique we have the conditional probability of its variables being the contraction of the messages with the cliques cores, that is
	\begin{align}
		\contractionof{\extnet}{\catvariableof{\nodes_i}} =
		\contractionof{\{ \mesfromtoat{\secclusterenumerator}{\clusterenumerator}{\catvariableof{\nodesof{\clusterenumerator}\cap\nodesof{\secclusterenumerator}}}  \, : \, j \in N(i) \} \cup \{\tnetof{\enc}\}}{\catvariableof{\nodes_i}} \, .
	\end{align}
\end{theorem}
\begin{proof}
	We use the topological order $\prec$ of the clusters by the tree, when choosing a root by cluster $\enc$.

	The claim then follows from \theref{the:contractionClusterSplit} and \lemref{lem:clusterContractionMessage}.
%
%	Thus, we find to each cluster $\cluster_l$ exactly one neighboring cluster $j\in N(i)$ with $\cluster_l \prec \cluster_j$.
%	We use \theref{the:contractionClusterSplit} and reorder the contractions as
%	\begin{align*}
%		\contractionof{\extnet}{\catvariableof{\nodes_i}} =
%		\contractionof{\bigcup_{j\in N(i)}
%		\{\contractionof{\tnetof{\cluster_l}}{\catvariableof{\nodes_l \cap (\tilde{\nodes}_i\cup\secnodes)}}   \, : \, \cluster_l \prec \cluster_j \}
%		\cup \{\tnetof{\enc}\}}{\catvariableof{\nodes_i}} \, .
%	\end{align*}
%	By \lemref{lem:clusterContractionMessage}, the contractions to the clusters preceeding $\cluster_l$ are exactly the messages sent to the root cluster.
%	Therefore we have
%	\begin{align}
%		\contractionof{\extnet}{\catvariableof{\nodes_i}} =
%		\contractionof{\{ \upmes{\secclusterenumerator}{\clusterenumerator}  \, : \, j \in N(i) \} \cup \{\tnetof{\enc}\}}{\catvariableof{\nodes_i}} \, .
%	\end{align}

%	We then show by	induction, that any message
%	\begin{align*}
%		\upmes{\secclusterenumerator}{\clusterenumerator}
%		= \contractionof{
%			\{ \tnetof{\cluster} \, : \, \cluster \prec \clusterof{\secclusterenumerator} \}
%		}{\catvariableof{\nodes_\secclusterenumerator \cup \nodes_{\clusterenumerator}}}
%	\end{align*}
%	To this end, use \theref{the:contractionClusterSplit}.
%	Having established the induction, we use this result for the messages and get the claim.
%	By \theref{the:splittingContractions} we split into contractions of the clusters up and down of the respective neighbors and apply the above lemma.
\end{proof}


% Downward messages
While we have defined message passing along the topological order of a graph, we can also define messages against the topological order, that is
\begin{align*}
	\downmes{i}{j}  = \contractionof{\{\downmes{\tilde{j}}{i} \, : \,  \enc \prec  \thirdenc\} \cup \tnetof{\enc}}{\catvariableof{\nodes_i\cap \nodes_j}} \,
\end{align*}
To this end, we can get a similar statement for nodes, which are not the rotes of the cluster tree.
The constractions at each cluster can then be computed batchwise, based on message passed along a topological order and against.

%
These message passing schemes can be derived from Lagrangian parameters given a local consistency polytope \cite{wainwright_graphical_2008}.



\subsect{Variable Elimination Cluster Graphs}


\begin{remark}[Construction of Cluster Graphs by Variable Elimination]
	% Build a cluster graph
	Following an elimination order of the colors, mark those tensors containing the colors, which have not been marked before, as the cluster.
	% Extension to clique tree
	A clique tree can be constructed by these cluster, when iterating through the clusters and either connect them to previous disconnected clusters or leave the current cluster disconnected.
	Add the disconnected clusters with the current cluster in case there are overlaps of their open colors.
	If the disconnected cluster added has more open colors, 
\end{remark}


\subsect{Bethe Cluster Graphs}


\begin{figure}[t]
\begin{center}
	\input{./PartIII/tikz_pics/message_passing/data_example.tex}
\end{center}
\caption{Example of a Bethe Cluster Graph.
	a) Example of a Tensor Network $\tnetof{\graph}$, which represents the by $\lambda$ averaged evaluation of the formula $(a\land b)\lor c$ on data $\datamap$.
	b) Corresponding Bethe Cluster Hypergraph, which dual is bipartite by the sets $\Delta$ and $\tilde{\edges}$.
	}
\label{fig:betheDataExample} 
\end{figure}

By adding delta tensors to each node $\node\in\nodes$ and defining its leg variables by $\node^{\edge}$ for $\edge\in\edges$.
We mark each such delta tensor by a cluster in $\Delta^{\graph}$, as defined in the following (see also \figref{fig:betheDataExample}).

\begin{definition}
	Given a tensor network $\tnetof{\graph}$ on a decorated hypergraph $\graph$, we define the Bethe Cluster Hypergraph $\secgraph$ as
	$(\secnodes, \secedges \cup \Delta^{\graph})$ where we have
	\begin{itemize}
		\item Recolored Edges $\secedges = \{\tilde{\edge} \, : \, \edge\in \edges\}$ where $\tilde{\edge} = \{\node^{\edge} \, : \, \node\in\edge\}$, which decoration tensor has same coordinates as $\hypercoreof{\edge}$
		\item Nodes $\secnodes = \bigcup_{\edge\in\edges}\tilde{\edge}$ %$\secnodes = \bigcup_{\edge\in\edges}\{\node^{\edge} \, : \, \node\in\edge \}$ 
		\item Delta Edges $\Delta^{\graph} =  \big\{ \{\node^{\edge} \, : \, \edge\ni\node \} \, : \, \node\in\nodes \big\} $, each of which decorated by a delta tensor $\delta^{\{\node^{\edge} \, : \, \edge\ni\node \}}$
	\end{itemize}
\end{definition}

By \lemref{lem:deltification} this construction does not change contractions.

% Dual graph
The dual is bipartite, since any variable appears exactly in one cluster in $\secedges$ and in one cluster of $\Delta^{\graph}$.
This further makes the dual of the Bethe Cluster Hypergraph a proper graph (i.e. edges consistent of node pairs). 





%\subsect{Computational Complexity}
%
%%\red{Tree-width here: By building a cluster tree to any partition, simply by including variables for the running intersection property.
%%The maximum number of variables at a cluster is the tree-width and provides a complexity bound for the local contractions.}
%
%Naive execution of $\contractionof{\tnetof{\graph}}{\secnodes}$: $\prod_{\node\in\nodes} \catdimof{\node}$ many products are built and summed up.
%When splitting contractions into local subcontractions, the product can be turned into sums with tremendous decrease in complexity.
%




\sect{Boolean Message Passing}\label{sec:supportContractionEquations}

Instead of the exact calculation of a contraction, let us now investigate schemes to sparsify the tensors before a contraction.
To this end, we first show underlying properties of contractions enabling these schemes.


\subsect{Monotonicity of tensor contraction}

To state the next theorem we use the nonzero function $\nonzerofunction: \rr \rightarrow [2]$ by $\nonzeroof{x}=1$ if $x\neq0$ and $\nonzeroof{x}=0$ else.
Applied coordinatewise on tensors it marks the nonzero coordinates by $1$.

We show that adding boolean tensor cores to an contraction orders the results by the partial ordering introduced in \defref{def:partialOrder}.

\begin{theorem}[Monotonicity of Tensor Contractions]\label{the:monotonicityBinaryContractions}
	Let $\extnet, \secextnet$ be tensor network of non-negative tensors and $\catvariableof{\secnodes}$ an arbitrary set of random variables. %, and $\tilde{\theta}$ another binary tensor.
	Then we have
	\begin{align*}
		\nonzeroof{\contractionof{\extnet\cup\secextnet}{\catvariableof{\secnodes}}} \prec
		\nonzeroof{\contractionof{\extnet}{\catvariableof{\secnodes}}} \, .
	\end{align*}
\end{theorem}
\begin{proof}
	It suffices to show that for any $\catindexof{\secnodes}$ with
		\[ \nonzeroof{\contractionof{\extnet\cup\secextnet}{\indexedcatvariableof{\secnodes}}}=1 \]
	we also have
		\[ \nonzeroof{\contractionof{\extnet}{\indexedcatvariableof{\secnodes}}}=1 \, . \]
	For any $\catindexof{\secnodes}$ satisfying the first equation we find an extension $\catindexof{\nodes}$ to all variables of the tensor networks such that
		\[ \contractionof{\extnet\cup\secextnet}{\indexedcatvariableof{\nodes}} > 0 \]
	and it follows that
		\[ \contractionof{\extnet}{\indexedcatvariableof{\nodes}} > 0 \andspace  \contractionof{\secextnet}{\indexedcatvariableof{\nodes}} > 0  \, . \]
	But this already implies, that
	\begin{align*}
		\nonzeroof{\contractionof{\extnet}{\indexedcatvariableof{\secnodes}}}=1 \, . & \qedhere
	\end{align*}
\end{proof}

\subsect{Invariance of adding subcontractions}

%We now show \theref{the:booleanContractionInvariance} of \charef{cha:logicalReasoning}.
Let us now state an equivalence of the contraction, when we add the result of the same contraction.
This property was used in the proof of \theref{the:soundnessKnowledgePropagation}.

\begin{theorem}[Invariance under adding subcontractions]\label{the:invarianceAddingSubcontractions}
	Let $\extnet$ be a tensor network of non-negative tensors with variables $\catvariableof{\nodes}$ and let $\secextnet$ be a subset.
	Then we have for any subset $\catvariableof{\secnodes}$ of $\catvariableof{\nodes}$
		\[ \contractionof{\extnet \cup\{
			\nonzeroof{
			\contractionof{\secextnet}{\catvariableof{\secnodes}}
			}
		\}}{\catvariableof{\nodes}}
		= \contractionof{\extnet}{\catvariableof{\nodes}}
		\, . \]
\end{theorem}
\begin{proof}
	For any $\catindexof{\nodes}$ with
		\[ \contractionof{\extnet}{\indexedcatvariableof{\nodes}} = 0 \]
	we also have
		\[ \contractionof{\extnet \cup\{
			\nonzeroof{
			\contractionof{\secextnet}{\catvariableof{\secnodes}}
			}
		\}}{\indexedcatvariableof{\nodes}} = 0 \, . \]
	For any $\catindexof{\nodes}$ with
		\[ \contractionof{\extnet}{\indexedcatvariableof{\nodes}} \neq 0 \]
	we have for the reduction $\catindexof{\secnodes}$ of the index $\catindexof{\nodes}$ that
		\[  \contractionof{\secextnet}{\indexedcatvariableof{\secnodes}} \neq 0 \]
	and thus
	\begin{align*}
		\contractionof{\extnet \cup\{
			\nonzeroof{
			\contractionof{\secextnet}{\catvariableof{\secnodes}}
			}
		\}}{\indexedcatvariableof{\nodes}}
		&= \contractionof{\extnet}{\indexedcatvariableof{\nodes}} \cdot \nonzeroof{
			\contractionof{\secextnet}{\catvariableof{\secnodes}}
			}[\indexedcatvariableof{\secnodes}] \\
		&= \contractionof{\extnet}{\indexedcatvariableof{\nodes}} \, . \qedhere
	\end{align*}
%	When the subcore transformed by $\nonzeroof{\cdot}$ contains a zero slice, then this
%	 zero slice is also appearing in the rest contraction.
%	Multiplying a zero slice with zero does not affect the contraction, neither does multiplication with one on any slice.
\end{proof}


\begin{remark}
	Similar statements hold, when dropping the non-negativity assumption on the, but demanding that all variables are left open.
\end{remark}

\subsect{Basis Calculus as message passing scheme}

Message Passing of directed and boolean message by basis encoding of functions can be interpreted as function evaluation.
Each subfunction evaluation is passed in its one-hot encoding.

This is because any basis encoding of a function, the decomposition
\begin{align*}
	\bencodingof{\exfunction} = \sum_{y \in\imageof{\exfunction}} ( \sum_{i: \exfunction(i)=y}\onehotmapof{i} )  \otimes \onehotmapof{y}
\end{align*}
is a SVD of the matrification of $\bencodingof{\exfunction}$ with respect to incoming and outgoing legs.


Passing a message $\onehotmapof{i}$ in direction thus gives the message $\onehotmapof{\exfunction(i)}$.

Note, that this is exact, whenever the graph is directed and acyclic.
We do not need acyclicity of the underlying undirected graph.


%After having established a one-to-one connection between the directed and binary tensors with the encoding of functions, we now interpret contractions as evaluations of the respective functions.
%Applying this insight iteratively on composed functions we show the following theorem.

\begin{remark}[Basis Calculus as Message Passing]
	Given a tensor network of directed and binary tensor cores, each representing a function $\exfunctionof{\edge}$ depending on variables $\incomingnodes$.
	When there are not directed cycles, we define the compositions of $\exfunctionof{\edge}$ to be the function $\exfunction$ from the nodes $\nodesone$ not appearing as incoming nodes to the nodes $\nodestwo$ not appearing as outgoing nodes in an edge.
	Choosing arbitrary $\catindexof{\node}\in[\catdimof{\node}]$ for $\node\in\nodesone$ we have
	\begin{align*}
		\contractionof{\{\bencodingofat{\exfunctionof{\edge}}{\catvariableof{\outgoingnodes},\catvariableof{\incomingnodes}} \, : \edge=(\outgoingnodes,\incomingnodes)\in\edges\}}{\nodestwo}
	= \onehotmapof{\exfunction(\catindexof{\node} \, : \, \node\in\nodesone)}\, .
	\end{align*}
\end{remark}



\subsect{Application}

% Application
This properties can be applied as a sparsification of tensors before the execution of a contraction.
The Knowledge Propagation \algoref{alg:knowledgePropagation} produces in the knowledge cores conditions on non-vanishing coordinates.
Thus, the knowledge cores can be locally contracted with the tensors, as a sparsification before performing a global contraction.






\sect{Discussion}

Computing contractions by message passing is known to the graphical model community as belief propagation.
There, the objective is the calculation of marginal probabilities of Markov Networks, which involve contractions of the corresponding factor tensors.

\begin{remark}[Approximate Message Passing Schemes]
	\red{When the cluster graphs are not trees, we cannot find a topological order of the clusters any more.
	Messages can still be defined implicitly by received neighbored messages, but the equivalence with global contractions cannot be established in general.}

	Such algorithms are known in the graphical model community as loopy belief propagation.
\end{remark}




% Application: Dynamic programming
When queries share same parts, can perform their contraction using dynamic programming.
For conditional probability queries, which variables are the clusters of a cluster tree, this results in belief propagation.









