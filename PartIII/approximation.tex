\chapter{\chatextapproximation}\label{cha:approximation}

Often reasoning requires the execution of demanding contractions of tensors networks, or combinatorical search of maximum coordinates.
We in this chapter investigate methods, to replace hard to be sampled tensor networks by approximating tensor networks, which then serve as a proxy in inference tasks.


\sect{Selection tensor networks for $\cpformat$ decompositions}

In this section, we show that the set of tensors respresentable in specific $\cpformat$ formats coincides with the expressivity of tailored selection architectures (see \charef{cha:formulaSelection}).
We first define a basis+ $\cpformat$ selecting tensor and then show its decomposition into a formula selecting neural network.

\begin{definition}
    \label{def:cpSelection}
    Given a set of categorical variables $\shortcatvariables$ with $\catdim=\max_{\catenumeratorin}\catdimof{\catenumerator}$, a $\cpformat$ selecting tensor of maximal cardinality $\sliceorder$ is the tensor
    \begin{align*}
        \sliceselectionmapat{\shortcatvariables,\selvariableof{0,0},\ldots,\selvariableof{\sliceorder-1,0},\selvariableof{0,1},\ldots,\selvariableof{\sliceorder-1,1}}
    \end{align*}
    with dimensions
    \begin{align*}
        \seldimof{\selenumerator,0} = \catdim+1, \seldimof{\selenumerator,1} = \atomorder \quad \text{for} \quad \selenumerator\in[\sliceorder]
    \end{align*}
    and coordinates
    \begin{align*}
        & \sliceselectionmapat{\shortcatvariables=\shortcatindices,\indexedselvariableof{0,0},\ldots,\indexedselvariableof{\sliceorder-1,0},\indexedselvariableof{0,1},\ldots,\indexedselvariableof{\sliceorder-1,1}} \\
        & \quad = \begin{cases}
                      1 & \text{if} \quad
                      \forall{\atomenumeratorin,\selenumerator\in[\sliceorder]} : \, \big(\selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq \catdim \big) \Rightarrow  (\selindexof{\selenumerator,0} = \catindexof{\atomenumerator})  \\
                      0 & \text{else}
        \end{cases} \, .
    \end{align*}
\end{definition}

% Variable and State selectors
Intuitively, the selection variables $\selvariableof{\selenumerator,1}$ of $\sliceselectionmapof{\catorder,\sliceorder}$ select a variable out of $[\atomorder]$ to be included in $\variableset$ and the selection variables $\selvariableof{\selenumerator,0}$ select a corresponding state to that variable.
As in \charef{cha:formulaSelection} we refer to variables $\selvariableof{\selenumerator,1}$ as variable selectors and $\selvariableof{\selenumerator,0}$ as state selectors.
When $\selvariableof{\selenumerator,0}=\catdim$, the slice is left trivial, that is the selected variable is effectively not included in $\variableset$.
This then allows to also represent slices where $\cardof{\variableset}<\sliceorder$.
We in the following prove this more formally.

\begin{theorem}
    Let the non-vanishing indices of $\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}$ denote by $\{\selindexof{[\sliceorder]\times[\catdim+1]}^{\decindex} \, : \, \decindexin\}$.
    Let further $\mathcal{M}\subset[\decdim]$ be the set of agreeing selection indices, that is
    \begin{align*}
        \mathcal{M} = \{ \decindex \, : \, \decindexin,\,
        \forall{\selenumerator,\secselenumerator\in[\sliceorder]} : \big( \selindexof{\selenumerator,1}^\decindex= \selindexof{\secselenumerator,1}^\decindex  \land \selindexof{\selenumerator,1}^\decindex\neq\catdim \land \selindexof{\secselenumerator,1}^\decindex \neq \catdim \big)
        \Rightarrow \big(\selindexof{\selenumerator,1}^\decindex = \selindexof{\secselenumerator,1}^\decindex \big)
        \}
    \end{align*}
    Then
    \begin{align*}
        &\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{[\sliceorder]\times[\catdim+1]}},\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}}{\shortcatvariables} \\
        &\quad = \sum_{\decindex\in\mathcal{M}} \canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}=\selindexof{[\sliceorder]\times[\catdim+1]}^{\decindex}}
        \contractionof{\{\onehotmapofat{\selindexof{\selenumerator,0}}{\catvariableof{\selindexof{\selenumerator,1}}} \, : \, \selenumerator\in[\sliceorder], \, \selindexof{\selenumerator,0}\neq\catdim \}}{\shortcatvariables}
    \end{align*}
    Further we have
    \begin{align*}
        \baspluscprankof{\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{[\sliceorder]\times[\catdim+1]}},\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}}{\shortcatvariables}}
        \leq \sparsityof{\canparam} \, .
    \end{align*}
    %Thus, the slice selecting network represents $\cpformat$ decompositions of size $\cardof{\mathcal{M}}\leq \sparsityof{\canparam}$.
\end{theorem}
\begin{proof}
    Let us notice, that whenever $\decindex\notin\mathcal{M}$ then
    \begin{align*}
        \sliceselectionmapat{\shortcatvariables,\indexedselvariableof{[\sliceorder]\times[\catdim+1]}}
        = \zerosat{\shortcatvariables} \, ,
    \end{align*}
    since the condition for non-vanishing coordinates
    \begin{align*}
        \forall{\atomenumeratorin,\selenumerator\in[\sliceorder]} : \, \big(\selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq \catdim \big) \Rightarrow  (\selindexof{\selenumerator,0} = \catindexof{\atomenumerator})
    \end{align*}
    in \defref{def:cpSelection} cannot be satisfied for any $\shortcatindices$.
    For $\decindex\notin\mathcal{M}$ we have
    \begin{align*}
        \sliceselectionmapat{\shortcatvariables,\indexedselvariableof{[\sliceorder]\times[\catdim+1]}}
        =  \contractionof{\{\onehotmapofat{\selindexof{\selenumerator,0}}{\catvariableof{\selindexof{\selenumerator,1}}} \, : \, \selenumerator\in[\sliceorder], \, \selindexof{\selenumerator,0}\neq\catdim \}}{\shortcatvariables} \, .
    \end{align*}
    We now use these insights and linearity of contraction to get
    \begin{align*}
        &\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{[\sliceorder]\times[\catdim+1]}},\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}}{\shortcatvariables} \\
        & \quad = \sum_{\decindexin} \canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}=\selindexof{[\sliceorder]\times[\catdim+1]}^{\decindex}}
        \sliceselectionmapat{\shortcatvariables,\indexedselvariableof{[\sliceorder]\times[\catdim+1]}} \\
        & \quad = \sum_{\decindex\in\mathcal{M}} \canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}=\selindexof{[\sliceorder]\times[\catdim+1]}^{\decindex}}
        \contractionof{\{\onehotmapofat{\selindexof{\selenumerator,0}}{\catvariableof{\selindexof{\selenumerator,1}}} \, : \, \selenumerator\in[\sliceorder], \, \selindexof{\selenumerator,0}\neq\catdim \}}{\shortcatvariables}
    \end{align*}
    This shows the decomposition claim.
    We notice, that this is a basis+ $\cpformat$ decomposition of size $\mathcal{M}$ and thus
    \begin{align*}
        \baspluscprankof{\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{[\sliceorder]\times[\catdim+1]}},\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}}{\shortcatvariables}}
        \leq \cardof{\mathcal{M}} \leq \sparsityof{\canparam} \, .
    \end{align*}
\end{proof}

Towards a practicle usage of this representation scheme for basis+ $\cpformat$ decompositions, let us show that the $\cpformat$ selecting tensor coincides with a formula selecting network.

\begin{lemma}
    \label{lem:fsnnRepresentingSliceSelector}
    The $\cpformat$ selection tensor coincides with a formula selecting neural network with neurons (see Figure~\ref{fig:sliceSelectingNN}):
    \begin{itemize}
        \item unary state selecting neurons enumerated by $\selenumerator$, selecting one of the $\shortcatvariables$ with the variable $\selvariableof{\selenumerator,1}$ and selecting a state, extended by a possible choice of trivial legs $\ones$
%        \item unary input neuron enumerated by $\selenumerator$, selecting one of the $\shortcatvariables$ with the variable $\selvariableof{\selenumerator,1}$ and selecting a connective in $\{\lnot, \mathrm{Id}, \mathrm{True}\}$ by $\selvariableof{\selenumerator,0}$
        \item $\sliceorder$-ary output neuron fixed to the $\land$ connective.
    \end{itemize}
\end{lemma}
\begin{proof}
    This can be easily checked on each coordinate.
\end{proof}

% Case of leg-dimension 2
If $\catdimof{\catenumerator}=2$ for $\catenumeratorin$, the state selector chooses between the connectives $\{\lnot, \mathrm{Id}, \mathrm{True}\}$.
We can in that case understand each slice by a logical term.

%
%
%In the next two lemmata we first show that the defined slice selecting tensors indeed selects slices and then provide a representation as a formula selecting network.
%
%\begin{lemma}
%    \label{lem:sliceFromSliceSelector}
%    If all input neurons with same selection index are agreeing on the connective index, the selected formula does not vanish and coincides with a slice to the set
%    \[ \variableset = \{ \atomenumerator \, : \, \exists_{\selenumeratorin}: \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \} \]
%    and for $\atomenumerator\in\variableset$
%    \[ \catindexof{\atomenumerator} = \selindexof{\selenumerator,0} \quad \text{if} \quad \selindexof{\selenumerator,1} = \atomenumerator \, . \]
%\end{lemma}
%\begin{proof}
%    We need to show that
%    \begin{align}
%        \label{eq:sliceFromSliceSelector}
%        \sliceselectionmapat{\shortcatvariables,\indexedselvariableof{0,0},\ldots,\indexedselvariableof{\sliceorder-1,0},\indexedselvariableof{0,1},\ldots,\indexedselvariableof{\sliceorder-1,1}} = \onehotmapofat{\catindexof{\variableset}}{\shortcatvariables} \, .
%    \end{align}
%    If and only if an index $\tilde{\catindex}_{[\atomorder]}$ reduced on $\variableset$ does not coincide with $\catindexof{\variableset}$, we have $\onehotmapofat{\catindexof{\variableset}}{\shortcatvariables=\tilde{\catindex}_{[\atomorder]}}=0$ and otherwise $\onehotmapofat{\catindexof{\variableset}}{\shortcatvariables=\tilde{\catindex}_{[\atomorder]}}=1$ .
%    Let us notice, that this condition is equivalent to
%    \[ \forall_{\atomenumerator,\selenumerator} : \big(  \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \big) \Rightarrow  (\selindexof{\selenumerator,0} = \catindexof{\atomenumerator}) \]
%    and thus \eqref{eq:sliceFromSliceSelector} holds.
%\end{proof}
%
%

%
%It follows, that the expressivity of the slice selecting neural network coincides with the set of tensors with a bound on their slice sparsity, when $\sliceorder\geq\atomorder$.
%For arbitrary $\sliceorder$, the following theorem holds.
%
%\begin{theorem}
%    Let $\sliceselectionmapat{\shortcatvariables,\selvariable}$ be a slice selecting tensor.
%    For any parameter tensor $\canparamat{\selvariable}$ we have
%    \[ \baspluscprankof{\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables}} \leq \bascprankof{\canparamat{\selvariable}} \, . \]
%\end{theorem}
%\begin{proof}
%    Each non-vanishing coordinate of $\canparam$ represents by \lemref{lem:fsnnRepresentingSliceSelector} a slice and their weighted sum is thus a monomial decomposition.
%%	\red{Use the above lemma for that on each slice.}
%%	Show, how a coordinate of $\canparam$ corresponds with a slice determining tuple: $\sliceset$ determined by the selection indices.
%\end{proof}


\begin{figure}[h]
    \begin{center}
        \input{PartIII/tikz_pics/sparse_calculus/slice_selecting_nn.tex}
    \end{center}
    \caption{Representation of a basis+ Tensor by the contraction of a parameter tensor $\canparam$ with a $\cpformat$ selecting architecture $\fselectionmap$, which has a decomposition as a formula selecting neural network (see \lemref{lem:fsnnRepresentingSliceSelector}).
    The nonzero coordinates of $\canparam$ represent slices of the $\cpformat$ decomposition.
    }
\end{figure}\label{fig:sliceSelectingNN}


\subsect{Applications}

One application is as a parametrization scheme in the approximation of a tensor by a slice-sparse tensor, see \charef{cha:approximation}.
The approximated parameter can then be used as a proxy energy to be maximized.
When choosing $\sliceorder=2$, the approximating tensor contains only quadratic slices, which then poses a QUBO problem.

\begin{remark}[Extension to arbitrary $\cpformat$ formats]
    Select at each input neuron a specific leg.
    For finite number of legs, as it is the case in the binary, basis and basis+ formats, we can enumerate all possibilities by the selection variable.
    For the basis+ format, in case of binary leg dimensions, we here exemplified the approach, by enumerating the three possibilities $\onehotmapof{0},\onehotmapof{1},\onesat{1}$.
    This approach, however, fails as a generic representation of the directed format, since the directed legs are continuous and there therefore are infinite choosable legs.
\end{remark}





\sect{Approximation of Energy tensors}

The Hilbert-Schmidt norm of a tensor is the contraction of the coordinatewise transform with the square function
\begin{align*}
	\|\hypercorewith\|_2= \sqrt{\contraction{\left(\hypercorewith\right)2}} \, .
\end{align*}

Approximation involving a selection architecture $\fselectionmap$ is the problem
	\[ \argmin_{\canparam\in\Gamma^{\graph}} \|\energytensor - \contractionof{\sencodingof{\fselectionmap},\canparam}{\shortcatvariables}\|^2 \, . \]

Direct approximation is then the choice of the minterm statistic
	\[ \argmin_{\canparam\in\Gamma^{\graph}} \|\energytensorat{\shortcatvariables} - \canparamat{\shortcatvariables}\|^2 \, . \]

In a tensor network diagram we depict this as
\begin{center}
    \input{PartIII/tikz_pics/approximation/least_squares.tex}
\end{center}


\begin{example}[Approximate based on a slice sparsity selecting architecture]
	Use a term selecting neural network (conjunction neuron on $\atomorder$ unary neurons selecting a variable and $\mathrm{Id},\lnot,\mathrm{True}$ as connective selector.
	Demand the parameter tensor $\canparam$ to be in a basis CP format, then each slice of the parameter tensor corresponds with the slice of the energy.
	The use the approximation for MAP search.
	Same construction possible for probability tensors, but often more involved to instantiate them as tensor network.
\end{example}



\sect{Transformation of Maximum Search to Risk Minimization}

By the squares risk trick, maximum coordinate searches involving contractions with boolean tensors can be turned into squares risk minimization problems.
This trick can be applied in MAP inference of MLN and the proposal distribution.

\subsect{Weighted Squares Loss Trick}

\begin{lemma}
	Let $\hypercore$ be a boolean tensor, that is $\imageof{\hypercore}\subset\{0,1\}$.
	Then
		\[ \hypercoreat{\shortcatvariables} = \onesat{\shortcatvariables} - \left( \hypercoreat{\shortcatvariables} - \onesat{\shortcatvariables} \right)^2  \]
	where $\ones$ is a tensor with same shape as $\hypercore$ and all coordinates being $1$.
\end{lemma}
\begin{proof}
	Since for each $\shortcatindices\in\facstates$ we have $\hypercore[\shortcatvariables=\shortcatindices]\in\{0,1\}$, it holds that
		\[ \hypercoreat{\shortcatvariables=\shortcatindices} = 1 - (\hypercoreat{\shortcatvariables=\shortcatindices}-1)^2 \]
	and thus in coordinatewise calculus
		\[ \hypercoreat{\shortcatvariables} = \onesat{\shortcatvariables} - \left( \hypercoreat{\shortcatvariables} - \onesat{\shortcatvariables} \right)^2 \, .   \]
\end{proof}

We apply this property to reformulate optimization problems over boolean tensors into weighted least squares problems.

\begin{theorem}[Weighted Squares Loss Trick]\label{the:reweightedLeastSquares}
	Let $\Gamma$ be a set of boolean tensors in $\facspace$ and $\importancetensor\in\facspace$ arbitrary.
	Then we have
	\begin{align}
		\argmax_{\hypercore\in\Gamma} \contraction{\importancetensor,\hypercore} 
		= \argmin_{\hypercore\in\Gamma} \contraction{\importancetensor, (\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2}
	\end{align} 
\end{theorem}
\begin{proof}
	Using the Lemma above, $\hypercore$ is identical to $\onesat{\shortcatvariables}-(\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2$ and we get
	\begin{align*}
		 \contraction{\importancetensor,\hypercore} 
		 &=  \contraction{\importancetensor,\onesat{\shortcatvariables}}-\contraction{\importancetensor,(\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2} 
	\end{align*}
	Since the first term does not depend on $\hypercore$, it can be dropped in the maximization problem.
	The $(-1)$ factor then turns the maximization into a minimization problem.
\end{proof}

% Interpretation and Importance Tensor
\theref{the:reweightedLeastSquares} reformulates maximation of binary tensors with respect to an angle to another tensor into minimization of a squares risk.
This squares risk trick is especially useful when combining it with a relaxation of $\Gamma$ to differentiably parametrizable sets, since then common squares risk solvers can be applied.
We will call $\importancetensor$ in the \theref{the:reweightedLeastSquares} importance tensor, since it manipulates the relevance of each coordinate in the squares loss.

%
As a result, we interpret the objective
	\[ \contraction{\importancetensor, (\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2} \]
as a weighted squares loss.

\begin{example}[Proposal distribution maxima]
	The Problem~\ref{prob:greedyGrad} of finding the maximal coordinate of the proposal distribution can thus be turned into
	\begin{align*}
		&\argmax_{\shortselindices} \contractionof{(\empdistribution-\currentdistribution),\fselectionmap}{\shortselvariables=\shortselindices}  \\
		&\quad = \argmin_{\shortselindices} \contraction{(\empdistribution-\currentdistribution),
		\left(\contractionof{\fselectionmap,\onehotmapofat{\shortselindices}{\shortselvariables}}{\shortcatvariables}-\onesat{\shortcatvariables}\right)^2} \, . 
	\end{align*}
\end{example}


\subsect{Problem of the trivial tensor}

By the above we motivated least squares problems on the set of one-hot encoded states.
One is tempted to extend this set to $\mnexpfamily$ for efficient solutions by alternating algorithms.

However, for any hypergraph $\graph$ we have $\onesat{\shortcatvariables}\in\mnexpfamily$.
In many situations (e.g. disjoint model sets supported at positive data) the objective is more in favor at the trivial tensor than at the one-hot encoding.
As a result, we do not solve the previously posed one-hot encoding problem, when allowing such an hypothesis embedding.


\begin{example}[Fitting a boolean tensor by a formula tensor]\label{exa:formulaFitting}
	Given a tensor $\hypercore$, we want to find a formula $\exformula\in\formulaset$ such that it approximates $\hypercore$.

	If $\hypercore$ is a binary tensor, we understand it as a formula and want to find an $\exformula$ such that its number of worlds is maximal, that is solve the problem
		\[ \argmax_{\exformula\in\formulaset}\contraction{\exformula\Leftrightarrow\hypercore}  \, . \]

	We can use the squares risk trick and get an equivalent problem
		\[ \argmin_{\exformula\in\formulaset} \|\contractionof{\exformula\Leftrightarrow\hypercore}{\shortcatvariables}  - \onesat{\shortcatvariables} \|^2 \, . \]

	We have since $\hypercore$ and $\exformula$ are boolean
	\begin{align*}
		\|\contractionof{\exformula\Leftrightarrow\hypercore}{\shortcatvariables}  - \onesat{\shortcatvariables} \|^2
		= 	\|\contractionof{\exformula}{\shortcatvariables}  - \hypercoreat{\shortcatvariables} \|^2
	\end{align*}

	Now, when representing $\formulaset$ in a formula selecting architecture we have
		\[ \argmin_{\canparam\in\Gamma_1} \|\contractionof{\fselectionmap,\canparam}{\shortcatvariables}  - \hypercoreat{\shortcatvariables}\|^2 \, . \]
	where $\Gamma_1$ is the set of basis tensors.

	When we extend $\Gamma_1$ to a set including the trivial tensor $\onesat{\selvariable}$, when the formulas $\exformula$ are pairwise disjoint and $\hypercorewith=\onesat{\shortcatvariables}$, then the solution would be $\onesat{\selvariable}$.

\end{example}

%\begin{remark}{Least Squares Loss by Tensor Fitting}
%	\red{Alternative approach to least squares problems: Tensor Fitting}
%	And, if the target is another formula $y$, such that $\exformula$ conincides with $\tilde{f} \iff y $ we have
%		\[ \left(\polynomialof{\exformula}(\datamap)-1\right)^2 = \left(  \polynomialof{\tilde{f}}(\datamap) - y(\datamap) \right)^2  \]
%	This is exactly the least squares loss, which would appear in a supervised interpretation of the learning.
%\end{remark}




\sect{Alternating Solution of Least Squares Problems}

When the parameter tensor $\canparam$ is only restricted to have a decomposition as a tensor network on $\graph$, we can iteratively update each core.
The resulting algorithm is called Alternating Least Squares (ALS) (see Algorithm \ref{alg:ALS}).

\begin{algorithm}[hbt!]
\caption{Alternating Least Squares (ALS)}\label{alg:ALS}
\begin{algorithmic}
	\Require Target tensor $\targettensor[\shortcatvariables]$, hypergraph $\graph=(\nodes,\edges)$ with $[\catorder]\subset\nodes$ specifying the approximation format
	\Ensure Approximation of $\targettensor[\shortcatvariables]$ by a tensor network $\contractionof{\{\hypercoreofat{\edge}{\catvariableof{\edge}}\wcols\edge\in\edges\}}{\shortcatvariables}$
	\hrule
\For{$\edgein$}
	\State Set $\hypercoreofat{\edge}{\catvariableof{\edge}}$ to a random element in $\bigotimes_{\atomenumerator\in\edge}\rr^{\catdimof{\atomenumerator}}$
\EndFor
%\For{$\atomenumeratorin$}
%	\State Set $\varcore{\atomenumerator}$ to a random element in $\rr^{\atomlegdimof{\atomenumerator}}$ 
%\EndFor
\While{Stopping criterion is not met}
\For{$\edgein$}
	\State Set $\hypercoreofat{\edge}{\catvariableof{\edge}}$ to a solution of the local problem, that is
	\[ 
	\hypercoreofat{\edge}{\catvariableof{\edge}}
	 \algdefsymbol 
	 \argmin_{\hypercoreofat{\edge}{\catvariableof{\edge}}} 
	 \contraction{\importancetensor, (\contractionof{\fselectionmap,\canparam}{\shortcatvariables} - \targettensor[\shortcatvariables])^2}
	 \]
\EndFor
\EndWhile
	\State \Return $\{\hypercoreofat{\edge}{\catvariableof{\edge}}\wcols\edge\in\edges\}$
\end{algorithmic}
\end{algorithm}


\subsect{Choice of Representation Format}

\red{The choice of the hypergraph $\graph$ used for approximation bears a tradeoff between expressivity and complexity in sampling.
Hidden variables, that is variables only present in $\graph$, but not in the sensing matrix, increase the expressivity, especially when assigning large dimensions to them.
When there are no hidden variables, the maximum of $\canparam$ can be found by maximum calibration through a message passing algorithm, since no hidden variable has to marginalized.}


%In case of skeleton expressions with many placeholders further decomposition for algorithmic efficiency are required.
%\begin{itemize}
%	\item Elementary Format ($\elformat$-Format):
%	\item $\cpformat$-Format: Closest to sum of formula tensors (when all vectors are basis, then have a sum).
%	\item $\ttformat$-Format: Showed better heuristic performance in optimization
%\end{itemize}

%For any tensor network decomposition into cores $\canparamof{\selenumerator}$ have the derivative $\frac{\partial}{\partial \canparamof{\selenumerator}} \canparam$ as the tensor network with out the core $\canparamof{\selenumerator}$.

%\begin{remark}[Parametercores being basis tensors]
%	When the parameter core is a basis tensor, the contraction with the parametercore coincides with the respective formula tensor.
%	Thus, we will search for basis tensors optimizing in contractions objectives to specific reasoning tasks, and add them iteratively to the network at hand.
%\end{remark}


%\sect{Projection onto Basis Tensors}
%\red{This is sampling!}
%We project onto basis tensors to achieve single formulas.


\sect{Regularization and Compressed Sensing}


When regularizing the least squares problem by enforcing the sparsity of $\canparam$, we arrive at the compressed sensing problem
\begin{align}
	\argmin_{\canparamat{\selvariable}} \sparsityof{\canparam} 
	\quad \text{subject to } \quad 
	\left\| \contractionof{\sencsstat,\canparam}{\shortcatvariables} - \energytensorat{\shortcatvariables} \right\|_2 \leq \eta
\end{align}
Here, the sensing matrix is the selection tensor.


\begin{example}[Formula fitting to an example]
	Choosing the best formula fitting data (see Example~\ref{exa:formulaFitting}) is the problem
	\begin{align}
	\argmin_{\canparamat{\selvariable}\, : \,  \sparsityof{\canparam}=1} \left\| \contractionof{\importancetensor,\sencsstat,\canparam}{\shortcatvariables} - \targettensor \right\|_2 
	\end{align}
	where $\importancetensor$ has nonzero entries at marked coordinates and $\targettensor$ stores in Boolean coordinates whether the marked coordinates are positive or negative examples.
	\red{When the number of positive and negative examples are identical, we can linearly transform the objective to that of a grafting instance, where the current model is the empirical distribution of negative examples and the data consists of the positive examples.}
\end{example}

% Usage as sparse tensor
The sparse tensor solving the problem then has a small number of nonzero coordinates and the selection tensor can be restricted to those.
As a consequence, inference can be performed more efficiently.

% Algorithmic solution
The algorithmic solution of these problems can be done by greedy algorithms, thresholding based algorithms or optimization based algorithms \cite{foucart_mathematical_2013}.

% Guarantees
Guarantees for the success of the algorithms depend on the properties of the sensing matrices.
Here the sensing matrices are deterministic, since constructed as selection tensors, and concentration based approaches towards probabilistic bounds on these properties (see \cite{goesmann_uniform_2021}) are not applicable.





\begin{example}[Sensing matrix for propositional Formulas]
	Let there be a set $\formulaset$ of formulas, then we have
	\begin{align*}
		\contractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}},\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\outsymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}}
		= \contraction{\formulaof{\selindexof{\insymbol}}, \formulaof{\selindexof{\outsymbol}}} \, .
	\end{align*}
	If the formulas have disjoint model sets then 
	\begin{align*}
		\contractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}},\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}}
		= \begin{cases}
			\contraction{\formulaof{\insymbol}} & \text{if } \selindexof{\insymbol}=\selindexof{\outsymbol} \\
			0 & \text{else} 
		\end{cases} \, . 
	\end{align*}
	In that case, the sensing matrix is a restricted isometry, in the sense that the norm of any mapped vector is its norm mutliplied by a factor between the smallest and the largest $\contraction{\formulaof{\insymbol}}$.
\end{example}


\begin{example}[Sensing matrix for slice selection networks]

	For the slice selection network
	\begin{align*}
		& \contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{\insymbol}},\sliceselectionmapat{\shortcatvariables,\selvariableof{\outsymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}} \\
		& \quad = \begin{cases}
			0 & \text{if for a }\seccatenumerator\in\variablesetof{\selindexof{\insymbol}}\cap\variablesetof{\selindexof{\outsymbol}}\text{ we have }\catindexof{\seccatenumerator}^{\selindexof{\insymbol}}\neq\catindexof{\seccatenumerator}^{\selindexof{\outsymbol}} \\
			\prod_{\seccatenumerator\notin\variablesetof{\selindexof{\insymbol}}\cup\variablesetof{\selindexof{\outsymbol}}} \catdimof{\seccatenumerator}& \text{else} 
		\end{cases} \, . 
	\end{align*}

	Given a fixed $\selindexof{\insymbol}$, the maximum value in the respective slice is thus taken at $\selindexof{\insymbol}=\selindexof{\outsymbol}$.
\end{example}


\sect{Discussion and Outlook}

The slice selection network described here have been taylored to the $\cpformat$ format.

We can extend the slice selection network to parametrize more general tensor network formats than the $\cpformat$ format.
Here the graph structure of the format itself can be optimized, by the usage of variable selectors.
The activation selectors, generalizing the connective selection, are then choices of specific cores in the format.
Thus, each neuron represents an hypercore $\hypercoreofat{\edge}{\catvariableof{\edge}}$, where $\edge\subset\nodes$ is selected by variable selectors and the values of the tensor $\hypercoreof{\edge}$ by activation selectors.

