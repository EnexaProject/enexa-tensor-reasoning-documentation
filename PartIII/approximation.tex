\chapter{\chatextapproximation}\label{cha:approximation}

We in this chapter study the search for the maximal coordinate in a tensor, which has a sparse \basplusDecomposition{}.
We then investigate methods to approximate tensors by sparse \basplusDecomposition{}.
%Often reasoning requires the execution of demanding contractions of tensors networks, or combinatorical search of maximum coordinates.
%We in this chapter investigate methods, to replace hard to be sampled tensor networks by approximating tensor networks, which then serve as a proxy in inference tasks.


\sect{Optimization of sparse tensors}

Let us now study the problem of searching for the maximal coordinate in a tensor represented by a monomial decomposition.
Given a tensor $\hypercorewith$ we state this as the problem:
\begin{align}
    \label{prob:maxCoordinate}\tag{$\probtagtypeinst{\maximizationsymbol}{\hypercore}$}
    \argmax_{\shortcatindices\in\facstates} \hypercoreat{\indexedshortcatvariables}
\end{align}

\probref{prob:maxCoordinate} can be reformulated as optimization over the standard simplex
\begin{align*}
    \meansetof{\mlnmintermsymbol} = \convhullof{\onehotmapofat{\shortcatindices}{\shortcatvariables} \wcols \shortcatindices\in\facstates}
\end{align*}
as
\begin{align*}
    \argmax_{\meanparamat{\selvariableof{[\catorder]}}\in\meansetof{\mlnmintermsymbol}} \contraction{\meanparam,\hypercore} \, .
\end{align*}

\begin{example}[Mode search in exponential families]
    % \red{This transforms the mean parameter polytope by contracting with some core, here the selection encoding of the statistic!}
    Given a statistic $\sstat$, a canonical parameter $\canparam$ and a boolean base measure $\basemeasure$, the mode search problem for the member $\expdistof{\sstat,\canparam,\basemeasure}$ of the exponential family $\expfamilyof{\sstat,\basemeasure}$ is
    \begin{align*}
        \max_{\shortcatindices\in\atomstates \wcols \basemeasureat{\indexedshortcatvariables}=1} \contraction{\sencsstatat{\indexedshortcatvariables,\selvariable},\canparamat{\selvariable}}
        = \max_{\meanparam\in\meansetof{\sstat,\basemeasure}} \contraction{\meanparamat{\selvariable},\canparamat{\selvariable}} \, .
    \end{align*}
    Such mode search problems have appeared as generic MAP queries (see \charef{cha:probReasoning}).
    In \charef{cha:networkReasoning} we have discussed them for the specific cases of \HybridLogicNetworks{} and grafting proposal distributions.
% Appearance of mode search
%    The search for maximal coordinates appears in various reasoning tasks:
%    \begin{itemize}
%        \item MAP query as mode search of MLN: $\hypercore$ is the contraction of evidence with the distribution, leaving the query variables open.
%        \item Grafting as mode search of proposal distribution: $\hypercore$ is the contraction of the gradient of the likelihood with the basis encoding of the hypothesis.
%    \end{itemize}
%Both tasks have been formulated as mode search problems in exponential families.
\end{example}



\subsect{Unconstrained Binary Optimization}

For leg dimensions $\catdimof{\atomenumerator}=2$, \probref{prob:maxCoordinate} is known as the unconstrained binary optimization.
\probref{prob:maxCoordinate} is a Higher-Order Unconstrained Binary Optimization (HUBO), when $\hypercorewith$ has a when $\hypercore$ has a monomial decomposition (see \defref{def:polynomialSparsity}) with $\cardof{\variablesetof{\decindex}}\leq\sliceorder$ for all $\decindexin$, that is when $\slicerankwrtof{\sliceorder}{\hypercore}<\infty$.

\begin{definition}
    Let $\hypercorewith$ be a tensor with a monomial decomposition $\enumeratedslices$, where $\max_{\decindexin}\cardof{\variablesetof{\decindex}}=\sliceorder$.
    Se then call \probref{prob:maxCoordinate} a $\sliceorder$-Order Unconstrained Binary Optimization (HUBO), which we denote as
    \begin{align}
        \label{prob:HUBO}\tag{$\probtagtypeinst{\hubosymbol}{\hypercore}$}
        \argmax_{\shortcatindices\in\atomstates} \quad
        \sum_{\decindexin} \slicescalar^{\decindex} \contractionof{\onehotmapofat{\catindexof{\variablesetof{\decindex}}^{\decindex}}{\catvariableof{\variablesetof{\decindex}}}}{\indexedshortcatvariables} \, .
    \end{align}
%    The binary optimization of a tensor $\hypercorewith\in\atomstates$ is the problem
%    \begin{align}\tag{$\probtagtypeinst{\mathrm{HUBO}}{\hypercore}$}\label{prob:HUBO}
%        \argmax_{\shortcatindices\in\atomstates} \quad \hypercoreat{\indexedshortcatvariables}
%    \end{align}
%    We call Problem~\ref{prob:HUBO} a Higher-Order Unconstrained Binary Optimization (HUBO) problem of order $\sliceorder$ and sparsity $\slicerankwrtof{\sliceorder}{\hypercore}$, when $\hypercore$ has a monomial decomposition (see \defref{def:polynomialSparsity}) with $\cardof{\variablesetof{\decindex}}\leq\sliceorder$ for all $\decindexin$, that is when $\slicerankwrtof{\sliceorder}{\hypercore}<\infty$.
\end{definition}

\begin{remark}[Leg dimensions larger than 2]
    We demanded leg dimensions $\catdimof{\atomenumerator}=2$ to have boolean valued variables $\catvariableof{\catenumerator}$, which is required to connect with the formalism of binary optimization.
    Categorical variables with larger dimensions can be represented by atomization variables, which are created by contractions with categorical constraint tensors (see \secref{sec:categoricalTN}).
\end{remark}

% Interpretation of sparsity
The sparsity $\slicerankwrtof{\sliceorder}{\hypercore}$ is the minimal number of monomials, for which a weighted sum is equal to $\hypercore$.
Thus we interpret \probref{prob:HUBO} as searching for the maximum in a polynomial consistent of $\slicerankwrtof{\sliceorder}{\hypercore}$ monomial terms.
%\red{Each monomial is also refered to as potential.}

\probref{prob:HUBO} is called Quadratic Unconstrained Binary Optimization problems, if $\sliceorder=2$.
We can transform certain Higher-Order Unconstrained Binary Optimization (HUBO) problems into Quadratic Unconstrained Binary Optimization (QUBO) problems by introducing auxiliary variables.
An example of such an transform is provided by the next lemma.

%% Slack variables
\begin{lemma}
    \label{lem:monomialToQUBO}
    For any $\atomindices\in[2]$ and $\variableset\subset[\atomorder]$ we have
    \begin{align*}
        \left( \prod_{\atomenumerator\in\variableset} \atomlegindexof{\atomenumerator } \right)  \left(  \prod_{\atomenumerator\notin\variableset} (1- \atomlegindexof{\atomenumerator }) \right) =
        \max_{\slackvariable\in[2]} \slackvariable \cdot 2 \cdot \left( \sum_{\atomenumerator\in\variableset}\atomlegindexof{\atomenumerator}  - \cardof{\variableset} - \sum_{\atomenumerator\notin\variableset}\atomlegindexof{\atomenumerator} + \frac{1}{2} \right) \, . % Alternative: no factor 2, but + 1 instead of +1/2 (->pyqubo)
    \end{align*}
\end{lemma}
\begin{proof} %Proof by case distinction
    Only if $\atomlegindexof{\atomenumerator}=1$ for $\atomenumerator\in\variableset$ and $\atomlegindexof{\atomenumerator}=0$ else we have
    \[ \left( \sum_{\atomenumerator\in\variableset}\atomlegindexof{\atomenumerator}  - \cardof{\variableset} - \sum_{\atomenumerator\notin\variableset}\atomlegindexof{\atomenumerator} + \frac{1}{2} \right) \geq 0 \, . \]
    In this case the maximum is taken for $\slackvariable=1$ and we have
    \[ \max_{\slackvariable\in[2]} \slackvariable \cdot 2 \cdot \left( \sum_{\atomenumerator\in\variableset}\atomlegindexof{\atomenumerator}  - \cardof{\variableset} - \sum_{\atomenumerator\notin\variableset}\atomlegindexof{\atomenumerator} + \frac{1}{2} \right)
    = 1 = \left( \prod_{\atomenumerator\in\variableset} \atomlegindexof{\atomenumerator } \right)  \left(  \prod_{\atomenumerator\notin\variableset} (1- \atomlegindexof{\atomenumerator }) \right) \, . \]
    In all other cases, the maximum is taken for $\slackvariable=0$ and thus vanishes, that is
    \[ \max_{\slackvariable\in[2]} \slackvariable \cdot 2 \cdot \left( \sum_{\atomenumerator\in\variableset}\atomlegindexof{\atomenumerator}  - \cardof{\variableset} - \sum_{\atomenumerator\notin\variableset}\atomlegindexof{\atomenumerator} + \frac{1}{2} \right)
    = 0 = \left( \prod_{\atomenumerator\in\variableset} \atomlegindexof{\atomenumerator } \right)  \left(  \prod_{\atomenumerator\notin\variableset} (1- \atomlegindexof{\atomenumerator }) \right) \, . \]
    Thus, the claim holds in all cases.
\end{proof}

\subsect{Integer Linear Programming}

Let us now show how optimization problems can be represented as linear programming problems.
% State vector
To this end, we understand each index tuple $\shortcatindices\in\facstates$ as a vector $\statevectorofat{\shortcatindices}{\selvariable}\in\rr^{\catorder}$ with coordinates
\begin{align*}
    \statevectorofat{\shortcatindices}{\selvariable=\catenumerator} = \catindexof{\catenumerator} \, .
\end{align*}

\begin{definition}
    The integer linear program (ILP) of $\matrixat{\datvariable,\selvariable}\in\rr^{n \times d}$, $\rhssymbol[\datvariable]\in\rr^{n}$ and $c\in\rr^{\catorder}$ is the problem
    \begin{align}
        \tag{$\probtagtypeinst{\mathrm{ILP}}{\objectivesymbol,\exmatrix,\rhssymbol}$}
        \argmax_{\shortcatindices\in\facstates} \contraction{\objectivesymbol[\selvariable], \statevectorofat{\shortcatindices}{\selvariable}}
        \quad \text{subject to } \quad \contraction{\matrixat{\datvariable,\selvariable},\statevectorofat{\shortcatindices}{\selvariable}} \prec \rhssymbol[\datvariable] \, ,
    \end{align}
    where by $\prec$ we denote partial ordering of tensors (see \defref{def:partialOrder}).
\end{definition}


We now show that any binary optimization problem of a tensor can be transformed into a integer linear program, given a monomial decomposition of the tensor $\hypercorewith$ by $\sliceset=\enumeratedslices$.
For this we choose state indices by vectors
\begin{align*}
    \seccatindex_{[\catorder+\decdim]} = \catindexof{0},\ldots,\catindexof{\catorder-1},\slackindexof{0},\ldots\slackindexof{\decdim-1} \in \left(\bigtimes_{\catenumeratorin}[2]\right) \times  \left(\bigtimes_{\decindexin}[2]\right) \, ,
\end{align*}
that is we added for each monomial an index $\slackindexof{\decindex}$, which will represent the evaluations of the respective monomial.


We furthermore define a vector $\objofat{\sliceset}{\selvariable}$, where $\selvariable$ takes values in $[\catorder+\decdim]$, as
\begin{align}
    \label{eq:ilpPotential}
    \objofat{\sliceset}{\indexedselvariable} =
    \begin{cases}
        \slicescalarof{\selindex-\catorder} & \text{if} \quad \selindex>\catorder \\% \decindexin \text{ we have } \selindex = \catorder + \decindex \\
        0 & \text{else}
    \end{cases} \, .
\end{align}

To construct a matrix $\matrixat{\datvariable,\selvariable}$ and a vector $b[\datvariable]$ to the monomial decomposition $\sliceset$, we now introduce a variable $\datvariable$ enumerating linear inequalities, which takes values in $[\datanum]$, where
\[ \datanum =  \sum_{\decindexin} \left(\cardof{\variablesetof{\decindex}} +1\right) \, . \]
We define for each $\decindexin$ an auxiliary number
\[ \datanum_{\decindex} = \sum_{\tilde{\decindex}=0}^{\decindex} \left(\cardof{\variablesetof{\tilde{\decindex}}} +1\right) \]
and further enumerate the set $\variablesetof{\decindex}$ by a function $\indexinterpretation: [\cardof{\variablesetof{\decindex}}] \rightarrow \variablesetof{\decindex}$.

We then construct a matrix $\matrixofat{\sliceset}{\datvariable,\selvariable}$, were for $\selindex\in[\catorder+\decdim]$, $\decindexin$ and $\datindex\in[\cardof{\variablesetof{\decindex}}]$ we have
\begin{align}
    \label{eq:ilpMatrix}
    \matrixofat{\sliceset}{\datvariable=\datanum_{\decindex}+\datindex,\indexedselvariable} =
    \begin{cases}
        1 - 2 \cdot \catindexof{\indexinterpretationat{\datindex}}^{\decindex} & \text{if} \quad \datindex < \cardof{\variablesetof{\decindex}}, \,\, \datindex=\selindex \text{  and  } \selindex = \indexinterpretationat{\datindex} \\ % Upper bounds on z, x position
        1  & \text{if} \quad \datindex < \cardof{\variablesetof{\decindex}} \text{  and  } \selindex = \catorder + \decindex \\ % Upper bound on z, z position
        -\catindexof{\indexinterpretationat{\datindex}}^{\decindex} & \text{if }  \datindex = \cardof{\variablesetof{\decindex}}    \text{ and }  \selindex=\indexinterpretationat{\datindex}  \\ % Last condition on $x$
        -1 & \text{if }  \datindex = \cardof{\variablesetof{\decindex}}  \text{ and }  \selindex = \catorder + \decindex \\ % Last condition on $x$
        0 & \text{else} \\
    \end{cases} \, .
\end{align}
%All further coordinates of $\matrixofat{\sliceset}{\datvariable,\selvariable}$ not reached by this construction are set to $0$.
Similarly, we define $\rhsofat{\sliceset}{\datvariable}$ as the vector which nonvanishing coordinates are for $\decindexin$ at
\begin{align}
    \label{eq:ilpRhs}
    \rhsofat{\sliceset}{\datvariable=\datanum_{\decindex}+\datindex} =
    \begin{cases}
        1 - \catindexof{\indexinterpretationat{\datindex}}^{\decindex} & \text{if }  \datindex < \cardof{\variablesetof{\decindex}} \\ % Upper bounds on z
        %\datindex = \catenumerator \text{ and } \catindexof{\catenumerator}^{\decindex} = 0 \\ % First $\catorder$ conditions, on x
        -1 + \cardof{\{\catenumerator\in\variablesetof{\decindex} \wcols  \catindexof{\catenumerator}^{\decindex} = 1\}}  & \text{if }  \datindex = \cardof{\variablesetof{\decindex}} \\ % \text{ and }  \selindex = \catorder + \decindex \\ % Last condition on $x$
    \end{cases} \, .
\end{align}


% Intuition
Informally, we pose for each tuple $\slicetupleof{}$ $\cardof{\variableset}+1$ linear equations.
The first $\cardof{\variableset}$ enforce, that the slice representing variable $\slackvariable$ is zero once a leg is $0$.
The last enforces that the slice representing variable is $1$.
We prove this claim more formally in the next theorem.

\begin{theorem}
    Given a monomial decomposition $\sliceset=\enumeratedslices$ of a tensor $\hypercore$, let  $\seccatindex^{ILP,\sliceset}$ be a solution of the integer linear program defined by the matrix and vectors in equations \eqref{eq:ilpPotential}, \eqref{eq:ilpMatrix} and \eqref{eq:ilpRhs}.
    Then we have
    \begin{align*}
        \restrictionofto{\seccatindex^{ILP,\sliceset}}{[\catorder]} \in \argmax_{\shortcatindices\in\atomstates} \quad \hypercoreat{\indexedshortcatvariables} \, .
    \end{align*}
    where by  $\restrictionofto{\seccatindex^{ILP,\sliceset}}{[\catorder]}$ we denote the restriction of the index tuple $\seccatindex^{ILP,\sliceset}$ to the first $\catorder$.
\end{theorem}
\begin{proof}
    We show that the linear constraints by
    \begin{align*}
        \contraction{\matrixofat{\sliceset}{\datvariable,\selvariable},\statevectorofat{\seccatindexof{[\atomorder+\decdim]}}{\selvariable}} \prec \rhsofat{\sliceset}{\datvariable}
    \end{align*}
    are satisfied for a vector $\seccatindexof{[\atomorder+\decdim]}=(\catindexof{[\atomorder]},\slackindexof{[\decdim]})$, if and only if for all $\decindexin$ the product constraints
    \begin{align}
        \label{eq:slackindexInequalityILPProof}
        \slackindexof{\decindex}
        = \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^{\decindex} = 0} (1 - \catindexof{\catenumerator} \right)
        \cdot \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^{\decindex} = 1}  \catindexof{\catenumerator} \right) \,
    \end{align}
    hold.
    We will see, that the linear constraints where $\datvariable$ takes indices in $\datanum_{\decindex}+[\cardof{\variablesetof{\decindex}}]$ are equivalent to the upper bound on $\slackindexof{\decindex}$ and the constraint to $\datvariable=\datanum_{\decindex}+\cardof{\variablesetof{\decindex}}$ is equivalent to an lower bound on $\slackindexof{\decindex}$.
    To show the upper bound, we notice that for any $\datindex\in[\cardof{\variablesetof{\decindex}}]$ the constraint $\datvariable = \datanum_{\decindex}+\datindex$ is
    \begin{align*}
        \slackindexof{\decindex} \leq
        \begin{cases}
            \catindexof{\indexinterpretationat{\datindex}}  & \text{if} \quad \catindexof{\indexinterpretationat{\datindex}}^\decindex = 1 \\
            (1- \catindexof{\indexinterpretationat{\datindex}}) & \text{if} \quad \catindexof{\indexinterpretationat{\datindex}}^\decindex = 0 \\
        \end{cases} \, .
    \end{align*}
    Thus, whenever a factor on the right side of \eqref{eq:slackindexInequalityILPProof} is $0$, we have $\slackindexof{\decindex}=0$ if the respective constraint is satisfied.
    We conclude, that
    \begin{align*}
        \slackindexof{\decindex}
        \leq \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 0} (1 - \catindexof{\catenumerator}) \right)
        \cdot \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 1}  \catindexof{\catenumerator} \right) \, .
    \end{align*}
    To show the lower bound, we have the constraint to $\datvariable=\datanum_{\decindex}+\cardof{\variablesetof{\decindex}}$ by
    \begin{align*}
        \slackindexof{\decindex} \geq 1 -
        \left( \sum_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 0} \catindexof{\catenumerator} \right)
        + \left( \sum_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 1}  (\catindexof{\catenumerator} - 1 )\right) \, .
    \end{align*}
    The right side of this inequality is $1$, if and only if all factors on the right side of \eqref{eq:slackindexInequalityILPProof} are $1$, and less or equal to $0$ else.
    Thus, whenever this constraint is satisfied, we have
    \begin{align*}
        \slackindexof{\decindex}
        \geq \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 0} (1 - \catindexof{\catenumerator}) \right)
        \cdot \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 1}  \catindexof{\catenumerator} \right)  \, .
    \end{align*}

    In summary, the equation \eqref{eq:slackindexInequalityILPProof} holds, if and only if the constraints where $\datvariable$ takes indices in $\datanum_{\decindex}+[\cardof{\variablesetof{\decindex}}+1]$ are satisfied.
%    In summary we arrive at
%    \[ \slackindexof{\decindex}
%    = \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 0} (1 - \catindexof{\catenumerator}^\decindex) \right)
%    \cdot \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 1}  \catindexof{\catenumerator}^\decindex \right)
%    \]
%    if and only if the indices $\seccatindex_{[\catorder+\decdim]}$ are feasible.

    This characterization of the constraints implies, that for any $\shortcatindices\in\facstates$ there is exactly one feasible index $\seccatindex_{[\catorder+\decdim]}$ with $\restrictionofto{(\seccatindex_{[\catorder+\decdim]})}{[\catorder]}=\shortcatindices$, and the objective takes for this index the value
    \begin{align*}
        \contraction{\objectivesymbol[\selvariable], \statevectorofat{\seccatindex_{[\catorder+\decdim]}}{\selvariable}}
        &= \sum_{\decindexin} \slicescalarof{\decindex} \cdot \slackindexof{\decindex} \\
        &= \sum_{\decindexin} \slicescalarof{\decindex} \cdot \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 0} (1 - \catindexof{\catenumerator}^\decindex) \right)
        \cdot \left( \prod_{\catenumerator\in\variablesetof{\decindex} \, , \,  \catindexof{\catenumerator}^\decindex = 1}  \catindexof{\catenumerator}^\decindex \right)  \\
        & = \hypercoreat{\indexedshortcatvariables} \, .
    \end{align*}
    Therefore, any solution of the ILP reduced to the first $\catorder$ indices corresponding with the axis of $\hypercore$, is a solution of the binary optimization problem to $\hypercore$.
\end{proof}


% Sparsity
In order to achieve a sparse linear program it is benefitial to use a monomial decomposition with small order and rank.
Beside this sparsity, the matrix $\matrixofat{\sliceset}{\datvariable,\selvariable}$ is often $\ell_0$-sparse, and has thus an efficient representation in a basis CP format.
More precisely we have by the above construction
\begin{align*}
    \sparsityof{\matrixofat{\sliceset}{\datvariable,\selvariable}} \leq \sum_{\decindexin} 3 \cdot \cardof{\variablesetof{\decindex}} +1 \, .
\end{align*}


\sect{Selection tensor networks for $\cpformat$ decompositions}

In this section, we show that the set of tensors respresentable in specific $\cpformat$ formats coincides with the expressivity of tailored selection architectures (see \charef{cha:formulaSelection}).
We first define a basis+ $\cpformat$ selecting tensor and then show its decomposition into a formula selecting neural network.

\begin{definition}
    \label{def:cpSelection}
    Given a set of categorical variables $\shortcatvariables$ with $\catdim=\max_{\catenumeratorin}\catdimof{\catenumerator}$, a $\cpformat$ selecting tensor of maximal cardinality $\sliceorder$ is the tensor
    \begin{align*}
        \sliceselectionmapat{\shortcatvariables,\selvariableof{0,0},\ldots,\selvariableof{\sliceorder-1,0},\selvariableof{0,1},\ldots,\selvariableof{\sliceorder-1,1}}
    \end{align*}
    with dimensions
    \begin{align*}
        \seldimof{\selenumerator,0} = \catdim+1, \seldimof{\selenumerator,1} = \atomorder \quad \text{for} \quad \selenumerator\in[\sliceorder]
    \end{align*}
    and coordinates
    \begin{align*}
        & \sliceselectionmapat{\shortcatvariables=\shortcatindices,\indexedselvariableof{0,0},\ldots,\indexedselvariableof{\sliceorder-1,0},\indexedselvariableof{0,1},\ldots,\indexedselvariableof{\sliceorder-1,1}} \\
        & \quad = \begin{cases}
                      1 & \text{if} \quad
                      \forall{\atomenumeratorin,\selenumerator\in[\sliceorder]} : \, \big(\selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq \catdim \big) \Rightarrow  (\selindexof{\selenumerator,0} = \catindexof{\atomenumerator})  \\
                      0 & \text{else}
        \end{cases} \, .
    \end{align*}
\end{definition}

% Variable and State selectors
Intuitively, the selection variables $\selvariableof{\selenumerator,1}$ of $\sliceselectionmapof{\catorder,\sliceorder}$ select a variable out of $[\atomorder]$ to be included in $\variableset$ and the selection variables $\selvariableof{\selenumerator,0}$ select a corresponding state to that variable.
As in \charef{cha:formulaSelection} we refer to variables $\selvariableof{\selenumerator,1}$ as variable selectors and $\selvariableof{\selenumerator,0}$ as state selectors.
When $\selvariableof{\selenumerator,0}=\catdim$, the slice is left trivial, that is the selected variable is effectively not included in $\variableset$.
This then allows to also represent slices where $\cardof{\variableset}<\sliceorder$.
We in the following prove this more formally.

\begin{theorem}
    Let the non-vanishing indices of $\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}$ denote by $\{\selindexof{[\sliceorder]\times[\catdim+1]}^{\decindex} \, : \, \decindexin\}$.
    Let further $\mathcal{M}\subset[\decdim]$ be the set of agreeing selection indices, that is
    \begin{align*}
        \mathcal{M} = \{ \decindex \, : \, \decindexin,\,
        \forall{\selenumerator,\secselenumerator\in[\sliceorder]} : \big( \selindexof{\selenumerator,1}^\decindex= \selindexof{\secselenumerator,1}^\decindex  \land \selindexof{\selenumerator,1}^\decindex\neq\catdim \land \selindexof{\secselenumerator,1}^\decindex \neq \catdim \big)
        \Rightarrow \big(\selindexof{\selenumerator,1}^\decindex = \selindexof{\secselenumerator,1}^\decindex \big)
        \}
    \end{align*}
    Then
    \begin{align*}
        &\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{[\sliceorder]\times[\catdim+1]}},\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}}{\shortcatvariables} \\
        &\quad = \sum_{\decindex\in\mathcal{M}} \canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}=\selindexof{[\sliceorder]\times[\catdim+1]}^{\decindex}}
        \contractionof{\{\onehotmapofat{\selindexof{\selenumerator,0}}{\catvariableof{\selindexof{\selenumerator,1}}} \, : \, \selenumerator\in[\sliceorder], \, \selindexof{\selenumerator,0}\neq\catdim \}}{\shortcatvariables}
    \end{align*}
    Further we have
    \begin{align*}
        \baspluscprankof{\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{[\sliceorder]\times[\catdim+1]}},\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}}{\shortcatvariables}}
        \leq \sparsityof{\canparam} \, .
    \end{align*}
    %Thus, the slice selecting network represents $\cpformat$ decompositions of size $\cardof{\mathcal{M}}\leq \sparsityof{\canparam}$.
\end{theorem}
\begin{proof}
    Let us notice, that whenever $\decindex\notin\mathcal{M}$ then
    \begin{align*}
        \sliceselectionmapat{\shortcatvariables,\indexedselvariableof{[\sliceorder]\times[\catdim+1]}}
        = \zerosat{\shortcatvariables} \, ,
    \end{align*}
    since the condition for non-vanishing coordinates
    \begin{align*}
        \forall{\atomenumeratorin,\selenumerator\in[\sliceorder]} : \, \big(\selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq \catdim \big) \Rightarrow  (\selindexof{\selenumerator,0} = \catindexof{\atomenumerator})
    \end{align*}
    in \defref{def:cpSelection} cannot be satisfied for any $\shortcatindices$.
    For $\decindex\notin\mathcal{M}$ we have
    \begin{align*}
        \sliceselectionmapat{\shortcatvariables,\indexedselvariableof{[\sliceorder]\times[\catdim+1]}}
        =  \contractionof{\{\onehotmapofat{\selindexof{\selenumerator,0}}{\catvariableof{\selindexof{\selenumerator,1}}} \, : \, \selenumerator\in[\sliceorder], \, \selindexof{\selenumerator,0}\neq\catdim \}}{\shortcatvariables} \, .
    \end{align*}
    We now use these insights and linearity of contraction to get
    \begin{align*}
        &\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{[\sliceorder]\times[\catdim+1]}},\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}}{\shortcatvariables} \\
        & \quad = \sum_{\decindexin} \canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}=\selindexof{[\sliceorder]\times[\catdim+1]}^{\decindex}}
        \sliceselectionmapat{\shortcatvariables,\indexedselvariableof{[\sliceorder]\times[\catdim+1]}} \\
        & \quad = \sum_{\decindex\in\mathcal{M}} \canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}=\selindexof{[\sliceorder]\times[\catdim+1]}^{\decindex}}
        \contractionof{\{\onehotmapofat{\selindexof{\selenumerator,0}}{\catvariableof{\selindexof{\selenumerator,1}}} \, : \, \selenumerator\in[\sliceorder], \, \selindexof{\selenumerator,0}\neq\catdim \}}{\shortcatvariables}
    \end{align*}
    This shows the decomposition claim.
    We notice, that this is a basis+ $\cpformat$ decomposition of size $\mathcal{M}$ and thus
    \begin{align*}
        \baspluscprankof{\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{[\sliceorder]\times[\catdim+1]}},\canparamat{\selvariableof{[\sliceorder]\times[\catdim+1]}}}{\shortcatvariables}}
        \leq \cardof{\mathcal{M}} \leq \sparsityof{\canparam} \, .
    \end{align*}
\end{proof}

Towards a practicle usage of this representation scheme for basis+ $\cpformat$ decompositions, let us show that the $\cpformat$ selecting tensor coincides with a formula selecting network.

\begin{lemma}
    \label{lem:fsnnRepresentingSliceSelector}
    The $\cpformat$ selection tensor coincides with a formula selecting neural network with neurons (see Figure~\ref{fig:sliceSelectingNN}):
    \begin{itemize}
        \item unary state selecting neurons enumerated by $\selenumerator$, selecting one of the $\shortcatvariables$ with the variable $\selvariableof{\selenumerator,1}$ and selecting a state, extended by a possible choice of trivial legs $\ones$
%        \item unary input neuron enumerated by $\selenumerator$, selecting one of the $\shortcatvariables$ with the variable $\selvariableof{\selenumerator,1}$ and selecting a connective in $\{\lnot, \mathrm{Id}, \mathrm{True}\}$ by $\selvariableof{\selenumerator,0}$
        \item $\sliceorder$-ary output neuron fixed to the $\land$ connective.
    \end{itemize}
\end{lemma}
\begin{proof}
    This can be easily checked on each coordinate.
\end{proof}

% Case of leg-dimension 2
If $\catdimof{\catenumerator}=2$ for $\catenumeratorin$, the state selector chooses between the connectives $\{\lnot, \mathrm{Id}, \mathrm{True}\}$.
We can in that case understand each slice by a logical term.

%
%
%In the next two lemmata we first show that the defined slice selecting tensors indeed selects slices and then provide a representation as a formula selecting network.
%
%\begin{lemma}
%    \label{lem:sliceFromSliceSelector}
%    If all input neurons with same selection index are agreeing on the connective index, the selected formula does not vanish and coincides with a slice to the set
%    \[ \variableset = \{ \atomenumerator \, : \, \exists_{\selenumeratorin}: \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \} \]
%    and for $\atomenumerator\in\variableset$
%    \[ \catindexof{\atomenumerator} = \selindexof{\selenumerator,0} \quad \text{if} \quad \selindexof{\selenumerator,1} = \atomenumerator \, . \]
%\end{lemma}
%\begin{proof}
%    We need to show that
%    \begin{align}
%        \label{eq:sliceFromSliceSelector}
%        \sliceselectionmapat{\shortcatvariables,\indexedselvariableof{0,0},\ldots,\indexedselvariableof{\sliceorder-1,0},\indexedselvariableof{0,1},\ldots,\indexedselvariableof{\sliceorder-1,1}} = \onehotmapofat{\catindexof{\variableset}}{\shortcatvariables} \, .
%    \end{align}
%    If and only if an index $\tilde{\catindex}_{[\atomorder]}$ reduced on $\variableset$ does not coincide with $\catindexof{\variableset}$, we have $\onehotmapofat{\catindexof{\variableset}}{\shortcatvariables=\tilde{\catindex}_{[\atomorder]}}=0$ and otherwise $\onehotmapofat{\catindexof{\variableset}}{\shortcatvariables=\tilde{\catindex}_{[\atomorder]}}=1$ .
%    Let us notice, that this condition is equivalent to
%    \[ \forall_{\atomenumerator,\selenumerator} : \big(  \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \big) \Rightarrow  (\selindexof{\selenumerator,0} = \catindexof{\atomenumerator}) \]
%    and thus \eqref{eq:sliceFromSliceSelector} holds.
%\end{proof}
%
%

%
%It follows, that the expressivity of the slice selecting neural network coincides with the set of tensors with a bound on their slice sparsity, when $\sliceorder\geq\atomorder$.
%For arbitrary $\sliceorder$, the following theorem holds.
%
%\begin{theorem}
%    Let $\sliceselectionmapat{\shortcatvariables,\selvariable}$ be a slice selecting tensor.
%    For any parameter tensor $\canparamat{\selvariable}$ we have
%    \[ \baspluscprankof{\contractionof{\sliceselectionmapat{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables}} \leq \bascprankof{\canparamat{\selvariable}} \, . \]
%\end{theorem}
%\begin{proof}
%    Each non-vanishing coordinate of $\canparam$ represents by \lemref{lem:fsnnRepresentingSliceSelector} a slice and their weighted sum is thus a monomial decomposition.
%%	\red{Use the above lemma for that on each slice.}
%%	Show, how a coordinate of $\canparam$ corresponds with a slice determining tuple: $\sliceset$ determined by the selection indices.
%\end{proof}


\begin{figure}[h]
    \begin{center}
        \input{PartIII/tikz_pics/sparse_calculus/slice_selecting_nn.tex}
    \end{center}
    \caption{Representation of a basis+ Tensor by the contraction of a parameter tensor $\canparam$ with a $\cpformat$ selecting architecture $\fselectionmap$, which has a decomposition as a formula selecting neural network (see \lemref{lem:fsnnRepresentingSliceSelector}).
    The nonzero coordinates of $\canparam$ represent slices of the $\cpformat$ decomposition.
    }
\end{figure}\label{fig:sliceSelectingNN}


\subsect{Applications}

One application is as a parametrization scheme in the approximation of a tensor by a slice-sparse tensor, see \charef{cha:approximation}.
The approximated parameter can then be used as a proxy energy to be maximized.
When choosing $\sliceorder=2$, the approximating tensor contains only quadratic slices, which then poses a QUBO problem.

\begin{remark}[Extension to arbitrary $\cpformat$ formats]
    Select at each input neuron a specific leg.
    For finite number of legs, as it is the case in the binary, basis and basis+ formats, we can enumerate all possibilities by the selection variable.
    For the basis+ format, in case of binary leg dimensions, we here exemplified the approach, by enumerating the three possibilities $\onehotmapof{0},\onehotmapof{1},\onesat{1}$.
    This approach, however, fails as a generic representation of the directed format, since the directed legs are continuous and there therefore are infinite choosable legs.
\end{remark}





\sect{Approximation in the Hilbert-Schmidt norm}

The Hilbert-Schmidt norm of a tensor is the contraction of the coordinatewise transform with the square function
\begin{align*}
	\|\hypercorewith\|_2= \sqrt{\contraction{\left(\hypercorewith\right)^2}} \, .
\end{align*}
Approximation involving a selection architecture $\fselectionmap$ is the problem
	\[ \argmin_{\canparam\in\Gamma^{\graph}} \|\energytensor - \contractionof{\sencodingof{\fselectionmap},\canparam}{\shortcatvariables}\|^2 \, . \]
As an example, using the universal selection architecture (corresponding with minterm formulas), the approximation problem is
	\[ \argmin_{\canparam\in\Gamma^{\graph}} \|\energytensorat{\shortcatvariables} - \canparamat{\shortcatvariables}\|^2 \, . \]
In a tensor network diagram we depict this as
\begin{center}
    \input{PartIII/tikz_pics/approximation/least_squares.tex}
\end{center}


%\begin{example}[Approximate based on a slice sparsity selecting architecture]
%	Use a term selecting neural network (conjunction neuron on $\atomorder$ unary neurons selecting a variable and $\mathrm{Id},\lnot,\mathrm{True}$ as connective selector.
%	Demand the parameter tensor $\canparam$ to be in a basis CP format, then each slice of the parameter tensor corresponds with the slice of the energy.
%	The use the approximation for MAP search.
%	Same construction possible for probability tensors, but often more involved to instantiate them as tensor network.
%\end{example}



\subsect{Approximation for Mode queries}

By the squares risk trick, maximum coordinate searches involving contractions with boolean tensors can be turned into squares risk minimization problems.
This trick can be applied in MAP inference of \HybridLogicNetworks{} and the proposal distribution of grafting.

\subsubsect{Weighted Squares Loss Trick}

\begin{lemma}
	Let $\hypercore$ be a boolean tensor, that is $\imageof{\hypercore}\subset\{0,1\}$.
	Then
		\[ \hypercoreat{\shortcatvariables} = \onesat{\shortcatvariables} - \left( \hypercoreat{\shortcatvariables} - \onesat{\shortcatvariables} \right)^2  \]
	where $\ones$ is a tensor with same shape as $\hypercore$ and all coordinates being $1$.
\end{lemma}
\begin{proof}
	Since for each $\shortcatindices\in\facstates$ we have $\hypercore[\shortcatvariables=\shortcatindices]\in\{0,1\}$, it holds that
		\[ \hypercoreat{\shortcatvariables=\shortcatindices} = 1 - (\hypercoreat{\shortcatvariables=\shortcatindices}-1)^2 \]
	and thus in coordinatewise calculus
		\[ \hypercoreat{\shortcatvariables} = \onesat{\shortcatvariables} - \left( \hypercoreat{\shortcatvariables} - \onesat{\shortcatvariables} \right)^2 \, .   \]
\end{proof}

We apply this property to reformulate optimization problems over boolean tensors into weighted least squares problems.

\begin{theorem}[Weighted Squares Loss Trick]\label{the:reweightedLeastSquares}
	Let $\Gamma$ be a set of boolean tensors in $\facspace$ and $\importancetensor\in\facspace$ arbitrary.
	Then we have
	\begin{align}
		\argmax_{\hypercore\in\Gamma} \contraction{\importancetensor,\hypercore} 
		= \argmin_{\hypercore\in\Gamma} \contraction{\importancetensor, (\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2}
	\end{align} 
\end{theorem}
\begin{proof}
	Using the Lemma above, $\hypercore$ is identical to $\onesat{\shortcatvariables}-(\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2$ and we get
	\begin{align*}
		 \contraction{\importancetensor,\hypercore} 
		 &=  \contraction{\importancetensor,\onesat{\shortcatvariables}}-\contraction{\importancetensor,(\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2} 
	\end{align*}
	Since the first term does not depend on $\hypercore$, it can be dropped in the maximization problem.
	The $(-1)$ factor then turns the maximization into a minimization problem.
\end{proof}

% Interpretation and Importance Tensor
\theref{the:reweightedLeastSquares} reformulates maximation of binary tensors with respect to an angle to another tensor into minimization of a squares risk.
This squares risk trick is especially useful when combining it with a relaxation of $\Gamma$ to differentiably parametrizable sets, since then common squares risk solvers can be applied.
We will call $\importancetensor$ in the \theref{the:reweightedLeastSquares} importance tensor, since it manipulates the relevance of each coordinate in the squares loss.

%
As a result, we interpret the objective
	\[ \contraction{\importancetensor, (\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2} \]
as a weighted squares loss.

\begin{example}[Proposal distribution maxima]
	The Problem~\ref{prob:greedyGrad} of finding the maximal coordinate of the proposal distribution can thus be turned into
	\begin{align*}
		&\argmax_{\shortselindices} \contractionof{(\empdistribution-\currentdistribution),\fselectionmap}{\shortselvariables=\shortselindices}  \\
		&\quad = \argmin_{\shortselindices} \contraction{(\empdistribution-\currentdistribution),
		\left(\contractionof{\fselectionmap,\onehotmapofat{\shortselindices}{\shortselvariables}}{\shortcatvariables}-\onesat{\shortcatvariables}\right)^2} \, . 
	\end{align*}
\end{example}


\subsect{Problem of the trivial tensor}

By the above we motivated least squares problems on the set of one-hot encoded states.
One is tempted to extend this set to $\mnexpfamily$ for efficient solutions by alternating algorithms.

However, for any hypergraph $\graph$ we have $\onesat{\shortcatvariables}\in\mnexpfamily$.
In many situations (e.g. disjoint model sets supported at positive data) the objective is more in favor at the trivial tensor than at the one-hot encoding.
As a result, we do not solve the previously posed one-hot encoding problem, when allowing such an hypothesis embedding.


\begin{example}[Fitting a boolean tensor by a formula tensor]\label{exa:formulaFitting}
	Given a tensor $\hypercore$, we want to find a formula $\exformula\in\formulaset$ such that it approximates $\hypercore$.

	If $\hypercore$ is a binary tensor, we understand it as a formula and want to find an $\exformula$ such that its number of worlds is maximal, that is solve the problem
		\[ \argmax_{\exformula\in\formulaset}\contraction{\exformula\Leftrightarrow\hypercore}  \, . \]

	We can use the squares risk trick and get an equivalent problem
		\[ \argmin_{\exformula\in\formulaset} \|\contractionof{\exformula\Leftrightarrow\hypercore}{\shortcatvariables}  - \onesat{\shortcatvariables} \|^2 \, . \]

	We have since $\hypercore$ and $\exformula$ are boolean
	\begin{align*}
		\|\contractionof{\exformula\Leftrightarrow\hypercore}{\shortcatvariables}  - \onesat{\shortcatvariables} \|^2
		= 	\|\contractionof{\exformula}{\shortcatvariables}  - \hypercoreat{\shortcatvariables} \|^2
	\end{align*}

	Now, when representing $\formulaset$ in a formula selecting architecture we have
		\[ \argmin_{\canparam\in\Gamma_1} \|\contractionof{\fselectionmap,\canparam}{\shortcatvariables}  - \hypercoreat{\shortcatvariables}\|^2 \, . \]
	where $\Gamma_1$ is the set of basis tensors.

	When we extend $\Gamma_1$ to a set including the trivial tensor $\onesat{\selvariable}$, when the formulas $\exformula$ are pairwise disjoint and $\hypercorewith=\onesat{\shortcatvariables}$, then the solution would be $\onesat{\selvariable}$.

\end{example}

%\begin{remark}{Least Squares Loss by Tensor Fitting}
%	\red{Alternative approach to least squares problems: Tensor Fitting}
%	And, if the target is another formula $y$, such that $\exformula$ conincides with $\tilde{f} \iff y $ we have
%		\[ \left(\polynomialof{\exformula}(\datamap)-1\right)^2 = \left(  \polynomialof{\tilde{f}}(\datamap) - y(\datamap) \right)^2  \]
%	This is exactly the least squares loss, which would appear in a supervised interpretation of the learning.
%\end{remark}




\subsect{Alternating Solution of Least Squares Problems}

When the parameter tensor $\canparam$ is only restricted to have a decomposition as a tensor network on $\graph$, we can iteratively update each core.
The resulting algorithm is called Alternating Least Squares (ALS) (see Algorithm \ref{alg:ALS}).

\begin{algorithm}[hbt!]
\caption{Alternating Least Squares (ALS)}\label{alg:ALS}
\begin{algorithmic}
	\Require Target tensor $\targettensor[\shortcatvariables]$, hypergraph $\graph=(\nodes,\edges)$ with $[\catorder]\subset\nodes$ specifying the approximation format
	\Ensure Approximation of $\targettensor[\shortcatvariables]$ by a tensor network $\contractionof{\{\hypercoreofat{\edge}{\catvariableof{\edge}}\wcols\edge\in\edges\}}{\shortcatvariables}$
	\iosepline
\ForAll{$\edgein$}
	\State Set $\hypercoreofat{\edge}{\catvariableof{\edge}}$ to a random element in $\bigotimes_{\atomenumerator\in\edge}\rr^{\catdimof{\atomenumerator}}$
\EndFor
\While{Stopping criterion is not met}
\ForAll{$\edgein$}
	\State Set $\hypercoreofat{\edge}{\catvariableof{\edge}}$ to a solution of the local problem, that is
	\[ 
	\hypercoreofat{\edge}{\catvariableof{\edge}}
	 \algdefsymbol 
	 \argmin_{\hypercoreofat{\edge}{\catvariableof{\edge}}} 
	 \contraction{\importancetensor, (\contractionof{\fselectionmap,\canparam}{\shortcatvariables} - \targettensor[\shortcatvariables])^2}
	 \]
\EndFor
\EndWhile
	\State \Return $\{\hypercoreofat{\edge}{\catvariableof{\edge}}\wcols\edge\in\edges\}$
\end{algorithmic}
\end{algorithm}

%\subsect{Choice of Representation Format}

\red{The choice of the hypergraph $\graph$ used for approximation bears a tradeoff between expressivity and complexity in sampling.
Hidden variables, that is variables only present in $\graph$, but not in the sensing matrix, increase the expressivity, especially when assigning large dimensions to them.
When there are no hidden variables, the maximum of $\canparam$ can be found by maximum calibration through a message passing algorithm, since no hidden variable has to marginalized.}


%In case of skeleton expressions with many placeholders further decomposition for algorithmic efficiency are required.
%\begin{itemize}
%	\item Elementary Format ($\elformat$-Format):
%	\item $\cpformat$-Format: Closest to sum of formula tensors (when all vectors are basis, then have a sum).
%	\item $\ttformat$-Format: Showed better heuristic performance in optimization
%\end{itemize}

%For any tensor network decomposition into cores $\canparamof{\selenumerator}$ have the derivative $\frac{\partial}{\partial \canparamof{\selenumerator}} \canparam$ as the tensor network with out the core $\canparamof{\selenumerator}$.

%\begin{remark}[Parametercores being basis tensors]
%	When the parameter core is a basis tensor, the contraction with the parametercore coincides with the respective formula tensor.
%	Thus, we will search for basis tensors optimizing in contractions objectives to specific reasoning tasks, and add them iteratively to the network at hand.
%\end{remark}


%\sect{Projection onto Basis Tensors}
%\red{This is sampling!}
%We project onto basis tensors to achieve single formulas.


\subsect{Regularization and Compressed Sensing}


When regularizing the least squares problem by enforcing the sparsity of $\canparam$, we arrive at the compressed sensing problem
\begin{align}
	\argmin_{\canparamat{\selvariable}} \sparsityof{\canparam} 
	\stspace
	\left\| \contractionof{\sencsstat,\canparam}{\shortcatvariables} - \energytensorat{\shortcatvariables} \right\|_2 \leq \eta
\end{align}
Here, the sensing matrix is the selection tensor.


\begin{example}[Formula fitting to an example]
	Choosing the best formula fitting data (see Example~\ref{exa:formulaFitting}) is the problem
	\begin{align}
	\argmin_{\canparamat{\selvariable}\, : \,  \sparsityof{\canparam}=1} \left\| \contractionof{\importancetensor,\sencsstat,\canparam}{\shortcatvariables} - \targettensor \right\|_2 
	\end{align}
	where $\importancetensor$ has nonzero entries at marked coordinates and $\targettensor$ stores in Boolean coordinates whether the marked coordinates are positive or negative examples.
	\red{When the number of positive and negative examples are identical, we can linearly transform the objective to that of a grafting instance, where the current model is the empirical distribution of negative examples and the data consists of the positive examples.}
\end{example}

% Usage as sparse tensor
The sparse tensor solving the problem then has a small number of nonzero coordinates and the selection tensor can be restricted to those.
As a consequence, inference can be performed more efficiently.

% Algorithmic solution
The algorithmic solution of these problems can be done by greedy algorithms, thresholding based algorithms or optimization based algorithms \cite{foucart_mathematical_2013}.

% Guarantees
Guarantees for the success of the algorithms depend on the properties of the sensing matrices.
Here the sensing matrices are deterministic, since constructed as selection tensors, and concentration based approaches towards probabilistic bounds on these properties (see \cite{goesmann_uniform_2021}) are not applicable.

\begin{example}[Sensing matrix for propositional Formulas]
	Let there be a set $\formulaset$ of formulas, then we have
	\begin{align*}
		\contractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}},\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\outsymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}}
		= \contraction{\formulaof{\selindexof{\insymbol}}, \formulaof{\selindexof{\outsymbol}}} \, .
	\end{align*}
	If the formulas have disjoint model sets then 
	\begin{align*}
		\contractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}},\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}}
		= \begin{cases}
			\contraction{\formulaof{\insymbol}} & \text{if } \selindexof{\insymbol}=\selindexof{\outsymbol} \\
			0 & \text{else} 
		\end{cases} \, . 
	\end{align*}
	In that case, the sensing matrix is a restricted isometry, in the sense that the norm of any mapped vector is its norm mutliplied by a factor between the smallest and the largest $\contraction{\formulaof{\insymbol}}$.
\end{example}


\begin{example}[Sensing matrix for slice selection networks]

	For the slice selection network
	\begin{align*}
		& \contractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{\insymbol}},\sliceselectionmapat{\shortcatvariables,\selvariableof{\outsymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}} \\
		& \quad = \begin{cases}
			0 & \text{if for a }\seccatenumerator\in\variablesetof{\selindexof{\insymbol}}\cap\variablesetof{\selindexof{\outsymbol}}\text{ we have }\catindexof{\seccatenumerator}^{\selindexof{\insymbol}}\neq\catindexof{\seccatenumerator}^{\selindexof{\outsymbol}} \\
			\prod_{\seccatenumerator\notin\variablesetof{\selindexof{\insymbol}}\cup\variablesetof{\selindexof{\outsymbol}}} \catdimof{\seccatenumerator}& \text{else} 
		\end{cases} \, . 
	\end{align*}

	Given a fixed $\selindexof{\insymbol}$, the maximum value in the respective slice is thus taken at $\selindexof{\insymbol}=\selindexof{\outsymbol}$.
\end{example}


\sect{Discussion}

The selection network described here have been taylored to the $\cpformat$ format.
We can extend the selection network to parametrize more general tensor network formats than the $\cpformat$ format.
Here the graph structure of the format itself can be optimized, by the usage of variable selectors.
The activation selectors, generalizing the connective selection, are then choices of specific cores in the format.
Thus, each neuron represents an hypercore $\hypercoreofat{\edge}{\catvariableof{\edge}}$, where $\edge\subset\nodes$ is selected by variable selectors and the values of the tensor $\hypercoreof{\edge}$ by activation selectors.

