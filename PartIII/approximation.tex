\section{Reasoning by Tensor Approximation}\label{cha:tensorApproximation}

Often reasoning requires the execution of demanding contractions of tensors networks, or combinatorical search of maximum coordinates.
We in this chapter investigate methods, to replace hard to be sampled tensor networks by approximating tensor networks, which then serve as a proxy in inference tasks.


\subsection{Approximation of Energy tensors}

\subsubsection{Direct Approximation}

Direct approximation is the problem
	\[ \argmin_{\canparam\in\Gamma^{\graph}} \|\energytensorat{\shortcatvariables} - \canparamat{\shortcatvariables}\|^2 \, . \]


\subsubsection{Approximation involving Selection Architectures}

Approximation involving a selection architecture $\fselectionmap$ is the problem
	\[ \argmin_{\canparam\in\Gamma^{\graph}} \|\energytensor - \sbcontractionof{\sencodingof{\fselectionmap},\canparam}{\shortcatvariables}\|^2 \, . \]

In a tensor network diagram we depict this as
\begin{center}
    \input{PartIII/tikz_pics/approximation/least_squares.tex}
\end{center}


\begin{example}[Approximate based on a slice sparsity selecting architecture]
	Use a term selecting neural network (conjunction neuron on $\atomorder$ unary neurons selecting a variable and $\mathrm{Id},\lnot,\mathrm{True}$ as connective selector.
	Demand the parameter tensor $\canparam$ to be in a basis CP format, then each slice of the parameter tensor corresponds with the slice of the energy.
	The use the approximation for MAP search.
	Same construction possible for probability tensors, but often more involved to instantiate them as tensor network.
\end{example}



\subsection{Transformation of Maximum Search to Risk Minimization}

By the squares risk trick, maximum coordinate searches involving contractions with boolean tensors can be turned into squares risk minimization problems.
This trick can be applied in MAP inference of MLN and the proposal distribution.

\subsubsection{Weighted Squares Loss Trick}

\begin{lemma}
	Let $\hypercore$ be a Boolean tensor, that is $\imageof{\hypercore}\subset\{0,1\}$.
	Then
		\[ \hypercoreat{\shortcatvariables} = \onesat{\shortcatvariables} - \left( \hypercoreat{\shortcatvariables} - \onesat{\shortcatvariables} \right)^2  \]
	where $\ones$ is a tensor with same shape as $\hypercore$ and all coordinates being $1$.
\end{lemma}
\begin{proof}
	Since for each $\shortcatindices\in\facstates$ we have $\hypercore[\shortcatvariables=\shortcatindices]\in\{0,1\}$, it holds that
		\[ \hypercoreat{\shortcatvariables=\shortcatindices} = 1 - (\hypercoreat{\shortcatvariables=\shortcatindices}-1)^2 \]
	and thus in coordinatewise calculus
		\[ \hypercoreat{\shortcatvariables} = \onesat{\shortcatvariables} - \left( \hypercoreat{\shortcatvariables} - \onesat{\shortcatvariables} \right)^2 \, .   \]
\end{proof}

We apply this property to reformulate optimization problems over boolean tensors into weighted least squares problems.

\begin{theorem}[Weighted Squares Loss Trick]\label{the:reweightedLeastSquares}
	Let $\Gamma$ be a set of boolean tensors in $\facspace$ and $\importancetensor\in\facspace$ arbitrary.
	Then we have
	\begin{align}
		\argmax_{\hypercore\in\Gamma} \contraction{\importancetensor,\hypercore} 
		= \argmin_{\hypercore\in\Gamma} \contraction{\importancetensor, (\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2}
	\end{align} 
\end{theorem}
\begin{proof}
	Using the Lemma above, $\hypercore$ is identical to $\onesat{\shortcatvariables}-(\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2$ and we get
	\begin{align*}
		 \contraction{\importancetensor,\hypercore} 
		 &=  \contraction{\importancetensor,\onesat{\shortcatvariables}}-\contraction{\importancetensor,(\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2} 
	\end{align*}
	Since the first term does not depend on $\hypercore$, it can be dropped in the maximization problem.
	The $(-1)$ factor then turns the maximization into a minimization problem.
\end{proof}

% Interpretation and Importance Tensor
\theref{the:reweightedLeastSquares} reformulates maximation of binary tensors with respect to an angle to another tensor into minimization of a squares risk.
This squares risk trick is especially useful when combining it with a relaxation of $\Gamma$ to differentiably parametrizable sets, since then common squares risk solvers can be applied.
We will call $\importancetensor$ in the \theref{the:reweightedLeastSquares} importance tensor, since it manipulates the relevance of each coordinate in the squares loss.

%
As a result, we interpret the objective
	\[ \contraction{\importancetensor, (\hypercoreat{\shortcatvariables}-\onesat{\shortcatvariables})^2} \]
as a weighted squares loss.

\begin{example}[Proposal distribution maxima]
	The Problem~\ref{prob:steepestAscent} of finding the maximal coordinate can thus be turned into
	\begin{align*}
		\argmax_{\shortselindices} \contractionof{(\empdistribution-\currentdistribution),\fselectionmap}{\shortselvariables=\shortselindices}  
		= \argmin_{\shortselindices} \sbcontraction{(\empdistribution-\currentdistribution),
		\left(\contractionof{\fselectionmap,\onehotmapofat{\shortselindices}{\shortselvariables}}{\shortcatvariables}-\onesat{\shortcatvariables}\right)^2} \, . 
	\end{align*}
\end{example}


\subsubsection{Problem of the trivial tensor}

By the above we motivated least squares problems on the set of one-hot encoded states.
One is tempted to extend this set to $\mnexpfamily$ for efficient solutions by alternating algorithms.

However, for any hypergraph $\graph$ we have $\onesat{\shortcatvariables}\in\mnexpfamily$.
In many situations (e.g. disjoint model sets supported at positive data) the objective is more in favor at the trivial tensor than at the one-hot encoding.
As a result, we do not solve the previously posed one-hot encoding problem, when allowing such an hypothesis embedding.


\begin{example}[Fitting a tensor by a formula tensor]\label{exa:formulaFitting}
	Task: Given a tensor $\hypercore$, find a formula $\exformula\in\formulaset$ such that it coincides with $\hypercore$.

	If $\hypercore$ is a binary tensor, we understand it as a formula and want to find an $\exformula$ such that its number of worlds is maximal, that is solve the problem
		\[ \argmax_{\exformula\in\formulaset}\sbcontraction{\exformula\Leftrightarrow\hypercore}  \, . \]

	We can use the squares risk trick and get an equivalent problem
		\[ \argmin_{\exformula\in\formulaset} \|Â \sbcontractionof{\exformula\Leftrightarrow\hypercore}{\shortcatvariables}  - \onesat{\shortcatvariables} \|^2 \, . \]
\end{example}

%\begin{remark}{Least Squares Loss by Tensor Fitting}
%	\red{Alternative approach to least squares problems: Tensor Fitting}
%	And, if the target is another formula $y$, such that $\exformula$ conincides with $\tilde{f} \iff y $ we have
%		\[ \left(\polynomialof{\exformula}(\datamap)-1\right)^2 = \left(  \polynomialof{\tilde{f}}(\datamap) - y(\datamap) \right)^2  \]
%	This is exactly the least squares loss, which would appear in a supervised interpretation of the learning.
%\end{remark}




\subsection{Alternating Solution of Least Squares Problems}

When the parameter tensor $\canparam$ is only restricted to have a decomposition as a tensor network on $\graph$, we can iteratively update each core.
The resulting algorithm is called Alternating Least Squares (ALS) (see Algorithm \ref{alg:ALS}).

\begin{algorithm}[hbt!]
\caption{Alternating Least Squares (ALS)}\label{alg:ALS}
\begin{algorithmic}
\For{$\edgein$}
	\State Set $\hypercoreofat{\edge}{\catvariableof{\edge}}$ to a random element in $\bigotimes_{\atomenumerator\in\edge}\rr^{\catdimof{\atomenumerator}}$
\EndFor
%\For{$\atomenumeratorin$}
%	\State Set $\varcore{\atomenumerator}$ to a random element in $\rr^{\atomlegdimof{\atomenumerator}}$ 
%\EndFor
\While{Stopping criterion is not met}
\For{$\edgein$}
	\State Set $\hypercoreofat{\edge}{\catvariableof{\edge}}$ to a to a solution of the local problem, that is
	\[ 
	\hypercoreofat{\edge}{\catvariableof{\edge}}
	 \algdefsymbol 
	 \argmin_{\hypercoreofat{\edge}{\catvariableof{\edge}}} 
	 \contraction{\importancetensor, (\contractionof{\fselectionmap,\canparam}{\shortcatvariables} - \targettensor[\shortcatvariables])^2}
	 \]
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}


\subsubsection{Choice of Representation Format}

\red{The choice of the hypergraph $\graph$ used for approximation bears a tradeoff between expressivity and complexity in sampling.
Hidden variables, that is variables only present in $\graph$, but not in the sensing matrix, increase the expressivity, especially when assigning large dimensions to them.
When there are no hidden variables, the maximum of $\canparam$ can be found by maximum calibration through a message passing algorithm, since no hidden variable has to marginalized.}


In case of skeleton expressions with many placeholders further decomposition for algorithmic efficiency are required.
\begin{itemize}
	\item Elementary Format ($\elformat$-Format): 
	\item $\cpformat$-Format: Closest to sum of formula tensors (when all vectors are basis, then have a sum).
	\item $\ttformat$-Format: Showed better heuristic performance in optimization
\end{itemize}

For any tensor network decomposition into cores $\canparamof{\selenumerator}$ have the derivative $\frac{\partial}{\partial \canparamof{\selenumerator}} \canparam$ as the tensor network with out the core $\canparamof{\selenumerator}$.

%\begin{remark}[Parametercores being basis tensors]
%	When the parameter core is a basis tensor, the contraction with the parametercore coincides with the respective formula tensor.
%	Thus, we will search for basis tensors optimizing in contractions objectives to specific reasoning tasks, and add them iteratively to the network at hand.
%\end{remark}


%\subsection{Projection onto Basis Tensors}
%\red{This is sampling!}
%We project onto basis tensors to achieve single formulas.


\subsection{Regularization and Compressed Sensing}


When regularizing the least squares problem by enforcing the sparsity of $\canparam$, we arrive at the compressed sensing problem
\begin{align}
	\argmin_{\canparamat{\selvariable}} \sparsityof{\canparam} 
	\quad \text{subject to } \quad 
	\left\| \contractionof{\sencsstat,\canparam}{\shortcatvariables} - \energytensorat{\shortcatvariables} \right\|_2 \leq \eta
\end{align}
Here, the sensing matrix is the selection tensor.


\begin{example}[Formula fitting to an example]
	Choosing the best formula fitting data (see Example~\ref{exa:formulaFitting}) is the problem
	\begin{align}
	\argmin_{\canparamat{\selvariable}\, : \,  \sparsityof{\canparam}=1} \left\| \contractionof{\importancetensor,\sencsstat,\canparam}{\shortcatvariables} - \targettensor \right\|_2 
	\end{align}
	where $\importancetensor$ has nonzero entries at marked coordinates and $\targettensor$ stores in Boolean coordinates whether the marked coordinates are positive or negative examples.
	\red{When the number of positive and negative examples are identical, we can linearly transform the objective to that of a grafting instance, where the current model is the empirical distribution of negative examples and the data consists of the positive examples.}
\end{example}

% Usage as sparse tensor
The sparse tensor solving the problem then has a small number of nonzero coordinates and the selection tensor can be restricted to those.
As a consequence, inference can be performed more efficiently.

% Algorithmic solution
The algorithmic solution of these problems can be done by greedy algorithms, thresholding based algorithms or optimization based algorithms \cite{foucart_mathematical_2013}.

% Guarantees
Guarantees for the success of the algorithms depend on the properties of the sensing matrices.
Here the sensing matrices are deterministic, since constructed as selection tensors, and concentration based approaches towards probabilistic bounds on these properties (see \cite{goesmann_uniform_2021}) are not applicable.





\begin{example}[Propositional Formulas]
	Let there be a set $\formulaset$ of formulas, then we have
	\begin{align*}
		\sbcontractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}},\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\outsymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}}
		= \sbcontraction{\formulaof{\selindexof{\insymbol}}, \formulaof{\selindexof{\outsymbol}}} \, . 
	\end{align*}
	If the formulas have disjoint model sets then 
	\begin{align*}
		\sbcontractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}},\sencodingofat{\formulaset}{\shortcatvariables,\selvariableof{\insymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}}
		= \begin{cases}
			\sbcontraction{\formulaof{\insymbol}} & \text{if } \selindexof{\insymbol}=\selindexof{\outsymbol} \\
			0 & \text{else} 
		\end{cases} \, . 
	\end{align*}
\end{example}


\begin{example}[Slice selection networks]
	
	For the slice selection network
	\begin{align*}
		\sbcontractionof{\sliceselectionmapat{\shortcatvariables,\selvariableof{\insymbol}},\sliceselectionmapat{\shortcatvariables,\selvariableof{\outsymbol}}}{\indexedselvariableof{\insymbol},\indexedselvariableof{\outsymbol}}
		= \begin{cases}
			0 & \text{if for a }\seccatenumerator\in\variablesetof{\selindexof{\insymbol}}\cap\variablesetof{\selindexof{\outsymbol}}\text{ we have }\catindexof{\seccatenumerator}^{\selindexof{\insymbol}}\neq\catindexof{\seccatenumerator}^{\selindexof{\outsymbol}} \\
			\prod_{\seccatenumerator\notin\variablesetof{\selindexof{\insymbol}}\cup\variablesetof{\selindexof{\outsymbol}}} \catdimof{\seccatenumerator}& \text{else} 
		\end{cases} \, . 
	\end{align*}

	Given a fixed $\selindexof{\insymbol}$, the maximum value in the respective slice is thus taken at $\selindexof{\insymbol}=\selindexof{\outsymbol}$
\end{example}







