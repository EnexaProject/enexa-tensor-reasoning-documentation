\chapter{\chatextbasisCalculus}\label{cha:basisCalculus}

\red{
Basis Calculus stores informations in the selection of basis elements, while coordinate calculus uses the coordinates to each index for storage.
While coordinate calculus is more expressive, basis calculus can be exploited in sparse representations of composed functions.
}

\sect{Classification of tensors}

\red{
    We in this chapter investigate the properties of tensors, which where arising in probabilistic and logical reasoning.
    We observed already before, that
}
\begin{itemize}
    \item Conditional probability tensors are directed tensors.
    \item Logical formulas are boolean tensors.
\end{itemize}

\red{
    Thus, the set of tensors, which are both directed and boolean is of much interest.
    We will show in this chapter, that they are equal to the set of relational encodings of functions.
}

\begin{figure}[h]
	\begin{center}
		\input{PartIII/tikz_pics/basis_calculus/directed_binary_sketch.tex}
	\end{center}
	\caption{Sketch of the tensors with non-negative coordinates.
	We investigate in this chapter tensors, which are directed and boolean.}
\end{figure}

\sect{Encoding of Subsets and Relations}

Based on the concept of one-hot encodings of states we in this chapter develop the construction of encodings to sets, relations and functions.
We start with the definition of subset encodings, which represent set memberships in their boolean coordinates.

\begin{definition}[Subset Encoding]\label{def:subsetEncoding}
	We say that an arbitrary set $\arbset$ is enumerated by an enumeration variable $\indvariableof{\arbset}$ taking values in $[\inddimof{\arbset}]$, when $\inddimof{\arbset}=\absof{\arbset}$ and there is a bijective function
	\begin{align*}
		\indexinterpretation : [\inddimof{\arbset}] \rightarrow \arbset \, .
	\end{align*}
	Given an set $\arbset$ enumerated by the variable $\indvariableof{\arbset}$, any subset $\arbsubset\subset\arbset$ is encoded by the tensor $\onehotmapto{\arbsubset}[\indvariable]$ defined for $\indindex\in[\absof{\arbset}]$ as
	\begin{align*}
	 	\onehotmapofat{\arbsubset}{\indexedindvariable}
		= \begin{cases}
		1 & \text{if} \indexinterpretationat{\indindex} \in \arbsubset \\
		0 & \text{else}
		\end{cases} \, . 
	\end{align*}
\end{definition}

% Decomposition
In a one-hot basis decomposition we have
\begin{align*}
	\onehotmapofat{\arbsubset}{\indvariable}
	\coloneqq \sum_{\indindex\in[\cardof{\arbset}]\,:\,\indexinterpretationat{\indindex}\in\arbsubset}\onehotmapofat{\indindex}{\indvariable} \, .
\end{align*}

% Explanation
%Encoding of subsets as vectors: Each coordinate associated with a possible element, $\{0,1\}$ encoding whether in subset.
%The encodings is thus a boolean tensor.
%Any subset encoding is a boolean tensor.

% Relation
Since relations are subsets of cartesian products between two sets, their encoding is a straightforward generalization of \defref{def:subsetEncoding}.

\begin{definition}[Relation Encoding]
	A relation between two finite sets $\inset$ and $\outset$ is a subset of their cartesian product
	\begin{align*}
		 \exrelation \subset \inset \times \outset \, .
	\end{align*}
	Given an enumeration of $\inset$ and $\outset$ by the categorical variables $\indvariableof{\insymbol}$ and $\indvariableof{\outsymbol}$ and interpretation maps $\indexinterpretationof{\insymbol}$, $\indexinterpretationof{\outsymbol}$, we define the encoding of this subset as the tensor $\onehotmapto{\exrelation}[\indvariableof{\insymbol},\indvariableof{\outsymbol}]$ with the coordinates
	\begin{align*}
		\onehotmapofat{\exrelation}{\indexedindvariableof{\insymbol},\indexedindvariableof{\outsymbol}}
		= \begin{cases}
		1 & \text{if } (\indexinterpretationofat{\insymbol}{\indindexof{\insymbol}},\indexinterpretationofat{\outsymbol}{\indindexof{\outsymbol}}) \in \exrelation \\
		0 & \text{else}
		\end{cases} \, . 
	\end{align*}
\end{definition}

% Decomposition
The relation encoding has a decomposition into one-hot encodings as
\begin{align*}
	\onehotmapofat{\exrelation}{\indvariableof{\insymbol},\indvariableof{\outsymbol}}
	= \sum_{\indindexof{\insymbol},\indindexof{\outsymbol} \, : \, (\indexinterpretationofat{\insymbol}{\indindexof{\insymbol}},\indexinterpretationofat{\outsymbol}{\indindexof{\outsymbol}}) \in \exrelation}
	\onehotmapofat{\indindexof{\insymbol}}{\indvariableof{\insymbol}}  \otimes \onehotmapofat{\indindexof{\outsymbol}}{\indvariableof{\outsymbol}}  \, .
\end{align*}

Relational encodings have a matrix structure by the cartesian product, which can be further folded to tensors, when the sets itself are cartesian products.
The relational encoding is a bijection between the relations of two sets and the boolean tensors with their enumeration variables.

%They provide representations of generic relations by boolean tensors, in the sense that each relation between two sets is represented
%\begin{theorem}
%	The relational encoding is a bijection between the set of relations and the set of boolean tensors.
%\end{theorem}
%\begin{proof}
%	% =>
%	By definition, a relational encoding is the encoding of a subset and thus a boolean tensor.
%	% <=
%	Any matrification of a boolean tensor marks by its $1$ coordinates the elements of a relation.
%\end{proof}
%
%% Significance
%We can thus understand any matrification of a boolean tensor as the encoding of a relation and vice versa.



\subsect{Higher order relations}

We can extend this contraction to relations of higher order, and arrive at encoding schemes usable for relational databases.

\begin{definition}\label{def:daryRelation}
	Given sets $\arbsetof{\atomenumerator}$ for $\atomenumeratorin$, a $\atomorder$-ary relation is a subset of a their cartesian product, that is
	\begin{align*}
		\exrelation \subset\bigtimes_{\atomenumeratorin} \arbsetof{\atomenumerator} \, .
	\end{align*}
	Given an enumeration of each set $\arbsetof{\atomenumerator}$ by a variable $\indvariableof{\atomenumerator}$ and an interpretation map $\indexinterpretationof{\atomenumerator}$, we define the encoding of the relation as the tensor $\onehotmapto{\exrelation}[\indvariableof{[\atomorder]}]$ with coordinates
	\begin{align*}
		\onehotmapofat{\exrelation}{\indexedindvariableof{[\catorder]}}
		= \begin{cases}
		1 & \text{if} \quad (\indexinterpretationofat{0}{\indindexof{0}},\ldots,\indexinterpretationofat{\atomorder-1}{\indindexof{\atomorder-1}}) \in \exrelation \\
		0 & \text{else}
		\end{cases} \, .
	\end{align*}
\end{definition}

\begin{example}[Propositional Formulas]
	Let there be for $\atomenumeratorin$ sets $\arbsetof{\atomenumerator}$ of truth assignments to the $\atomenumerator$-th atom, which are all enumerated by $[2]$.
	A propositional formula then corresponds with a $\atomorder$-ary relation and we directly defined them in \defref{def:formulas} by their relational encoding.
\end{example}

\begin{example}[Relational Databases]
	Relational Databases can be encoded as tensors using the relation encoding scheme.
	Each column is thereby understood as an eunumeration variable, which values form the sets $\arbsetof{\catenumerator}$.
\end{example}

% Sparse Representations
Let us notice, that the dimensionality of the tensor space used for representing a relation is
\begin{align*}
	\prod_{\catenumeratorin} \cardof{\arbsetof{\catenumerator}}
\end{align*}
and therefore growing exponentially with the number of variables.
Relations are however often sparse, in the sense that
\begin{align*}
	 \cardof{\exrelation} << \prod_{\catenumeratorin} \cardof{\arbsetof{\catenumerator}} \, .
\end{align*}
It is therefore often benefitially to choose sparse encoding schemes, for example by restricted CP formats (see \charef{cha:sparseCalculus}) to represent $\onehotmapof{\exrelation}$.


\sect{Encoding of Functions}

Let us now restrict to relations, which have an expression by functions.
We in this section then show, how contractions of their encodings can be exploited in function evaluation.

\subsect{Relational Encoding of Functions}

%We now generalize the representation scheme towards maps between arbitrary unstructured sets.

\begin{definition}[Relational Encoding of Maps]\label{def:functionRelationEncoding}
	Any map
	\begin{align*}
		\exfunction : \inset \rightarrow \outset
	\end{align*}
	can be represented by a relation
	\begin{align*}
		\exrelationof{\exfunction} \coloneqq \left\{ (x,\exfunction(x) \, : \, x \in\inset )\right\} \subset \inset \times \outset \, .
	\end{align*}
	Given a enumeration of the sets by $\indvariableof{\insymbol}$ and $\indvariableof{\outsymbol}$ we define the relational encoding of $\exfunction$ as the tensor
	\begin{align*}
		\rencodingofat{\exfunction}{\indvariableof{\insymbol},\indvariableof{\outsymbol}}
		= \onehotmapofat{\exrelationof{\exfunction}}{\indvariableof{\insymbol},\indvariableof{\outsymbol}}  \, .
	\end{align*}
\end{definition}

\begin{remark}[Reduction to images]
	% Image enumeration
	When $\exfunction$ maps into a set of infinite cardinality, we restrict $\outset$ to the image of $\exfunction$ and enumerate the image by a variable $\indvariableof{\exfunction}$.
	This scheme is applied, when $\exfunction$ is itself a tensor, i.e. $\outset=\rr$.
	While the variable $\indvariableof{\exfunction}$ can in general be of the same cardinality as the domain set $\inset$, it will be valued in $[2]$ when considering boolean tensors.
\end{remark}

% Characterization of the directed and boolean tensors
We notice, that any relational representation of a function is also a directed tensor with incoming variables to the domain and outgoing variables to the image.
It furthermore holds, that the set of directed and boolean tensors is characterized by the relational encoding of functions.
This is shown in the next theorem, by the claim that any boolean tensor which is directed is the relational representation of a function.

\begin{theorem}\label{the:rencodingDirected}
	Let $\inset,\outset$ be sets and $\exrelation\subset\inset\times\outset$ a relation.
	If and only if there exists a map $\exfunction:\inset\rightarrow\outset$ such that $\exrelation=\exrelationof{\exfunction}$, the relational encoding $\rencodingof{\exfunction}$ is a directed tensor with $\indvariableof{\insymbol}$ incoming and $\indvariableof{\outsymbol}$ outgoing.
\end{theorem}
\begin{proof}
	\proofrightsymbol:
	When $\exfunction$ is a function, we have for any $\indindexofin{\insymbol}$
	\begin{align*}
		\sum_{\indindexofin{\outsymbol}} \rencodingofat{\exfunction}{\indexedindvariableof{\insymbol},\indexedindvariableof{\outsymbol}}
		=  \rencodingofat{\exfunction}{\indexedindvariableof{\insymbol},\indvariableof{\outsymbol}=\invindexinterpretationofat{\outsymbol}{\exfunctionat{\indexinterpretationofat{\insymbol}{\indindexof{\insymbol}}}}}
		= 1 \, .
	\end{align*}
	Thus, $\rencodingofat{\exfunction}{\indvariableof{\outsymbol},\indvariableof{\insymbol}}$ is a directed tensor with variables $\indvariableof{\insymbol}$ incoming and $\indvariableof{\outsymbol}$ outgoing.

	\proofleftsymbol:
	Conversely let there be a relation $\exrelation$, such that $\rencodingof{\exrelation}$ is directed.
	To this end, we observe that for any $\indindexofin{\insymbol}$ the tensor
	\begin{align*}
		\onehotmapofat{\exrelation}{\indexedindvariableof{\insymbol},\indvariableof{\outsymbol}}
	\end{align*}
	is a boolean tensor with coordinate sum one and therefore a basis vector.
	It follows that the function $\exfunction : \inset \rightarrow \outset $ defined for $x\in\inset$ as
	\begin{align*}
		\exfunctionat{x}
		= \indexinterpretationofat{\outsymbol}{\invonehotmapof{\onehotmapofat{\exrelation}{\indvariableof{\insymbol}=\indexinterpretationofat{\insymbol}{x},\indvariableof{\outsymbol}}}}
	\end{align*}
	is well-defined.
	We then have by construction
	\begin{align*}
		\rencodingofat{\exfunction}{\indvariableof{\outsymbol},\indvariableof{\insymbol}}
		& = \sum_{\indindexofin{\insymbol}}
		\onehotmapofat{\exfunction(\indindexof{\insymbol})}{\indvariableof{\outsymbol}} \otimes
		\onehotmapofat{\indindexof{\insymbol}}{\indvariableof{\insymbol}} \\
		& =  \sum_{\indindexofin{\insymbol}} \onehotmapofat{\exrelation}{\indexedindvariableof{\insymbol},\indvariableof{\outsymbol}} \otimes
		\onehotmapofat{\indindexof{\insymbol}}{\indvariableof{\insymbol}} \\
		& = \onehotmapofat{\exrelation}{\indvariableof{\outsymbol},\indvariableof{\insymbol}}
	\end{align*}
	and therefore by \defref{def:functionRelationEncoding} $\exrelation=\exrelationof{\exfunction}$.
\end{proof}

% Grid sets
We are specially interested in sets of states of a factored system, which amounts to the case in \defref{def:functionRepresentation}.
Those state sets have a decomposition into a cartesian product of $\atomorder$ sets
	\[ \arbset = \facstates \, . \]
The most obvious enumeration of the set $\arbset$ is therefore by the collection of state variables $\{\catvariableof{\atomenumerator}Â \, : \, \atomenumeratorin \}$.
Functions between states of factored systems with $\atomorder_{\insymbol}$ and $\atomorder_{\outsymbol}$ state variables can be represented by $\atomorder_{\insymbol}+\atomorder_{\outsymbol}$-ary relations and \defref{def:functionRelationEncoding} has an obvious generalization to this case with multiple enumeration variables.

%% NOT NEEDED -> Done in propositional logics
%% Conditional
%Since the relational encoding of any map between factored systems is directed, it can be interpreted by a conditional probability tensor, as we state next.
%
%%% Maps
%\begin{corollary}%\label{the:condProbFunctionRepresentation}
%	The relational encoding $\rencodingof{\exfunction}$ (see \defref{def:functionRepresentation}) of a function $\exfunction$ between factored systems is a conditional probability tensor, where the legs to the image system are the conditions and the legs to the target system the distribution legs.
%\end{corollary}
%
%%% Deterministic by construction
%These are deterministic conditional probability tensors, in the sense that any slice with respect to the input variables is a basis tensor.
%Through contractions with distribution tensors (e.g. distributions in domain systems) they get stochastic.
%This is for example the case in the empirical distribution, which can be understood as the forwarding of the uniform distribution on the sample enumeration.

\subsect{Function Evaluation}

We now justify the nomenclature of basis calculus, by showing that contraction with basis elements produce the one-hot encoded function evaluation.

\begin{theorem}[Basis Calculus]\label{the:basisCalculus}
	Retrieving the value of the function $\exfunction$ at a specific state is then the contraction of the tensor representation with the one-hot encoded state.
	For any $\arbelement\in\inset$ we have
	\begin{align*}
		\onehotmapofat{\invindexinterpretationofat{\outsymbol}{\exfunctionat{\arbelement}}}{\indvariableof{\outsymbol}}
		= \contractionof{
			\rencodingofat{\exformula}{\indvariableof{\outsymbol},\indvariableof{\insymbol}},
			\onehotmapofat{\indexinterpretationofat{\insymbol}{\arbelement}}{\indvariableof{\insymbol}}
		}{\indvariableof{\outsymbol}} \, .
	\end{align*}
	Thus, we can retrieve the function evaluation by the inverse one-hot mapping as
	\begin{align*}
		\exfunctionat{\arbelement} = \invonehotmapof{\contractionof{
			\rencodingofat{\exformula}{\indvariableof{\outsymbol},\indvariableof{\insymbol}},
			\onehotmapofat{\indexinterpretationofat{\insymbol}{\arbelement}}{\indvariableof{\insymbol}}
		}{\indvariableof{\outsymbol}}} \, .
\end{align*}
\end{theorem}
\begin{proof}
	From the representation
	\begin{align*}
		\rencodingofat{\exfunction}{\indvariableof{\outsymbol},\indvariableof{\insymbol}}
		& =  \sum_{\indindexofin{\insymbol}}
			\onehotmapofat{(\invindexinterpretationof{\outsymbol} \circ \exfunction \circ \indexinterpretationof{\insymbol}) \indindexof{\insymbol}
				}{\indvariableof{\insymbol}}
			\otimes
			\onehotmapofat{\indindexof{\insymbol}}{\indvariableof{\insymbol}}
	\end{align*}
	and the orthonormality of the one-hot encodings of the input enumeration we get
	\begin{align*}
		 \contractionof{
			\rencodingofat{\exformula}{\indvariableof{\outsymbol},\indvariableof{\insymbol}},
			\onehotmapofat{\indexinterpretationofat{\insymbol}{\arbelement}}{\indvariableof{\insymbol}}
		}{\indvariableof{\outsymbol}}
		= \onehotmapofat{\invindexinterpretationofat{\outsymbol}{\exfunctionat{\arbelement}}}{\indvariableof{\outsymbol}} \, .
	\end{align*}
\end{proof}

%% Usage: Basis Calculus
We can thus use tensor contractions to calculate the values of functions.
Since basis vectors being the one-hot encoding of the domain system are mapped to basis vectors being the encoding of the image system, we call these contraction basis calculus.



\sect{Calculus of relational encodings}

We now show the utility of relational encodings for functions, by developing tensor network representation to composed functions.
\red{We in this section use the notation of factored system representation, as developed in \parref{par:one} and enumerate states of factored systems by variables $\catvariable$ with states in $[\catdim]$, instead of combinations of variables $\indvariable$ with index interpretation functions $\indexinterpretation$ enumerating arbitrary sets.}

\subsect{Composition of function}

We have already used (see \theref{the:formulaDecomposition}), that combination of propositional formulas by connectives can be represented by contractions.
We now show in a more general perspective, that in basis calculus, any composition of functions in its relational encoding the contraction of the encoded functions.

\begin{theorem}[Composition of Functions]\label{the:compositionByContraction}
	Let there be two maps between factored systems
	\begin{align*}
		\exfunction : \nodestatesof{\nodesone} \rightarrow \nodestatesof{\nodestwo}
	\end{align*}
	and
	\begin{align*}
		\secexfunction : \nodestatesof{\nodestwo} \rightarrow \nodestatesof{\nodesthree}
	\end{align*}
	with the image system of $\exfunction$ is the domain system of $\secexfunction$.
	Then the relational encoding of the composition
	\begin{align*}
		\compositionof{\secexfunction}{\exfunction} : \nodestatesof{\nodesone} \rightarrow \nodestatesof{\nodesthree}
	\end{align*}
	is the contraction
	\begin{align*}
		\rencodingofat{\compositionof{\secexfunction}{\exfunction}}{\catvariableof{\nodesthree},\catvariableof{\nodesone}}
		= \contractionof{
			\rencodingofat{\secexfunction}{\catvariableof{\nodesthree},\catvariableof{\nodestwo}},
			\rencodingofat{\exfunction}{\catvariableof{\nodestwo},\catvariableof{\nodesone}},
		}{\catvariableof{\nodesthree},\catvariableof{\nodesone}} \, .
	\end{align*}
\end{theorem}
\begin{proof}
	By definition we have the relational encoding of the composition as
	\begin{align*}
		\rencodingofat{\compositionof{\secexfunction}{\exfunction}}{\catvariableof{\nodesthree},\catvariableof{\nodesone}}
		= \sum_{\catindexof{\nodesone}\in\nodestatesof{\nodesone}}
		\onehotmapofat{\compositionofat{\secexfunction}{\exfunction}{\catindexof{\nodesone}}}{\catvariableof{\nodesthree}} \otimes
		\onehotmapofat{\catindexof{\nodesone}}{\catvariableof{\nodesone}}  \, .
	\end{align*}
	By using a similar representation for $\rencodingof{\secexfunction}$ and $\rencodingof{\exfunction}$ we now show, that this coincides with the contraction of these relational encodings with closed variables $\catvariableof{\nodestwo}$.
	By the linearity of the contraction operation we get
	\begin{align*}
		\contractionof{\rencodingof{\exfunction},\rencodingof{\secexfunction}}{\catvariableof{\nodesthree},\catvariableof{\nodesone}}
		& = \sum_{\catindexof{\nodesone}\in\bigtimes_{\node\in\nodesone}[\catdimof{\node}]}
			\sum_{\catindexof{\nodestwo} \in \bigtimes_{\node\in\nodestwo}[\catdimof{\node}]}
			\breakablecontractionof{
				\left( \onehotmapofat{\secexfunctionat{\catindexof{\nodestwo}}}{\catvariableof{\nodesthree}} \otimes
				\onehotmapofat{\catindexof{\nodestwo}}{\catvariableof{\nodestwo}} \right), \\
				& \hspace{4.5cm} \left( \onehotmapofat{\exfunctionat{\catindexof{\nodesone}}}{\catvariableof{\nodestwo}} \otimes
				\onehotmapofat{\catindexof{\nodesone}}{\catvariableof{\nodesone}} \right)
			}{\catvariableof{\nodesthree},\catvariableof{\nodesone}} \\
		& = \sum_{\catindexof{\nodesone}\in\bigtimes_{\node\in\nodesone}[\catdimof{\node}]}
			\delta_{\catindexof{\nodestwo},\catindexof{\nodesone}} \, \cdot \,
			\onehotmapofat{\secexfunctionat{\catindexof{\nodestwo}}}{\catvariableof{\nodesthree}} \otimes
			\onehotmapofat{\catindexof{\nodesone}}{\catvariableof{\nodesone}} \\
		& = \sum_{\catindexof{\nodesone}\in\nodestatesof{\nodesone}}
		\onehotmapofat{\compositionofat{\secexfunction}{\exfunction}{\catindexof{\nodesone}}}{\catvariableof{\nodesthree}} \otimes
		\onehotmapofat{\catindexof{\nodesone}}{\catvariableof{\nodesone}} \\
		& = \rencodingofat{\compositionof{\secexfunction}{\exfunction}}{\catvariableof{\nodesthree},\catvariableof{\nodesone}} \, ,
	\end{align*}
	where we exploited the orthonormality of the one-hot encodings to the states of $\catvariableof{\nodestwo}$, which contraction thus results in the delta symbol $\delta$ applied on the respective states.
\end{proof}

% Iterative usage
We can use \theref{the:compositionByContraction} iteratively to further decompose the function $\secexfunction$.
In this way, the relational encoding of a function consistent of multiple compositions can be represented as the contractions of all the functions.
This has been applied in \theref{the:formulaDecomposition} to efficiently represent propositional formulas, for which syntactical expressions are given.

\subsect{Compositions with real functions}

We here investigate how the composition of a tensor
\begin{align*}
	\hypercore : \facstates \rightarrow \rr
\end{align*}
with arbitrary functions
\begin{align*}
	\chainingfunction: \rr \rightarrow \rr
\end{align*}
can be represented.
This is for example relevant, when representing coordinatewise tensor transforms (see \secref{sec:coordinatewiseTransforms}) based on tensor network contractions.
% Strategy
%Our main strategy is in understanding the tensor $\hypercore$ as a map to its finite image, seen as the enumerated states of a categorical variable building a factored system.
To this end we understand the tensor $\hypercoreat{\shortcatvariables}$ as a map of the states $\facstates$ onto its by a variable $\indvariableof{\hypercore}$ and index interpretation $\indexinterpretation$ enumerated image $\imageof{\hypercore}$.
We then define the restriction of $\chainingfunction$ onto $\imageof{\hypercore}$ as the tensor $\restrictionofto{\chainingfunction}{\imageof{\hypercore}}\left[\indvariableof{\hypercore}\right]$ with coordinates $\indindexof{\hypercore}$
\begin{align*}
	\restrictionofto{\chainingfunction}{\imageof{\hypercore}}\left[\indexedindvariableof{\hypercore}\right]
	= \compositionofat{\chainingfunction}{\indexinterpretation}{\indindexof{\hypercore}} \, .
\end{align*}

%By $\restrictionofto{\chainingfunction}{\mathcal{M}}$ we further denote the restriction of a real function $\chainingfunction$ to an enumerated set $\mathcal{M} =\{x_i \, : \, i \in [\cardof{\imageof{\hypercore}}]\} \subset \rr$, i.e. the vector
%	\[ \restrictionofto{\chainingfunction}{\mathcal{M}} : [\cardof{\mathcal{M}}] \rightarrow \rr \]
%defined for $i \in [\cardof{\mathcal{M}}]$ as
%	\[ \restrictionofto{\chainingfunction}{\mathcal{M}}(i) = \chainingfunction(x_i) \, . \]


\begin{theorem}\label{the:tensorFunctionComposition}
	The coordinatewise transform of any tensor $\hypercore$ (see \defref{def:coordinatewiseTransform}) by a real function $\chainingfunction$ is the contraction (see \figref{fig:tensorFunctionComposition})
	\begin{align*}
		\chainingfunction(\hypercore)[\shortcatvariables]
		= \contractionof{\rencodingofat{\hypercore}{\indvariableof{\hypercore},\shortcatvariables},\restrictionofto{\chainingfunction}{\imageof{\hypercore}}\left[\indvariableof{\hypercore}\right] }{\shortcatvariables} \, .
	\end{align*}
\end{theorem}
\begin{proof}
	By the basis calculus \theref{the:basisCalculus} we have for any state $\shortcatindices\in\facstates$, that
	\begin{align*}
		\contractionof{\rencodingofat{\hypercore}{\indvariableof{\hypercore},\shortcatvariables},\restrictionofto{\chainingfunction}{\imageof{\hypercore}}\left[\indvariableof{\hypercore}\right]}{\indexedshortcatvariables}
		&= \contraction{\rencodingofat{\hypercore}{\indvariableof{\hypercore},\indexedshortcatvariables},\restrictionofto{\chainingfunction}{\imageof{\hypercore}}\left[\indvariableof{\hypercore}\right]} \\
		& = \contraction{\onehotmapofat{\indexinterpretationof{\hypercoreat{\indexedshortcatvariables}}}{\indvariableof{\hypercore}},\restrictionofto{\chainingfunction}{\imageof{\hypercore}}\left[\indvariableof{\hypercore}\right]} \\
		& = \chainingfunction(\hypercore)[\indexedshortcatvariables] \, .
	\end{align*}
	Since both tensors coincide on all coordinates, they are equal.
\end{proof}

\begin{figure}[h]
\begin{center}
	\input{./PartIII/tikz_pics/basis_calculus/chaining_formula.tex}
\end{center}
\caption{Representation of the composition of a tensor $\hypercore$ with a real function $\chainingfunction$.}
\label{fig:tensorFunctionComposition} 
\end{figure}


\begin{corollary}\label{cor:rhoToNormal}
	For any tensor $\hypercoreat{\shortcatvariables}$ we have
	\begin{align*}
		\hypercoreat{\shortcatvariables}
		= \contractionof{\rencodingofat{\hypercore}{\indvariableof{\hypercore},\shortcatvariables},\idrestrictedto{\imageof{\hypercore}}\left[\indvariableof{\hypercore}\right]}{\shortcatvariables} \, .
	\end{align*}
\end{corollary}
\begin{proof}
	This follows from \theref{the:tensorFunctionComposition} using $\chainingfunction=\idsymbol$ and by noticing that
	\begin{align*}
		\hypercoreat{\shortcatvariables} = \idsymbol(\hypercore)[\shortcatvariables] \, .
	\end{align*}
\end{proof}

\begin{corollary}\label{cor:onesHead}
	For any tensor $\hypercore$, which is directed with $\shortcatvariables$ incoming, we have
		\[ \onesat{\shortcatvariables} = \contractionof{\rencodingof{\hypercore}}{\shortcatvariables} \, . \]
\end{corollary}
\begin{proof}
	This follows from \theref{the:tensorFunctionComposition} using $\chainingfunction=\ones$ and by noticing that
	\begin{align*}
		\onesat{\shortcatvariables} = \ones(\hypercore)[\shortcatvariables] \, .
	\end{align*}
\end{proof}


%% COULD STATE SLICING THEOREM AS A COMPOSITION OF CHAININGS! But unclear, wheter needed
%\begin{theorem}
%	\begin{align*}
%		\coordinatetrafowrtofat{\chainingfunction}{\contractionof{\exvector[\indvariableof{\exfunction}],\rencodingofat{\hypercore}{\indvariableof{\exfunction},\shortcatvariables}}{\shortcatvariables}}{\shortcatvariables}
%		= \coordinatetrafowrtofat{\left(\coordinatetrafowrtofat{\chainingfunction}{\exvector}{\indvariableof{\exfunction}}\right)}{\hypercore}{\shortcatvariables}
%	\end{align*}
%\end{theorem}
%\begin{proof}
%	Simply by compositions of transforms.
%\end{proof}
%
%
%% Replacement of Slicing Theorem
%\begin{corollary}\label{cor:directedTrafo}
%	Let $\basisslices$ be a directed and boolean tensor with incoming variables being $\shortcatvariables$, and $\gentensor$ a tensor, which variables are the outgoing variables of $\basisslices$.
%	Let further $\chainingfunction:\rr\rightarrow\rr$ be any real function.
%	Then
%		\[ \chainingfunction \circ \contractionof{\basisslices,\gentensor}{\shortcatvariables}
%		= \contractionof{\basisslices,\chainingfunction\circ\gentensor}{\shortcatvariables} \, . \]
%\end{corollary}
%\begin{proof}
%	Since $\basisslices$ is a directed and boolean tensor, we find a map
%		\[ \exfunction : \facstates \rightarrow \secfacstates \]
%	such that $\basisslices=\rencodingof{\exfunction}$ and a map $V$ such that $\gentensor=\restrictionofto{V}{\imageof{\exfunction}}$.
%	Then \theref{the:tensorFunctionComposition} implies that
%		\[ \contractionof{\basisslices,\gentensor}{\shortcatvariables} = V \circ \exfunction \, . \]
%	It follows that
%	\begin{align*}
%		\chainingfunction \circ \contractionof{\basisslices,\gentensor}{\shortcatvariables} = \chainingfunction \circ V \circ \exfunction
%	\end{align*}
%	and by another application of Theorem~\ref{the:tensorFunctionComposition} that
%	\begin{align*}
%		\chainingfunction \circ V \circ \exfunction
%		& = \contractionof{\rencodingof{\exfunction}, \restrictionofto{\chainingfunction \circ V}{\imageof{\exfunction}}}{\shortcatvariables} \\
%		& = \contractionof{\basisslices,\chainingfunction\circ\gentensor}{\shortcatvariables} \, .
%	\end{align*}
%	The claim follows as a combination of both equations.
%\end{proof}





\subsect{Decomposition in case of structured images}

When a set is structured as the cartesian product of other sets, that is
\begin{align*}
	\outset = \bigtimes_{\catenumeratorin} \arbsetof{\catenumerator} \, ,
\end{align*}
we can enumerate it by a collection $\{\indvariableof{\catenumerator} \, : \, \catenumeratorin\}$ of enumeration variables, each with respective index interpretation maps.
When the image of a function admits such a cartesian representation, we now show that the relational encoding can be represented by a contraction of relational encodings to each image coordinate.

\begin{theorem}\label{the:functionImageDecompositionContraction}
	Let $\exfunction$ be a function between factored systems
	\begin{align*}
		\exfunction : [\catdim] \rightarrow  \facstates
	\end{align*}
	and denote by
	\begin{align*}
		\exfunctionof{\catenumerator} : [\catdim] \rightarrow [\catdimof{\catenumerator}]
	\end{align*}
	the image coordinate restrictions of $\exfunction$, that is we have $\exfunction=(\exfunctionof{0},\ldots,\exfunctionof{\catorder-1})$.
	Let us assign the variable $\catvariable$ to the factored system in the domain system of $\exfunction$ and the variables $\catvariableof{\atomenumerator}$ for $\atomenumeratorin$ to the image system of $\exfunction$.
	We can then decompose the relational encoding of $\exfunction$ into the relational encodings of its image coordinate restrictions, that is
	\begin{align*}
		\rencodingofat{\exfunction}{\shortcatvariables,\catvariable}
		= \contractionof{
		\{\rencodingofat{\exfunctionof{\atomenumerator}}{\catvariableof{\atomenumerator},\catvariable} : \atomenumeratorin \} 
		}{\shortcatvariables,\catvariable} \, .
	\end{align*}
\end{theorem}
\begin{proof}
	For any $\catindexin$ we have
	\begin{align*}
		\rencodingofat{\exfunction}{\shortcatvariables,\indexedcatvariable}
		&= \onehotmapofat{\exfunctionat{\catindex}}{\shortcatvariables} \\
		&= \bigotimes_{\atomenumeratorin} \rencodingofat{\exfunctionof{\atomenumerator}}{\catvariableof{\atomenumerator},\indexedcatvariable} \\
		&= \contractionof{
		\{\rencodingofat{\exfunctionof{\atomenumerator}}{\catvariableof{\atomenumerator},\indexedcatvariable} : \atomenumeratorin\}
		}{\shortcatvariables} \\
		&= \contractionof{
		\{\rencodingofat{\exfunctionof{\atomenumerator}}{\catvariableof{\atomenumerator},\catvariable} : \atomenumeratorin\}
		}{\shortcatvariables,\indexedcatvariable}
	\end{align*}
	and therefore equality of both tensors.
\end{proof}

% Continue discussion in Sparse TC
In \charef{cha:sparseTC} we will apply \theref{the:functionImageDecompositionContraction} in \theref{the:functionDecompositionBasisCP} to show sparse basis CP decompositions to $\rencodingof{\exfunction}$.
These decompositions are then applied for efficient the representation of empirical distribution, which involve the relational encoding of data maps (see \exaref{exa:empDistCP}), and for exponential families, which statistics have images, which are included in cartesian products of the images to each coordinate (see \exaref{exa:expFamCP}).


\sect{Effective Coordinate Calculus}\label{sec:effectiveCalculus} % -> Part III

% Motivation: Effective Coordinate Calculus
In some situations, we can perform basis calculus more effectively by avoiding image enumeration variables, and instead apply coordinatewise transforms on tensors (see \defref{def:coordinatewiseTransform}).
As we show here, these include conjunctions, which correspond with coordinatewise multiplication, and negation, which correspond with coordinatewise substraction from 1.
Such schemes are applied for example in \cite{tsilionis_tensor-based_2024} in batchwise logical inference.

\begin{figure}
\begin{center}
	\input{./PartIII/tikz_pics/basis_calculus/skeleton_elements.tex}
\end{center}
\caption{Decomposition schemes by effective calculus, using coordinatewise transforms of tensors (see \defref{def:coordinatewiseTransform}).
	a) Conjunction performed by coordinatewise multiplications, b) Negations performed by coordinatewise substraction from one.}\label{fig:ConNegDecomposition}
\end{figure}

\begin{theorem}\label{the:effectiveConjunction}
	For any formulas $\exformula,\secexformula$ we have
	\begin{align*}
		\sbcontractionof{
			\rencodingofat{\land}{\headvariableof{\exformula\land\secexformula},\catvariableof{\exformula},\catvariableof{\secexformula}},\tbasisat{\headvariableof{\exformula\land\secexformula}}
		}{\catvariableof{\exformula},\catvariableof{\secexformula}}
		= \tbasisat{\catvariableof{\exformula}} \otimes \tbasisat{\catvariableof{\secexformula}} \, . 
	\end{align*}
	In particular, it holds that (see Figure~\ref{fig:ConNegDecomposition}a)
	\begin{align*}
		(\exformula\land\secexformula)[\shortcatvariables] = \sbcontractionof{\exformula,\secexformula}{\shortcatvariables} \, . 
	\end{align*}
\end{theorem}
\begin{proof}
	We decompose 
	\begin{align*}
		\rencodingofat{\land}{\headvariableof{\exformula\land\secexformula},\catvariableof{\exformula},\catvariableof{\secexformula}}
		= \tbasisat{\headvariableof{\exformula\land\secexformula}} \otimes \tbasisat{\catvariableof{\exformula}} \otimes \tbasisat{\catvariableof{\secexformula}}
		+ \fbasisat{\headvariableof{\exformula\land\secexformula}} \left( \onesat{\catvariableof{\exformula},\catvariableof{\secexformula}} -  \tbasisat{\catvariableof{\exformula}} \otimes \tbasisat{\catvariableof{\secexformula}} \right)
	\end{align*}
	and get the first claim as
	\begin{align*}
		\sbcontractionof{
			\rencodingofat{\land}{\headvariableof{\exformula\land\secexformula},\catvariableof{\exformula},\catvariableof{\secexformula}},\tbasisat{\headvariableof{\exformula\land\secexformula}}
		}{\catvariableof{\exformula},\catvariableof{\secexformula}}
		& = \sbcontractionof{
			\tbasisat{\headvariableof{\exformula\land\secexformula}} \otimes \tbasisat{\catvariableof{\exformula}} \otimes \tbasisat{\catvariableof{\secexformula}},\tbasisat{\headvariableof{\exformula\land\secexformula}}
		}{\catvariableof{\exformula},\catvariableof{\secexformula}} \\
		& = \tbasisat{\catvariableof{\exformula}} \otimes \tbasisat{\catvariableof{\secexformula}} \, . 
	\end{align*}
	To show the second claim we use
	\begin{align*}
		(\exformula\land\secexformula)[\shortcatvariables] 
		&= \sbcontractionof{
			\rencodingofat{\exformula}{\catvariableof{\exformula},\shortcatvariables},
			\rencodingofat{\secexformula}{\catvariableof{\secexformula},\shortcatvariables},
			\rencodingofat{\land}{\headvariableof{\exformula\land\secexformula},\catvariableof{\exformula},\catvariableof{\secexformula}},
			\tbasisat{\headvariableof{\exformula\land\secexformula}}
			}{\shortcatvariables} \\
		&  = \sbcontractionof{
			\rencodingofat{\exformula}{\catvariableof{\exformula},\shortcatvariables},
			\rencodingofat{\secexformula}{\catvariableof{\secexformula},\shortcatvariables},
			(\tbasisat{\catvariableof{\exformula}}\otimes \tbasisat{\catvariableof{\secexformula}})
			%\rencodingofat{\land}{\catvariableof{\exformula},\catvariableof{\secexformula},\headvariableof{\exformula\land\secexformula}}
			}{\shortcatvariables} \\
		&= \sbcontractionof{\exformula,\secexformula}{\shortcatvariables} \, . 
	\end{align*}
\end{proof}

A similar decomposition holds for negations, as we show next.

\begin{theorem}
	For any formula $\exformula$ we have
	\begin{align*}
		\sbcontractionof{
			\rencodingofat{\lnot}{\headvariableof{\lnot\exformula},\catvariableof{\exformula}},\tbasisat{\headvariableof{\lnot\exformula}}
		}{\catvariableof{\exformula}}
		= \fbasisat{\catvariableof{\exformula}} =  \onesat{\catvariableof{\exformula}} - \tbasisat{\catvariableof{\exformula}} \, .
	\end{align*}
	and
	\begin{align*}
		\sbcontractionof{
			\rencodingofat{\lnot}{\catvariableof{\exformula},\headvariableof{\lnot\exformula}},\fbasisat{\headvariableof{\lnot\exformula}}
		}{\catvariableof{\exformula}}
		= \tbasisat{\catvariableof{\exformula}} \, . 
	\end{align*}
	In particular, it holds that (see Figure~\ref{fig:ConNegDecomposition}b)
	\begin{align*}
		(\lnot\exformula)[\shortcatvariables] = \onesat{\shortcatvariables} - \formulaat{\shortcatvariables}  \, . 
	\end{align*}
\end{theorem}
\begin{proof}
	Using that for two dimensional variables we have $\onesat{\catvariable}=\fbasisat{\catvariable}+\tbasisat{\catvariable}\, .$
\end{proof}

% Usage
These theorems provide a mean to represent logical formulas by sums of one-hot encodings.
Since any propositional formula can be represented by compositions of negations and conjunctions, they are universal.
We further notice, that the resulting decomposition is a basis+ CP format, as further discussed in \charef{cha:sparseCalculus}.
In Figure~\ref{fig:DecompositionExample} we provide an example of this decomposition.


\begin{figure}
\begin{center}
	\input{./PartIII/tikz_pics/basis_calculus/skeleton_example.tex}
\end{center}
\caption{
	Example of a decomposition by effective calculus of a formula $\exformula(\catvariableof{1},\catvariableof{2}) = \textcolor{blue}{\lnot} \secexformula^{(1)}(\catvariableof{1},\catvariableof{2}) \textcolor{red}{\land}  \secexformula^{(2)}(\catvariableof{1},\catvariableof{2})$ into a sum of contractions.}
	\label{fig:DecompositionExample}
\end{figure}


% Calculus against the direction
\red{In an alternative perspective, effective calculus amounts to an contraction against the directionality of the relational encodings.}
For specific functions, slices of the relational encodings with respect to head variables are basis vectors.
In that case, we can perform basis calculus in the inverse direction than suggested by the directions of the tensors.
We examplify this situation in the following theorem for relational encodings of logical conjunctions and negations.




\sect{Applications in Machine Learning}

The neural paradigm of Machine Learning describes the relevance of sparse function to be effective models in the sense of learning and approximation.

% Neural Paradigm by Tensor Network Decompositions
Our model of the neural paradigm are tensor network decompositions, seen as decomposition of functions into smaller functions, which take each other as input.
Summations along input axis are avoided, when having directed and boolean tensor networks with basis calculus interpretation.

% Basis Calculus
We have already observed in Theorem~\ref{the:basisCalculus}, that the value of discrete maps can be calculated by contractions of the directed boolean relation encodings.
This has been framed as Basis Calculus.
What is more, tensor network decompositions into directed boolean tensors correspond with representation of functions as compositions of smaller functions.
We can understand each composition as marking a neuron in an architecture and thus have established a neural perspective on boolean directed tensor networks.
