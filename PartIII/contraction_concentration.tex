%\section{Concentration of the Expected Sufficient Statistics}
\section{Uniform Concentration of Random Contractions}\label{cha:widthBounds}

We here derive bounds on the uniform concentration of contractions with random tensors.




	
	
	

The width of a vector $\noisetensor$ is the supremum of contractions with respect to a set $\Gamma$ is
\begin{align}
	\widthwrtof{\Gamma}{\noisetensor}
	= \sup_{\theta\in\Gamma} \sbcontraction{\theta,\noisetensor} \, . 
	%= \sup_{\theta\in\Gamma} \contractionof{\{\theta, \noisetensor\}}{\varnothing} \, . 
\end{align}

We are interested in the random vector
	\[ \noisetensor^{\formulaset,\gendistribution,\datamap} = \sbcontractionof{\empdistribution,\formulaset}{\selvariable} -  \sbcontractionof{\gendistribution,\formulaset}{\selvariable}  \,  \]
	%\essdistof{\empdistribution}-\expectationof{\essdistof{\empdistribution}} \]
which is the difference between the mean parameters given the empirical distribution and the underlying generating distribution.

We derive bounds for the hypothesis $\Gamma$ being the set of basis vectors (i.e. for feature search) and being the set of normed vectors (i.e. for feature calibration).





%\subsection{Binomials}
%\subsubsection{Concentration Bounds for Binomials}





\subsection{Naive bounds given binomial coordinates}


We here investigate width bounds on random tensors $\noisetensor$, which coordinates have marginal distributions by Binomials.

% MLN
This is the case for $\noisetensor^{\fselectionmap,\gendistribution,\datanum}$.

Naive means, that we do not exploit the dependencies of the coordinates on each other, as would be the case in more sophisticated chaining approaches.

\subsubsection{Basis Vectors}

We exploit the sub-Gaussian Norm (Def 2.5.6 in CITE Vershynin Book) to state concentration inequalities.

\begin{definition}[Sub-Gaussian Norm]
	The sub-Gaussian norm of a random variable $X$ is defined as
		\[ \sgnormof{X} = \inf \left\{ C > 0 \, : \expectationof{\frac{X^2}{C^2} } \leq 2 \right\} \, .  \]
\end{definition}

\begin{theorem}
	Any coordinate of $\noisetensor$ is sub-Gaussian with 
		\[ \sgnormof{\noisetensor_i} \leq \frac{C}{\sqrt{\ln2}} \frac{1}{\sqrt{m}} \]
\end{theorem}
\begin{proof}
	Centered Bernoulli is bounded and therefore sub-Gaussian.
	Binomial is a sum and we apply Proposition 2.6.1 in CITE Vershynin Book.
\end{proof}


\begin{theorem}\label{the:basisTensorWidthBound}
	Let 
		\[ \Gamma = \{\onehotmapof{i} : i \in [p] \}\]
	and $\noisetensor$ be a random vector in $\rr^p$, which coordinates have a marginal distribution being centered Binomials with a fixed $\datanum\in\nn$.
	Then
		\[ \sgnormof{\widthatof{\Gamma}{\noisetensor}} \leq C \sqrt{\frac{\ln p}{m}} \, . \]
	where $C>0$ is a universal constant.
\end{theorem}
\begin{proof}
	Supremum of Sub-Gaussian variables.
\end{proof}


\subsubsection{Sphere}

We first provide a Chebyshev bound on the width of the sphere.

\begin{theorem}\label{the:sphereBoundVariance}
	Let $\noisetensor$ be a random tensor with marginal coordinate distributions by binomials with parameters $(\fprobof{\catindex},\datanum)$.

	For any $\failprob>0$, $\precision>0$ and $\datanum\in\nn$ with probability at least $1-\failprob$ we have
		\[ \normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} \leq   \precision \, \]
	provided that
		\[ \datanum \geq  \frac{ \sum_{\catindex\in\facstates} \fprobof{\catindex}(1-\fprobof{\catindex})}{\precision^2 \failprob} \, . \]
\end{theorem}
\begin{proof}
	%We estimate with the Cauchy Shwartz inequality
	%	\[ \braket{\theta,\noisetensor} \leq \|\noisetensor\| \|\theta\| \, . \]
	Since the squared norm of the noise is the sum of squared centered and averaged Binomials, we have
		\[  \expectationof{\normof{\noisetensor-\expectationof{\noisetensor}}^2}  
		= \sum_{\catindex\in\facstates} \frac{\fprobof{\catindex}(1-\fprobof{\catindex})}{\datanum} \]
	Here we used that the variance of Binomials with parameters $(\fprobof{\catindex},\datanum)$ is $\fprobof{\catindex}(1-\fprobof{\catindex}) \datanum$.
	
	If follows, that 
		\[ \expectationof{\left(\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}}\right)^2} =  \frac{ \sum_{\catindex\in\facstates} \fprobof{\catindex}(1-\fprobof{\catindex})}{\datanum} \, . \]
	
	Then we apply a Chebyshev Bound to get for any $\precision>0$
	\begin{align}
		\probof{\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} > \precision} 
		= \probof{\left(\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}}\right)^2 > \precision^2} 
		\leq \frac{ \sum_{\catindex\in\facstates} \fprobof{\catindex}(1-\fprobof{\catindex})}{\datanum \cdot \precision^2}
	\end{align} 
	For a $\failprob>0$ we choose any $\datanum$ with
		\[ \datanum \geq  \frac{ \sum_{\catindex\in\facstates} \fprobof{\catindex}(1-\fprobof{\catindex})}{\precision^2 \failprob} \, \]
	and get 
	\begin{align}
		\probof{\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} > \precision} \leq \failprob \, . 
	\end{align} 
	Thus, we have 
	\begin{align}
		\probof{\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} \leq \precision} = 1 - \probof{\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} > \precision}  \geq 1-\failprob \, . 
	\end{align} 
\end{proof}


% Multinomial
For $\sstat=\identity$ the noise tensor is a rescaled and centered multinomial.

\begin{corollary}
	Let there be multinomial variable with parameters $(\fprob,\datanum)$ where $\fprob\in\facspace$ a positive and normed tensor.
	Let $\datamap$ be a set of independent samples
	For any $\failprob>0$, $\precision>0$ and $\datanum\in\nn$ with probability at least $1-\failprob$ we have
		\[ \normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} \leq   \precision \, \]
	provided that
		\[ \datanum \geq  \frac{(1-\sbcontraction{(\fprob)^2})}{\precision^2 \failprob} \, . \]
\end{corollary}
\begin{proof}
	Theorem~\ref{the:sphereBoundVariance} with 
		\[ \sum_{\catindex\in\facstates} \fprobof{\catindex}(1-\fprobof{\catindex}) = \sum_{\catindex\in\facstates} \fprobof{\catindex} - \sum_{\catindex\in\facstates} (\fprobof{\catindex})^2 = 1-\sbcontraction{(\fprob)^2} \, . \]
\end{proof}



\subsubsection{Bounds based on the sub-gaussian norm}

\red{Unclear whether this is needed.}

A faster tail decay can be achieved, when bounding sub-gaussian norms.


\begin{theorem}
	Let $\noisetensor$ be a random vector in $\rr^p$, which coordinates have a marginal distribution being centered Binomials with a fixed $\datanum\in\nn$.
	Then
		\[ \sgnormof{\widthatof{\subsphere}{\noisetensor}} \leq C \sqrt{\frac{p}{m}} \, . \]
	where $C>0$ is a universal constant.
\end{theorem}
\begin{proof}
	Using that each coordinate has Sub-gaussian norm of at most $1$.
	\red{Asymptotically, the binomial tends to a gaussian, which has a smaller sg norm. 
	But the binomial has a sub-exponential regime preventing tighter sg bounds.}

	
	Norm of a Sub-gaussian vector, another application of Proposition 2.6.1 in CITE Vershynin Book.
\end{proof}




\subsection{Chaining bounds given binomial coordinates}

To proceed with the uniform concentration investigation, we need a concentration bound on Binomials.

\begin{theorem}
	For any $p\in[0,1]$ and $\datanum\in\mathbb{N}$ any $X \sim \bidistof{p,\datanum}$ satisfies for any $t>0$
		\[ \probof{X-\expectationof{X} > t}  \leq \expof{- \frac{t^2}{2\datanum p + \frac{2t}{3}} } \]
	and
		\[ \probof{\expectationof{X} - X > t}  \leq \expof{- \frac{t^2}{2\datanum p}} \]
	Thus
		\[ \probof{|\expectationof{X} - X| > t} \leq  2 \expof{- \frac{t^2}{2\datanum p + \frac{2t}{3}} }  \]
\end{theorem}
\begin{proof}
	See e.g.
	\href{https://mathweb.ucsd.edu/~fan/wp/concen.pdf}{https://mathweb.ucsd.edu/~fan/wp/concen.pdf}
%	The proof uses the Chernoff bound applying the moment generating function, which is for Binomial variables X and $\lambda\geq0$
%	\begin{align}
%	 	\expectationof{\expof{\lambda (X - \expectationof{X})}} 
%		= & \left( (1-p) \cdot \expof{\lambda -p} + p \cdot \expof{\lambda (1-p)}\right)^\datanum \\
%		= & \expof{-\lambda p \datanum} \left(1 + p(\expof{\lambda}-1) \right)^\datanum \, .
%	\end{align}
%	The Chernoff bound used for any $\lambda>0,t>0$ the Markov inequality
%	\begin{align}
%		\probof{X-\expectationof{X} > t} 
%		= \probof{\expof{\lambda(X-\expectationof{X})} > \expof{\lambda t} }
%		\leq \frac{\expectationof{\expof{\lambda(X-\expectationof{X})}}}{\expof{\lambda t} } 
%	\end{align}
\end{proof}

The binomial thus has a sub-gaussian and a sub-exponential regime.

\begin{theorem}
	For any $p\in[0,1]$ and $\datanum\in\mathbb{N}$ any $X \sim \bidistof{p,\datanum}$ satisfies for any $t>0$
		\[ \probof{|\expectationof{X} - X| > \sqrt{4\datanum p t} + 2 t} \leq  2 \expof{- t }  \]
\end{theorem}
\begin{proof}
	For any s>0 we choose t>0 such that 
		\[ s = - \frac{t^2}{2\datanum p + \frac{t}{3}}  \]
	and observe 
		\[ \min\left( \frac{t^2}{4\datanum p},\frac{t^2}{2t} \right) \]
	and
		\[ t \leq \max(\sqrt{4\datanum p s},2s) \leq  \sqrt{4\datanum p s} + 2s \, . \]
	With the above bound it holds, that
		\[  \probof{|\expectationof{X} - X| >  \sqrt{4\datanum p s} + 2s}
		\leq \probof{|\expectationof{X} - X| > t}
		\leq 2 \expof{- \frac{t^2}{2\datanum p + \frac{t}{3}} } 
		\leq 2 \expof{-s} \, . \]
\end{proof}

We apply this on the variable  $\braket{\ftensor,\noisetensor}$.

\begin{theorem}
	Let $\ftensor = \sum_{\mlnformulain} \weightof{\exformula}\exformula$, then
	\[ \probof{\datanum |\braket{\ftensor,\noisetensor}|\geq \sqrt{4 \datanum} \cdot \left( \sum_{\mlnformulain} |\weightof{\exformula}| \sqrt{\fprobof{\exformula}} \right) \sqrt{t}  + 2 \cdot \left( \sum_{\mlnformulain} |\weightof{\exformula}|  \right) t } \leq 2|\mlnformulaset | \cdot \expof{- t} \]
\end{theorem}
\begin{proof}
	We can not assume independence of the $\braket{\exformula,\noisetensor}$ (in that case we could use a Bernstein inequality) and instead take the naive bound over all formulas in $\mlnformulaset$ 
	\begin{align}
		& \probof{|\braket{\ftensor,\noisetensor}|\geq
		 \sqrt{4 \datanum} \cdot \sum_{\mlnformulain} |\weightof{\exformula}| \sqrt{\fprobof{\exformula}}\sqrt{t}  
		 + 2 \cdot \sum_{\mlnformulain} |\weightof{\exformula}|  t } \\
		& \quad \quad \leq \probof{\exists \mlnformulain \, : \, |\braket{\exformula,\noisetensor}|\geq  \sqrt{4 \datanum}  |\weightof{\exformula}| \sqrt{\fprobof{\exformula}}\sqrt{t}  + 2 |\weightof{\exformula}|  t  }\\
		& \quad \quad \leq \sum_{\mlnformulain} \probof{ |\braket{\exformula,\noisetensor}|\geq  \sqrt{4 \datanum}  |\weightof{\exformula}| \sqrt{\fprobof{\exformula}}\sqrt{t}  + 2 |\weightof{\exformula}|  t  }\\
		& \quad \quad  \leq 2|\mlnformulaset| \cdot  \expof{-t} \, .
	\end{align}
\end{proof}

We can thus proof uniform concentration bounds given covering bounds of a hypothesis set in $\ell_1$ norm (and the reweighted one).

\begin{remark}[Small Formula Probabilities]	
	Directions with small $\fprobof{\exformula}$ will require larger covering sets and thus have large contributions to the bounds.
	Intuitively, they correspond with exceptional special cases, which need many samples to be observed. 
\red{Strange: Should also $1-\fprobof{\exformula}$ small intuitely be an issue?}
\end{remark}


\subsubsection{Generic Width Bounds}

We define the maps
	\[ \nu^p_2(\ftensor) =  \inf_{\mlnparameters : \sum_{\mlnformulain}\weightof{\exformula}\exformula = \ftensor}  \left( \sum_{\mlnformulain} |\weightof{\exformula}| \sqrt{\fprobof{\exformula}} \right) \]
and
	\[ \nu_1(\ftensor) =  \inf_{\mlnparameters : \sum_{\mlnformulain}\weightof{\exformula}\exformula = \ftensor} \left( \sum_{\mlnformulain} |\weightof{\exformula}| \sqrt{\fprobof{\exformula}} \right)  \]
corresponding to the sub-gaussian and sub-exponential regimes.

%This is almost the mixed tail definition of the thesis, just a factor on the probability

\begin{theorem}
	Let $\mlnformulaset$ be a set of formulas and $W\subset\rr^{|\mlnformulaset|}$ a set of weight vectors.
	Then with probability at least $1- |\mlnformulaset| C_1 \expof{-\frac{u^2}{2}} $ we have for the set
		\[ \Gamma = \big\{ \sum_{\mlnformulain}\weightof{\exformula}\exformula \, : \, (\weightof{\exformula})_{\mlnformulain} \in W \big\} \]
	the bound
		\[ \omega_\Gamma(\noisetensor)  \leq \frac{C_2 \gamma_{2}(\Gamma,\nu^p_2)}{\sqrt{\datanum}} + \frac{C_3 \gamma_{1}(\Gamma,\nu_1)}{\datanum} \, . \]
\end{theorem}



%% OLD: These situations are addressed more directly

%\subsubsection{Recovery of atom assignment in skeleton}
%
%We select for each placeholder in the skeleton an atom of $\variableorder$ possible choices.
%The set of formula tensors resulting from these choices is
%	\[ \Gamma^{\skeleton} = \left\{ \exformula \, : \, \exformula = \skeleton(\atomindices), \atomindices \in [\variableorder] \right\} \, .\]
%We can estimate the cardinality by
%	\[ \cardof{\Gamma^{\skeleton}} \leq \variableorder^\atomorder \, . \]
%This is just an inequality, since assignments of atoms to placeholders of the skeleton can result in identical formulas.
%
%When restricting choices by a candidatesdict, the bound can be sharpened by the product of the cardinality at each placeholder.
%
%\begin{theorem}
%	Let $\skeleton$ be a skeleton formula with $\atomorder$ placeholders and $\variableorder$ atoms, which can be selected at each position.
%	Then we have with probability at least $1-C_1\expof{-\frac{u^2}{2}}$
%		\[ \omega_\skeleton(\noisetensor)  \leq 2C_2 \sqrt{\frac{ \atomorder \ln\variableorder}{\datanum}} + 2C_3\frac{\atomorder\ln\variableorder}{\datanum}  \]
%\end{theorem}
%\begin{proof}
%	Application of the generic Dudleys entropy bound.
%\end{proof}
%
%Thus
%	\[ \datanum \sim \atomorder\ln(\variableorder) \]
%is enough for a sharp Kullback Leibler bound of the solution.
%
%
%\subsection{Recovery of weight parameters }
