\section{Sparse Tensor Representations}\label{cha:sparseTC}

We in this chapter investigate, which sparsity notations enable tensors to be representable as contractions of tensor networks.


\subsection{CP Formats}

The CP Decomposition is one way to generalize the ranks of matrices to tensors.
It is oriented on the Singular Value Decomposition of matrices, providing a representation of the matrix as a weighed sum of the tensor product of singular vectors.
Given a tensor of higher order, each such tensor product is over multiple vectors, 

\begin{definition}\label{def:cpFormats}
	A CP Decomposition of rank $\decdim$ of a tensor $\hypercore\in\facspace$ is a collections of tensors $\scalarcoreat{\decvariable}$ and $\legcoreofat{\atomenumerator}{\decvariable,\catvariableof{\atomenumerator}}$ for $\atomenumeratorin$, where $\decvariable$ takes values in $[\decdim]$, such that
		\[  \hypercoreat{\shortcatvariables}
		= \contractionof{
		\{\scalarcoreat{\decvariable}\} \cup \{ \legcoreofat{\atomenumerator}{\decvariable,\catvariableof{\atomenumerator}} \, : \, \atomenumeratorin \}
		}{\shortcatvariables} \, . 
		\]
%	where for each $\decindexin$ and $\atomenumeratorin$ we have $\scalarcoreat{\decindex} \in \rr$ and $\legcoreof{\atomenumerator,\decindex}\in\rr^{\catindexof{\atomenumerator}}$.
	We say that the CP Decomposition is
	\begin{itemize}
		\item directed, when for each $\atomenumerator$ the core $\legcoreof{\atomenumerator}$ is directed with $\decvariable$ incoming and $\catvariableof{\atomenumerator}$ outgoing.
		\item binary, when for each $\atomenumerator$ the core $\legcoreof{\atomenumerator}$ is binary.
		\item basis, where we demand both properties, that is for each $\atomenumeratorin$ and $\decindexin$ 
			\[ \legcoreofat{\atomenumerator}{\inddecvar,\catvariableof{\atomenumerator}}\in \{ \onehotmapofat{[\catindexof{\atomenumerator}]}{\catvariableof{\atomenumerator}} \catindexof{\atomenumerator}\in[\catdimof{\atomenumerator}] \}\, . \]
		\item basis+, when for each $\atomenumeratorin$ and $\decindexin$  %$\legcoreof{\atomenumerator,\decindex}\in\onehotmapof{[\catindexof{\atomenumerator}]}$ or $\legcoreof{\atomenumerator,\decindex}=\ones$.
			\[ \legcoreofat{\atomenumerator}{\inddecvar,\catvariableof{\atomenumerator}}\in \{ \onehotmapofat{[\catindexof{\atomenumerator}]}{\catvariableof{\atomenumerator}} \catindexof{\atomenumerator}\in[\catdimof{\atomenumerator}] \} \cup \{\onesat{\catvariableof{\atomenumerator}}\}\, . \]
	\end{itemize}
	We denote by $\cprankof{\hypercore}$, respectively $\bincprankof{\hypercore}$, $\bascprankof{\hypercore}$ and $\baspluscprankof{\hypercore}$ the minimal cardinality such that $\hypercore$ has a CP Decomposition with directed cores, respectively binary cores, basis cores and basis+ cores.
\end{definition}

% Sum of elementary tensors
We have by definition
	\[ \hypercoreat{\shortcatvariables} = \sum_{\decindexin} \scalarcoreat{\inddecvar} \left( \bigotimes_{\atomenumeratorin} \legcoreofat{\atomenumerator}{\inddecvar,\catvariableof{\atomenumerator}} \right) \, . \]
The right side can be seen as an alternative definition of CP Decompositions by summations of elementary tensors.


\begin{figure}[h]
	\begin{center}
		\input{PartIII/tikz_pics/sparse_tensor_calculus/cp_decomposition.tex}
	\end{center}
	\caption{Tensor Network diagram of a generic CP decomposition (see Definition~\ref{def:cpFormats})}
\end{figure}

We introduce different notions of sparsities based on CP Decomposition with different properties of their leg cores.

\subsubsection{Directed Leg Cores}

This is the canonical CP Decomposition, where the vectors $\legcoreof{\atomenumerator,\decindex}$ are interpreted as generalized singular vectors.
Any CP decomposition can be transformed into a directed CP decomposition without enlarging the index set $\indexset$, simply by diving the vectors by their norms and multiplying it to $\scalarcoreat{\inddecvar}$.

% Directionality
We then have a partially directed Tensor Network representing the decomposed tensor.
The only undirected core is $\scalarcore$, since we do not demand it to be normed.
In many applications applications, however, also the $\scalarcore$ is directed with a single outgoing leg (see for example the empirical distributions as discussed in Section~\ref{sec:empDistribution}).
In that case, also the decomposed tensor is directed with outgoing legs.



\subsubsection{Basis Leg Cores}\label{sec:basisCP}

% From FOL Chapter: Bayesian Network interpretation of Basis CP
%	The basis CP can further be understood as a Bayesian network, where we understand $\dataindex$ as condition and each decomposition core as a conditional probability distribution.
%	We notice that in this interpretation the direction of the dependency is inversed compared with previous representation of grounding tensors in Figure~\ref{fig:groundingCP}. 


Directed and binary leg cores have incoming slices being basis vectors, we thus call them basis CP Decomposition.
This allows the interpretation of the directed and binary CP decomposition in terms of mapping to nonzero coordinates.
We start by defining the number of nonzero coordinates of tensors by the $\ell_0$-norm.

\begin{definition}
	The $\ell_0$-norm counts the nonzero coordinates of a tensor by
		\[ \sparsityof{\hypercore} = \#\big\{ \catindices \, : \, \hypercore_{\catindices }\neq 0 \big\} \, . \]
\end{definition}

The $\ell_0$-norm is not a proper norm itself, but the limit of $\ell_p$-norms (where $p \rightarrow 0$) of the flattened tensor (which are norms for $p\geq1$).

% Interpretation
The $\ell_0$ norm is the number of nonzero coordinates. 
We understand the leg cores as the relational encoding of functions mapping to the slices of these coordinates given an enumeration.
This is consistent with the previous analysis of Chapter~\ref{cha:directedTC}, where we characterized binary and directed cores by the encoding of associated functions.
Based on this idea, we can proof, that any tensor has a directed and binary CP decomposition with rand $\sparsityof{\hypercore}$.


\begin{theorem}\label{the:sparseBasisCP}
	For any tensor $\hypercore$ we have
		\[ \bascprankof{\hypercore} = \sparsityof{\hypercore} \, .  \]	
\end{theorem}
\begin{proof}
	We find a map 
		\[ \exfunction : [\sparsityof{\hypercore}] \rightarrow  \facstates \, , \] 
	which image is the set of nonzero coordinates of $\hypercore$.
	Denoting its image coordinate maps by $\exfunction^{\atomenumerator}$ we have
		\[ \hypercore = \sum_{\dataindexin} \scalarcoreof{\exfunction(\dataindex)} \left( \bigotimes_{\atomenumeratorin} \onehotmapof{\exfunction^\atomenumerator(\dataindex)} \right) \, . \]
	This is a basis CP Decomposition with rank $\sparsityof{\hypercore}$.
	Conversely, any basis CP Decomposition of $\hypercore$ with dimension $r$ would have at most $r$ coordinates different from zero and thus $\sparsityof{\hypercore}\leq r$.
	Thus, there cannot be a CP Decomposition with a dimension $r\leq\sparsityof{\hypercore}$.
\end{proof}

%
The next theorem relates the basis CP decomposition with encodings of $\atomorder$-ary relations (see Definition~\ref{def:daryRelation}).

\begin{theorem}
	If any only if $\hypercore\in\facspace$ has a basis decomposition with slices $\{\catindex_{[\atomorder]}^{\decindex} \, : \, \decindexin \}$ and trivial cores, it coincides with the encoding of the $\atomorder$-ary relation 
		\[ \exrelation = \{\catindex_{[\atomorder]}^{\decindex} \, : \, \decindexin \} \, . \]
%	To each basis CP decompositions with pairwise different slices and trivial scalar cores we find a $d$-ary relation, such that 
\end{theorem}


If in addition $\catdimof{\atomenumerator}=2$, we can interpret basis CP decompositions as propositional formulas.

% Knowledge Bases
\begin{theorem}
	If $\hypercore\in\atomspace$ has a basis decomposition with slices $\{\catindex_{[\atomorder]}^{\decindex} \, : \, \decindexin \}$ and trivial cores, it coincides with the propositional formula
		\[ \formulaat{\shortcatvariables} = 
		\bigvee_{\decindexin} \termof{\catindex_{[\atomorder]}^{\decindex}} \, . \]
\end{theorem}
\begin{proof}
	This is a generalization of Theorem~\ref{the:maximalClausesRepresentation}, which follows from Theorem~\ref{the:tensorToMaxMinTerms}.
\end{proof}


% Storage
The storage demand of any CP decomposition is at most linear in the dimension and the sum of its leg dimension.
When we have a basis CP decomposition, this demand can be further improved.
The basis vectors can be stored by its preimage of the one hot encoding $\onehotmapof{\cdot}$, that is the number of the basis vector in $[\catdim]$.
This reduces the storage demand of each basis vector to the logarithms of the space dimension without the need of storing the full vector.

% Matrix Representation
More precisely, we can store the CP Decomposition by the matrix
	\[ \matrixat{\decvariable,\selvariable} \in \rr^{\datanum \times (\atomorder+1)} \]
defined for $\atomenumeratorin$
	\[ \matrixat{\inddecvar,\selvariable=\atomenumerator} 
	= \invonehotmapof{\legcoreofat{\atomenumerator}{\inddecvar,\catvariableof{\atomenumerator}}}\]
and
	\[ \matrixat{\inddecvar,\selvariable=\atomorder}  
	= \scalarcoreat{\inddecvar} \, . \]
	
This is a typical tabular format to store relational databases.

\subsubsection{Basis+ Leg Cores}

The minimal rank of CP Decompositon is closely related to polynomial sparsity of the map $\hypercore$, which we will define next.

\begin{definition}\label{def:polynomialSparsity}
	A monomial decomposition of a tensor $\hypercore\in\facspace$ 
	%consists of index sets $\indexsetof{\variableset}$ to each $\variableset\subset[\atomorder]$ and values $\scalarcoreat{\variableset, \catindexof{\variableset}}\in\rr$ for each $\catindexof{\variableset}\in \indexsetof{\variableset}$ such that
	is a set $\sliceset$ of tuples $\slicetupleof{}$ where $\slicescalar\in\rr, \variableset\subset[\atomorder]$ and $\catindexof{\variableset}\in\bigtimes_{\atomenumerator\in\variableset} [\catdimof{\atomenumerator}]$ such that
	\begin{align}\label{eq:decIntoMonomials}
		\hypercoreat{\shortcatvariables} = \sum_{\slicetupleof{}\in\sliceset} \slicescalar \cdot \contractionof{\onehotmapof{\catindexof{\variableset}}}{\shortcatvariables} \, .
	\end{align}
%	\begin{align}
%		\hypercore 
%			= \sum_{\variableset\subset[\atomorder]} \sum_{\catindexof{\variableset}\in \indexsetof{\variableset}}  
%			\scalarcoreat{\variableset, \catindexof{\variableset}} \cdot \left( \onehotmapof{\catindexof{\variableset}} \otimes \onesof{[\atomorder]/\variableset} \right)   \, . 
%	\end{align}
	For any tensor $\hypercore\in\facspace$ we define its polynomial sparsity as
	\begin{align*}
		\slicesparsityof{\hypercore} =
		 \min \left\{ \cardof{\sliceset} \, : \, 
		 	\hypercoreat{\shortcatvariables} = \sum_{\slicetupleof{}\in\sliceset} \slicescalar \cdot \contractionof{\onehotmapof{\catindexof{\variableset}}}{\shortcatvariables} 
		 \right\}
	\end{align*}
%	\begin{align}
%		\slicesparsityof{\hypercore} =
%		 \min \left\{ \# \left( \bigcup_{\variableset\subset[\atomorder]} \indexsetof{\variableset} \right) \, : \, \exists \scalarcoreat{\variableset, \catindexof{\variableset}} \in \rr : 
%		 \hypercore 
%		 	= \sum_{\variableset\subset[\atomorder]} \sum_{\catindexof{\variableset}\in \indexsetof{\variableset}}  
%			\scalarcoreat{\variableset, \catindexof{\variableset}} \cdot \left( \onehotmapof{\catindexof{\variableset}} \otimes \onesof{[\atomorder]/\variableset} \right)   
%		 \right\}
%	\end{align}
\end{definition}


% Explanation of monomials
We refer to the terms in a decomposition \eqref{eq:decIntoMonomials} in Definition~\ref{def:polynomialSparsity} as monomials of binary variables, which are enumerated by pairs $(\atomenumerator,\catindexof{\atomenumerator})$ and indicate whether the variable $\catvariableof{\atomenumerator}$ is in state $\catindexof{\atomenumerator}\in[\catdimof{\atomenumerator}]$.
Such indicators are represented by the one-hot encodings
	\[ \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}} \, . \]
The monomial of multiple such binary variables indicated, whether all variables labelled by a set $\variableset$ are in the state $\catvariableof{\variableset}$, which is represented by
	\[ \onehotmapofat{\catindexof{\variableset}}{\catvariableof{\variableset}} = \bigotimes_{\atomenumerator\in\variableset} \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}}  \, . \]
The states of the variables labeled by $\atomenumerator\in[\atomorder]/\variableset$ are not specified in the monomial and the monomial is trivially extended to
	\[ \contractionof{\onehotmapof{\catindexof{\variableset}}}{\shortcatvariables}  = \onehotmapofat{\catindexof{\variableset}}{\catvariableof{\variableset}} \otimes \onesat{\catvariableof{[\atomorder]/\variableset}} \, .   \]

%% Interpretation as Monomial Sparsity
%Each tensor $\lambda \cdot \contractionof{\onehotmapof{\catindexof{\variableset}}}{\shortcatvariables}$ is a by $\lambda\in\rr$ weighted monomial of the variables (or their negations) in $\variableset$.
%The polynomial sparsity is thus the minimal number of monomials that sum up to the function $\hypercore$.
%Given any monomial decomposition of a tensor, we can alternatively write
%	\begin{align*}
%		\hypercore = \sum_{\variableset\subset[\atomorder]} \sum_{\catindexof{\variableset}\in \indexsetof{\variableset}}  
%			\scalarcoreat{\variableset, \catindexof{\variableset}} \left( \prod_{\atomenumeratorin} \catvariableof{\atomenumerator} == (\catindexof{\variableset})_{\atomenumerator} \right) \, . 
%	\end{align*}
%where by $ \catvariableof{\atomenumerator} == (\catindexof{\variableset})_{\atomenumerator}$ we denote the atomic variables, indicating whether the variable $\catvariableof{\atomenumerator}$ is in state $ (\catindexof{\variableset})_{\atomenumerator}$.
%
%% Failing to be directed.
%Note, that the leg cores fail to be directed, when for some $\variableset\neq[\atomorder]$ the set $\indexsetof{\variableset}$ is not empty.


\begin{theorem}
	For any tensor $\hypercore\in\facspace$ we have
		\[ \slicesparsityof{\hypercore} = \baspluscprankof{\hypercore} \, . \]
	When $\catindexof{\atomenumerator}=2$ for all $\atomenumeratorin$, we also have
		\[ \bincprankof{\hypercore} = \slicesparsityof{\hypercore}  \, . \]
\end{theorem}
\begin{proof}
	To proof the first claim, we construct a basis+ CP decomposition given a monomial decomposition and vice versa.
	Let there be a tensor $\hypercoreat{\shortcatvariables}$ with a monomial decomposition by $\sliceset$ with $\cardof{\sliceset}=m$ and let us enumerate the elements in $\sliceset$ by $\slicetupleof{\decindex}$ for $\decindexin$.
	 We define for each $\atomenumeratorin$ the tensors
	 \begin{align*}
		\legcoreofat{\atomenumerator}{\decvariable,\catvariableof{\atomenumerator}}
		 = \left( \sum_{\decindexin \, : \, \atomenumerator\in\variableset} \onehotmapofat{\decindex}{\decvariable} \otimes \onehotmapofat{\catindexof{\atomenumerator}^{\decindex}}{\catvariableof{\atomenumerator}} \right)
		 + \left(\sum_{\decindexin \, : \, \atomenumerator\notin\variableset} \onehotmapofat{\decindex}{\decvariable} \otimes \onesat{\catvariableof{\atomenumerator}} \right)
	\end{align*}
	and 
	\begin{align*}
		\scalarcoreat{\decvariable} = \sum_{\decindexin} \slicescalar^{\decindex} \cdot \onehotmapofat{\decindex}{\decvariable}
	\end{align*}
	 and notice that	 
	\begin{align*}
		\hypercoreat{\shortcatvariables} 
		& = \sum_{\decindexin} \slicescalar^{\decindex} \cdot \contractionof{\onehotmapof{\catindexof{\variableset}^{\decindex}}}{\shortcatvariables} \\
		& = \sum_{\decindexin} \left(  \scalarcoreat{\inddecvar} \cdot \bigotimes_{\atomenumeratorin} \legcoreofat{\atomenumerator}{\inddecvar, \catvariableof{\atomenumerator}} \right) \\
		& = \contractionof{
		\{\scalarcoreat{\decvariable}\} \cup \{\legcoreofat{\atomenumerator}{\decvariable,\catvariableof{\atomenumerator}} \, : \, \atomenumeratorin \}
		}{\shortcatvariables} \, . 
	\end{align*}
	By construction this is a basis+ CP decomposition with rank $\decdim$.
	Since any monomial decomposition can be transformed into a basis+ CP decomposition with same rank we have
	\begin{align*}
		\slicesparsityof{\hypercore} \geq \baspluscprankof{\hypercore} \, . 
	\end{align*}
	
	Let there now be a basis+ CP decomposition we define for each $\decindexin$ 
	\begin{align*}
		\variableset^{\decindex} = \{\atomenumeratorin : \legcoreofat{\atomenumerator}{\inddecvar, \catvariableof{\atomenumerator}} \neq \onesat{\catvariableof{\atomenumerator}} \}
		 \quad \text{and} \quad 
		 \catindexof{\variableset}^{\decindex} = \{\invonehotmapof{\legcoreofat{\atomenumerator}{\inddecvar, \catvariableof{\atomenumerator}} } \, : \atomenumerator\in\variableset\}
	\end{align*}
	where by $\invonehotmapof{\cdot}$ we denote the inverse of the one-hot encoding.
	
	We notice that this is a monomial decomposition of $\hypercoreat{\shortcatvariables}$ to the tuple set
	\begin{align*}
		\sliceset = \{(\scalarcoreat{\inddecvar}, \variableset^{\decindex}, \catindexof{\variableset^{\decindex}}^{\decindex} ) \, : \, \decindexin \} \, . 
	\end{align*}
	It follows from this that
	\begin{align*}
		\slicesparsityof{\hypercore} \leq \baspluscprankof{\hypercore} \, 
	\end{align*}
	and the first claim is shown.
	
	The second claim follows from the observation, that whenever $\catindexof{\atomenumerator}=2$ for all $\atomenumeratorin$ the binary CP decompositions with non-vanishing slices $\legcoreofat{\atomenumerator}{\inddecvar, \catvariableof{\atomenumerator}}$ for $\atomenumeratorin$ and $\decindexin$ are also basis+ CP decompositions and vice versa.
%
%	
%
%	We proof the claim by establishing a one-to-one map between any binary CP decomposition of a a tensor $\hypercore$ and a monomial decomposition of $\hypercore$.
%	% CP Decomposition to monomial decomposition
%	Let there be a binary CP Decomposition of $\hypercore$ with the leg tensors $\{\legcoreof{\atomenumerator}\, :\, \atomenumeratorin\}$ and the scalar core $\scalarcore$.
%	For any index $\decindexin$ and $\atomenumeratorin$ we have
%		\[ \legcoreof{\atomenumerator}_{\decindex} \in \{\onehotmapof{0},\onehotmapof{1},\ones\} \, . \]
%	We define for any $\decindexin$ the sets 
%		\[ \variableset^{\decindex}=\big\{\atomenumerator \, : \, \legcoreof{\atomenumerator}_{\decindex} \in \{\onehotmapof{0},\onehotmapof{1}\}  \big\}\]
%	and an index tuple $\catindexof{\variableset}^\decindex \in \bigotimes_{\atomenumerator\in\variableset}[2]$ by
%	\begin{align*}
%		(\catindexof{\variableset}^\decindex)_\atomenumerator = 
%		\begin{cases} 
%			0 & \text{  if  } \legcoreof{\atomenumerator}_{\decindex} = \onehotmapof{0} \\1 & \text{  if  } \legcoreof{\atomenumerator}_{\decindex} = \onehotmapof{1} 
%		\end{cases} \, . 
%	\end{align*}   
%	Then we have by construction that 
%	\begin{align*}
%		\hypercore = \sum_{\decindexin} \scalarcoreat{\decindex} \cdot \left( \onehotmapof{\catindexof{\variableset}^\decindex} \otimes \onesof{[\atomorder]/\variableset^{\decindex}} \right) \, . 
%	\end{align*}
%	When regrouping the sum over the decomposition index by a sum over possible sets $\variableset\subset[\atomorder]$ and a sum over appearing index tuples $\catindexof{\variableset}$, this is a monomial decomposition of $\hypercore$ with
%		\[ \#\left(\indexset\right) = \# \left( \bigcup_{\variableset\subset[\atomorder]} \indexsetof{\variableset} \right) \, . \]
%	 % Monomial decomposition to CP Decomposition
%	 Vise versa we can construct a binary CP Decomposition given any monomial decomposition of $\hypercore$ 
%	 	\[ \hypercore 
%			= \sum_{\variableset\subset[\atomorder]} \sum_{\catindexof{\variableset}\in \indexsetof{\variableset}}  
%			\scalarcoreat{\variableset, \catindexof{\variableset}} \cdot \left( \onehotmapof{\catindexof{\variableset}} \otimes \onesof{[\atomorder]/\variableset} \right)  \, . \]
%	 To this end, we enumerate the set $\bigcup_{\variableset\subset[\atomorder]} \indexsetof{\variableset}$ by an additional index $\decindexin$ and define leg cores by
%	\begin{align*}
%		\legcoreof{\atomenumerator}_{\decindex} = 
%		\begin{cases} 
%			\onehotmapof{0} & \text{  if  }  \atomenumerator \in \variableset^{\decindex} \text{  and  } (\catindexof{\variableset}^\decindex)_\atomenumerator = 0 \\
%			\onehotmapof{1} & \text{  if  } \atomenumerator \in \variableset^{\decindex} \text{  and  } (\catindexof{\variableset}^\decindex)_\atomenumerator = 1 \\
%			\ones & \text{  if   } \atomenumerator \notin \variableset^{\decindex}
%		\end{cases} 
%	\end{align*}   
%	and a scalar core by coordinates
%		\[ \scalarcoreat{\decindex} = \scalarcoreat{\variableset^{\decindex},\catindexof{\variableset}^\decindex} \, . \]
%	Then, the monomial decomposition coincides with a basis CP Decomposition with the same dimension.
%	Thus, both $\bincprankof{\hypercore}$ and $\slicesparsityof{\hypercore}$ are the minima of identical sets and thus identical.
\end{proof}



\begin{example}[Propositional Formulas]
	When all leg dimensions of a binary tensor $\hypercore$ are $2$, we can interpret $\hypercore$ as a logical formula.
	We can use the binary CP decomposition of any tensor $\sechypercore$ with $\nonzeroof{\sechypercore}=\hypercore$ as a CNF of $\hypercore$.
	Finding the sparsest CNF thus amounts to finding the $\sechypercore$ with minimal $\slicesparsityof{\sechypercore}$ such that $\nonzeroof{\sechypercore}=\hypercore$.
\end{example}






\subsection{Representation involving selection architectures}

The set of slice-sparse tensors coincides with the expressivity of specific selection architecture.
We first define a slice selecting tensor and then show its decomposition into a formula selecting neural network.

\begin{definition}
	Given a set of atomic variables $\shortcatvariables$, a slice selecting tensor of maximal cardinality $\selorder$ is the tensor
		\[ \fselectionmapat{\shortcatvariables,\selvariableof{0,0},\ldots,\selvariableof{\selorder-1,0},\selvariableof{0,1},\ldots,\selvariableof{\selorder-1,1}} \]
	with dimensions
		\[ \seldimof{\selenumerator,0} = 2, \seldimof{\selenumerator,1} = \atomorder \]
	and coordinates
	\begin{align*}
		& \fselectionmapat{\shortcatvariables=\shortcatindices,\indexedselvariableof{0,0},\ldots,\indexedselvariableof{\selorder-1,0},\indexedselvariableof{0,1},\ldots,\indexedselvariableof{\selorder-1,1}} \\
		& \quad = \begin{cases}
			1 & \text{if} \quad 
			\forall_{\atomenumerator,\selenumerator} : \big(  \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \big) \rightarrow  \selindexof{\selenumerator,0} = \catindexof{\atomenumerator}  \\
			0
		\end{cases} \, . 
	\end{align*}
\end{definition}


\begin{lemma}\label{lem:sliceFromSliceSelector}
	If all input neurons with same selection index are agreeing have the same connective index, the selected formula does not vanish and coincides with a slice to the set
		\[ \sliceset = \{ \atomenumerator \, : \, \exists_{\selenumeratorin}: \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \} \]
	and
		\[ \catindexof{\atomenumerator} = \selindexof{\selenumerator,0} \quad \text{if} \quad \selindexof{\selenumerator,1} = \atomenumerator \, . \]
\end{lemma}


\begin{lemma}\label{lem:fsnnRepresentingSliceSelector}
	The slice selection tensor coincides with a formula selecting neural network with neurons
	\begin{itemize}
		\item unary input neuron enumerated by $\selenumerator$, selecting one of the $\shortcatvariables$ with the variable $\selvariableof{\selenumerator,1}$ and selecting a connective in $\{\lnot, \mathrm{Id}, \mathrm{True}\}$ by $\selvariableof{\selenumerator,0}$
		\item $\selorder$-ary output neuron fixed to the $\land$ connective.
	\end{itemize}
\end{lemma}
\begin{proof}
	This can be easily checked on each input coordinate.
\end{proof}








Let us now show, that the expressivity of the slice selecting neural network coincides with the set of tensors with 

\begin{theorem}
	Let $\fselectionmap$ be a slice selecting tensor.
	For any parameter tensor $\canparam$ we have
		\[ \bascprankof{\canparam} \geq \baspluscprankof{\contractionof{\fselectionmap},\canparam} \, . \]
\end{theorem}
\begin{proof}
	\red{Use the above lemma for that on each slice.}
	Show, how a coordinate of $\canparam$ corresponds with a slice determining tuple: $\sliceset$ determined by the selection indices.
\end{proof}


\begin{figure}[h]
	\begin{center}
		\input{PartIII/tikz_pics/sparse_tensor_calculus/slice_selecting_nn.tex}
	\end{center}
	\caption{Representation of a basis+ Tensor by the contraction of a parameter tensor $\canparam$ with a slice selecting architecture $\fselectionmap$, which has a decomposition as a formula selecting neural network (see Theorem~\ref{lem:fsnnRepresentingSliceSelector}).
	The nonzero coordinates of $\canparam$ represent the (see Lemma~\ref{lem:sliceFromSliceSelector}).}
\end{figure}\label{fig:sliceSelectingNN}


\subsubsection{Applications}

One application is as a parametrization scheme in the approximation of a tensor by a slice-sparse tensor, see Chapter~\ref{cha:tensorApproximation}.


\red{
The approximated parameter can then be used as a proxy energy to be maximized.
When choosing $\selorder=2$, the approximating tensor contains only quadratic slices, which then poses a QUBO problem.
}

\begin{remark}[Extension to arbitrary CP-formats]
	Select at each input neuron a specific leg.
	For finite number of legs, as it is the case in the binary, basis and basis+ formats, we can enumerate all possibilities by the selection variable.
	For the basis+ format, in case of binary leg dimensions, we here exemplified the approach, by enumerating the three possibilities $\onehotmapof{0},\onehotmapof{1},\onesat{1}$.
	This approach, however, fails as a generic representation of the directed format, since the directed legs are continuous and there therefore are infinite choosable legs.
\end{remark}





\subsection{Constructive Bounds on CP Ranks}

After having defined three CP Decompositions, let us investigate bounds on their ranks which proofs come with constructions of the cores.


\subsubsection{Format Transformations}

%% Case of binary legs
Especially useful, when the leg dimensions are two, where the slice decomposition shows decomposition of the tensor into monomials.


\begin{theorem}\label{the:sliceToCP}
	For any tensor $\hypercoreat{\shortcatvariables}\in\facspace$ we have
		\[ \cprankof{\hypercore} \leq \bincprankof{\hypercore} \leq \baspluscprankof{\hypercore} \leq \bascprankof{\hypercore} \, . \]
\end{theorem}	
\begin{proof}
	% First bound
	Since any CP decomposition into binary leg cores can be normed to a CP decomposition with directed leg cores, the first bound holds.
	% Second bound
	The second bound holds analogously, since any CP decomposition with basis leg cores is also a CP decomposition with binary leg cores.
\end{proof}

	%Taking only $\variableset=[\atomorder]$ and the index set being the nonzero coordinates of $\hypercore$ we get 
	%	\[ \slicesparsityof{\hypercore} \leq \#{\catindices: \hypercore_{\catindices}\neq 0} = \sparsityof{\hypercore} \, . \]

%% Tightness of the Bounds
Consider for example the tensor $\ones$ having maximal $\ell_0$-norm being the dimension of the tensor space, but, since it is elementary, a CP decomposition with rank $1$.


\subsubsection{Summation of CP Decompositions}

\begin{theorem}\label{the:CPrankSumBound}
	For any collections of tensors $\{T^{l}[\catvariableof{\nodes}] : l \in [n]\}$ with identical variables and scalars $\lambda^{l} \in \rr$ for $l\in[n]$  we have
		\[ \cprankof{\sum_{l \in [n]} \lambda^{l} \cdot T^{l}} \leq \sum_{l\in[n]}  \cprankof{T^{l}}  \, . \]
	The bound still holds, when we replace on both sides $\cprankof{\cdot}$ by $\bincprankof{\cdot}$, by $\bascprankof{\cdot}$ or by $\baspluscprankof{\cdot}$.
\end{theorem}
\begin{proof}
	Products with scalars do not change the rank, since they just rescale the core $\scalarcore$.
	The sum of CP Decomposition is just the combination of all slices, thus the rank is at most additive.
\end{proof}

\subsubsection{Contractions of CP Decompositions}

More general, we can bound the sparsity of any contraction by the product of sparsities of affected tensors.

\begin{theorem}\label{the:CPrankContractionBound}
	For any tensor network with variables $\nodes$ and edges $\edges$ we have for any subset $\secnodes\subset\nodes$
		\[ \cprankof{\contractionof{\{\hypercoreof{\edge} : \edge\in\edges \}}{\secnodes}} \leq 
		\prod_{\edge\in\edges \, : \, \secnodes\cap\edge \neq \varnothing} \cprankof{\hypercoreof{\edge}} \, . \]
	The bound still holds, when we replace on both sides $\cprankof{\cdot}$ by $\bincprankof{\cdot}$, by $\bascprankof{\cdot}$ or by $\baspluscprankof{\cdot}$.
\end{theorem}


Remarkably, in Theorem~\ref{the:CPrankContractionBound} the upper bound on the CP rank is build only by the ranks of the tensor cores, which have remaining open edges.
We prepare for its proof by first showing the following Lemmata.

\begin{lemma}\label{lem:sparsityGeneralContraction}
	For any tensors $\hypercoreofat{1}{\catvariableof{\nodes_1}}$ and $\hypercoreofat{2}{\catvariableof{\nodes_2}}$ and any set of variables $\secnodes\subset\nodes_1\cup\nodes_2$ we have
		\[ \cprankof{\contractionof{\{\hypercoreof{1},\hypercoreof{2}\}}{\secnodes}} \leq \cprankof{\hypercoreof{1}} \cdot \cprankof{\hypercoreof{2}} \, . \]
	The bound still holds, when we replace on both sides $\cprankof{\cdot}$ by $\bincprankof{\cdot}$, by $\bascprankof{\cdot}$ or by $\baspluscprankof{\cdot}$.
\end{lemma}
\begin{proof}
	By connecting the cores and restoring the binary or basis properties.
\end{proof}

When one core of the contracted tensor network does not contain variables which are left open, we can drastically sharpen the bound provided by Lemma~\ref{lem:sparsityGeneralContraction} as we show next.

\begin{lemma}\label{lem:sparsityDisjointContraction}
	For any tensor network consistent of two tensors $\hypercoreofat{1}{\catvariableof{\nodes_1}}$ and $\hypercoreofat{2}{\catvariableof{\nodes_2}}$ and any set $\secnodes$ with $\secnodes\cap\nodes_2=\varnothing$ we have
		\[ \cprankof{\contractionof{\{\hypercoreof{1},\hypercoreof{2}\}}{\secnodes}} \leq \cprankof{\hypercoreof{1}} \, . \]
	The bound still holds, when we replace on both sides $\cprankof{\cdot}$ by $\bincprankof{\cdot}$ or by $\bascprankof{\cdot}$.
\end{lemma}
\begin{proof}
	We show the lemma by constructing a CP decomposition of $\cprankof{\contractionof{\{\hypercoreof{1},\hypercoreof{2}\}}{\secnodes}} $ for any CP decomposition of $\hypercoreof{1}$.
	Let therefore take any CP decomposition of $\hypercoreof{1}$ consistent of the leg cores $\{\legcoreof{\node} \, : \, \node \in \nodes_1 \}$ and a scalar core $\scalarcore$.
	Then we define a new $\scalarcore$ by
		\[ \tilde{\scalarcore} = \contractionof{\{\scalarcore\}\cup \{\legcoreof{\node} \, : \, \node \in \nodes_1 , \node \notin \secnodes \} \cup \{\hypercoreof{2}\} }{\decvariable} \, . \]
	Then, the leg cores $\{\legcoreof{\node} \, : \, \node \in \secnodes \}$ build with the scalar core $\tilde{\scalarcore}$ a CP decomposition of $\contractionof{\{\hypercoreof{1},\hypercoreof{2}\}}{\secnodes}$.
	% Binary of basis
	When the CP decomposition of $\hypercoreof{1}$ was binary, basis or basis+, this property is also satisfied by the constructed CP decomposition.
	Thus the bound also holds for the ranks $\bincprankof{\cdot}$ or $\bascprankof{\cdot}$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{the:CPrankContractionBound}]
	Use delta tensor representation to represent contractions by graphs.
	We then iterate through the cores and contract them to the previously contracted tensor, where we apply Lemma~\ref{lem:sparsityGeneralContraction} when the tensor core has variables left open and Lemma~\ref{lem:sparsityDisjointContraction} if not.
\end{proof}


\begin{example}[Composition of formulas with connectives]
	For any formula $\exformula$ we have $1-\exformula$ = $\lnot\exformula$.
	The CP rank bound brings an increase by at most factor $2$ when taking the contraction with $\concoreof{\lnot}$ which has slice sparsity of $2$.
	This is not optimal, since $\lnot\exformula$ has at most an absolute slice sparsity increase of $1$.
	
	For any formulas $\exformula$ and $\secexformula$ we have $\exformula\cdot\secexformula = \exformula\land\secexformula$.
	Here the CP rank bounds on contractions can also be further tightened.
\end{example}


\begin{example}[Distributions of independent variables]
	Independence means factorization, conditional independence means sum over factorizations.
	Again, the $\ell_0$ norm is bounded by the product of the $\ell_0$ norm of the factors.
\end{example}


\subsubsection{Sparse Encoding of Functions}

%Using the proof idea of Theorem~\ref{the:sparseBasisCP}, we can state a more general CP bound on the encoding of functions.

We now state that the basis CP rank of relational encodings is equal to the cardinality of the domain.
The basis CP format can therefore not provide a sparse representation when the factored system contains many categorical variables.

\begin{theorem}\label{the:rencodingBasCP}
	For any function
		\[ \exfunction : \facstates \rightarrow  \secfacstates \]
	between factored systems we have
		\[ \bascprankof{\rencodingof{\exfunction}} =  \facdim \, . \]
\end{theorem}
\begin{proof}
	With Theorem~\ref{the:sparseBasisCP}, the basis CP rank coincides with the number of not vanishing coordinates, which is the cardinality of the domain of $\exfunction$.
\end{proof}

Allowing for trivial leg vectors can decrease the CP rank, as we show next.

\begin{theorem}
	We have
		\[ \baspluscprankof{\rencodingof{\exfunction}} \leq  \sum_{y \in \imageof{\exfunction}} \baspluscprankof{\ones_{\exfunction == y} } \, , \]
	where by $\ones_{\exfunction == y} $ we denote the indicator, whether the function $\exfunction$ evaluates to $y$.
\end{theorem}
\begin{proof}
	We have
		\[ \rencodingof{\exfunction} = \sum_{y \in \imageof{\exfunction}} \ones_{\exfunction == y}[\catvariable]  \otimes \onehotmapofat{y}{\catvariableof{\exfunction}} \, . \]
	For each $y \in \imageof{\exfunction}$ we represent $\onehotmapofat{y}{\catvariableof{\exfunction}}$ in an basis+ CP format with $\baspluscprankof{\ones_{\exfunction == y} } $ summands and arrive at a basis+ CP decomposition of $\rencodingof{\exfunction}$ with $\sum_{y \in \imageof{\exfunction}} \baspluscprankof{\ones_{\exfunction == y} } $ summands.
\end{proof}

The above claim still holds when replacing $\baspluscprankof{\cdot}$ with the ranks $\bascprankof{\cdot}$ or $\bincprankof{\cdot}$.
For the rank $\bascprankof{\cdot}$ it leads to the bound of Theorem~\ref{the:rencodingBasCP}, since summing the number of non zero coordinators of the indicators is the cardinality of the domain.

\begin{example}{Conjunction of variables}
	For the propositional formula $\exformula = \catvariableof{0} \land \catvariableof{1}$ we have
		\[ \rencodingofat{\exformula}{\catvariableof{0},\catvariableof{1}}
		 = \onehotmapofat{1,1}{\catvariableof{0},\catvariableof{1}} \otimes \onehotmapofat{1}{\catvariableof{\exformula}}
		  +  (\onesat{\catvariableof{0},\catvariableof{1}} - \onehotmapofat{1,1}{\catvariableof{0},\catvariableof{1}}) \otimes \onehotmapofat{0}{\catvariableof{\exformula}}  \]
	and thus 
		\[ \baspluscprankof{\rencodingof{\exformula}} \leq 3\]
	while $\bascprankof{\rencodingof{\exformula}} = 4$.
	\red{Especially useful for $d$-ary conjunctions, see Remark~\ref{rem:naryConnectives}!}
\end{example}




\subsubsection{Construction by averaging the incoming legs}

Basis CP Decompositions can be constructed by understanding the variable $\indvariableof{\insymbol}$ of the relational encoding of a function $\exfunction:\inset \rightarrow \outset$ as the slice selection variable.

\begin{example}{Empirical distributions, see Theorem~\ref{the:empCPRep}}
	Let there be a data map 
		\[ \datamap : [\datanum] \rightarrow \facstates \, . \]
	We can use Theorem~\ref{the:functionDecompositionBasisCP} to find a tensor network representation fo $\rencodingof{\datamap}$ as
	\begin{align*}
		\rencodingofat{\datamap}{\catvariable,\shortcatvariables}  
		= \contractionof{
		\{\rencodingofat{\datamap^{\atomenumerator}}{\catvariable,\catvariableof{\atomenumerator}} : \atomenumeratorin \} 
		}{\catvariable,\shortcatvariables} \, . 
	\end{align*}
	This representation is in the CP format, when adding trivial scalar core and and delta tensor to the data index.
	It is furthermore in a basis CP format, since all $\rencodingof{\datamap^{\catenumerator}}$ are directed and binary tensors.
	Normation to get the empirical distribution amounts to setting a slice core with coordinates $\frac{1}{\datanum}$.
\end{example}





%\subsection{Manipulations of Binary CP Decomposition}\label{sec:BinaryCPManipulation}
%
%Since the coordinates on the legs are binary, operations like contractions, slicing and marginalization are especially efficient given binary CP Decompositions.
%
%
%\begin{example}[Hypertrie Format]
%	Hypertries are another efficient implementation of the slicing operations.
%%	Hypertries make use of the Basic TT Decomposition, given any permutation of the tensor legs.
%%	In addition, they eliminate storage redundancies when representing all permuted TT Decompositions, by referencing to same subnetworks (e.g. when slicing wrt leg 1 and then 2 or slicing wrt to 2 and then leg 1 will leave the same tensors to be further decomposed).
%\end{example}






%%% NEEDED?
%\subsection{Basis Tensor Networks}
%
%
%\begin{definition}
%	We call a tensor network, which cores are directed and binary a basis tensor network. %also acyclic?
%\end{definition}
%
%%\begin{definition}
%%	We call a Tensor Network with open legs $V$ basis, when for any tensor core in the network any slicing of the closed legs is parallel to a basic tensor (that is has $\ell_0$ norm of at most $1$).
%%\end{definition}
%
%
%\subsubsection{Basis elementary decomposition}
%
%Elementary tensors are tensor products of vectors.
%Demanding each vector in the product to be a basis vector leads to basis tensors.
%Thus the tensors which poses a basis elementary decompositions coincide with the basis tensors.
%
%\begin{theorem}
%	Given axis dimensions $\catdimof{\atomenumerator}\in\mathbb{N}$ for $\atomenumeratorin$, the one-hot encoding is a bijection between $\facstates$ and the basis tensors of $\facspace$ with unit norm.
%\end{theorem}
%\begin{proof}
%	The one-hot encoding to the basis tensors is injective, since each state is mapped to a different basis tensor.
%	The one-hot encoding is further surjective, since every basis tensor has a preimage state by the indices of the $1$ coordinate.
%	Therefore the one-hot encoding is a bijection.
%\end{proof}
%
%
%\subsubsection{Basis CP Decomposition}\label{sec:basisCP}
%
%We here provide with the CP Decomposition of binary tensors as ways to overcome the storage overhead of $\ell_0$-sparse (of tensor flattening) tensors.
%The key idea is to enumerate the nonzero coordinates by introducing an additional axis carrying the data index.
%Keeping the such introduced hidden rank as constant then results in an elementary tensor, which has a representation with linear demand.
%We can thus represent the vectors of each such elementary tensor in a matrix and get the cores of the CP decomposition.
%
%
%\subsubsection{Basis TT Decomposition}
%
%Exploiting vanishing slices, thus exploiting a form of block $\ell_0$-sparsity (where full slizes are vanishing).
%
%Can be generated from the basic CP decomposition, by the CP cores contracted with partial $\delta$ tensors.
%The rank will thus not be larger than the rank of the $\cpformat$.
%
%
%
%\subsubsection{Basic HT Decomposition}
%
%The constraint that networks need to be basic just affects the leaf cores.






\subsection{Subspaces of formulas}\label{sec:HT}

\red{
Formula Tensors have Tensor Network decomposition, which are best represented in a $\htformat$ decomposition.
We here describe this perspective and show applications of this formalism in the recovery/learning of formula tensors.
}
The decomposition of formula tensors is basic, since atomic formula tensors being on the leafs consist of basic vectors in the respective legs.


\subsubsection{Formula Subspaces}

Each formula tensor defines the subspace of $\atomspace$
\begin{align}
	\subspaceof{\exformula} = \mathrm{span} \left\{ \lnot\exformula,\exformula \right\} = \mathrm{im}\left(\ftensorof{\exformula} \right)
	%\mathrm{span} \left\{ \braket{\atombasisvector_1,\ftensorof{\exformula}}, \braket{\atombasisvector_0,\ftensorof{\exformula}} \right\}
\end{align}

Let us notice that the spanning vector of the subspace $\subspaceof{\exformula}$ are binary tensors summing up to the tensor of ones.

%\subsubsection{Atomic Tensor Spaces}

For each atom $\atomicformulaof{\atomenumerator}$ we have
	\[ \subspaceof{\atomicformulaof{\atomenumerator}} = \rr^2 \, . \]
The tensor space carrying the factored representation of the worlds is thus
\begin{align}
	\bigotimes_{\atomenumeratorin}\subspaceof{\atomicformulaof{\atomenumerator}} \, .
\end{align}

\subsubsection{Formula Decomposition as a Subspace Choice}

Given a formula $\exformula\exconnective\secexformula$ composed of formulas $\exformula$ and $\secexformula$ containing different atoms we have
\begin{align}
	\subspaceof{\exformula\exconnective\secexformula} 
	\subset \subspaceof{\exformula} \otimes \subspaceof{\secexformula}
\end{align}

A connective $\exconnective$ thus determines the selection of a two-dimensional subspace in the four-dimensional tensor product of subspaces to both subformulas.

%\subsubsection{Approximation problems}

Reconstruction of a formula given its formula tensor amounts to finding the HT Decomposition under the constraints of subspace choices according to the allowed logical connectives.

Given a set of positive and negative examples of a formula poses further an approximation problem of the examples by a HT Decomposition.

Advantages of this perspective are
\begin{itemize}
	\item Given a $\htformat$ the best approximation always exists (Theorem 11.58 in \cite{hackbusch_tensor_2012}), but need to further restrict to cores given by logical connectives 
	\item Apply Approximation algorithms: ALS or HOSVD
\end{itemize}

















