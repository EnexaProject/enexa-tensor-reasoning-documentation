\section{Sparse Tensor Representations}\label{cha:sparseTC}

We in this chapter investigate, which sparsity notations enable tensors to be representable as contractions of tensor networks.


\subsection{CP Formats}

The CP Decomposition is one way to generalize the ranks of matrices to tensors.
It is oriented on the Singular Value Decomposition of matrices, providing a representation of the matrix as a weighed sum of the tensor product of singular vectors.
Given a tensor of higher order, each such tensor product is over multiple vectors, 

\begin{definition}\label{def:cpFormats}
	A CP Decomposition of rank $\decdim$ of a tensor $\hypercore\in\facspace$ is a collections of tensors $\scalarcoreat{\decvariable}$ and $\legcoreofat{\atomenumerator}{\decvariable,\catvariableof{\atomenumerator}}$ for $\atomenumeratorin$, where $\decvariable$ takes values in $[\decdim]$, such that
		\[  \hypercoreat{\shortcatvariables}
		= \contractionof{
		\{\scalarcoreat{\decvariable}\} \cup \{ \legcoreofat{\atomenumerator}{\decvariable,\catvariableof{\atomenumerator}} \, : \, \atomenumeratorin \}
		}{\shortcatvariables} \, . 
		\]
%	where for each $\decindexin$ and $\atomenumeratorin$ we have $\scalarcoreat{\decindex} \in \rr$ and $\legcoreof{\atomenumerator,\decindex}\in\rr^{\catindexof{\atomenumerator}}$.
	We say that the CP Decomposition is
	\begin{itemize}
		\item directed, when for each $\atomenumerator$ the core $\legcoreof{\atomenumerator}$ is directed with $\decvariable$ incoming and $\catvariableof{\atomenumerator}$ outgoing.
		\item binary, when for each $\atomenumerator$ the core $\legcoreof{\atomenumerator}$ is binary.
		\item basis, where we demand both properties, that is for each $\atomenumeratorin$ and $\decindexin$ 
			\[ \legcoreofat{\atomenumerator}{\inddecvar,\catvariableof{\atomenumerator}}\in \{ \onehotmapofat{[\catindexof{\atomenumerator}]}{\catvariableof{\atomenumerator}} \catindexof{\atomenumerator}\in[\catdimof{\atomenumerator}] \}\, . \]
		\item basis+, when for each $\atomenumeratorin$ and $\decindexin$  %$\legcoreof{\atomenumerator,\decindex}\in\onehotmapof{[\catindexof{\atomenumerator}]}$ or $\legcoreof{\atomenumerator,\decindex}=\ones$.
			\[ \legcoreofat{\atomenumerator}{\inddecvar,\catvariableof{\atomenumerator}}\in \{ \onehotmapofat{[\catindexof{\atomenumerator}]}{\catvariableof{\atomenumerator}} \catindexof{\atomenumerator}\in[\catdimof{\atomenumerator}] \} \cup \{\onesat{\catvariableof{\atomenumerator}}\}\, . \]
	\end{itemize}
	We denote by $\cprankof{\hypercore}$, respectively $\bincprankof{\hypercore}$, $\bascprankof{\hypercore}$ and $\baspluscprankof{\hypercore}$ the minimal cardinality such that $\hypercore$ has a CP Decomposition with directed cores, respectively binary cores, basis cores and basis+ cores.
\end{definition}

% Sum of elementary tensors
We have by definition
	\[ \hypercoreat{\shortcatvariables} = \sum_{\decindexin} \scalarcoreat{\inddecvar} \left( \bigotimes_{\atomenumeratorin} \legcoreofat{\atomenumerator}{\inddecvar,\catvariableof{\atomenumerator}} \right) \, . \]
The right side can be seen as an alternative definition of CP Decompositions by summations of elementary tensors.


\begin{figure}[h]
	\begin{center}
		\input{PartIII/tikz_pics/sparse_tensor_calculus/cp_decomposition.tex}
	\end{center}
	\caption{Tensor Network diagram of a generic CP decomposition (see Definition~\ref{def:cpFormats})}
\end{figure}

We introduce different notions of sparsities based on CP Decomposition with different properties of their leg cores.

\subsubsection{Directed Leg Cores}

This is the canonical CP Decomposition, where the vectors $\legcoreof{\atomenumerator,\decindex}$ are interpreted as generalized singular vectors.
Any CP decomposition can be transformed into a directed CP decomposition without enlarging the index set $\indexset$, simply by diving the vectors by their norms and multiplying it to $\scalarcoreat{\inddecvar}$.

% Directionality
We then have a partially directed Tensor Network representing the decomposed tensor.
The only undirected core is $\scalarcore$, since we do not demand it to be normed.
In many applications applications, however, also the $\scalarcore$ is directed with a single outgoing leg (see for example the empirical distributions as discussed in Section~\ref{sec:empDistribution}).
In that case, also the decomposed tensor is directed with outgoing legs.



\subsubsection{Basis Leg Cores}\label{sec:basisCP}

% From FOL Chapter: Bayesian Network interpretation of Basis CP
%	The basis CP can further be understood as a Bayesian network, where we understand $\dataindex$ as condition and each decomposition core as a conditional probability distribution.
%	We notice that in this interpretation the direction of the dependency is inversed compared with previous representation of grounding tensors in Figure~\ref{fig:groundingCP}. 


Directed and binary leg cores have incoming slices being basis vectors, we thus call them basis CP Decomposition.
This allows the interpretation of the directed and binary CP decomposition in terms of mapping to nonzero coordinates.
We start by defining the number of nonzero coordinates of tensors by the $\ell_0$-norm.

\begin{definition}
	The $\ell_0$-norm counts the nonzero coordinates of a tensor by
		\[ \sparsityof{\hypercore} = \#\big\{ \catindices \, : \, \hypercore_{\catindices }\neq 0 \big\} \, . \]
\end{definition}

The $\ell_0$-norm is not a proper norm itself, but the limit of $\ell_p$-norms (where $p \rightarrow 0$) of the flattened tensor (which are norms for $p\geq1$).

% Interpretation
The $\ell_0$ norm is the number of nonzero coordinates. 
We understand the leg cores as the relational encoding of functions mapping to the slices of these coordinates given an enumeration.
This is consistent with the previous analysis of Chapter~\ref{cha:directedTC}, where we characterized binary and directed cores by the encoding of associated functions.
Based on this idea, we can proof, that any tensor has a directed and binary CP decomposition with rand $\sparsityof{\hypercore}$.


\begin{theorem}\label{the:sparseBasisCP}
	For any tensor $\hypercore$ we have
		\[ \bascprankof{\hypercore} = \sparsityof{\hypercore} \, .  \]	
\end{theorem}
\begin{proof}
	We find a map 
		\[ \exfunction : [\sparsityof{\hypercore}] \rightarrow  \facstates \, , \] 
	which image is the set of nonzero coordinates of $\hypercore$.
	Denoting its image coordinate maps by $\exfunction^{\atomenumerator}$ we have
		\[ \hypercore = \sum_{\dataindexin} \scalarcoreof{\exfunction(\dataindex)} \left( \bigotimes_{\atomenumeratorin} \onehotmapof{\exfunction^\atomenumerator(\dataindex)} \right) \, . \]
	This is a basis CP Decomposition with rank $\sparsityof{\hypercore}$.
	Conversely, any basis CP Decomposition of $\hypercore$ with dimension $r$ would have at most $r$ coordinates different from zero and thus $\sparsityof{\hypercore}\leq r$.
	Thus, there cannot be a CP Decomposition with a dimension $r\leq\sparsityof{\hypercore}$.
\end{proof}

%
The next theorem relates the basis CP decomposition with encodings of $\atomorder$-ary relations (see Definition~\ref{def:daryRelation}).

\begin{theorem}
	If any only if $\hypercore\in\facspace$ has a basis decomposition with slices $\{\catindex_{[\atomorder]}^{\decindex} \, : \, \decindexin \}$ and trivial cores, it coincides with the encoding of the $\atomorder$-ary relation 
		\[ \exrelation = \{\catindex_{[\atomorder]}^{\decindex} \, : \, \decindexin \} \, . \]
%	To each basis CP decompositions with pairwise different slices and trivial scalar cores we find a $d$-ary relation, such that 
\end{theorem}


If in addition $\catdimof{\atomenumerator}=2$, we can interpret basis CP decompositions as propositional formulas.

% Knowledge Bases
\begin{theorem}
	If $\hypercore\in\atomspace$ has a basis decomposition with slices $\{\catindex_{[\atomorder]}^{\decindex} \, : \, \decindexin \}$ and trivial cores, it coincides with the propositional formula
		\[ \formulaat{\shortcatvariables} = 
		\bigvee_{\decindexin} \termof{\catindex_{[\atomorder]}^{\decindex}} \, . \]
\end{theorem}
\begin{proof}
	This is a generalization of Theorem~\ref{the:maximalClausesRepresentation}, which follows from Theorem~\ref{the:tensorToMaxMinTerms}.
\end{proof}


% Storage
The storage demand of any CP decomposition is at most linear in the dimension and the sum of its leg dimension.
When we have a basis CP decomposition, this demand can be further improved.
The basis vectors can be stored by its preimage of the one hot encoding $\onehotmapof{\cdot}$, that is the number of the basis vector in $[\catdim]$.
This reduces the storage demand of each basis vector to the logarithms of the space dimension without the need of storing the full vector.

% Matrix Representation
More precisely, we can store the CP Decomposition by the matrix
	\[ \matrixat{\decvariable,\selvariable} \in \rr^{\datanum \times (\atomorder+1)} \]
defined for $\atomenumeratorin$
	\[ \matrixat{\inddecvar,\selvariable=\atomenumerator} 
	= \invonehotmapof{\legcoreofat{\atomenumerator}{\inddecvar,\catvariableof{\atomenumerator}}}\]
and
	\[ \matrixat{\inddecvar,\selvariable=\atomorder}  
	= \scalarcoreat{\inddecvar} \, . \]
	
This is a typical tabular format to store relational databases.

\subsubsection{Basis+ Leg Cores}

The minimal rank of CP Decompositon is closely related to polynomial sparsity of the map $\hypercore$, which we will define next.

\begin{definition}\label{def:polynomialSparsity}
	A monomial decomposition of a tensor $\hypercore\in\facspace$ is a set $\sliceset$ of tuples $\slicetupleof{}$ where $\slicescalar\in\rr, \variableset\subset[\atomorder]$ and $\catindexof{\variableset}\in\bigtimes_{\atomenumerator\in\variableset} [\catdimof{\atomenumerator}]$ such that
	\begin{align}\label{eq:decIntoMonomials}
		\hypercoreat{\shortcatvariables} = \sum_{\slicetupleof{}\in\sliceset} \slicescalar \cdot \contractionof{\onehotmapof{\catindexof{\variableset}}}{\shortcatvariables} \, .
	\end{align}
	For any tensor $\hypercore\in\facspace$ we define its polynomial sparsity of order $\sliceorder$ as
	\begin{align*}
		\slicerankwrtof{\sliceorder}{\hypercore} =
		 \min \left\{ \cardof{\sliceset} \, : \, 
		 	\hypercoreat{\shortcatvariables} = \sum_{\slicetupleof{}\in\sliceset} \slicescalar \cdot \contractionof{\onehotmapof{\catindexof{\variableset}}}{\shortcatvariables} \, , \, \forall_{\slicetupleof{}\in\sliceset} \cardof{\variableset} \leq \sliceorder \, . 
		 \right\}
	\end{align*}
\end{definition}


% Explanation of monomials
We refer to the terms in a decomposition \eqref{eq:decIntoMonomials} in Definition~\ref{def:polynomialSparsity} as monomials of binary variables, which are enumerated by pairs $(\atomenumerator,\catindexof{\atomenumerator})$ and indicate whether the variable $\catvariableof{\atomenumerator}$ is in state $\catindexof{\atomenumerator}\in[\catdimof{\atomenumerator}]$.
Such indicators are represented by the one-hot encodings
	\[ \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}} \, . \]
The monomial of multiple such binary variables indicated, whether all variables labelled by a set $\variableset$ are in the state $\catvariableof{\variableset}$, which is represented by
	\[ \onehotmapofat{\catindexof{\variableset}}{\catvariableof{\variableset}} = \bigotimes_{\atomenumerator\in\variableset} \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}}  \, . \]
The states of the variables labeled by $\atomenumerator\in[\atomorder]/\variableset$ are not specified in the monomial and the monomial is trivially extended to
	\[ \contractionof{\onehotmapof{\catindexof{\variableset}}}{\shortcatvariables}  = \onehotmapofat{\catindexof{\variableset}}{\catvariableof{\variableset}} \otimes \onesat{\catvariableof{[\atomorder]/\variableset}} \, .   \]


% Infinity
For some $\sliceorder<\catorder$ there are tensors $\hypercoreat{\shortcatvariables}$, which do not have a monomial decomposition of order $\sliceorder$.
In that case we the minimum is over an empty set and we define $\slicerankwrtof{\sliceorder}{\hypercore}=\infty$.
We characterize in the next theorem the set of tensors with monomial decompositions.


\begin{theorem}\label{the:polynomialSubspaces}
	For any $\atomorder, \sliceorder$, the set of tensors of $\catorder$ variables with leg dimension $\catdim$, which have a monomial decomposition of order $\sliceorder$, is a linear subspace $\subspaceof{\atomorder,\sliceorder}$ with dimension
		\[ \subspacedimof{\subspaceof{\atomorder,\sliceorder}} \leq  \sum_{s \in [\sliceorder]} \catdim^s \binom{\catorder}{s} \, .  \]
\end{theorem}
\begin{proof}
	Any sum of tensors with a monomial decomposition of order $\sliceorder$ admits again a monomial decomposition, which is the concatenation of both.
	The same holds for a scalar multiplication, and thus, the sets of such tensors form a linear subspace.

	The number of different tensors $\contractionof{\onehotmapof{\catindexof{\variableset}}}{\shortcatvariables}$ is
		\[  \sum_{s \in [\sliceorder]} \catdim^s \binom{\catorder}{s}  \]
	Since any tensor with a monomial decomposition is a weighted sum of those, 
	
	We notice, that the set of slices is in general not linear independent, and therefore forms a frame and not a linear basis \cite{casazza_introduction_2013}.
	The number of elements in the frame is therefore an upper bound on the dimension.
\end{proof}


% Infinite rank
Theorem~\ref{the:polynomialSubspaces} implies, that the tensors admiitting a monomial decomposition of a small order build a low-dimensional subspace in the $\catdim^\catorder$ dimensional space of tensors, since for $\sliceorder << \catorder $ we have
	\[ \subspacedimof{\subspaceof{\atomorder,\sliceorder}} << \catdim^{\catorder} \, . \]
If $\sliceorder\geq\catorder$, we always find a monomial decomposition by an enumeration of nonzero coordinates.
In the next theorem, we show that in that case the $\slicerankwrtof{\sliceorder}{\hypercore}$ furthermore coincides with the basis+ CP rank $\baspluscprankof{\hypercore}$.

\begin{theorem}
	For any tensor $\hypercore\in\facspace$ we have
		\[ \slicesparsityof{\hypercore} = \baspluscprankof{\hypercore} \, . \]
	When $\catindexof{\atomenumerator}=2$ for all $\atomenumeratorin$, we also have
		\[ \bincprankof{\hypercore} = \slicesparsityof{\hypercore}  \, . \]
\end{theorem}
\begin{proof}
	To proof the first claim, we construct a basis+ CP decomposition given a monomial decomposition and vice versa.
	Let there be a tensor $\hypercoreat{\shortcatvariables}$ with a monomial decomposition by $\sliceset$ with $\cardof{\sliceset}=m$ and let us enumerate the elements in $\sliceset$ by $\slicetupleof{\decindex}$ for $\decindexin$.
	 We define for each $\atomenumeratorin$ the tensors
	 \begin{align*}
		\legcoreofat{\atomenumerator}{\decvariable,\catvariableof{\atomenumerator}}
		 = \left( \sum_{\decindexin \, : \, \atomenumerator\in\variableset} \onehotmapofat{\decindex}{\decvariable} \otimes \onehotmapofat{\catindexof{\atomenumerator}^{\decindex}}{\catvariableof{\atomenumerator}} \right)
		 + \left(\sum_{\decindexin \, : \, \atomenumerator\notin\variableset} \onehotmapofat{\decindex}{\decvariable} \otimes \onesat{\catvariableof{\atomenumerator}} \right)
	\end{align*}
	and 
	\begin{align*}
		\scalarcoreat{\decvariable} = \sum_{\decindexin} \slicescalar^{\decindex} \cdot \onehotmapofat{\decindex}{\decvariable}
	\end{align*}
	 and notice that	 
	\begin{align*}
		\hypercoreat{\shortcatvariables} 
		& = \sum_{\decindexin} \slicescalar^{\decindex} \cdot \contractionof{\onehotmapof{\catindexof{\variableset}^{\decindex}}}{\shortcatvariables} \\
		& = \sum_{\decindexin} \left(  \scalarcoreat{\inddecvar} \cdot \bigotimes_{\atomenumeratorin} \legcoreofat{\atomenumerator}{\inddecvar, \catvariableof{\atomenumerator}} \right) \\
		& = \contractionof{
		\{\scalarcoreat{\decvariable}\} \cup \{\legcoreofat{\atomenumerator}{\decvariable,\catvariableof{\atomenumerator}} \, : \, \atomenumeratorin \}
		}{\shortcatvariables} \, . 
	\end{align*}
	By construction this is a basis+ CP decomposition with rank $\decdim$.
	Since any monomial decomposition can be transformed into a basis+ CP decomposition with same rank we have
	\begin{align*}
		\slicesparsityof{\hypercore} \geq \baspluscprankof{\hypercore} \, . 
	\end{align*}
	
	Let there now be a basis+ CP decomposition we define for each $\decindexin$ 
	\begin{align*}
		\variableset^{\decindex} = \{\atomenumeratorin : \legcoreofat{\atomenumerator}{\inddecvar, \catvariableof{\atomenumerator}} \neq \onesat{\catvariableof{\atomenumerator}} \}
		 \quad \text{and} \quad 
		 \catindexof{\variableset}^{\decindex} = \{\invonehotmapof{\legcoreofat{\atomenumerator}{\inddecvar, \catvariableof{\atomenumerator}} } \, : \atomenumerator\in\variableset\}
	\end{align*}
	where by $\invonehotmapof{\cdot}$ we denote the inverse of the one-hot encoding.
	
	We notice that this is a monomial decomposition of $\hypercoreat{\shortcatvariables}$ to the tuple set
	\begin{align*}
		\sliceset = \{(\scalarcoreat{\inddecvar}, \variableset^{\decindex}, \catindexof{\variableset^{\decindex}}^{\decindex} ) \, : \, \decindexin \} \, . 
	\end{align*}
	It follows from this that
	\begin{align*}
		\slicesparsityof{\hypercore} \leq \baspluscprankof{\hypercore} \, 
	\end{align*}
	and the first claim is shown.
	
	The second claim follows from the observation, that whenever $\catindexof{\atomenumerator}=2$ for all $\atomenumeratorin$ the binary CP decompositions with non-vanishing slices $\legcoreofat{\atomenumerator}{\inddecvar, \catvariableof{\atomenumerator}}$ for $\atomenumeratorin$ and $\decindexin$ are also basis+ CP decompositions and vice versa.
%
%	
%
%	We proof the claim by establishing a one-to-one map between any binary CP decomposition of a a tensor $\hypercore$ and a monomial decomposition of $\hypercore$.
%	% CP Decomposition to monomial decomposition
%	Let there be a binary CP Decomposition of $\hypercore$ with the leg tensors $\{\legcoreof{\atomenumerator}\, :\, \atomenumeratorin\}$ and the scalar core $\scalarcore$.
%	For any index $\decindexin$ and $\atomenumeratorin$ we have
%		\[ \legcoreof{\atomenumerator}_{\decindex} \in \{\onehotmapof{0},\onehotmapof{1},\ones\} \, . \]
%	We define for any $\decindexin$ the sets 
%		\[ \variableset^{\decindex}=\big\{\atomenumerator \, : \, \legcoreof{\atomenumerator}_{\decindex} \in \{\onehotmapof{0},\onehotmapof{1}\}  \big\}\]
%	and an index tuple $\catindexof{\variableset}^\decindex \in \bigotimes_{\atomenumerator\in\variableset}[2]$ by
%	\begin{align*}
%		(\catindexof{\variableset}^\decindex)_\atomenumerator = 
%		\begin{cases} 
%			0 & \text{  if  } \legcoreof{\atomenumerator}_{\decindex} = \onehotmapof{0} \\1 & \text{  if  } \legcoreof{\atomenumerator}_{\decindex} = \onehotmapof{1} 
%		\end{cases} \, . 
%	\end{align*}   
%	Then we have by construction that 
%	\begin{align*}
%		\hypercore = \sum_{\decindexin} \scalarcoreat{\decindex} \cdot \left( \onehotmapof{\catindexof{\variableset}^\decindex} \otimes \onesof{[\atomorder]/\variableset^{\decindex}} \right) \, . 
%	\end{align*}
%	When regrouping the sum over the decomposition index by a sum over possible sets $\variableset\subset[\atomorder]$ and a sum over appearing index tuples $\catindexof{\variableset}$, this is a monomial decomposition of $\hypercore$ with
%		\[ \#\left(\indexset\right) = \# \left( \bigcup_{\variableset\subset[\atomorder]} \indexsetof{\variableset} \right) \, . \]
%	 % Monomial decomposition to CP Decomposition
%	 Vise versa we can construct a binary CP Decomposition given any monomial decomposition of $\hypercore$ 
%	 	\[ \hypercore 
%			= \sum_{\variableset\subset[\atomorder]} \sum_{\catindexof{\variableset}\in \indexsetof{\variableset}}  
%			\scalarcoreat{\variableset, \catindexof{\variableset}} \cdot \left( \onehotmapof{\catindexof{\variableset}} \otimes \onesof{[\atomorder]/\variableset} \right)  \, . \]
%	 To this end, we enumerate the set $\bigcup_{\variableset\subset[\atomorder]} \indexsetof{\variableset}$ by an additional index $\decindexin$ and define leg cores by
%	\begin{align*}
%		\legcoreof{\atomenumerator}_{\decindex} = 
%		\begin{cases} 
%			\onehotmapof{0} & \text{  if  }  \atomenumerator \in \variableset^{\decindex} \text{  and  } (\catindexof{\variableset}^\decindex)_\atomenumerator = 0 \\
%			\onehotmapof{1} & \text{  if  } \atomenumerator \in \variableset^{\decindex} \text{  and  } (\catindexof{\variableset}^\decindex)_\atomenumerator = 1 \\
%			\ones & \text{  if   } \atomenumerator \notin \variableset^{\decindex}
%		\end{cases} 
%	\end{align*}   
%	and a scalar core by coordinates
%		\[ \scalarcoreat{\decindex} = \scalarcoreat{\variableset^{\decindex},\catindexof{\variableset}^\decindex} \, . \]
%	Then, the monomial decomposition coincides with a basis CP Decomposition with the same dimension.
%	Thus, both $\bincprankof{\hypercore}$ and $\slicesparsityof{\hypercore}$ are the minima of identical sets and thus identical.
\end{proof}



\begin{example}[Propositional Formulas]
	When all leg dimensions of a binary tensor $\hypercore$ are $2$, we can interpret $\hypercore$ as a logical formula.
	We can use the binary CP decomposition of any tensor $\sechypercore$ with $\nonzeroof{\sechypercore}=\hypercore$ as a CNF of $\hypercore$.
	Finding the sparsest CNF thus amounts to finding the $\sechypercore$ with minimal $\slicesparsityof{\sechypercore}$ such that $\nonzeroof{\sechypercore}=\hypercore$.
\end{example}











\subsection{Constructive Bounds on CP Ranks}

After having defined three CP Decompositions, let us investigate bounds on their ranks which proofs come with constructions of the cores.


\subsubsection{Format Transformations}

%% Case of binary legs
Especially useful, when the leg dimensions are two, where the slice decomposition shows decomposition of the tensor into monomials.


\begin{theorem}\label{the:sliceToCP}
	For any tensor $\hypercoreat{\shortcatvariables}\in\facspace$ we have
		\[ \cprankof{\hypercore} 
		\leq \bincprankof{\hypercore} 
		\leq \baspluscprankof{\hypercore} 
		\leq \bascprankof{\hypercore} \, . \]
\end{theorem}	
\begin{proof}
	% First bound
	Since any CP decomposition into binary leg cores can be normed to a CP decomposition with directed leg cores, the first bound holds.
	% Second bound
	The second bound holds analogously, since any CP decomposition with basis leg cores is also a CP decomposition with binary leg cores.
\end{proof}

	%Taking only $\variableset=[\atomorder]$ and the index set being the nonzero coordinates of $\hypercore$ we get 
	%	\[ \slicesparsityof{\hypercore} \leq \#{\catindices: \hypercore_{\catindices}\neq 0} = \sparsityof{\hypercore} \, . \]

%% Tightness of the Bounds
Consider for example the tensor $\ones$ having maximal $\ell_0$-norm being the dimension of the tensor space, but, since it is elementary, a CP decomposition with rank $1$.


\subsubsection{Summation of CP Decompositions}

\begin{theorem}\label{the:CPrankSumBound}
	For any collections of tensors $\{T^{l}[\catvariableof{\nodes}] : l \in [n]\}$ with identical variables and scalars $\lambda^{l} \in \rr$ for $l\in[n]$  we have
		\[ \cprankof{\sum_{l \in [n]} \lambda^{l} \cdot T^{l}} \leq \sum_{l\in[n]}  \cprankof{T^{l}}  \, . \]
	The bound still holds, when we replace on both sides $\cprankof{\cdot}$ by $\bincprankof{\cdot}$, by $\bascprankof{\cdot}$ or by $\baspluscprankof{\cdot}$.
\end{theorem}
\begin{proof}
	Products with scalars do not change the rank, since they just rescale the core $\scalarcore$.
	The sum of CP Decomposition is just the combination of all slices, thus the rank is at most additive.
\end{proof}

\subsubsection{Contractions of CP Decompositions}

More general, we can bound the sparsity of any contraction by the product of sparsities of affected tensors.

\begin{theorem}\label{the:CPrankContractionBound}
	For any tensor network with variables $\nodes$ and edges $\edges$ we have for any subset $\secnodes\subset\nodes$
		\[ \cprankof{\contractionof{\{\hypercoreof{\edge} : \edge\in\edges \}}{\secnodes}} \leq 
		\prod_{\edge\in\edges \, : \, \secnodes\cap\edge \neq \varnothing} \cprankof{\hypercoreof{\edge}} \, . \]
	The bound still holds, when we replace on both sides $\cprankof{\cdot}$ by $\bincprankof{\cdot}$, by $\bascprankof{\cdot}$ or by $\baspluscprankof{\cdot}$.
\end{theorem}


Remarkably, in Theorem~\ref{the:CPrankContractionBound} the upper bound on the CP rank is build only by the ranks of the tensor cores, which have remaining open edges.
We prepare for its proof by first showing the following Lemmata.

\begin{lemma}\label{lem:sparsityGeneralContraction}
	For any tensors $\hypercoreofat{1}{\catvariableof{\nodes_1}}$ and $\hypercoreofat{2}{\catvariableof{\nodes_2}}$ and any set of variables $\secnodes\subset\nodes_1\cup\nodes_2$ we have
		\[ \cprankof{\contractionof{\{\hypercoreof{1},\hypercoreof{2}\}}{\secnodes}} \leq \cprankof{\hypercoreof{1}} \cdot \cprankof{\hypercoreof{2}} \, . \]
	The bound still holds, when we replace on both sides $\cprankof{\cdot}$ by $\bincprankof{\cdot}$, by $\bascprankof{\cdot}$ or by $\baspluscprankof{\cdot}$.
\end{lemma}
\begin{proof}
	By connecting the cores and restoring the binary or basis properties.
\end{proof}

When one core of the contracted tensor network does not contain variables which are left open, we can drastically sharpen the bound provided by Lemma~\ref{lem:sparsityGeneralContraction} as we show next.

\begin{lemma}\label{lem:sparsityDisjointContraction}
	For any tensor network consistent of two tensors $\hypercoreofat{1}{\catvariableof{\nodes_1}}$ and $\hypercoreofat{2}{\catvariableof{\nodes_2}}$ and any set $\secnodes$ with $\secnodes\cap\nodes_2=\varnothing$ we have
		\[ \cprankof{\contractionof{\{\hypercoreof{1},\hypercoreof{2}\}}{\secnodes}} \leq \cprankof{\hypercoreof{1}} \, . \]
	The bound still holds, when we replace on both sides $\cprankof{\cdot}$ by $\bincprankof{\cdot}$ or by $\bascprankof{\cdot}$.
\end{lemma}
\begin{proof}
	We show the lemma by constructing a CP decomposition of $\cprankof{\contractionof{\{\hypercoreof{1},\hypercoreof{2}\}}{\secnodes}} $ for any CP decomposition of $\hypercoreof{1}$.
	Let therefore take any CP decomposition of $\hypercoreof{1}$ consistent of the leg cores $\{\legcoreof{\node} \, : \, \node \in \nodes_1 \}$ and a scalar core $\scalarcore$.
	Then we define a new $\scalarcore$ by
		\[ \tilde{\scalarcore} = \contractionof{\{\scalarcore\}\cup \{\legcoreof{\node} \, : \, \node \in \nodes_1 , \node \notin \secnodes \} \cup \{\hypercoreof{2}\} }{\decvariable} \, . \]
	Then, the leg cores $\{\legcoreof{\node} \, : \, \node \in \secnodes \}$ build with the scalar core $\tilde{\scalarcore}$ a CP decomposition of $\contractionof{\{\hypercoreof{1},\hypercoreof{2}\}}{\secnodes}$.
	% Binary of basis
	When the CP decomposition of $\hypercoreof{1}$ was binary, basis or basis+, this property is also satisfied by the constructed CP decomposition.
	Thus the bound also holds for the ranks $\bincprankof{\cdot}$ or $\bascprankof{\cdot}$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{the:CPrankContractionBound}]
	Use delta tensor representation to represent contractions by graphs.
	We then iterate through the cores and contract them to the previously contracted tensor, where we apply Lemma~\ref{lem:sparsityGeneralContraction} when the tensor core has variables left open and Lemma~\ref{lem:sparsityDisjointContraction} if not.
\end{proof}


\begin{example}[Composition of formulas with connectives]
	For any formula $\exformula$ we have $1-\exformula$ = $\lnot\exformula$.
	The CP rank bound brings an increase by at most factor $2$ when taking the contraction with $\concoreof{\lnot}$ which has slice sparsity of $2$.
	This is not optimal, since $\lnot\exformula$ has at most an absolute slice sparsity increase of $1$.
	
	For any formulas $\exformula$ and $\secexformula$ we have $\exformula\cdot\secexformula = \exformula\land\secexformula$.
	Here the CP rank bounds on contractions can also be further tightened.
\end{example}


\begin{example}[Distributions of independent variables]
	Independence means factorization, conditional independence means sum over factorizations.
	Again, the $\ell_0$ norm is bounded by the product of the $\ell_0$ norm of the factors.
\end{example}


\subsubsection{Normations of CP Decompositions}

\red{As a theorem: If any of the above CP Decomposition is normable, the normation has the same CP ranks.
Especially interesting when learning Bayesian Networks, where each core has a CP bound by the number of datapoints.}

\subsubsection{Sparse Encoding of Functions}

%Using the proof idea of Theorem~\ref{the:sparseBasisCP}, we can state a more general CP bound on the encoding of functions.

We now state that the basis CP rank of relational encodings is equal to the cardinality of the domain.
The basis CP format can therefore not provide a sparse representation when the factored system contains many categorical variables.

\begin{theorem}\label{the:rencodingBasCP}
	For any function
		\[ \exfunction : \facstates \rightarrow  \secfacstates \]
	between factored systems we have
		\[ \bascprankof{\rencodingof{\exfunction}} =  \facdim \, . \]
\end{theorem}
\begin{proof}
	With Theorem~\ref{the:sparseBasisCP}, the basis CP rank coincides with the number of not vanishing coordinates, which is the cardinality of the domain of $\exfunction$.
\end{proof}

Allowing for trivial leg vectors can decrease the CP rank, as we show next.

\begin{theorem}
	We have
		\[ \baspluscprankof{\rencodingof{\exfunction}} \leq  \sum_{y \in \imageof{\exfunction}} \baspluscprankof{\ones_{\exfunction == y} } \, , \]
	where by $\ones_{\exfunction == y} $ we denote the indicator, whether the function $\exfunction$ evaluates to $y$.
\end{theorem}
\begin{proof}
	We have
		\[ \rencodingof{\exfunction} = \sum_{y \in \imageof{\exfunction}} \ones_{\exfunction == y}[\catvariable]  \otimes \onehotmapofat{y}{\catvariableof{\exfunction}} \, . \]
	For each $y \in \imageof{\exfunction}$ we represent $\onehotmapofat{y}{\catvariableof{\exfunction}}$ in an basis+ CP format with $\baspluscprankof{\ones_{\exfunction == y} } $ summands and arrive at a basis+ CP decomposition of $\rencodingof{\exfunction}$ with $\sum_{y \in \imageof{\exfunction}} \baspluscprankof{\ones_{\exfunction == y} } $ summands.
\end{proof}

The above claim still holds when replacing $\baspluscprankof{\cdot}$ with the ranks $\bascprankof{\cdot}$ or $\bincprankof{\cdot}$.
For the rank $\bascprankof{\cdot}$ it leads to the bound of Theorem~\ref{the:rencodingBasCP}, since summing the number of non zero coordinators of the indicators is the cardinality of the domain.

\begin{example}{Conjunction of variables}
	For the propositional formula $\exformula = \catvariableof{0} \land \catvariableof{1}$ we have
		\[ \rencodingofat{\exformula}{\catvariableof{0},\catvariableof{1}}
		 = \onehotmapofat{1,1}{\catvariableof{0},\catvariableof{1}} \otimes \onehotmapofat{1}{\catvariableof{\exformula}}
		  +  (\onesat{\catvariableof{0},\catvariableof{1}} - \onehotmapofat{1,1}{\catvariableof{0},\catvariableof{1}}) \otimes \onehotmapofat{0}{\catvariableof{\exformula}}  \]
	and thus 
		\[ \baspluscprankof{\rencodingof{\exformula}} \leq 3\]
	while $\bascprankof{\rencodingof{\exformula}} = 4$.
	\red{Especially useful for $d$-ary conjunctions, see Remark~\ref{rem:naryConnectives}!}
\end{example}




\subsubsection{Construction by averaging the incoming legs}

Basis CP Decompositions can be constructed by understanding the variable $\indvariableof{\insymbol}$ of the relational encoding of a function $\exfunction:\inset \rightarrow \outset$ as the slice selection variable.

\begin{example}{Empirical distributions, see Theorem~\ref{the:empCPRep}}
	Let there be a data map 
		\[ \datamap : [\datanum] \rightarrow \facstates \, . \]
	We can use Theorem~\ref{the:functionDecompositionBasisCP} to find a tensor network representation fo $\rencodingof{\datamap}$ as
	\begin{align*}
		\rencodingofat{\datamap}{\catvariable,\shortcatvariables}  
		= \contractionof{
		\{\rencodingofat{\datamap^{\atomenumerator}}{\catvariable,\catvariableof{\atomenumerator}} : \atomenumeratorin \} 
		}{\catvariable,\shortcatvariables} \, . 
	\end{align*}
	This representation is in the CP format, when adding trivial scalar core and and delta tensor to the data index.
	It is furthermore in a basis CP format, since all $\rencodingof{\datamap^{\catenumerator}}$ are directed and binary tensors.
	Normation to get the empirical distribution amounts to setting a slice core with coordinates $\frac{1}{\datanum}$.
\end{example}





%\subsection{Manipulations of Binary CP Decomposition}\label{sec:BinaryCPManipulation}
%
%Since the coordinates on the legs are binary, operations like contractions, slicing and marginalization are especially efficient given binary CP Decompositions.







\subsection{Representation by slice selection architectures}

The set of slice-sparse tensors coincides with the expressivity of specific selection architecture.
We first define a slice selecting tensor and then show its decomposition into a formula selecting neural network.

\begin{definition}
	Given a set of atomic variables $\shortcatvariables$, a slice selecting tensor of maximal cardinality $\sliceorder$ is the tensor
		\[ \fselectionmapat{\shortcatvariables,\selvariableof{0,0},\ldots,\selvariableof{\sliceorder-1,0},\selvariableof{0,1},\ldots,\selvariableof{\sliceorder-1,1}} \]
	with dimensions
		\[ \seldimof{\selenumerator,0} = 2, \seldimof{\selenumerator,1} = \atomorder \]
	and coordinates
	\begin{align*}
		& \fselectionmapat{\shortcatvariables=\shortcatindices,\indexedselvariableof{0,0},\ldots,\indexedselvariableof{\sliceorder-1,0},\indexedselvariableof{0,1},\ldots,\indexedselvariableof{\sliceorder-1,1}} \\
		& \quad = \begin{cases}
			1 & \text{if} \quad 
			\forall_{\atomenumerator,\selenumerator} : \big(  \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \big) \Rightarrow  (\selindexof{\selenumerator,0} = \catindexof{\atomenumerator})  \\
			0
		\end{cases} \, . 
	\end{align*}
\end{definition}

In the next two Lemmata we first show that the defined slice selecting tensors indeed selects slices and then provide a representation as a formula selecting network.

\begin{lemma}\label{lem:sliceFromSliceSelector}
	If all input neurons with same selection index are agreeing on the connective index, the selected formula does not vanish and coincides with a slice to the set
		\[ \variableset = \{ \atomenumerator \, : \, \exists_{\selenumeratorin}: \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \} \]
	and for $\atomenumerator\in\variableset$
		\[ \catindexof{\atomenumerator} = \selindexof{\selenumerator,0} \quad \text{if} \quad \selindexof{\selenumerator,1} = \atomenumerator \, . \]
\end{lemma}
\begin{proof}
	We need to show that 
	\begin{align}\label{eq:sliceFromSliceSelector}
	  	\fselectionmapat{\shortcatvariables,\indexedselvariableof{0,0},\ldots,\indexedselvariableof{\sliceorder-1,0},\indexedselvariableof{0,1},\ldots,\indexedselvariableof{\sliceorder-1,1}} = \onehotmapofat{\catindexof{\variableset}}{\shortcatvariables} \, . 
	\end{align}
	If and only if an index $\tilde{\catindex}_{[\atomorder]}$ reduced on $\variableset$ does not coincide with $\catindexof{\variableset}$, we have $\onehotmapofat{\catindexof{\variableset}}{\shortcatvariables=\tilde{\catindex}_{[\atomorder]}}=0$ and otherwise $\onehotmapofat{\catindexof{\variableset}}{\shortcatvariables=\tilde{\catindex}_{[\atomorder]}}=1$ .
	Let us notice, that this condition is equivalent to 
		\[ \forall_{\atomenumerator,\selenumerator} : \big(  \selindexof{\selenumerator,1} = \atomenumerator \land \selindexof{\selenumerator,0} \neq 2 \big) \Rightarrow  (\selindexof{\selenumerator,0} = \catindexof{\atomenumerator}) \]
	and thus \eqref{eq:sliceFromSliceSelector} holds.
\end{proof}


\begin{lemma}\label{lem:fsnnRepresentingSliceSelector}
	The slice selection tensor coincides with a formula selecting neural network with neurons (see Figure~\ref{fig:sliceSelectingNN}):
	\begin{itemize}
		\item unary input neuron enumerated by $\selenumerator$, selecting one of the $\shortcatvariables$ with the variable $\selvariableof{\selenumerator,1}$ and selecting a connective in $\{\lnot, \mathrm{Id}, \mathrm{True}\}$ by $\selvariableof{\selenumerator,0}$
		\item $\sliceorder$-ary output neuron fixed to the $\land$ connective.
	\end{itemize}
\end{lemma}
\begin{proof}
	This can be easily checked on each coordinate.
\end{proof}

It follows, that the expressivity of the slice selecting neural network coincides with the set of tensors with a bound on their slice sparsity, when $\sliceorder\geq\atomorder$.
For arbitrary $\sliceorder$, the following theorem holds.

\begin{theorem}
	Let $\fselectionmap$ be a slice selecting tensor.
	For any parameter tensor $\canparam$ we have
		\[ \baspluscprankof{\contractionof{\fselectionmap,\canparam}{\shortcatvariables}} \leq \bascprankof{\canparam} \, . \]
\end{theorem}
\begin{proof}
	Each non-vanishing coordinate of $\canparam$ represents by Lemma~\ref{lem:fsnnRepresentingSliceSelector} a slice and their weighted sum is thus a monomial decomposition.
%	\red{Use the above lemma for that on each slice.}
%	Show, how a coordinate of $\canparam$ corresponds with a slice determining tuple: $\sliceset$ determined by the selection indices.
\end{proof}


\begin{figure}[h]
	\begin{center}
		\input{PartIII/tikz_pics/sparse_tensor_calculus/slice_selecting_nn.tex}
	\end{center}
	\caption{Representation of a basis+ Tensor by the contraction of a parameter tensor $\canparam$ with a slice selecting architecture $\fselectionmap$, which has a decomposition as a formula selecting neural network (see Lemma~\ref{lem:fsnnRepresentingSliceSelector}).
	The nonzero coordinates of $\canparam$ represent the (see Lemma~\ref{lem:sliceFromSliceSelector}).}
\end{figure}\label{fig:sliceSelectingNN}


\subsubsection{Applications}

One application is as a parametrization scheme in the approximation of a tensor by a slice-sparse tensor, see Chapter~\ref{cha:tensorApproximation}.


\red{
The approximated parameter can then be used as a proxy energy to be maximized.
When choosing $\sliceorder=2$, the approximating tensor contains only quadratic slices, which then poses a QUBO problem.
}

\begin{remark}[Extension to arbitrary CP-formats]
	Select at each input neuron a specific leg.
	For finite number of legs, as it is the case in the binary, basis and basis+ formats, we can enumerate all possibilities by the selection variable.
	For the basis+ format, in case of binary leg dimensions, we here exemplified the approach, by enumerating the three possibilities $\onehotmapof{0},\onehotmapof{1},\onesat{1}$.
	This approach, however, fails as a generic representation of the directed format, since the directed legs are continuous and there therefore are infinite choosable legs.
\end{remark}





\subsection{Optimization of sparse tensors}

Let us now study the problem of searching for the maximal coordinate in a tensor represented by a monomial decomposition. 


\subsubsection{Mode search in exponential families}

Mode search 
\begin{align*}
	\max_{\shortcatindices\in\atomstates} \sbcontraction{\sencsstatat{\indexedshortcatvariables,\selvariable},\canparam} 
	= \max_{\meanparam\in\meanset} \sbcontraction{\meanparamat{\selvariable},\canparamat{\selvariable}}
\end{align*}


% Appearance of mode search
The search for maximal coordinates appears in various reasoning tasks:
\begin{itemize}
	\item MAP query as mode search of MLN: $\hypercore$ is the contraction of evidence with the distribution, leaving the query variables open.
	\item Grafting as mode search of proposal distribution: $\hypercore$ is the contraction of the gradient of the likelihood with the relational encoding of the hypothesis.
\end{itemize}
Both tasks have been formulated as mode search problems in exponential families.



\subsubsection{Higher-Order Unconstrained Binary Optimization (HUBO)}

\red{
Here binary refers to the leg dimensions $\catdimof{\atomenumerator}$ being 2, not to binary coordinates as often refered to in this work.
}


\begin{definition}
	The binary optimization of a tensor $\hypercoreat{\shortcatvariables}\in\atomstates$ is the problem
	\begin{align}\tag{$\mathrm{P}_{\hypercore}$}\label{prob:HUBO}
		\argmax_{\shortcatindices\in\atomstates} \quad \hypercoreat{\indexedshortcatvariables} 
	\end{align}
	
	We call Problem~\ref{prob:HUBO} a Higher Order Unconstrained Binary Optimization (HUBO) problem of order $\sliceorder$ and sparsity $\slicerankwrtof{\sliceorder}{\hypercore}$, when $\hypercore$ has a monomial decomposition (see Definition~\ref{def:polynomialSparsity}) with $\cardof{\variablesetof{\decindex}}\leq\sliceorder$ for all $\decindexin$, that is when $\slicerankwrtof{\sliceorder}{\hypercore}<\infty$.
	
	
\end{definition}


\begin{remark}[Leg dimensions larger than 2]
% Leg dimension needs to be 2
	We demanded leg dimensions $\catdimof{\atomenumerator}=2$ to have binary valued variables $\catvariableof{\catenumerator}$, which is required to connect with the formalism of binary optimization.
	Categorical variables with larger dimensions can be represented by atomization variables, which are created by contractions with categorical constraint tensors (see Section~\ref{sec:categoricalTN}).
\end{remark}


% Interpretation of sparsity
The sparsity $\slicerankwrtof{\sliceorder}{\hypercore}$ is the minimal number of monomials, for which a weighted sum is equal to $\hypercore$.
Thus we interpret Problem~\ref{prob:HUBO} as searching for the maximum in a polynomial consistent of $\slicerankwrtof{\sliceorder}{\hypercore}$ monomial terms.
\red{Each monomial is also refered to as potential.}



\subsubsection{Quadratic Unconstrained Binary Optimization (QUBO)}

\red{Quadratic Unconstrained Binary Optimization problems are HUBOs of order $\sliceorder=2$.}

We refine the monomial decomposition of tensors (see Definition~\ref{def:polynomialSparsity}) by demanding that monomials consist of at most two variables.

\begin{definition}
	We call a monomial decomposition $\sliceset$ of a tensor $\hypercore\in\atomspace$ a quadratic decomposition, if $\cardof{\variableset}\leq 2$ for all $(\lambda,\variableset,\catindexof{\variableset}) \in \sliceset$.
	We denote the smallest cardinality $\cardof{\sliceset}$ among quadratic decompositions of $\hypercore$ by $\quacprankof{\hypercore}$.

	If a tensor $\hypercore\in\bigotimes_{\atomenumeratorin}\rr^2$ has a quadratic decomposition, we call Problem~\ref{prob:HUBO} a Quadratic Unconstrained Binary Optimization (QUBO) problem of sparsity $\quacprankof{\hypercore}$.
\end{definition}

% CP Decompositions
%Analogously to monomial decompositions, quadratic decompositions have an equivalence in a CP decomposition of $\hypercore$.
%Beyond being binary tensors, the leg cores are further restricted that for each slice $\decindexin$ at most two of them are basis vectors and the rest trivial vectors $\ones$.

%% REPETITION Existence
%We notice, that there are tensors, for which no quadratic decomposition exists.
%This is already obvious from the fact, that the tensors with a quadratic decomposition build a subspace of dimensions bounded by $2 \atomorder + 2^2 \cdot \binom{\atomorder}{2}$ dimensional submanifold in the $2^\atomorder$ dimensional tensor space.
%This is in contrast with monomial decompositions of order $\sliceorder=\atomorder$, where one can always construct a decomposition.



%% OLD THEOREM: FALSE!
%However, for any non-negative tensor $\hypercore$ the Problem~\ref{prob:HUBO} is equivalent to a QUBO problem of possibly larger order as we state next.
%To turn HUBO problems into QUBO we need the slack variable trick, as described in the next lemma.

%\begin{theorem}\label{the:HUBOtoQUBO}
%	Let there be a tensor $\hypercore\in\in\bigotimes_{\atomenumeratorin}\rr^2$, which has a monomial decomposition with dimension $r$ and non-negative scalar core $\scalarcore$.
%	Then, the HUBO defined by $\hypercore$ is equivalent to a QUBO of order at most $\atomorder+r$ and sparsity at most $\atomorder \cdot r $.
%%	The maximal coordinate problem to any tensor $\hypercore\in\bigotimes_{\atomenumeratorin}\rr^2$ is equivalent to a QUBO with at most $\atomorder+\slicesparsityof{\hypercore}$ variables.
%%	\red{Need positive coordinates!}
%\end{theorem}

%To show the theorem we state the following lemma.


We can transform certain HUBO problems in QUBO problems with the usage of auxiliary variables, as we show in the next lemma.

%% Slack variables
\begin{lemma}\label{lem:monomialToQUBO}
	For any $\atomindices\in[2]$ and $\variableset\subset[\atomorder]$ we have 
		\[ \left( \prod_{\atomenumerator\in\variableset} \atomlegindexof{\atomenumerator } \right)  \left(  \prod_{\atomenumerator\notin\variableset} (1- \atomlegindexof{\atomenumerator }) \right)
		=
		\max_{\slackvariable\in[2]} \slackvariable \cdot 2 \cdot \left( \sum_{\atomenumerator\in\variableset}\atomlegindexof{\atomenumerator}  - \cardof{\variableset} - \sum_{\atomenumerator\notin\variableset}\atomlegindexof{\atomenumerator} + \frac{1}{2} \right) \, . % Alternative: no factor 2, but + 1 instead of +1/2 (->pyqubo)
 		\]
\end{lemma}
\begin{proof} %Proof by case distinction
	Only if $\atomlegindexof{\atomenumerator}=1$ for $\atomenumerator\in\variableset$ and $\atomlegindexof{\atomenumerator}=0$ else we have
		\[ \left( \sum_{\atomenumerator\in\variableset}\atomlegindexof{\atomenumerator}  - \cardof{\variableset} - \sum_{\atomenumerator\notin\variableset}\atomlegindexof{\atomenumerator} + \frac{1}{2} \right) \geq 0 \, . \]
	In this case the maximum is taken for $\slackvariable=1$ and we have
		\[ \max_{\slackvariable\in[2]} \slackvariable \cdot 2 \cdot \left( \sum_{\atomenumerator\in\variableset}\atomlegindexof{\atomenumerator}  - \cardof{\variableset} - \sum_{\atomenumerator\notin\variableset}\atomlegindexof{\atomenumerator} + \frac{1}{2} \right) 
		= 1 = \left( \prod_{\atomenumerator\in\variableset} \atomlegindexof{\atomenumerator } \right)  \left(  \prod_{\atomenumerator\notin\variableset} (1- \atomlegindexof{\atomenumerator }) \right) \, . \]
	In all other cases, the maximum is taken for $\slackvariable=0$ and thus vanishes, that is 
		\[ \max_{\slackvariable\in[2]} \slackvariable \cdot 2 \cdot \left( \sum_{\atomenumerator\in\variableset}\atomlegindexof{\atomenumerator}  - \cardof{\variableset} - \sum_{\atomenumerator\notin\variableset}\atomlegindexof{\atomenumerator} + \frac{1}{2} \right) 
		= 0 = \left( \prod_{\atomenumerator\in\variableset} \atomlegindexof{\atomenumerator } \right)  \left(  \prod_{\atomenumerator\notin\variableset} (1- \atomlegindexof{\atomenumerator }) \right) \, . \]
	Thus, the claim holds in all cases.
\end{proof}	


%\begin{proof}[Proof of Theorem~\ref{the:HUBOtoQUBO}]
%	For each summand in the monomial decomposition apply Lemma~\ref{lem:monomialToQUBO}.
%\end{proof}



\subsubsection{Integer Linear Programming}

Let us now show how optimization problems can be represented as linear programming problems.

\begin{definition}
	The integer linear program (ILP) of $A\in\rr^{n \times d}$ and $b\in\rr^{n}$ is the problem
	\begin{align*}
		\max_{\shortcatindices\in\facstates} \sum_{\catenumeratorin} c[\selvariable=\catenumerator] \cdot \catindexof{\catenumerator} 
		 \quad \text{subject to } \quad A \shortcatindices \leq b \, . 
	\end{align*}
\end{definition}

%\begin{definition}
%	A Binary Integer Linear Program (ILP) is a problem of the form
%	\begin{align*}
%		\max_{x \in\{0,1\}^n} c^T x \quad \text{subject to } \quad A^{upper} x \leq b^{upper} , A^{lower} x \geq b^{lower} 
%	\end{align*}
%	where $A^{upper}\in\rr^{n^{upper}\times n}$, $b^{upper}\in\rr^{n^{upper}}$, $A^{lower}\in\rr^{n^{lower}\times n}$, $b^{lower}\in\rr^{n^{lower}}$.
%\end{definition}

\begin{theorem}
	Given a monomial decomposition $\sliceset=\enumeratedslices$ of a tensor $\hypercore$ we define an Binary ILP as the maximation of 
	\begin{align*}
		\sum_{\decindexin} \slicescalar^{\decindex} \slackvariable^{\decindex} 
	\end{align*}
	with the constraints for any $\decindex$
	\begin{itemize}
		\item 
		\begin{align*}
			\slackvariable^{\decindex}  \leq \catvariableof{\atomenumerator} \quad \text{for} \quad \atomenumerator\in\variableset^j , \catindexof{\atomenumerator} = 1
		\end{align*}
		\item 
		\begin{align*}
			\slackvariable^{\decindex}  \leq (1-\catvariableof{\atomenumerator}) \quad \text{for} \quad \atomenumerator\in\variableset^j , \catindexof{\atomenumerator} = 0
		\end{align*}
		\item 
		\begin{align*}
			\slackvariable^{\decindex} \geq 1 + \sum_{\atomenumerator\in\variableset^{\decindex} : \catindexof{\atomenumerator} = 1} (\catvariableof{\atomenumerator} -1)
		- \sum_{\atomenumerator\in\variableset^{\decindex} : \catindexof{\atomenumerator} = 0} \catvariableof{\atomenumerator} 
		\end{align*}
	\end{itemize}
	The solution $\catindex^{ILP,\sliceset}$ of this ILP and the solution $\catindex^{HUBO,\sliceset}$ of the HUBO coincide on the variables of hypercore, i.e.
		\[ \catindex^{ILP,\sliceset}|_{[d]} =  \catindex^{HUBO,\sliceset} \, . \]
\end{theorem}
\begin{proof}
	We have to show that the constraints are satisfied if and only if $\slackvariable^{\decindex}=\onehotmapofat{\catvariableof{\variableset^{\decindex}}^{\decindex}}{\indexedcatvariableof{\variableset^{\decindex}}}$.
\end{proof}









\subsection{Subspaces of formulas}\label{sec:HT}

\red{
Formula Tensors have Tensor Network decomposition, which are best represented in a $\htformat$ decomposition.
We here describe this perspective and show applications of this formalism in the recovery/learning of formula tensors.
}
The decomposition of formula tensors is basic, since atomic formula tensors being on the leafs consist of basic vectors in the respective legs.


\subsubsection{Formula Subspaces}

Each formula tensor defines the subspace of $\atomspace$
\begin{align}
	\subspaceof{\exformula} = \mathrm{span} \left\{ \lnot\exformula,\exformula \right\} = \mathrm{im}\left(\ftensorof{\exformula} \right)
	%\mathrm{span} \left\{ \braket{\atombasisvector_1,\ftensorof{\exformula}}, \braket{\atombasisvector_0,\ftensorof{\exformula}} \right\}
\end{align}

Let us notice that the spanning vector of the subspace $\subspaceof{\exformula}$ are binary tensors summing up to the tensor of ones.

%\subsubsection{Atomic Tensor Spaces}

For each atom $\atomicformulaof{\atomenumerator}$ we have
	\[ \subspaceof{\atomicformulaof{\atomenumerator}} = \rr^2 \, . \]
The tensor space carrying the factored representation of the worlds is thus
\begin{align}
	\bigotimes_{\atomenumeratorin}\subspaceof{\atomicformulaof{\atomenumerator}} \, .
\end{align}

\subsubsection{Formula Decomposition as a Subspace Choice}

Given a formula $\exformula\exconnective\secexformula$ composed of formulas $\exformula$ and $\secexformula$ containing different atoms we have
\begin{align}
	\subspaceof{\exformula\exconnective\secexformula} 
	\subset \subspaceof{\exformula} \otimes \subspaceof{\secexformula}
\end{align}

A connective $\exconnective$ thus determines the selection of a two-dimensional subspace in the four-dimensional tensor product of subspaces to both subformulas.

%\subsubsection{Approximation problems}

Reconstruction of a formula given its formula tensor amounts to finding the HT Decomposition under the constraints of subspace choices according to the allowed logical connectives.

Given a set of positive and negative examples of a formula poses further an approximation problem of the examples by a HT Decomposition.

Advantages of this perspective are
\begin{itemize}
	\item Given a $\htformat$ the best approximation always exists (Theorem 11.58 in \cite{hackbusch_tensor_2012}), but need to further restrict to cores given by logical connectives 
	\item Apply Approximation algorithms: ALS or HOSVD
\end{itemize}




%%% NEEDED? This would be a description of the relational encoding of decompositions
%\subsection{Basis Tensor Networks}
%
%
%\begin{definition}
%	We call a tensor network, which cores are directed and binary a basis tensor network. %also acyclic?
%\end{definition}













