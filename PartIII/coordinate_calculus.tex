\section{Coordinate Calculus} \label{cha:coordinateCalculus}

In the previous chapters, information to states has been stored in coordinates of a tensor.
To distinguish from other schemes of calculus, we call this scheme of storing and retrieving information the coordinate calculus.
We in this chapter investigate in more depth, which operations can be performed based on such tensors and proof the applied properties.

\red{Add summations and further operations?}

\red{Discuss things like $\expof{\hypercore}$ as coordinatewise operations, by circles in the factor graph diagrams.}

\subsection{One-hot encodings as basis}

\begin{lemma}[Basis of tensor spaces]\label{lem:tensorBasisDecomposition}
	The image of the one-hot encoding map is a linear basis of the tensor space $\facspace$.
	Any element $\exformula\in\facspace$ has a decomposition 
		\[ \exformula[\catvariables] = \sum_{\catindices} \exformula[\indexedcatvariables] \cdot \onehotmapof{\catindices}[\catvariables] \, . \]
	We notice that the coordinates are the weights to the basis elements in the one-hot decomposition.
\end{lemma}
\begin{proof}
	For any $\tildecatindices\in\facstates$ we have
		\[ \formulaat{\shortcatvariables=\tilde{\catindex}_{[\catorder]}}
		= \sum_{\catindices} \exformula[\indexedcatvariables] \cdot \onehotmapofat{\catindices}{\shortcatvariables=\tilde{\catindex}_{[\catorder]}} 
		= \formulaat{\shortcatvariables=\tilde{\catindex}_{[\catorder]}} \cdot \onehotmapofat{\catindices}{\shortcatvariables=\tilde{\catindex}_{[\catorder]}}   \]
\end{proof}


\begin{definition}
	Given any real-valued function 
		\[ \exfunction : \facstates \rightarrow \rr \]
	we define the coordinate encoding by
		\[ \hypercoreof{\exfunction} = \sum_{\catindices\in\facstates} \exfunction(\catindices) \cdot \onehotmapof{\catindices} \, . \]
\end{definition}


\begin{theorem}[Coordinate Calculus]\label{the:coordinateCalculus}
	Given any tensor $\hypercoreat{\catvariables}$ can retrieve its coordinate indexed by $\catindices$ as
		\[ \hypercoreat{\indexedcatvariables} = \sbcontraction{\hypercore, \onehotmapof{\catindices}} \, . \]
\end{theorem}
\begin{proof}
	We use the decomposition in \lemref{lem:tensorBasisDecomposition} and have
	\begin{align*}
		\contractionof{\{\hypercore, \onehotmapof{\catindices}\}}{\varnothing} 
		& = \sum_{\tildecatindices} \hypercoreat{\tildeindexedcatvariables} \cdot \contractionof{\{\onehotmapof{\tildecatindices} ,\onehotmapof{\catindices} \}}{\catvariables} \\
		& =  \sum_{\tildecatindices} \hypercoreat{\tildeindexedcatvariables} \cdot \delta_{(\tildecatindices),(\catindices)} \\
		& = \hypercoreat{\indexedcatvariables} \, ,
	\end{align*}
	where we used that one-hot encodings are orthonormal.
\end{proof}

% Coordinate Calculus
Coordinate calculus is the representation of real-valued functions as tensors, from which its evaluations can be retrieved by the scheme of Theorem~\ref{the:coordinateCalculus}.


%\red{Incorporate theorem: Contracting tensor network conditioned on indices can be done by contracting the conditioned.}


% Retrieval of Coorinates from tensor networks
Tensors of large orders often admit a decomposition by tensor networks.
We in the next theorem show, how such a decomposition can be exploited for efficient contraction and in particular coordinate retrieval.


\begin{theorem}\label{the:slicedContractionToCores}
	Given a tensor network $\tnetof{\graph}$ on a hypergraph $\graph=(\nodes,\edges)$, disjoint subsets $\nodesa,\nodesb\subset\nodes$ and $\catindexofin{\nodesb}$, we have
		\[ \contractionof{\tnetof{\graph}}{\catvariableof{\nodesa},\indexedcatvariableof{\nodesb}} 
		=  \contractionof{\{
			\sbcontractionof{\hypercoreof{\edge}}{\catvariableof{\edge/\nodesb},\indexedcatvariableof{\nodesb}} \, : \, \edge\in\edges
		\}}{\catvariableof{\nodesa}} \, .
		\]
\end{theorem}
\begin{proof}
	Using a delta tensor, which copies basis vector when contracting with one.
\end{proof}

% Special case of retrieving single coordinates
If we retrieve a single coordinate of a tensor, we have the situation $\nodesa=\varnothing$, $\nodesb=\nodes$.
In that case, Theorem~\ref{the:slicedContractionToCores} shows, that the coordinate is the product of the coordinates of the cores. % Thus no contraction required!


\subsection{Coordinatewise Transforms}


\red{This is central to coordinate calculus!}

\begin{definition}\label{def:coordinateCalculus}
	Let $\exfunction: \rr^{\seldim} \rightarrow \rr$ be a function.
	Then the coordinatewise transform of tensors $\hypercoreofat{\selindex}{\shortcatvariables}$, where $\selindexin$, under $\exfunction$ is the tensor
		\[ \exfunction(\hypercoreof{0},\ldots,\hypercoreof{\seldim-1})[\shortcatvariables] \]
	with coordinates
		\[ \exfunction(\hypercoreof{0},\ldots,\hypercoreof{\seldim-1})[\indexedshortcatvariables] 
		= \exfunction(\hypercoreofat{0}{\indexedshortcatvariables},\ldots,\hypercoreofat{\seldim-1}{\indexedshortcatvariables}) \, .  \]	
	To abbreviate notation, we will denote the transform by $\exfunction(\hypercoreofat{0}{\shortcatvariables},\ldots,\hypercoreofat{\seldim-1}{\shortcatvariables})$ and $\exfunction(\hypercoreof{0},\ldots,\hypercoreof{\seldim-1})$.
\end{definition}

% Examples
\begin{example}[Hadamard product]
	Hadamard products of tensors are a special way of coordinate calculus, where the transform is the product and thus
		\[ \cdot \, (\hypercoreof{0},\ldots,\hypercoreof{\seldim-1})[\shortcatvariables] 
		= \contractionof{\{\hypercoreofat{\selindex}{\shortcatvariables} \, : \, \selindexin \}}{\shortcatvariables} \, . \]
	\red{Applications of this are the computation of conjunctions, as we will exploit in effective calculus.}
\end{example}




\subsection{Differentiation of Contraction}

We add adiitional variables $\seccatvariable$ selecting a coordinate of a tensor, which is varied in a differentiation.

\begin{lemma}\label{lem:difMNprob}
	For any tensor network $\extnet$ with positive $\hypercoreof{\edge}$ we have
	\begin{align*}
		\difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}} \extnetdist
		& = \sbcontractionof{
	 	\identityat{\seccatvariableof{\edge},\edgevariables}, 
		\frac{\contractionof{\extnet}{\edgevariables}}{\hypercoreofat{\edge}{\edgevariables}}, 
		\normationofwrt{\extnet}{\catvariableof{\nodes/\edge}}{\edgevariables} }{\seccatvariableof{\edge},\nodevariables} \\
		& \quad -  \extnetdist \otimes \sbcontractionof{\frac{\contractionof{\extnet}{\seccatvariableof{\edge}}}{\hypercoreofat{\edge}{\seccatvariableof{\edge}}}
		}{\seccatvariableof{\edge}} \, .
	\end{align*}
\end{lemma}
\begin{proof}
	By multilinearity of tensor network contractions we have
	\begin{align*}
		\difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}} \contractionof{\extnet}{\nodevariables}
		& = \contractionof{\{\identityat{\seccatvariableof{\edge},\edgevariables}\}\cup\{\hypercoreofat{\secedge}{\catvariableof{\secedge}} \, : \, \secedge\neq\edge \}}{\seccatvariableof{\edge},\nodevariables}
	\end{align*}
	and thus	
	\begin{align*}
		\difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}} \contraction{\extnet}
		& = \contractionof{\{\identityat{\seccatvariableof{\edge},\edgevariables}\}\cup\{\hypercoreofat{\secedge}{\catvariableof{\secedge}} \, : \, \secedge\neq\edge \}}{\seccatvariableof{\edge}} \, . 
	\end{align*}
		
	Using both we get
	\begin{align}
		\difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}} \extnetdist 
		& = \difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}}  \frac{\contractionof{\extnet}{\nodevariables}}{\contraction{\extnet}} \nonumber \\
		& = \frac{ \difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}} \contractionof{\extnet}{\nodevariables}}{\contraction{\extnet}} 
		- \frac{ \contractionof{\extnet}{\nodevariables} \difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}} \contraction{\extnet} }{(\contraction{\extnet})^2} \nonumber \\
		& = \frac{ \contractionof{\{\identityat{\seccatvariableof{\edge},\edgevariables}\}\cup\{\hypercoreofat{\secedge}{\catvariableof{\secedge}} \, : \, \secedge\neq\edge \}}{\seccatvariableof{\edge},\nodevariables}}{\contraction{\extnet}} \nonumber \\
		& \quad\quad - \extnetdist \cdot  \frac{\contractionof{\{\identityat{\seccatvariableof{\edge},\edgevariables}\}\cup\{\hypercoreofat{\secedge}{\catvariableof{\secedge}} \, : \, \secedge\neq\edge \}}{\seccatvariableof{\edge}}}{\contraction{\extnet}} \label{eq:differentiatingMNpreresult}
		% = \contractionof{\{\identityat{\seccatvariableof{\edge},\edgevariables}\}\cup\{\hypercoreofat{\secedge}{\catvariableof{\secedge}} \, : \, \secedge\neq\edge \}}{\seccatvariableof{\edge},\nodevariables}
	\end{align}
		
	For the first term we get with a normation equation (see Theorem~\ref{the:normationContractionEQ}) that
	\begin{align*}
		\frac{ \contractionof{\{\identityat{\seccatvariableof{\edge},\edgevariables}\}\cup\{\hypercoreofat{\secedge}{\catvariableof{\secedge}} \, : \, \secedge\neq\edge \}}{\seccatvariableof{\edge},\nodevariables}}{\contraction{\extnet}} 	
		&= \frac{\contractionof{\{\identityat{\seccatvariableof{\edge},\edgevariables}\}\cup\{\hypercoreofat{\secedge}{\catvariableof{\secedge}} \, : \, \secedge\in\edges \}}{\seccatvariableof{\edge},\nodevariables}}{\hypercoreofat{\edge}{\edgevariables}  \cdot \contraction{\extnet}} \\
		&= \frac{
		\sbcontractionof{\identityat{\seccatvariableof{\edge},\edgevariables},\extnetdist}{\seccatvariableof{\edge},\nodevariables}
		}{\hypercoreofat{\edge}{\edgevariables}}  \\ 
		&= \frac{\sbcontractionof{\identityat{\seccatvariableof{\edge},\edgevariables},
			\normationof{\extnet}{\edgevariables},
			\normationofwrt{\extnet}{\catvariableof{\nodes/\edge}}{\edgevariables}
			}{\seccatvariableof{\edge},\nodevariables}
		}{\hypercoreofat{\edge}{\edgevariables}}  \, . 
	\end{align*}
	
	Analogously, we have 
	\begin{align*}
		\frac{ \contractionof{\{\identityat{\seccatvariableof{\edge},\edgevariables}\}\cup\{\hypercoreofat{\secedge}{\catvariableof{\secedge}} \, : \, \secedge\neq\edge \}}{\seccatvariableof{\edge}}}{\contraction{\extnet}} 	
		&= \frac{\sbcontractionof{\identityat{\seccatvariableof{\edge},\edgevariables},
			\normationof{\extnet}{\edgevariables}%,
			%\normationofwrt{\extnet}{\catvariableof{\nodes/\edge}}{\edgevariables}
			}{\seccatvariableof{\edge}}
		}{\hypercoreofat{\edge}{\edgevariables}}  \, . 
	\end{align*}
		
	With \eqref{eq:differentiatingMNpreresult}, we arrive at the claim 
	\begin{align*}
		\difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}} \extnetdist
		& = \sbcontractionof{
	 	\identityat{\seccatvariableof{\edge},\edgevariables}, 
		\frac{\contractionof{\extnet}{\edgevariables}}{\hypercoreofat{\edge}{\edgevariables}}, 
		\normationofwrt{\extnet}{\catvariableof{\nodes/\edge}}{\edgevariables} }{\seccatvariableof{\edge},\nodevariables} \\
		& \quad -  \extnetdist \otimes \sbcontractionof{\frac{\contractionof{\extnet}{\seccatvariableof{\edge}}}{\hypercoreofat{\edge}{\seccatvariableof{\edge}}}
		}{\seccatvariableof{\edge}} \, .
	\end{align*}
	
\end{proof}


% Could put it into contraction equations?
\begin{lemma}\label{lem:difMNExpectation}
	%See Proposition 11.9 in Koller Book.
	For any function $\exfunction(\hypercoreof{\edge})[\nodevariables]$ we have
	\begin{align*}
		 \difwrt{\hypercoreofat{\edge}{\seccatvariableof{\edge}}} &
		\sbcontraction{\extnetdist,\exfunction(\hypercoreof{\edge})[\nodevariables]} \\
		= & 
		\frac{\normationof{\extnet}{\indexedcatvariableof{\edge}}}{\hypercoreofat{\edge}{\indexedcatvariableof{\edge}}} 
		\Big( \sbcontraction{\normationofwrt{\extnet}{\catvariableof{\nodes/\edge}}{\indexedcatvariableof{\edge}}, \exfunction(\hypercoreof{\edge})[\nodevariables,\seccatvariableof{\edge}]} \\
		& \quad \quad \quad \quad \quad - \sbcontraction{\extnetdist, \exfunction(\hypercoreof{\edge})[\nodevariables]}
		\Big) \\
		& + \contraction{ \extnetdist
		\difofwrt{\exfunction(\hypercoreof{\edge})[\nodevariables]}{\hypercoreofat{\edge}{\seccatvariableof{\edge}}}
		}
	\end{align*}
\end{lemma}
\begin{proof}
	By product rule of differentiation we have
	\begin{align*}
		\difwrt{\hypercoreofat{\edge}{\indexedcatvariableof{\edge}}} \sbcontraction{\extnetdist,\exfunction(\hypercoreof{\edge})[\nodevariables]} 
		& =  \sbcontraction{\difwrt{\hypercoreofat{\edge}{\indexedcatvariableof{\edge}}}\extnetdist,\exfunction(\hypercoreof{\edge})[\nodevariables]} \\
		& \quad +  \sbcontraction{\extnetdist,\difwrt{\hypercoreofat{\edge}{\indexedcatvariableof{\edge}}}\exfunction(\hypercoreof{\edge})[\nodevariables]}  \, . 
	\end{align*}
	The claim now follows with the application of \lemref{lem:difMNprob} on the first term.
\end{proof}






\subsection{Selection Encodings}

Selection encodings as introduced in \defref{def:selectionEncoding} are best understood in terms of linear mapping interpretations of tensors.
We will first provide by basis encodings a generic relation between the coordinatewise tensor definitions in this work and linear maps.

We then show the utility of this perspective in the representation of composed linear functions.
The results are applicable in the exponential family theory, in the tensor representation of energies and means.

\subsubsection{Tensors as linear maps}

The state sets $\facstates$ can be interpreted as an enumeration of basis elements $\onehotmapof{\catindex}$ of the tensor space $\facspace$.

Along this interpretation, tensors have an interpretation as maps between tensor spaces.

\red{Any tensor and any partition of its variables into two sets can be interpreted as the basis elements of a linear map between the tensor spaces of the respective variables.}

Tensor valued functions on state sets $\facstates$ are an intermediate representation.

\begin{definition}
	Let there be two tensor spaces $V_1$ and $V_2$ with basis by sets $\mathcal{M}_1\subset V_1$ and $\mathcal{M}_2\subset V_2$ of cardinality $\catdimof{1}$ and $\catdimof{2}$, which are enumerated by variables $\individualvariableof{1}$ and $\individualvariableof{2}$.
	The basis encoding of a linear map $\exfunction\in\linmapspace(V_1,V_2)$ is the tensor
		\[ \bencodingof{\exfunction}[\individualvariableof{1},\individualvariableof{2}] \in \rr^{\catdimof{1}} \otimes \rr^{\catdimof{2}} \, . \] 
\end{definition}

% Matrices
Basis encodings are standard linear algebra tools, where matrices are understood as linear maps between vector spaces.

\begin{theorem}\label{the:linearCompositionBasisEncoding}
	If $\linmapof{1}$ is a linear function between $V_1$ and $V_2$  and $\linmapof{2}$ between $V_2$ and $V_3$, and let $\individualvariableof{1},\,\individualvariableof{2}$ and $\individualvariableof{3}$ be enumerations of chosen bases in the spaces.
	We have
	\begin{align*}
		\bencodingofat{\linmapof{2}\circ\linmapof{1}}{\individualvariableof{1},\individualvariableof{3}} 
		= \sbcontractionof{
		\bencodingofat{\linmapof{2}}{\individualvariableof{2},\individualvariableof{3}}, \bencodingofat{\linmapof{1}}{\individualvariableof{1},\individualvariableof{2}}
		}{\individualvariableof{1},\individualvariableof{3}}  \, . 
	\end{align*}
\end{theorem}
\begin{proof}
	By basis decompositions and representations of linear maps as contractions.
\end{proof}

% Matrix Multiplication
A typical instance is matrix multiplication, where matrices understood as representations of linear maps.


\subsubsection{Selection encodings}

Selection encodings (see \defref{def:selectionEncoding}) are related to basis encodings of linear maps as we show in the next theorem.

\begin{theorem}\label{the:selectionToBasisEncoding}
	Given a function 
		\[ \exfunction : \facstates \rightarrow \parspace \]
	we define a linear map $\linmapof{\exfunction}\in\linmapspace(\facspace,\parspace)$ by the basis elements to $\catindexof{1} \in\facstates$ and $\catindexof{2}\in\parstates$ by
	\begin{align*}
	 	\linmapof{\exfunction}(\onehotmapof{\catindexof{1}},\onehotmapof{\catindexof{2}}) 
		= \sbcontraction{\exfunction(\onehotmapof{\catindexof{1}}),\onehotmapof{\catindexof{2}}} \, .  
	\end{align*}
	We then have
	\begin{align*}
		\sencodingof{\exfunction} = \bencodingof{\linmapof{\exfunction}} \, . 
	\end{align*}
\end{theorem}



%\red{Selection encodings are interpretations of matrifications.
%Along that, maps between factored systems are understood as basis decompositions of linear maps between the tensor spaces.}


% Comparison with relational encodings - definition
While relational encoding works for maps from $\facstates$ to arbitrary sets (which are enumerated), selection encodings as introduced in \defref{def:selectionEncoding} require and exploit that their image is embedded in a tensor space.

% Slicing
Given a selection encoding of a function, the function is retrieved by slicing with respect to the 
	\[ \exfunction(\catindex) = \sencodingofat{\exfunction}{\indexedcatvariableof{},\selvariable} \, . \]
More generally, we show in the next Lemma how to construct to any tensor and any partition of its variables functions by slicing operations, such that the tensor is the selection encoding of the function.

\begin{lemma}\label{lem:inverseSelectionEncoding} % To be used for MLN - proposal distribution
	Let $\hypercoreat{\nodevariables}$ be a tensor in $\bigotimes_{\nodein}\rr^{\catdimof{\node}}$ and let $\nodesa$, $\nodesb$ be a disjoint partition of $\nodes$, that is $\nodesa\dot{\cup}\nodesb=\nodes$.
	Then the function
		\[ \exfunction : \bigtimes_{\node\in\nodesa}[\catdimof{\node}] \rightarrow \bigotimes_{\node\in\nodesb} \rr^{\catdimof{\node}}  \]
	with coordinates
		\[ \exfunction(\catindexof{\nodesa}) = \hypercoreat{\indexedcatvariableof{\nodesa},\catvariableof{\nodesb}}  \]
	obeys
		\[ \sencodingofat{\exfunction}{\nodevariables} = \hypercoreat{\nodevariables} \, . \]
	Here we have renamed the selection variables $\selvariableof{\nodesa}$ by the categorical variables $\catvariableof{\nodesa}$. 
\end{lemma}
\begin{proof}
	From Theorem~\ref{the:linearCompositionBasisEncoding} using the basis encoding equivalence of Theorem~\ref{the:selectionToBasisEncoding}.
\end{proof}


\begin{example}[Markov Logic Networks and Proposal Distributions]
	% Via inverse selection encodings
	While the statistic of MLN (namely $\fselectionmap$) and the proposal distribution (namely $\tranfselectionmap$) have a common selection encoding, both result from the inverse selection encoding described in \lemref{lem:inverseSelectionEncoding}.
	We can construct $\tranfselectionmap$ by first building the selection encoding to $\fselectionmap$ and then applying the construction of \lemref{lem:inverseSelectionEncoding} with $\nodesa=\selvariable$ and $\nodesb=\shortcatvariables$.
\end{example}


% Composition
We use selection encodings to express compositions of functions, based on the next theorem.

\begin{theorem}[Selection Encoding for Linear Compositions]\label{the:linCompSelEncoding}
	Let $\sstat$ be a tensor valued function from $\facstates$ to $\simpleparspace$ with image coordinates $\sstat_\statenumerator$ and let $\exfunction$ be a tensor 
	Then
		\[ \left(\sum_{\statenumeratorin}\exfunction[\selvariableof{\sstat}=\statenumerator]\cdot \sstat_\statenumerator \right) [\shortcatvariables] : \facstates \rightarrow \rr \]
	is represented as
		\[ \left(\sum_{\statenumeratorin}\exfunction[\selvariableof{\sstat}=\statenumerator]\cdot \sstat_\statenumerator \right) [\shortcatvariables] 
		 = \sbcontractionof{\sencodingofat{\sstat}{\shortcatvariables,\selvariableof{\sstat}} , \exfunction[\selvariableof{\sstat}]}{\shortcatvariables} \, . \]
\end{theorem}
\begin{proof}
	The representation holds, since for any $\catindexof{[\atomorder]}\in\facstates$ we have
	\begin{align*}
		\sbcontractionof{\sencodingofat{\sstat}{\shortcatvariables,\selvariableof{\sstat}} , \exfunction[\selvariableof{\sstat}]}{\indexedcatvariableof{[\atomorder]}}  
		= \sum_{\statenumeratorin}\exfunction[\selvariableof{\sstat}=\statenumerator]\cdot\sstat_\statenumerator(\catindexof{[\atomorder]}) \, . 
	\end{align*} 
\end{proof}

% Linear 
This theorem shows, that while relation encodings can represent any composition with another function by a contractions, selection encodings can be used to represent linear transforms.
To see this, we interpret $\sstat$ and $\exfunction$ in Theorem~\ref{the:linCompSelEncoding} as basis decompositions of linear maps.


\subsection{Directed Tensors}

\red{
We in this chapter investigate the properties of tensors, which where arising in probabilistic and logical reasoning.
We observed already before, that
}
\begin{itemize}
	\item Conditional probability tensors are directed tensors.
	\item Logical formulas are binary tensors.
\end{itemize}

\red{
Thus, the set of tensors, which are both directed and binary is of much interest. 
We will show, that they are equal to the set of relational encodings of functions.
}

Directionality as defined in \defref{def:directedTensor} represents the constraints on the structure of tensors:
Summing over outgoing trivializes the tensor.
%We will use this property to find efficient algorithms.


\begin{definition}[Directed Hypergraph]
	A directed hyperedge is a hyperedge, which node set is split into disjoint sets of incoming and outgoing nodes.
	We say a hypercore $\hypercoreof{\edge}$ decorating a directed hyperedge respects the direction, when it is a conditional probability tensor with respect to the direction of the hyperedge. 
	The hypergraph is acyclic, when there is no nonempty cycle of node tuples $(\node_1,\node_2)$, such that $\node_1$ is an incoming node and $\node_2$ an outgoing node of the same hyperedge.
\end{definition}

%\begin{definition}
%	A directed Tensor Network is a directed hypergraph, which is decorated by tensors respecting the directionality of the respective edges.
%\end{definition}


% Multiple Directions possible
There can be multiple ways to direct a tensor, which an extreme example being the Dirac Delta Tensors.
Further example are relational encodings of invertible functions.

\begin{example}[Dirac Delta Tensors]
	Given a node set $\edge$ colored with a constant dimension $\catdim$ Diracs Delta Tensors are the tensors
		\[ \dirdeltaof{\edge,\catdim} \in \bigotimes_{\node\in\edge} \rr^{\catdim} \]
	with coordinates
	\begin{align}
		\dirdeltaof{\edge,\catdim}_{\catindices} = 
		\begin{cases}
			1 \quad & \text{if} \quad \catindexof{0} = \ldots = \catindexof{\atomorder-1} \\
			0 & \text{else}
		\end{cases} \, . 
	\end{align}
	The contractions with respect to subsets $\secnodes \subset \edge$ are
	\begin{align}
		\contractionof{\{\dirdeltaof{\edge,\catdim}\}}{\catvariableof{\secnodes}} = 
		\begin{cases}
			\catdim & \text{if} \quad \nodes = \varnothing \\ 
			\ones & \text{if} \quad \cardof{\secnodes} = 1\\ 
			\dirdeltaof{\secnodes,\catdim} & \text{else}
		\end{cases} \, .
	\end{align}
	Thus are directed for any orientation of the respective edge with exactly one incoming variable.
\end{example}




% TRUE?
%% Hypernetworks
% We can use Diracs Delta Tensors to represent a tensor network on a hypergraph by a tensor network on a graph (that is edges contain at most two nodes).


\begin{lemma}\label{lem:deltification}
%	Let $\graph$ be a directed hypergraph, such that each node is at most in one hyperedge appearing in the outgoing nodes.
	Let $\graph=(\nodes,\edges)$ be a hypergraph and $\extnet$ a tensor network on $\graph$.
	We build a graph $\secgraph=(\secnodes,\secedges\cup\Delta^{\graph})$ and a tensor network $\tnetof{\secgraph}$ by % ! See Bethe Cluster Graph definition !
	\begin{itemize}
		\item Recolored Edges $\secedges = \{\tilde{\edge} \, : \, \edge\in \edges\}$ where $\tilde{\edge} = \{\node^{\edge} \, : \, \node\in\edge\}$, which decoration tensor $\hypercoreof{\tilde{\edge}}$ has same coordinates as $\hypercoreof{\edge}$
		\item Nodes $\secnodes = \bigcup_{\edge\in\edges}\tilde{\edge}$ %$\secnodes = \bigcup_{\edge\in\edges}\{\node^{\edge} \, : \, \node\in\edge \}$ 
		\item Delta Edges $\Delta^{\graph} =  \big\{ \{\node\} \cup \{\node^{\edge} \, : \, \edge\ni\node \} \, : \, \node\in\nodes \big\} $ each of which decorated by a delta tensor $\delta^{\{\node^{\edge} \, : \, \edge\ni\node \}}$
	\end{itemize}
	 Then we have
	 	\[ \contractionof{\extnet}{\catvariableof{\nodes}} =  \contractionof{\tnetof{\secgraph}}{\catvariableof{\nodes}}  \, . \]
\end{lemma}
\begin{proof}
	For any $\catindexof{\nodes}$ we have 
	\begin{align*}
		 \contractionof{\tnetof{\secgraph}}{\indexedcatvariableof{\nodes}} 
		 & = \contractionof{\{\hypercoreof{\tilde{\edge}}[\catvariableof{\{\node^{\edge} : \node\in\edge\}}] : \edge \in \edges \}\cup 
		 	\{\delta^{\{\node\} \cup \{\node^{\edge} \, : \, \edge\ni\node \}}[\catvariableof{\{\node^{\edge} : \edge\ni\node \}}, \indexedcatvariableof{\node} ]  : \node\in\nodes \}
		 }{\varnothing} \\
		 & =  \contractionof{\{\hypercoreof{\tilde{\edge}}[\catvariableof{\{\node^{\edge} : \node\in\edge\}} = \catindexof{\{\node : \node\in\edge\}} ] : \edge \in \edges \}
		 }{\varnothing} \\
		 & = \contractionof{\extnet}{\indexedcatvariableof{\nodes}} \, ,
	\end{align*}
	which establishes the claim.
\end{proof}



\subsubsection{Normation}


Normed tensors (see \defref{def:normation}) are directed and directed tensors invariant under normation wrt their incoming and outgoing variable, as we show next.

\begin{theorem}\label{the:normationDirected}
	For any tensor network $\extnet$ on variables $\nodes$ that can be normed with respect to $\innodes$ and $\outnodes$, the normation is directed with $\innodes$ incoming and $\outnodes$ outgoing.
\end{theorem}
\begin{proof}
	We have for any incoming state ${\atomlegindexof{\innodes}\in\bigtimes_{\node\in\innodes}\catdimof{\node}}$ that
	\begin{align*}
		\contractionof{\{\normationofwrt{\extnet}{\innodes}{\outnodes}, \onehotmapof{\atomlegindexof{\innodes}} \}}{\varnothing} 
		& =  \frac{
		\contractionof{\extnet\cup\{\onehotmapof{\atomlegindexof{\innodes}}\}}{\varnothing}
		}{
		\contractionof{\extnet\cup\{\onehotmapof{\atomlegindexof{\innodes}}\}}{\varnothing}
		} \, .
	\end{align*}
	By \defref{def:directedTensor}, $\normationofwrt{\extnet}{\outnodes}{\innodes}$ is thus directed.
%
%	We have for any basis tensor $\onehotmapof{\atomlegindexof{\secnodes}}$
%	\begin{align}
%		\contractionof{
%			\{ \normationof{\hypercore}{\secnodes}, \onehotmapof{\atomlegindexof{\secnodes}} \}
%		}{
%			\varnothing
%		}
%		= \frac{
%			\contractionof{\{\hypercore, \onehotmapof{\atomlegindexof{\secnodes}} \}}{\varnothing}
%		}{
%			\contractionof{\{\hypercore, \onehotmapof{\atomlegindexof{\secnodes}} \}}{\varnothing}
%		} 
%		= 1 \, . 
%	\end{align}
%	Thus 
%	\begin{align}
%		\contractionof{
%			\{ \normationof{\hypercore}{\secnodes} \}
%		}{
%			\secnodes 
%		}
%		= \ones
%	\end{align}
%	and by \defref{def:directedTensor}, $\normationof{\hypercore}{\secnodes}$ is directed with the variables $\secnodes$ incoming.
\end{proof}


The normation operation coincides in cases of non-negative tensors with the conditioning of a Markov Network representing a probability distribution.


\subsubsection{Contraction of Directed Tensors}

Let us now investigate, which contractions inherit the directionality of the tensors.

%Next we state that specific contraction of conditional probability tensors are still conditional probability tensors.

%\red{Can be extended to single outgoing legs, by using delta tensors at hyperedges.}

%\begin{theorem}
%	Given a directed acyclic hypergraph, which hyperdedges are decorated by tensor cores respecting the direction.
%	Then the contractions, where all closed nodes appear exactly once as an incoming node and exactly once as an outgoing node, and where all open nodes appear in single hyperedges, are conditional probability tensors
%\end{theorem}
%\begin{proof}
%	It is enough to show this property on the contraction of two hypercores.
%	Since the hypergraph is acyclic, the coinciding nodes are all outgoing on the one and incoming to the other hyperedge.
%	Let the hyperedge with the incoming nodes $\edge_1$ and the one with the outgoing nodes $\edge_2$
%	We need to show that when further contracting the contraction with trivial tensors on the outgoing and basis tensors on the incoming legs we get $1$.
%	For any $\catindex$ and $\seccatindex$ this holds since

%	Here we used in the first equality, that $\hypercoreof{\edge_1}$ is a conditional probability tensor and in the second the same property for $\hypercoreof{\edge_2}$.
%\end{proof}

% Hadamard does not preserve probabilities
%We need to ass as assumption in Theorem~\ref{the:conditionalContractionPreservation}, that each node is to at most one hyperedge and to at most one hyperedge outgoing.
%This is due to the failure of Hadamard products of probability tensors to be probability tensors themself.


%\begin{lemma}\label{lem:twoDirectedContracted}
%	Given two 
%\end{lemma}





\begin{theorem}\label{the:conditionalContractionPreservation}
	Let $\graph=(\nodes,\edges)$ be a directed acyclic hypergraph, such that each node $\node\in\edge$ appears at most in one hyperedge as an outgoing variable and denote $\innodes$ as those nodes, which do not appear as outgoing variables.
	For any tensor network $\extnet$ respecting the direction of $\graph$ we have that
		\[ \contractionof{\extnet}{\catvariableof{\innodes}} = \onesat{\catvariableof{\innodes}} \, , \]
	that is $\contractionof{\extnet}{\catvariableof{\nodes}}$ is a directed tensor with $\innodes$ incoming and $\nodes/\innodes$ outgoing.
\end{theorem}
\begin{proof}
	We show the theorem only for the case of hypergraphs, where variables are appearing at most in two hyperedges.
	If a hypergraph fails to satisfy this assumption, we apply \lemref{lem:deltification} and add delta tensors copying the variables, which are appearing in multiple tensors, and arrive at a tensor network with nodes appearing in at most two hyperedges.
	When orienting each edge 
	
	% Approach: Contracting with ones
	We show the theorem over induction on the number $n$ of cores.
	\paragraph{$n=1$:} It holds trivially, when $\extnet$ consists of a single core
	\paragraph{$n\rightarrow n-1$:} Let us assume, that the claim holds for graphs with $n-1$ hyperedges and let $\tnetof{\graph}$ be a tensor network with $n$ hyperedges.
	Since the hypergraph is acyclic, we find an edge $\edge\in\edges$ such that all outgoing nodes of $\edge$ are not appearing as an incoming node in any edge. 
	We then apply Theorem~\ref{the:splittingContractions} and get
	\begin{align*}
		\contractionof{\extnet}{\catvariableof{\innodes}} 
		&= \contractionof{
			\tnetof{(\nodes,\edges/\{\edge\})} \cup \{\hypercoreofat{\edge}{\catvariableof{\incomingnodes},\catvariableof{\outgoingnodes}}\}
			}{\catvariableof{\innodes}} \\
		& = \contractionof{
			\tnetof{(\nodes,\edges/\{\edge\})} \cup \{\sbcontractionof{\hypercoreof{\edge}}{\catvariableof{\incomingnodes}} \}
			}{\catvariableof{\innodes}} \\
		& = \contractionof{
			\tnetof{(\nodes,\edges/\{\edge\})} \cup \{\onesat{\catvariableof{\incomingnodes}} \}
			}{\catvariableof{\innodes}} \\
		& \contractionof{
			\tnetof{(\nodes,\edges/\{\edge\})} \}
			}{\catvariableof{\innodes}} \, . 
	\end{align*}
	We then notice that the hypergraph $(\nodes,\edges/\{\edge\})$ has $n-1$ hyperedges and each node appears at most once as an incoming and at most once as an outgoing node.
	Thus, we apply the assumption of the induction and get
	\begin{align*}
		\contractionof{\extnet}{\catvariableof{\innodes}} = \contractionof{
			\tnetof{(\nodes,\edges/\{\edge\})} \}
			}{\catvariableof{\innodes}} = \onesat{\catvariableof{\innodes}} \, . 
	\end{align*}
	
%	% ALTERNATIVE APPROACH	
%	We then iteratively choose two neighbored tensors and replace them by their contraction, which is by \lemref{lem:twoDirectedContracted} itself directed and not changing the contraction by  Theorem~\ref{the:splittingContractions}.
%	When no such neighbored tensors are left, we have an tensor product of directed tensors, which are not sharing variables.
%	This outer product is trivially directed.
\end{proof}


% Overwork!
Schematically, the direction conservation property argument is depicted as:
\begin{center}
	\input{PartIII/tikz_pics/coordinate_calculus/cond_contraction_property.tex}
\end{center}






\subsection{Discussion}

Representations of linear maps is the typical application of tensors, reason for refering to tensor networks as multilinear algebra.
