\chapter{Notation and Basic Concepts}\label{cha:TensorNetworks}\label{cha:notation}

We here provide the fundamental definitions of tensors, which are essentiell for the content in \parref{par:one} and \parref{par:two}.
In \parref{par:three} we will further investigate the properties of tensors focusing on their contractions.

\sect{\bncategoricals}

We will in this work investigate systems, which are described by a set of properties, each called categorical variables.
This is called an ontological commitment, since it defines what properties a system has.

\begin{definition}
	An atomic representation of a system is described by a categorical variables $\catvariable$ taking values $\catindex$ in a finite set
		\[  [\catdim]\coloneqq \{0,\ldots, \catdim-1\} \]
	of cardinality $\catdim$.
\end{definition}

% Notation: Large and small literals
We will in this work always notate categorical variables by large literals and indices by small literals, possible with other letters such as $\catvariable,\selvariable,\indvariable,\datvariable$ and corresponding values $\catindex,\selindex,\indindex,\datindex$.

\begin{definition}
	A factored representation of a system is a set of categorical variables $\catvariableof{\atomenumerator}$, where $\atomenumeratorin$, taking values in $[\catdimof{\atomenumerator}]$.
\end{definition}

\sect{\bntensors}

% Gentle introduction sentences
Tensors are multiway arrays and a generalization of vectors and matrices to higher orders.
We will first provide a formal definition as real maps from index sets enumerating the coordinates of vectors, matrices and larger order tensors.

\begin{definition}[Tensor]\label{def:tensor}
	Let there be numbers $\catdimof{\atomenumerator}\in\nn$ for $\atomenumeratorin$ and categorical variables $\catvariableof{\atomenumerator}$ taking their values in $[\catdimof{\atomenumerator}]$.
	We call maps
	\begin{align*}
		\hypercoreat{\catvariables} : \bigtimes_{\atomenumeratorin} [\catdimof{\atomenumerator}] \rightarrow \rr
	\end{align*}
	tensor of order $\atomorder$ and leg dimensions $\catdimof{0},\ldots,\catdimof{\atomorder-1}$.
	Evaluations of these maps at indices $\catindices$ are denoted by
	\begin{align*}
		\hypercoreat{\indexedcatvariables} = \hypercoreat{\catvariables}(\catindices)\, .
	\end{align*}
%	with coordinates denoted by $\hypercore_{\catindices}$ is called a tensor of order $\atomorder$ and legs with the dimensions $\catdimof{0},\ldots,\catdimof{\atomorder-1}$.
	Tensors $\hypercoreat{\catvariables}$ are elements of the space
	\begin{align*}
		\bigotimes_{\atomenumeratorin} \rr^{\catdimof{\atomenumerator}} \,
	\end{align*}
	which is, with the operations of coordinatewise summation and scalar multiplication, a linear space called a tensor space.
\end{definition}

% Non-canonical 
We here introduced tensors in a non-canonical way based on categorical variables assigned to its axis.
While coming as syntactic sugar at this point, this will allow us to define contractions without further specification of axes, based on comparisons of shared categorical variables.
Especially, this eases the implementation of tensor network contractions without the need to further specify a graph (see Appendix~\ref{cha:implementation}).

% Further abbreviations
We abbreviate lists $\catvariables$ of categorical variables by $\shortcatvariables$, that is denote $\hypercoreat{\catvariables}$ by $\hypercoreat{\shortcatvariables}$.
Occasionally, when the categorical variables of a tensor are clear from the context, we will omit the notation of the variables. %further abbreviate $\hypercoreat{\catvariables}$ by $\hypercore$.

\begin{example}[Trivial Tensor]\label{exa:trivialTensor}
	The trivial tensor is defined as the map
		\[ \onesat{\shortcatvariables} : \facstates \rightarrow \{1\} \subset \rr \]
	with all coordinates being $1$, that is for all $\catindices\in\facstates$
		\[ \onesat{\indexedshortcatvariables} = 1 \, . \]
\end{example}

%\sect{Properties of Tensors}

%% Boolean
We will often encounter situations, where the coordinates of tensors are in $\{0,1\}=[2]$.

\begin{definition}\label{def:booleanTensor}
	We call a tensor $\hypercoreat{\shortcatvariables}$ boolean, when $\imageof{\hypercore}\subset[2]$, i.e. all coordinates are either $0$ or $1$.
\end{definition}

%\sect{One-hot encodings}

We are now ready to provide the link between tensors and states of systems with factored representations.
To this end, we define the one-hot encoding of a state, which is a bijection between the states and the basis elements of a tensor space.

\begin{definition}[One-hot encodings to Atomic Representations]
	Given an atomic system described by the categorical variable $\catvariable$, we define for each $\catindex\in[\catdim]$ the basis vector $\onehotmapofat{\catindex}{\catvariable}$ by
	\begin{align}
		\onehotmapofat{\catindex}{\catvariable=\tilde{\catindex}} = \begin{cases}
			1 & \text{if} \quad \catindex=\tilde{\catindex} \\
			0 & \text{else} \, .
		\end{cases}
	\end{align}
	The one-hot encoding of states $\catindex\in[\catdim]$ of the atomic system described by the categorical variable $\catvariable$ is the map
		\[ \onehotmap: [\catdim] \rightarrow \rr^\catdim \]
	which maps $\catindex \in [\catdim]$ to the basis vectors $\onehotmapofat{\catindex}{\catvariable}$.
\end{definition}

% Coordinatewise representation
The basis vectors $\onehotmapofat{\catindex}{\catvariable}$ are tensors of order $1$ and leg dimension $\catdim$ of the structure
\begin{align}
	\onehotmapofat{\catindex}{\catvariable} = \begin{bmatrix}
	0 & \cdots & 0 & 1 & 0 & \cdots & 0
	\end{bmatrix}^T \, ,
\end{align}
where the $1$ is at the $\catindex$th coordinate of the vector.

% Atomic -> Factored system
We have so far described one-hot representations of the states of a single categorical variable, which would suffice to encode the state of an atomic system.
In a factored system on the other side, we are dealing with multiple categorical variables.

\begin{definition}[One-hot encodings to Factored Representations]\label{def:oneHotEncoding}
	Let there be a factored system defined by a tuple $(\catvariables)$ of variables taking values in $\facstates$.
	The one-hot encoding of its states is the tensor product of the one-hot encoding to each categorical variables, that is the map
		\[ \onehotmap : \facstates \rightarrow  \facspace \]
	defined by mapping $\catindices=\shortcatindices$ to
	\begin{align*}
		 \onehotmapofat{\shortcatindices}{\shortcatvariables}
		=: \bigotimes_{\atomenumeratorin} \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}} \, .
	\end{align*}
	We will call one-hot representations \emph{tensor representations} and depict them as
	\begin{center}
		\input{OtherContent/tikz_pics/notation/one_hot_tensorproduct.tex}
	\end{center}
\end{definition}

In \charef{cha:coordinateCalculus} we will investigate the image of $\onehotmap$ in more detail and show that it is an orthonormal basis of the tensor space $\facspace$.

\begin{remark}[Flattening of Tensors]
	The use the tensor product to represent states of factored systems can be motivated by the reduction to atomic systems by enumeration of the states.
	We have this property reflected in the state encoding of factored systems, since the tensor space $\facspace$ is isomorphic to the vector spaces $\rr^{\prod_{\atomenumeratorin}\catdimof{\atomenumerator}}$.
	This operation is called flattening (or unfolding) of tensors with many axes to tensors of less axes.
\end{remark}

\sect{\bncontractions}

Contractions are the central manipulation operation on sets of tensors.
To introduce them, we will develop a graphical illustration of sets of tensors, which we also call tensor networks.
In \parref{par:three} we will further investigate the utility of contractions in representing specific calculations, which demand different encoding schemes.


\subsect{Graphical Illustrations}

% Hypergraph as capturing the categorical variable assignment to tensors
Sets of tensor with categorical variables assigned to each legs implicitly carry a notion of a hypergraph.
This perspective is especially useful, when some categorical variables are assigned to axis of multiple tensors, as it will often be the case in the applications considered in this work.
Each variable can then be labeled by a node and each tensor as a hyperedge containing the nodes to its axis variables.
Let us first formally introduce hypergraphs, which are generalizations of graphs allowing edges to be arbitrary nonempty subsets of the nodes, whereas canonical graphs demand a cardinality of two.

\begin{definition}\label{def:hypergraphs}
	A hypergraph is a pair $\graph=(\nodes,\edges)$ of a set of nodes $\nodes$ and a set of edges $\edges$, where each hyperedge $\edge\in\edges$ is a subset of the nodes $\nodes$.
	A directed hypergraph is a pair $\graph=(\nodes,\edges)$, such that each hyperedge $\edge\in\edges$ is the tuple of two disjoint sets $\incomingnodes,\outgoingnodes\subset\nodes$, that is
		\[ \edge = (\incomingnodes,\outgoingnodes)  \, . \]
\end{definition}

% Diagrammatic representation in factor graphs
We will use the standard visualization by factor graphs as a diagrammatic illustration of sets of tensors, where tensors are represented by block nodes and each axis assigned with by a categorical variable $\catvariableof{\atomenumerator}$ represented by a node, see Figure~\ref{fig:tensors}a).
%We further denote on Each axis of the tensor is represented by a node representing the variable $\catvariableof{\atomenumerator}$ and the tensor $\hypercore$ is associated with the hyperedge $\edge$ connecting all variables.
 %representing the choice of an element in the set $[\catdimof{\atomenumerator}]$
% Hyperedge view
Different simplifications of these factor graph depictions have been evolved in different research fields.
In the tradition of graphical models, which started with the work \cite{pearl_probabilistic_1988}, the categorical variables are highlighted and the tensor blocks just depicted by hyperedges.
To depict dependencies with causal interpretations, the edges are further decorated by directions in the depiction of Bayesian networks, see for example \cite{pearl_causality_2009}.

In the tensor network community on the other hand, a simplification scheme highlighting the tensors as blocks and omitting the depiction of categorical variables has been evolved.
The variables, or sometimes their index or dimension, are then directly assigned to the lines depicting the axes of the tensor blocks.
This depiction scheme has been established in the literature as wiring diagrams (see \cite{landsberg_tensors_2011} and dates back at least to the work \cite{penrose_spinors_1987}.

Both depiction schemes are simplifications of factor graphs, by highlighting the categorical variables in the depiction in Figure~\ref{fig:tensors}b) and the tensors in the depiction in Figure~\ref{fig:tensors}c).
We in this work will prefer the simplification of the tensor network community, depicted in Figure~\ref{fig:tensors}b).

% Duality
In another interpretation (see \cite{robeva_duality_2019}), both simplification schemes are different hypergraphs, which are dual to each other.

\begin{figure}[h!]
	\begin{center}
		\input{OtherContent/tikz_pics/notation/hypercore.tex}
	\end{center}
	\caption{Depiction of Tensors
	a) As a factor in a factor graph, depicted by a block, and connected to categorical variables assigned to nodes.
	b) Highlighting only the variable dependencies by a hyperedge connecting the variables $\catvariableof{\atomenumerator}$ to each axis $\atomenumeratorin$.
	c) Highlighting the tensor by a blockwise notation with axes denoted by open legs represented by the variables $\catvariableof{\atomenumerator}$.
	}\label{fig:tensors}
\end{figure}


% Diagramatic representation of vectors
To depict vector calculus and its generalizations, we will apply the graphical notation (mainly version b) introduced in \charef{cha:TensorNetworks}.
Along this line, we represent vectors and their generalization to tensors by blocks with legs representing its indices.
The basis vectors being one-hot encodings of states are in this scheme represented by
	\begin{center}
		\input{OtherContent/tikz_pics/notation/one_hot_atomic.tex}
	\end{center}
where $\tilde{\catindex}$ is an indexed represented by an open leg.
Assigning $\catindex$ to this index will retrieve the $\catindex$th coordinate (with value $1$), whereas all other assignments will retrieve the coordinate values $0$.


Drawing on the interpretation of tensors by hyeredges we can continue with the definition of tensor networks.

\begin{definition}\label{def:tensorNetwork}
	Let $\graph=(\nodes,\edges)$ be a hypergraph with nodes decorated by categorical variables $\catvariableof{\node}$ with dimensions
		\[ \catdimof{\node} \in \nn \]
	and hyperedges $\edge\in\edges$ decorated by core tensors
		\[ \hypercoreofat{\edge}{\catvariableof{\edge}} \in \bigotimes_{\node\in\edge}\rr^{\catdimof{\node}} \, , \]
	where we denote by $\catvariableof{\edge}$ the set of categorical variables $\catvariableof{\node}$ with $\node\in\edge$.
	Then we call the set
		\[ \tnetofat{\graph}{\catvariableof{\nodes}} = \{\hypercoreofat{\edge}{\catvariableof{\edge}}  \, : \, \edge\in\edges\} \]
	the Tensor Network of the decorated hypergraph $\graph$.
\end{definition}


\begin{figure}
	\begin{center}
		\input{OtherContent/tikz_pics/notation/network.tex}
	\end{center}
	\caption{
	Example of a tensor network on a
	a) hypergraph with edges $\edge_0=\{\catvariableof{0},\catvariableof{1},\catvariableof{2}\}$, $\edge_1=\{\catvariableof{1},\catvariableof{2}\}$ and $\edge_2=\{\catvariableof{2},\catvariableof{3}\}$,
	which is decorated by the tensor cores b), representing a contraction with leaving all variables open.
	}\label{fig:network}
\end{figure}

%%
%Diagrammatic notation: Best to do version a) as used in the definition, highlighting that tensors have shared categorical variables with fixed dimensions.




\subsect{Tensor Product}

% Diagrams -> Contractions
Let us now exploit the developed graphical representations to define contractions of tensor networks.
The simplest contraction is the tensor product, which maps a pair of two tensors with distinct variables onto a third tensor and has an interpretation by coordinatewise products.
Such a contraction corresponds with a tensor network of two tensors with disjoint variables, depicted as:
\begin{center}
	\input{OtherContent/tikz_pics/notation/tensor_product.tex}
\end{center}

\begin{definition}[Tensor Product]\label{def:tensorProduct}
	Let there be two tensor
	\begin{align*}
		\hypercoreofat{\edge_0}{\shortcatvariables} : \facstates \rightarrow \rr \quad \text{and} \quad  \hypercoreofat{\edge_1}{\secshortcatvariables} : \secfacstates \rightarrow \rr \,
	\end{align*}
	with different categorical variables assigned to its axes.
	Then there tensor product is the map
	\begin{align*}
		\contractionof{\hypercoreofat{\edge_0}{\shortcatvariables},\hypercoreofat{\edge_1}{\secshortcatvariables}}{\shortcatvariables,\secshortcatvariables} :  \left(\facstates\right) \times \left(\secfacstates\right) \rightarrow \rr
	\end{align*}
	defined for $\catindices\in\facstates$ and $\seccatindices\in\secfacstates$ as
	\begin{align*}
		& \contractionof{\hypercore,\sechypercore}{\indexedcatvariables,\indexedseccatvariables} \\
		&\quad\quad :=  \hypercoreofat{\edge_0}{\indexedcatvariables}\cdot \hypercoreofat{\edge_1}{\indexedseccatvariables} \, .
	\end{align*}
\end{definition}

% Other notations
Other popular standard notations of tensor products (see \cite{kolda_tensor_2009,hackbusch_tensor_2012,cichocki_tensor_2015})
	\[ \left(\hypercore \otimes \sechypercore\right) = \left(\hypercore \circ \sechypercore\right)
	= \contractionof{\hypercoreofat{\edge_0}{\shortcatvariables},\hypercoreofat{\edge_1}{\secshortcatvariables}}{\shortcatvariables,\secshortcatvariables}  \, . \]
We will avoid these notations in this work in favor of a consistent notation capable of depicting generic tensor network contractions.

When the tensor $\hypercoreofat{\edge_1}{\secshortcatvariables}$ coincides with the trivial tensor $\onesat{\secshortcatvariables}$ (see Example~\ref{exa:trivialTensor}), we further make a notation convention to omit that tensor, that is
\begin{align*}
	\contractionof{\hypercoreofat{\edge_0}{\shortcatvariables},\onesat{\secshortcatvariables}}{\shortcatvariables,\secshortcatvariables}
	= \contractionof{\hypercoreofat{\edge_0}{\shortcatvariables}}{\shortcatvariables,\secshortcatvariables} \, .
\end{align*}


\subsect{Generic Contractions}


Contractions of Tensor Networks $\extnet$ are operations to retrieve single tensors by summing products of tensors in a network over common indices.
We will define contractions formally by specifying just the indices not to be summed over.

When some of the variables are not appearing as leg variables, we define the contraction as being a tensor product with the trivial tensor $\ones$ carrying the legs of the missing variables.

\begin{definition}\label{def:contraction}
	Let $\tnetof{\graph}$ be a tensor network on a decorated hypergraph $\graph=(\nodes,\edges)$.
	For any subset $\secnodes\subset\nodes$ we define the contraction  to be the tensor
	\begin{align}
		\contractionof{\tnetof{\graph}}{\secnodevariables} \in \bigotimes_{\node\in\secnodes} \rr^{\catdimof{\node}}
	\end{align}
	defined coordinatewise by the sum
	\begin{align}
		\contractionof{\tnetof{\graph}}{\indexedcatvariableof{\secnodes}} =
		\sum_{\catindexof{\nodes/\secnodes} \in\,\nodestatesof{\nodes/\secnodes}}
		\left( \prod_{\edge\in\edges}\hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \right) \, .
	\end{align}
	We call $\secnodevariables$ the open variables of the contraction.
\end{definition}

To ease notation, we will often omit the set notation by brackets $\{\cdot\}$ and specify the tensors to be contracted with the delimiter "," (see e.g. \exaref{exa:matrixProduct}).

\begin{figure}
	\begin{center}
		\input{OtherContent/tikz_pics/notation/contraction.tex}
	\end{center}
	\caption{
		Example of a tensor network contraction of all but the variables $\catvariableof{1},\catvariableof{3}$.
		Contraction of variables can always be depicted by closing the open legs with trivial tensors $\ones$ performing index sums.
	}\label{fig:contraction}
\end{figure}

%%
%Diagrammatic notation: Best to do version b), since this is easiest to see how tensors combine to new tensors by contractions.

\begin{remark}[Alternative Notations]
	% Einstein summations
	Contractions can also denoted by the Einstein summations of the indices along connected edges, understood as scalar product in each subspace.
	This is as in \defref{def:contraction}, just omitting the sums.
	We found it useful in this work to do the diagrammatic representation instead, since it offers a better possibility to depict hierarchical arrangements of shared variables.
\end{remark}


% Mode products
Further notations without usage of axis variables are mode products (see \cite{kolda_tensor_2009,hackbusch_tensor_2012,cichocki_tensor_2015}), often denoted by the operation $\times_n$.
With our more generic variable-based notations, we can capture these more specific contractions by coloring the tensor axes, that is assignment of axis variables.

% Examples
To further gain familiarity with the generic contractions, we show the connection to two more popular examples.

%% Diagrammatic representation of Matrix Vector
\begin{example}{Matrix Vector Products}\label{exa:matrixProduct}
	The matrix vector product is a special case of tensor contractions, where a matrix $\matrixat{\exrandom,\secexrandom}$ shares a categorical variable with a vector $\vectorat{\secexrandom}$.
	When leaving the variable unique to the matrix open we get the matrix vector product as
		\[ \contractionof{\matrixat{\exrandom,\secexrandom},\vectorat{\secexrandom}}{\exrandom=\exrandind} = \sum_{\secexrandind\in[\secexranddim]} \matrixat{\exrandom=\exrandind,\secexrandom=\secexrandind} \cdot \vectorat{\secexrandom=\secexrandind} \, .  \]

	Exploiting the diagramatic tensor network visualization we depict matrix vector by: %in \figref{fig:matrixProduct}.
	\begin{center}
		\input{OtherContent/tikz_pics/notation/one_hot_matrix.tex}
	\end{center}
%	Here the index $j$ is represented by a closed edge, which means that it is eliminated by a sum.
\end{example}

%% Hadamard Product 
\begin{example}{Hadamard products of vectors}\label{exa:hadamard}
	A node appearing in arbitrary many hyperedges denotes a Hadamard product of the axis of the respective decorating tensors.
	To give an example, let $\vectorofat{\catenumerator}{\catvariable}\in\rr^\catdim$ be vectors for $\catenumeratorin$. Their hadamard product is the vector
		\[ \contractionof{\{\vectorofat{\catenumerator}{\catvariable} \, : \, \catenumeratorin\}}{\catvariable}  \in \rr^\catdim \]
	defined by
		\[ \contractionof{\{\vectorofat{\catenumerator}{\catvariable} \, : \, \catenumeratorin\}}{\indexedcatvariable}
		= \prod_{\atomenumeratorin} \vectorofat{\atomenumerator}{\indexedcatvariable}\, . \]
	In a contraction diagram the Hadamard product is depicted by: % in \figref{fig:hadamard}.
	\begin{center}
		\input{OtherContent/tikz_pics/notation/hadamard.tex}
	\end{center}
\end{example}



\subsect{Decompositions}

Tensors can be represented by tensor network decompositions, when the contraction of the network retrieves the tensor.

\begin{definition}\label{def:tnDecomposition}
	%Let $\hypercoreat{\nodevaraibles}$ be a tensor in $\extensorspace$.
	A Tensor Network Decomposition of a tensor $\hypercoreat{\nodevariables}$ is a Tensor Network $\tnetof{\graph}$ such that
		\[ \hypercoreat{\nodevariables}= \contractionof{\tnetof{\graph}}{\nodevariables} \, . \]
	We call the hypergraph $\graph$ the format of the decomposition.
\end{definition}





\subsect{Directed Tensors and normalizations}

%% Directionality
Directionality represents constraints on the structure of tensors, namely that the sum over outgoing trivializes the tensor.

\begin{definition}\label{def:directedTensor}
	A Tensor
		\[ \hypercoreat{\nodevariables} \in\bigotimes_{\nodein}\rr^{\catdimof{\node}} \]
	is said to be directed with incoming variables $\innodes$ and outgoing variables $\outnodes$, where $\nodes=\innodes\dot{\cup}\outnodes$, when
		\[ \contractionof{\hypercore}{\catvariablesinset{\outnodes}} =  \onesat{\catvariablesinset{\innodes}} \]
	where $\onesat{\catvariablesinset{\innodes}}$ denoted the trivial tensor in  $\bigotimes_{\node\in\innodes}\rr^{\catdimof{\node}}$ which coordinates are all $1$.
\end{definition}

While by default all legs are outgoing, we can change the direction by normalization.

\begin{definition}\label{def:normalization}
	A tensor $\hypercoreat{\nodevariables}$ is said to be normable on $\innodes\subset\nodes$, if for any $\catindexof{\innodes}\in\nodestatesof{\innodes}$ we have
		\[ \contraction{\hypercoreat{\nodevariables},\onehotmapofat{\atomlegindexof{\innodes}}{\catvariableof{\innodes}}} > 0 \, . \]
	The normalization of a on $\innodes\subset\nodes$ normable tensor is the tensor
	\begin{align*}
		\normalizationofwrt{\hypercoreat{\nodevariables}}{\catvariableof{\outnodes}}{\catvariableof{\innodes}} =
		\sum_{\catindexof{\innodes}\in\nodestatesof{\innodes}}
		\onehotmapofat{\atomlegindexof{\innodes}}{\catvariableof{\innodes}} \otimes \frac{
		\contractionof{\hypercoreat{\nodevariables},\onehotmapofat{\catindexof{\innodes}}{\catvariableof{\innodes}}}{\catvariableof{\outnodes}}
		}{
		\contraction{\hypercoreat{\nodevariables},\onehotmapofat{\catindexof{\innodes}}{\catvariableof{\innodes}}}
		}
	\end{align*}
	where $\outnodes = \nodes/\innodes$.
\end{definition}

We will investigate the contractions of directed tensors in \parref{par:three}, where we show in Theorem~\ref{the:normalizationDirected} that normalizations are directed tensors.


%% Diagrammatic notation
In our graphical tensor notation, we depict directed tensors by directed hyperedges (a), which are decorated by directed tensors (b), for example:
	\begin{center}
		\input{OtherContent/tikz_pics/notation/directed_core.tex}
	\end{center}



\sect{\bnencoding}

Tensors are defined here as real-valued functions on the state set of a system described by categorical variables.
We provide further schemes to represent functions in order to perform sparse calculus and to handle more generic functions.



%
%\subsect{Real-valued functions}
%\begin{example}[Uncertainty about States]\label{exa:onehotUncertainty}
%	The uncertainty about the state of a categorical variable $\catvariable$ can be expressed in vectors.
%	For example let there be real numbers $\probat{\catvariable=\catindex} \in [0,1]$ for $\catindex\in[\catdim]$ with $\sum_{\catindex\in[\catdim]}\probat{\catvariable=\catindex}=1$ with the interpretation that $\probat{\catvariable=\catindex}$ is the probability of a system being in state $\catindex$.
%	We can represent this uncertain state simply by a vector 
%		\[ \probat{\catvariable}\in\rr^{\catdim} \]
%	defined as the sum of one-hot representations weighted by $\probat{\catvariable=\catindex}$
%	\[ \sum_{\catindex\in[\catdim]} \probat{\catvariable=\catindex} \cdot \onehotmapofat{\catindex}{\catvariable} =
%		\begin{bmatrix}
%		\probat{\catvariable=0} & \probat{\catvariable=1} & \cdots & \probat{\catvariable=\catdim-1}
%		\end{bmatrix} \, . 
%	\]
%\end{example}



\subsect{Basis encodings}

%We have already observed in Example~\ref{exa:atomicFunction}, that any function of a categorical variable has a representation as a linear function acting on the one-hot encoding of the variable.
Let us now show how we can encode maps between factored systems.
The scheme is described in more generality and detail (encoding of subsets and relations) in \charef{cha:basisCalculus}, see \defref{def:functionRelationEncoding}.

\begin{definition}[Relation encoding of maps between Factored Systems]\label{def:functionRepresentation}
	Let $\exfunction$ be a function
		\[ \exfunction : \facstates \rightarrow  \secfacstates \]
	which maps the states of a factored system to variables $\catvariables$ to the states of another factored system with variables $\seccatvariables$.
	Then the tensor representation of $\exfunction$ is a tensor
		\[ \bencodingofat{\exformula}{\catvariables,\seccatvariables} \in  \left(\secfacspace\right) \otimes \left(\facspace\right)  \]
	defined by
	\begin{align*}
		& \bencodingofat{\exformula}{\seccatvariables,\catvariables} \\
		& \quad = \sum_{\catindices\in\facstates}
		\onehotmapofat{\exfunction(\catindices)}{\seccatvariables} \otimes  \onehotmapofat{\catindices}{\catvariables} \, .
	\end{align*}
\end{definition}

We depict basis encodings by directed tensors:
\begin{center}
	\input{OtherContent/tikz_pics/notation/bencoding.tex}
\end{center}


% Notation with image categorical variable
%When the categorical variables of the image factored system to a map $\exfunction$ are not specified otherwise, we will denote them by $\catvariableof{\exfunction}$.




\subsect{Tensor-valued functions}


%% TO DETAILLED HERE -> Part III?
\begin{definition}[Selection encoding of Maps between Factored Systems]\label{def:selectionEncoding}
	Given a tensor space $\selspace$ described by categorical variables $\selvariables$ and a tensor-valued function
		\[ \exfunction : \facstates \rightarrow \selspace \]
	the selection encoding of $\exfunction$ is a tensor
		\[ \sencodingofat{\exfunction}{\shortcatvariables,\shortselvariables} \in \left(\facspace\right) \otimes \left(\selspace\right) \]
	defined by the basis decomposition
		\[ \sencodingofat{\exfunction}{\shortcatvariables,\shortselvariables} = \sum_{\shortcatindicesin} \onehotmapofat{\catindices}{\shortcatvariables} \otimes \exfunction(\catindices)[\shortselvariables] \, .  \]
\end{definition}

%%
We call these tensor representation of maps selection encodings, since the coordinate of a function $\exfunction$ to be processed is selected by another argument to $\sencodingof{\exfunction}$.

%\begin{example}[Vector valued functions]\label{exa:atomicFunction} %% CONFUSIN, since already needs selection variables?
%	When using a one-hot representation of the state of a categorical variable, any real valued function has a representation by a real valued matrix acting on the one-hot encoding. 
%	Let there be a vector valued function
%		\[ \exformula : [\catdim] \rightarrow \rr^p \]
%	which maps $\catindex\in[\catdim]$ to the vector
%		\[ \exformula(\catindex)[\selvariable] \in \rr^p \, , \]
%	where we introduced the variable $\selvariable\in[p]$ selecting a coordinate of the image vector.
%	The 
%		\[ \exformula(\catindex)[\selvariable] = 
%		\contractionof{\{\onehotmapof{\catindex}[\catvariable] , \,\concore_{\exformula}[\catvariable,\selvariable]\}}{\selvariable}  \]
%	where $\concore_{\exformula} \in \rr^{\catdim \times p} $ is the matrix defined by the function evaluation vectors of $\exformula$ as
%		\[ \concore_{\exformula}[\catvariable,\selvariable] = \begin{bmatrix}
%			-- & \exformula(0) & -- \\
%			-- & \exformula(1) & -- \\
%			& \vdots &  \\
%			-- & \exformula(\catdim-1) & -- 
%		\end{bmatrix} \, . 
%		\]
%	This can easily be verified, since matrix multiplication with basis vectors amounts to selection of rows (when the basis vector is acting from the left) or columns (when the basis vector is acting from the right).
%	Thus, linear transforms (matrices) acting on the one-hot representation are sufficient to represent any vector valued function of the states of a categorical variable.
%\end{example} 


%% 
We will provide more detail to the tensor representation of functions in \parref{par:three}, where we distinguish between embeddings for basis and coordinate calculus. %where we show that domain encodings coincide with selection encodings.





