Recent models in artificial intelligence, despite performance breakthroughs in large language models, suffer from limited efficiency and explainability, which prevents them from unlocking their full application potential for economic and trustworthy use.
We in this work leverage the mathematical structure of tensor networks, which has been eminent in artificial intelligence from the beginning, to achieve the goals of efficiency and explainability.

While tensors appear naturally in artificial intelligence as factored representations of systems, their decompositions into tensor networks is necessary to avoid the curse of dimensionality. %improve the efficiency and explainability of several approaches.
Since the curse of dimensionality prevents feasible generic representations, logical and probabilistic reasoning approaches trade off efficiency and generality.
This work presents these tradeoffs in the tensor network formalism and formulates feasible reasoning algorithms involving tensor network contractions.
We review the classical logical and probabilistic approaches to reasoning in the first part and develop applications in neuro-symbolic AI in the second part.
In the third part we investigate in more detail schemes to exploit tensor network contractions for calculus.