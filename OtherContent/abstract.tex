While tensors appear naturally in artificial intelligence as factored representations of systems, their decompositions into networks improve the efficiency and explainability of several approaches.
Since the curse of dimensionality prevents feasible generic representations and reasoning, logical and probabilistic reasoning focuses on tradeoffs between efficiency and generality.
In this work we present these tradeoffs based on the tensor network formalism and formulate feasible reasoning algorithms involving tensor network contractions.
We review the classical logical and probabilistic approaches to reasoning in the first part and develop applications in neuro-symbolic AI in the second part.
In the third part we investigate in more detail schemes to exploit tensor network contractions for calculus.