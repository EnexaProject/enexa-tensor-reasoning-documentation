\chapter{Abstract}

Recent models in artificial intelligence, despite performance breakthroughs in large language models, suffer from limited efficiency and explainability, which prevents them from unlocking their full application potential for economic and trustworthy use.
To train and infer large black-box models, an evolving infrastructure of hardware and software frameworks capable of large-scale parallelizable linear algebra workload has been created.
We in this work develop an approach towards an alternative usage of this infrastructure as processing efficient and explainable models instead of black-box models.
To this end we leverage the mathematical structure of tensor networks, which has been eminent in the logical and probabilistic tradition of artificial intelligence.

While tensors appear naturally in artificial intelligence as factored representations of systems, their decompositions into tensor networks is necessary to avoid the curse of dimensionality. %improve the efficiency and explainability of several approaches.
Since the curse of dimensionality prevents feasible generic representations, logical and probabilistic reasoning approaches trade off efficiency and generality.
While logical approaches focus on models with sparse description in a logical syntax, probabilistic approaches exploit independencies to motivate sparse graphical models.
This work presents a unified treatment of these sparsity mechanisms in the tensor network formalism and formulates feasible reasoning algorithms involving tensor network contractions.

In the first part of this work, we review the classical logical and probabilistic tradition of artificial intelligence in the tensor network formalism.
Exploiting the common framework of tensor networks, the second part describes the integration of these approaches into a neuro-symbolic framework, which we call hybrid logic networks.
In the third part we investigate in more detail schemes to exploit tensor network contractions for calculus.

The concepts of this work are implemented in the open-source \python library \tnreason.
This library underlies a modular design and specifies reasoning workload by tensor network operations, which is deligated towards various software frameworks of artificial intelligence.
In this way, \tnreason enables the usage of modern artificial intelligence infrastructure for efficient and explainable reasoning.