\chapter{Abstract}

%% Improved Abstract (30.9.25)

Recent advances in artificial intelligence, particularly in large language models, have achieved impressive performance but remain limited in efficiency and explainability, restricting their practical and trustworthy application.
Training and inference of such black-box models have driven the development of extensive hardware and software infrastructures optimized for large-scale, parallelizable linear algebra workloads.
In this work, we propose an alternative approach that leverages this infrastructure to build efficient and interpretable models rather than black-box systems.
Specifically, we exploit the mathematical structure of tensor networks, a framework with deep roots in the logical and probabilistic traditions of artificial intelligence.

Tensors naturally arise in AI as factored representations of complex systems, but their full representations suffer from the curse of dimensionality.
Tensor network decompositions mitigate this issue, enabling tractable reasoning while preserving essential structure.
Classical logical and probabilistic AI approaches have historically employed related sparsity-inducing decompositions: logical approaches emphasize sparse syntactic descriptions, while probabilistic approaches exploit conditional independencies to develop sparse graphical models.
In this work, we unify these sparsity mechanisms within the tensor network formalism and develop feasible reasoning algorithms based on tensor network contractions.

The first part of this work reviews the classical logical and probabilistic AI traditions in the formalism of tensor networks.
Building on this foundation, the second part introduces a neuro-symbolic framework, \HybridLogicNetworks{}, integrating these approaches.
The third part explores tensor network contraction schemes for reasoning and calculus in more detail.

We implement these concepts in the open-source \python{} library \tnreason{}, designed modularly to specify reasoning tasks as tensor network operations, which are then executed on existing AI software frameworks.
This enables modern AI infrastructure to support reasoning that is both efficient and interpretable.

%% Old Abstract

%Recent models in artificial intelligence, despite performance breakthroughs in large language models, suffer from limited efficiency and explainability, which prevents them from unlocking their full application potential for economic and trustworthy use.
%To train and infer large black-box models, an evolving infrastructure of hardware and software frameworks capable of large-scale parallelizable linear algebra workload has been created.
%We in this work develop an approach towards an alternative usage of this infrastructure as processing efficient and explainable models instead of black-box models.
%To this end we leverage the mathematical structure of tensor networks, which has been eminent in the logical and probabilistic tradition of artificial intelligence.

%While tensors appear naturally in artificial intelligence as factored representations of systems, their decompositions into tensor networks are necessary to avoid the curse of dimensionality.
%Since the curse of dimensionality prevents feasible generic representations, logical and probabilistic reasoning approaches trade off efficiency and generality by using such decomposition schemes.
%While logical approaches focus on models with sparse description in a logical syntax, probabilistic approaches exploit independencies to motivate sparse graphical models.
%This work presents a unified treatment of these sparsity mechanisms in the tensor network formalism and formulates feasible reasoning algorithms involving tensor network contractions.

%In the first part of this work, we review the classical logical and probabilistic tradition of artificial intelligence in the tensor network formalism.
%Exploiting the common framework of tensor networks, the second part describes the integration of these approaches into a neuro-symbolic framework, which we call \HybridLogicNetworks{}.
%In the third part we investigate in more detail tensor network contraction schemes for calculus.

%The concepts of this work are implemented in the open-source \python{} library \tnreason{}.
%Based on a modular design, \tnreason{} specifies various reasoning tasks as tensor network operations, which are delegated towards various software frameworks of artificial intelligence.
%In this way, \tnreason{} enables the usage of modern artificial intelligence infrastructure for efficient and explainable reasoning.
