\section{Introduction}

% Explaining the title
Artificial intelligence is a long-standing dream of humanity, which has in recent years received enourmous attention, driven by breakthroughs in large language models.
Among the key priorities towards an economic and trustworthy usage remain the creation of efficient and the explainable models.

% Explainability
Instead of post-hoc explainability of a models inference given specific data, our aim in this work is the intrinsic human understandability of a model.
We are motivated by the theory of logic, which formalization of human thoughts serves as an interface between mechanized reasoning on a machine and human understandability.
Having established this advanced form of explainability enables novel forms of human interactions with a model based on verbalizations, manipulations and guarantees on the models inference output.

% Efficiency
The desire of an efficient model originates more from an economic perspective on the realizability of a model and its power consumption.
Tensors appear naturally as representations of a system with multiple variables, both in logical and probabilistic approaches towards artificial intelligence. % avoid factored at this point!
However, already for moderate numbers of variables, the curse of dimensionality prevents a typical machines memory to store a generic representation.
The careful design of representation formats is therefore a necessary task to avoid the exponential increase of storage demands and balance the expressivity and the efficiency of representation formats.

% Tensor Networks
We in this work exploit the formalism of tensor networks in the creation of efficient representation schemes.
The chosen tensor network formats are motivated from explainable learning architectures and provide a synergy between the aims of efficiency and explainability.
Tensor networks appear as the natural numerical structures in probabilistic graphical models and logical knowledge bases.
After presenting the probabilistic and logical approaches based on the tensor network formalism we develop novel applications schemes towards neuro-symbolic artificial intelligence.

\subsection{Background}

Before presenting an overview over the contents, we further motivate this work based on the broach approaches towards artificial intelligence and more recent developments.

\subsubsection{Classical Approached towards AI}

We start with ontological commitments in the description of a system and follow the book \cite{russell_artificial_2021} distinguishing atomic, factored and structured representations.
While in atomic representation, the states of a systems are enumerated and represented in a single variable, factored representations describe a systems state based on a collection of variables.
In the tensor formalism, each state of a system corresponds with a coordinate of a representing tensor.
The order of the tensor coincides therefore with the number of variables in a system.
In an atomic representation, where there is a single coordinate, each state corresponds with a coordinate of the representing vector being a tensor of order one.
Having a factored representation with two variables requires order two tensors or matrices, where a coordinate is specified by a row and a column index.
Given larger numbers of coordinates now extends this representation picture to tensors of larger orders, which have more abstract axes besides rows and columns.
The generalization of the atomic representation to a factored system thus corresponds with the generalization of vectors towards matrices and tensors of larger orders.
Along this line, we can always transform a factored representation of a system to an atomic one, just by enumerating the states of the factored system and interpreting them by a single variable.
This amounts to the flattening of a representing tensor to a vector.
However, by doing so, we would loose much of the structure of the representation, which we would like to exploit in reasoning processes.

% Structured Representations
A more generic representation of systems are structured representation.
Structured representations involve objects of differing numbers and relations between them.
As a consequence the numbers of variables can differ depending on the state of a system.
This poses a challenge to the tensor representation, since a fixed number of variables is required to motivate a tensor space of representations.
There are approaches to circumvent these difficulty by the development of template models such as Markov Logic Networks \cite{richardson_markov_2006}, which are instantiated on systems with differing number of objects.
We will discuss those in \charef{cha:folModels}.

% Continous vs discrete
In this work we treat discrete systems, where the number of states is finite.
One can understand them as a discretization of continuous variables and many results will generalize by the converse limit to the situation of continuous variables.

% Epistemologic
Besides ontological commitments in the choice of a representation scheme, modelling a system also requires epistemologic commitments, by defining what properties are to be reasoned about.
In logical approaches the properties of states are boolean values representing whether a state is consistent with known constraints.
Probabilistic approaches assign to the coordinates of the tensors numbers in $[0,1]$ encoding the probability of a state.
Compared with logical approached to reasoning, probabilistic approaches thus bear a more expressive modelling.

\subsubsection{Logic and Explainability in AI}

\textbf{Inductive Logic Programming:}
\begin{itemize}
    \item ILP is a classical task \cite{muggleton_inductive_1994}
    \item Amie \cite{galarraga_amie_2013} is a method of learning Horn clauses using a refinement operator.
    \item Class Expression Learning \cite{lehmann_class_2011} is a more recent approach to assist in the design of reasoning capabilities in Knowledge Graphs.
        However, problems arise from the expressivity of description logics and the efficient choice of formulas from exponentially large hypothesis sets.
    \item CEL has therefore recently received further popularity in combination with reinforcement learning \cite{demir_drill-_2021} and neural networks \cite{kouagou_neural_2022, pesquita_neural_2023}, which are methods searching efficienctly in exponentially large spaces of formulas.
\end{itemize}

\textbf{Statistical Relational AI:} \cite{getoor_introduction_2019}
\begin{itemize}
    \item Classical combination of logical and probabilistic approaches to reasoning
\end{itemize}

\textbf{Knowledge Graphs}
\cite{hogan_knowledge_2021}
\begin{itemize}
    \item The advent of large Knowledge Graphs enables explainable reasoning methods on structured data.
    \item Knowledge Graphs are stored in a sparse format, i.e. only true atoms instead of all + truth label.
\end{itemize}


\subsubsection{Tensor Networks in AI}

\textbf{Tensor Network formats}
\begin{itemize}
    \item HT Format \cite{hackbusch_new_2009}
    \item CP Format
\end{itemize}

\textbf{Tensor Networks as Regressors}
\begin{itemize}
    \item Dynamical Systems learning \cite{gels_multidimensional_2019, goesmann_tensor_2020}
    \item Supervised learning CITE: Stoudenmire etc.
\end{itemize}

\textbf{Tensor Representation of Logics}
\begin{itemize}
    \item Tensor Networks have been applied in the automatization of logic reasoning \cite{li_linear_2017, sato_linear_2017} apply Matrix multiplication in reasoning.
    \item \cite{nickel_review_2016} review over relational machine learning and latent features via matrix embeddings.
\end{itemize}

\textbf{Tensor Representation of Knowledge Graphs}
\begin{itemize}
    \item Effective representation of queries
    \item Usage of tensor networks in embeddings \cite{yang_embedding_2015} and using complex extensions \cite{trouillon_complex_2017, trouillon_knowledge_2017}
\end{itemize}

\textbf{Tensor Representation of Graphical Models}
\begin{itemize}
    \item Duality of Graphical Models and Tensor Networks:
    \cite{robeva_duality_2019}
    \item Expressivity studies \cite{glasser_expressive_2019}
\end{itemize}

\subsubsection{Infrastructure of AI}

The formalism of tensors and their network decompositions and contractions bears the potential of parallel computations exploited in the AI-dedicated soft and hardware.
\begin{itemize}
    \item Hardware: TPUs beyond GPUs
    \item Software: Tensors as basic data structure in TensorFlow, pyTorch etc., storing neural activations and model weights.
\end{itemize}



\subsection{Structure of the work}

The chapters are structured into three parts, and two focuses, as sketched by:
\input{./OtherContent/tikz_pics/introduction/chapter_overview.tikz}

\textbf{\parref{par:one}: \partonetext} \\
\ \\
The probabilistic and logical approaches towards artificial intelligence are reviewed in the tensor network formalism. \\

Tensors appear naturally in
\begin{itemize}
    \item Logics: Boolean tensors indicating models (propositional case) and interpretation tensors in first order logics
    \item Probability theory: Truth tables, which are tensors of probabilities for joint distibutions of categorical variables.
\end{itemize}

% Classical usage of tensor network decompositions
Tensor network decompositions as representation schemes appear in
\begin{itemize}
    \item Logics: Conjunctions of formulas are Hadamard products of the tensor representation of formulas (Coordinate Calculus/ Effective Calculus)
    \item Probability theory: Graphical models are tensor networks of the factors. Further sparsity schemes apply, when placing restrictions on the structure of each factor.
    \item Data bases: Relations encoded by lists as storage of nonvanishing coordinates of a relation encoding
\end{itemize}

% Classical usage of tensor network contractions
Tensor network contractions as reasoning schemes appear in
\begin{itemize}
    \item Logics: Model counts, used for satisfiablility decisions and entailment
    \item Probability theory: Marginal probability distributions, extended to conditional probability distributions through normations
\end{itemize}

\ \\
\textbf{\parref{par:two}: \parttwotext} \\
\ \\
Motivated by the classical approaches we apply the tensor network formalism towards learning and infering neuro-symbolic models. \\

\textbf{Neurosymbolic AI}
\begin{itemize}
    \item Required for more advanced AI \cite{hochreiter_toward_2022}
    \item Add the paradigm of neural computing to logical reasoning
    \item Potential benefits from Statistical Relational AI \cite{marra_statistical_2024}
    \item Tensor based approaches \cite{cohen_tensorlog_2020}
    \item \cite{badreddine_logic_2022} representation of logic using tensor networks and automated differentiation to optimize.
\end{itemize}

%\subsubsection{Neuro-Symbolic AI}

\textbf{Tensor Approaches to Neuro-Symbolic AI}
\begin{itemize}
    \item TensorLog \cite{cohen_tensorlog_2020}
    \item \cite{badreddine_logic_2022} representation of logic using tensor networks and automated differentiation to optimize.
\end{itemize}

%% Decomposition of Neural Networks
In Deep Neural Networks, functions between the input layer and the output layer are decomposed into neurons.
Typical neurons are linear transforms with an activation function.

%% Sparsity by fixed architecture
Sparsity means restriction to functions, which are decomposable into a small number of neurons.
Approximations of generic functions (see the universal approximation theorems) would require large amounts of neurons. % CITE!
When restricting to functions based on a fixed architecture, we restrict to a certain set of functions called the inductive bias of the architecture.

\ \\
\textbf{\parref{par:three}: \partthreetext}\\
\ \\
The applied schemes of calculus using tensor network contractions are investigated in more detail.
\ \\
% Representation
\textbf{\focusonespec}\\
\\\
Here we motivate and investigate the efficient representation of tensors based on tensor network decompositions. \\
\ \\
% Reasoning
\textbf{\focustwospec}\\
\ \\
We develop schemes to efficiently perform inductive and deductive reasoning based on information stored in decomposed tensor.


%\subsubsection{\focusonespec}
%
%\subsubsection{\focustwospec}
%
%\subsubsection{\parref{par:one}: \partonetext}
%
%\subsubsection{\parref{par:two}: \parttwotext}
%
%\subsubsection{\parref{par:three}: \partthreetext}