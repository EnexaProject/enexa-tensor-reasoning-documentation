\section{Introduction}

Reasoning based on tensors has the advantage to
\begin{itemize}
	\item Provide a unifying approach to logical and probabilistic reasoning
	\item Define necessary workload of algorithms by contractions
\end{itemize}

% Factored representations of systems -> Tensors
The storage of numeric information about the states of a system with multiple variables is naturally a tensor.
Tensors appear naturally in 
\begin{itemize}
	\item Logics: Boolean tensors indicating models (propositional case) and interpretation tensors in first order logics
	\item Probability theory: Truth tables, which are tensors of probabilities for joint distibutions of categorical variables.
\end{itemize}

% Curse of dimensionality -> Tensor Networks
The curse of dimensionality renders the representations of tensors by a list of basis elements infeasible.
Therefore, sparse representation formats by tensor networks have been investigated.

Tensor networks appear in
\begin{itemize}
	\item Logics: Conjunctions of formulas are Hadamard products of the tensor representation of formulas (Coordinate Calculus/ Effective Calculus)
	\item Probability theory: Graphical models are tensor networks of the factors
\end{itemize}


% Databases
Furthermore, relational databases can be formalized by tensors.


\section{Literature}

\subsection{Broad topics}

\textbf{Inductive Logic Programming:}
\begin{itemize}
	\item ILP is a classical task \cite{muggleton_inductive_1994}
	\item Amie \cite{galarraga_amie_2013} is a method of learning Horn clauses using a refinement operator.
	\item Class Expression Learning \cite{lehmann_class_2011} is a more recent approach which has recently seen further popularity in combination with reinforcement learning \cite{demir_drill-_2021} and neural networks \cite{kouagou_neural_2022, pesquita_neural_2023}
\end{itemize}

\textbf{Statistical Relational AI:} \cite{getoor_introduction_2019}
\begin{itemize}
	\item Classical combination of logical and probabilistic approaches to reasoning 
\end{itemize}

\textbf{Neurosymbolic AI}
\begin{itemize}
	\item Required for more advanced AI \cite{hochreiter_toward_2022}
	\item Add the paradigm of neural computing to logical reasoning 
	\item Potential benefits from Statistical Relational AI \cite{marra_statistical_2024}
	\item Tensor based approaches \cite{cohen_tensorlog_2020}
	\item \cite{badreddine_logic_2022} representation of logic using tensor networks and automated differentiation to optimize.
\end{itemize}


\textbf{Knowledge Graphs}
\cite{hogan_knowledge_2021}
\begin{itemize}
	\item The advent of large Knowledge Graphs enables reasoning methods. 
	\item The bottleneck is the efficiency of methods to identify formulas from data.
	\item Knowledge Graphs are stored in a sparse format, i.e. only true atoms instead of all + truth label.
\end{itemize}

Â 


\subsection{Tensors Representations}

Tensor Representation is natural, if systems are described in factored representation.

\textbf{Tensor Network formats}
\begin{itemize}
	\item HT Format \cite{hackbusch_scheme_2009}
	\item CP Format 
\end{itemize}


\textbf{Tensor Representation of Logics}
\begin{itemize}
	\item Tensor Networks have been applied in the automatization of logic reasoning \cite{li_linear_2017, sato_linear_2017} apply Matrix multiplication in reasoning.
	\item \cite{nickel_review_2016} review over relational machine learning and latent features via matrix embeddings.
\end{itemize}

\textbf{Tensor Representation of Knowledge Graphs}
\begin{itemize}
	\item Effective representation of queries 
	\item Usage of tensor networks in embeddings \cite{yang_embedding_2015} and using complex extensions \cite{trouillon_complex_2017, trouillon_knowledge_2017}
\end{itemize}


\textbf{Tensor Representation of Graphical Models}
\begin{itemize}
	\item Duality of Graphical Models and Tensor Networks:
\cite{robeva_duality_2019}
	\item Expressivity studies \cite{glasser_expressive_2019}
\end{itemize}



\textbf{Tensor Networks as Regressors}
\begin{itemize}
	\item Dynamical Systems learning \cite{gels_multidimensional_2019, goesmann_tensor_2020}
\end{itemize}



\subsubsection{Neuro-Symbolic AI}

\textbf{Tensor Approaches to Neuro-Symbolic AI}
\begin{itemize}
	\item TensorLog \cite{cohen_tensorlog_2020}
	\item \cite{badreddine_logic_2022} representation of logic using tensor networks and automated differentiation to optimize.
\end{itemize}



%% Decomposition of Neural Networks
In Deep Neural Networks, functions between the input layer and the output layer are decomposed into neurons.
Typical neurons are linear transforms with an activation function.

%% Sparsity by fixed architecture
Sparsity means restriction to functions, which are decomposability into a small number of neurons.
Approximations of generic functions (see the universal approximation theorems) would require 
When restricting to functions based on a fixed architecture, the sparsity of functions  








\subsection{Represention schemes of systems}

\red{Following the book of Russel: Atomic, Factored and Structured Representations, which are framed as ontological commitments.}

%\section{Tensor Representation of Factored Systems}\label{cha:factoredRepresentation}

Tensors have frequent appearance in machine learning.
We show in this chapter how they are natural choices to encode states the states of factored systems, which are systems described by a collection of categorical variables.

\subsubsection{Ontological Commitments}


% Atomic Representations
Differing ontological commitments are made by atomic systems, where we just enumerate the state of a system and understand them as assignments to a single variable.
We can always transform a factored representation of a system to an atomic one, just by enumerating the tuples of states of the factored system and interpreting them as a single variable.
However, by doing so we would loose much of the structure of the representation, which we would like to exploit in reasoning processes.

%% TO DO:
% Factored Representations

% Structured Representations
A more generic representation of systems are structured representation.
In structured systems the numbers of variables can differ depending on the state the system is in.
\red{Limitation to factored representations: 
Dealing with structured representations would mean differing order of the representing tensors.}

% Continous vs discrete
In this work we treat discrete systems, where the number of states is finite. 
One can understand them as a discretization of continuous variables and many results will generalize by the converse limit to the situation of continous variables.






\subsubsection{Logical and Probabilistic Approaches}

Besides ontological commitments in the choice of a representation scheme, one also needs to make epistomologic commitments, by defining what properties we can reason about.
We can encode different properties of the system in the axes of the tensor representation.
In logical approaches the properties of states are Boolean values representing, for example, whether a state is consistent with specific constraints.
Probabilistic approaches represent on the coordinates of the tensors real numbers in $[0,1]$ which can be interpreted as probability of a state.
Compared with logical approached to reasoning, probabilistic approaches thus bear a more expressive modelling.


Tensor Networks can represent both probabilistic and logical reasoning on factored systems, since in the tensor space we can
\begin{itemize}
	\item Represent probability distributions by storing probabilistic values in each coordinate
	\item Represent set of states by sums over different one-hot encodings (this enables logical calculus)
\end{itemize}


%\subsubsection{Comparison of Representation Frameworks}

Both probability and logic provide a human-understandable interface to machine learning. 
As we will describe in the following sections, they can be combined in one formalism providing efficient reasoning.
This formalism of tensors along their network decompositions and contractions bears the potential of parallel computations exploited in the AI-dedicated soft and hardware. 

% Same thesises repeated??

\textbf{Probability} represents the uncertainty of states.
The categorical variables are called random variables and their joint distribution is represented by a probability tensor.
Humans can interpret probabilities by Bayesian and frequentist approaches.
Reasoning based on Bayes Theorem has an intuitive interpretation in terms of evidence based update of prior distributions to posterior distributions.
However it is based on interpreting (large amounts) of numbers, which makes it hard for humans to assess the probabilistic reasoning process.

\textbf{Logics} explains relations between sets of worlds in a human understandable way.
Categorical variables have dimension $2$, where the first is interpreted as indicating a $\falsesymbol$ state and the second as a $\truesymbol$ state.
We mainly restrict to propositional logics, where there are finite sets of such variables called atomic formulas.
Using model-theoretic semantics it defines entailment of sets by other sets, which is understandable as a consequence relation.

\textbf{Tensors} unify both approaches since they are natural mathematical structures to represent properties of states in factored systems. 
The potential is then based in employing efficient multilinear algorithms based on tensor network alternations to solve reasoning problems.
Further, algorithms formulates in tensor networks have a high parallelization potential, which is why they are of central interest in the development of AI-dedicated software and hardware.


The different areas have developed separated languages to describe similar objects.
Here we want to provide a rough comparison of those in a dictionary.

\begin{tabular}{l|l|l|l}
						& \textbf{Probability Theory}  					& \textbf{Propositional Logic} 			& \textbf{Tensors} \\
	\hline
	\textit{Atomic System} 	& Random Variable 		& Atomic Formula 		& Vector \\
	\textit{Factored System} 	& Joint Distribution 		& Knowledge Base 		& Tensor \\
	\textit{Categorical Variable} & Random Variable 	& Atomic Formula 		& Axis of the Tensor
\end{tabular}

While the probability theory lacks to provide an intuition about sets of events, propositional syntax has limited functionality to represent uncertainties.
Tensors on the other side can build a bridge by representing both functionalities and relying on probability theory and logics for respective interpretations.



%\subsection{Dictionary of Terms}

%\begin{tabular}{l|l|l|l}
%	 					& \textbf{Probability Theory}  					& \textbf{Propositional Logic} 			& \textbf{Tensors} \\
%	\hline
%	\textit{Sets of States}		& \textit{Measurable space}			& \textit{Syntax}					& \textit{Boolean Tensors} \\
%	Single State 			& elementary event 							& maximal clause (single model)		& Basis Tensor \\
%	Sets of states 			& measurable events						& generic formula (set of models) 		& Binary Tensors \\
%	Sparsest set of states 	& event to single variable 				& atomic formula 					& Basis Vector\\
%	Sparse set of states 		& \red{Limited intuition}				& short formula						& Tensor Network \\
%	\hline
%	\textit{Relation between sets}	& 									& \textit{Model-theoretic Semantics}		& \\
%	Inclusion of states		& Inclusion of events							& Entailment						& Invariance under Hadamard \\
%	\hline 
%	\textit{Modelling of Beliefs} & & & \\
%						& Probability of events 						& \red{Limitation to True/False/None}		& Values of Real Tensors 
%\end{tabular}









\subsection{Outline}

Part I: Review of classical approaches using tensor calculus

Part II: Neuro-symbolic learning

Part III: Numerical and Stochastic investigation of the calculus with tensors

