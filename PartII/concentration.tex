\section{\chatextconcentration}\label{cha:concentration}

When drawing data independently from a random distribution, we are limited by random effects.
We in this chapter derive guarantees, that the learning methods introduced in \charef{cha:probReasoning} and \charef{cha:networkReasoning} are robust against such effects.

\subsection{Fluctuations of random data}

A random tensor is a random element of a tensor space $\facspace$, drawn from a probability distribution on $\facspace.$
In contrast to the discrete distributions investigated previously in this work, the random tensors are in most generality continuous distributions. % However, when drawing data they are 

\subsubsection{Fluctuation of the empirical distribution}

% Random one hot encodings
When drawing random states $\datapoint\in\facstates$ by a distribution $\gendistribution$, we use the one-hot encoding to forward each random state to the random tensor
\[ \onehotmapofat{\datapoint}{\shortcatvariables} \, . \]
The expectation of this random tensor is
\begin{align*}
    \expectationof{\onehotmapof{\datapoint}}
    = \sum_{\shortcatindices\in\facstates} \gendistributionat{\indexedshortcatvariables} \onehotmapofat{\shortcatindices}{\shortcatvariables}
    = \gendistributionat{\shortcatvariables} \, .
\end{align*}

The empirical distribution is then the average of independent random one-hot encodings, namely the random tensor
\[ \empdistribution = \frac{1}{\datanum} \sum_{\datindexin}  \onehotmapofat{\datapoint}{\shortcatvariables} \, . \]
To avoid confusion let us strengthen, that in this chapter we interpret $\empdistribution$ as a random tensor taking values in $\facspace$, whereas each supported value of $\empdistribution$ is an empirical distribution taking values in $\facstates$.
The forwarding of $\facstates$ under the one-hot encoding is a multinomial random variable, see \defref{def:mulinomialVariable}.


% Expectation -> Does not make use of independence here!
When the marginal of each datapoint is $\gendistribution$, the expectation of the empirical distribution is
\begin{align*}
    \expectationof{\empdistribution}
    = \frac{1}{\datanum} \sum_{\datindexin}  \expectationof{\onehotmapof{\datapoint}}
    = \gendistribution \, .
\end{align*}

% Law of large numbers
From the law of large numbers it follows, that in the limit of $\datanum\rightarrow\infty$ at any coordinate $\catindex\in\facstates$ almost everywhere
\[ \empdistributionat{\indexedshortcatvariables} \rightarrow \expectationof{\empdistributionat{\indexedshortcatvariables}} =  \gendistributionat{\indexedshortcatvariables} \, . \]

% Fluctuation
At finite $\datanum$ the empirical distribution differs from the by the difference
\[ \empdistribution - \gendistribution \]
which we call a fluctuation tensor.

\subsubsection{Mean parameter of the empirical distribution}

We now investigate the empirical mean parameter
\[
    \datameanat{\selvariable} = \contractionof{\sencsstatwith,\empdistributionat{\shortcatvariables}}{\selvariable} \, .
\]

Each coordinate of $\datamean$ is decomposed as
\[ \datameanat{\indexedselvariable} = \frac{1}{\datanum}\sum_{\datindexin} \sstatcoordinateofat{\selindex}{\datapointof{\datindex}} \]
and thus stores the empirical average of the feature $\sstatcoordinateof{\selindex}$ on the dataset $\data$.

% Expectation of the empirical mean
Since the mean parameter depends linearly on the corresponding distribution, we can show the following correspondence between the empirical and the expected mean parameter.

\begin{theorem}
    \label{the:expectedMeanParameter}
    When drawing data independently from $\gendistribution$, we have $\expectationof{\datameanat{\selvariable}}=\genmeanat{\selvariable}$, where we call
    \[
        \genmeanat{\selvariable} = \contractionof{\sencsstatwith,\empdistributionat{\shortcatvariables}}{\selvariable} \,
    \]
    the expected mean parameter.
\end{theorem}
\begin{proof}
    Since the expectation commutes with linear functions.
%    Since the mean parameter of a distribution depends linearly on the distribution.
\end{proof}


% Convergence by Law of Large Numbers and issues
For each $\selindexin$ the law of large numbers guarantees that $\genmeanat{\indexedselvariable}$ converges almost surely against $\genmeanat{\indexedselvariable}$ when $\datanum\rightarrow\infty$.
To utilize these we need to approach the following issues:
\begin{itemize}
    \item We need non-asymptotic convergence bounds, since one has access to finite data when learning
    \item The convergence has to happen uniformly for all $\selindexin$
    \item Guarantees on the result of an estimated model are more accessible when provided for quantities like the canonical parameter and KL-divergences of the learning result.
    Those, however, depend nonlinearly on $\datameanat{\selvariable}$ and therefore require further investigation.
\end{itemize}

\subsubsection{Noise tensor and its width}

% Definition of noise tensors
Motivated by \theref{the:expectedMeanParameter}, we build our derivation of probabilistic guarantees on non-asymptotic and uniform convergence bounds for $\datameanat{\selvariable}$.
Let us first define the fluctuations of the empirical mean parameter, when drawing the data independently from a random distribution, as the noise tensor.

\begin{definition}
    Given a statistic $\sstat$, $\datanum\in\nn$ and a distribution $\gendistribution$, we call
    \[ \sstatnoise = \sbcontractionof{(\empdistribution-\gendistribution),\sencsstat}{\selvariable} \]
    the \emph{noise tensor}, where $\datamap$ is a collection of $\datanum$ independent samples of $\gendistribution$.
\end{definition}

% Naive Ex
The fluctuation of the empirical distribution around the generating distribution corresponds in this notation with the minterm exponential family, taking the identity as statistics.
% Appearances
Besides this, fluctuation tensors appears in Markov Logic Networks as fluctuations of random mean parameters and in proposal distributions as fluctuation of random energy tensor.
We will discuss these examples in the following sections.


% Fluctuation of mean parameter
We notice, that the fluctuation tensor $\sstatnoise$ is the centered mean parameter to the empirical distribution, that is
\begin{align*}
    \datamean - \expectationof{\datamean} =  \sbcontractionof{\sencsstat,\empdistribution-\gendistribution}{\selvariable} \, .
\end{align*}

% Widths
In the following we will use the supremum of contractions with random tensors in the derivation of success guarantees for learning problems.
Such quantities are called widths.

\begin{definition}
    Given a set $\canparamhypothesis\subset\facspace$ and $\noisetensor$ a random tensor taking values in $\facspace$ we define the width as the random variable
    \[ \widthwrtof{\canparamhypothesis}{\noisetensor} = \sup_{\canparamin} \absof{\sbcontraction{\canparam,\noisetensor}} \, . \]
\end{definition}

% Uniform concentration events
Bounds on the widths are also called uniform concentration bounds \cite{goessmann_uniform_2021} and generic probabilistic bounds will be provided in \secref{sec:directWidthBounds} and  \secref{sec:chainingWidthBounds}.

\subsection{Error bounds based on the noise width}

We now derive error bounds for parameter estimation and structure learning, as introduced in \charef{cha:networkReasoning}.
When combined with probabilistic bounds on the noise width, they are probabilistic success guarantees.

\subsubsection{Parameter Estimation}

\red{We in this section always assume, that $\empdistribution$ is representable by the base measure $\basemeasure$ of the respective exponential families.}

Parameter Estimation is the M-projection of the empirical distribution onto an exponential family.
In \charef{cha:probReasoning} we have characterized those by the backward map acting on the mean parameter.
Thus, while we are interested in the expected canonical parameter
\[
    \gencanparamat{\selvariable} = \backwardmapof{\genmeanat{\selvariable}}
\]
we get an estimation by the empirical canonical parameter
\[
    \datacanparamat{\selvariable}  = \backwardmapof{\datameanat{\selvariable}} \, .
\]

% Nonlinearity
Unfortunately, since the backward map is not linear, we in general do not have that $\expectationof{\backwardmapof{\datamean}}$ coincides with $\backwardmapof{\genmean}$.
To build intuition on the concentration we recall the expression of the backward map as
% Concentration
\begin{align*}
    \backwardmapof{\meanparam}
    = \argmax_{\canparam} -\centropyof{\meanrepprob}{\stanexpdistof{\canparam}}
\end{align*}
where $\meanrepprob$ is any distribution reproducing the mean parameter.
We want to compare the solutions $\backwardmapof{\datamean}$ and $\backwardmapof{\genmean}$, in which case $\meanrepprob$ can be chosen as $\empdistribution$ and $\gendistribution$.
It is common to call the objectives $\centropyof{\empdistribution}{\stanexpdistof{\canparam}}$ and $\centropyof{\gendistribution}{\stanexpdistof{\canparam}}$ empirical and expected risk \cite{shalev-schwartz_understanding_2014,goesmann_uniform_2021}
Since the empirical risk has a linear dependence on $\datamean$, we have at each $\canparam$
\begin{align*}
    \expectationof{\centropyof{\empdistribution}{\stanexpdistof{\canparam}}}
    &= \expectationof{\contraction{\datamean,\canparam} - \cumfunctionof{\canparam}} \\
    &= \contraction{\expectationof{\datamean},\canparam} - \cumfunctionof{\canparam} \\
    &= \centropyof{\gendistribution}{\stanexpdistof{\canparam}}
\end{align*}
By the law of large numbers, in the limit $\datanum\rightarrow\infty$ we thus have at each $\canparam$ a convergence of the empirical risk to the expected risk.
However, since the backward map is defined by the minima of these risks, we need a uniform and non-asymptotical concentration guarantee to get more useful bounds.
To this end, we now relate the supremum on the differences between expected and empirical risks with the width of the noise tensor.

\begin{lemma}
    \label{lem:centropyWidthCharacterization}
    For any $\canparamhypothesis$ and $\datamap$ we have
    \begin{align*}
        \widthwrtof{\canparamhypothesis}{\noisetensor}
        = \sup_{\canparamin} \absof{\centropyof{\empdistribution}{\stanexpdistof{\canparam}} - \centropyof{\gendistribution}{\stanexpdistof{\canparam}}}
    \end{align*}
\end{lemma}
\begin{proof}
    For any $\canparam\in\canparamhypothesis$ and by $\meanrepprob$ realizable mean parameter $\meanparam$ we have
    \begin{align*}
        \centropyof{\meanrepprob}{\stanexpdistof{\canparam}}
        = - \contraction{\meanparam,\canparam} -
    \end{align*}

    Using the decomposition of cross entropy in the exponential family
    \[ \centropyof{\empdistribution}{\stanexpdistof{\canparam}}
    =\contraction{\probtensor,\lnof{\gendistribution}} - \cumfunctionof{\lnof{\gendistribution}} \, . \]
\end{proof}

As a direct consequence, we have at any $\canparam\in\canparamhypothesis$
\begin{align*}
    \absof{\centropyof{\empdistribution}{\stanexpdistof{\canparam}} - \centropyof{\gendistribution}{\stanexpdistof{\canparam}}}
    \leq \widthwrtof{\canparamhypothesis}{\noisetensor} \, .
\end{align*}
Thus, the absolute difference of the expected risk and the empirical risk is bounded by the width of the noise tensor.
This is especially useful for the solution $\datamean$ of the empirical risk minimization, where we can state
\begin{align*}
    \centropyof{\gendistribution}{\stanexpdistof{\datacanparam}}
    \leq \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}} + \widthwrtof{\canparamhypothesis}{\noisetensor} \, .
\end{align*}
At the solution of a empirical risk minimization problem over $\canparamhypothesis$, the expected risk exceeds the empirical risk at most by the noise tensor width.

% Further KL divergence bound when assuming gendistribution in the hypothesis
When the generating distribution is in the hypothesis, we can further show the following KL-divergence bound for the estimated distribution.

\begin{theorem}
    Let us assume that for $\gencanparam\in\canparamhypothesis$ we have $\gendistribution=\stanexpdistof{\gencanparam}$. %and that $\partitionfunctionof{\canparam}$ is constant among $\canparamin$.
    Then for any solution $\datacanparam$ of the empirical problem we have
    \begin{align}
        \kldivof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}} \leq 2\widthwrtof{\canparamhypothesis}{\noisetensor} \, .
    \end{align}
\end{theorem}
\begin{proof}
    For the solution $\datacanparam$ of the empirical risk minimization on $\canparamhypothesis$ we have since $\gencanparam\in\canparamhypothesis$ that
    \begin{align*}
        \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}}
        \leq \centropyof{\empdistribution}{\stanexpdistof{\gencanparam}} \, .
    \end{align*}
    It follows that
    \begin{align*}
        \kldivof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}}
        & \leq \kldivof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}}
        + \centropyof{\empdistribution}{\stanexpdistof{\gencanparam}}
        - \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}} \\
        & = \left(\centropyof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}} - \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}}\right) \\
        & \quad - \left(\centropyof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\gencanparam}} - \centropyof{\empdistribution}{\stanexpdistof{\gencanparam}}\right) \, ,
    \end{align*}
    where we expanded the KL-divergence as a difference of cross entropies.
    We apply \lemref{lem:centropyWidthCharacterization} to estimate the terms in brackets and get
    \begin{align*}
        \left(\centropyof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}} - \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}}\right)
        - \left(\centropyof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\gencanparam}} - \centropyof{\empdistribution}{\stanexpdistof{\gencanparam}}\right)
        \leq 2 \widthwrtof{\canparamhypothesis}{\noisetensor} \, .
    \end{align*}
    Combined with the above inequality we arrive at
    \begin{align}
        \kldivof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}} \leq 2\widthwrtof{\canparamhypothesis}{\noisetensor} \, . \qedhere
    \end{align}
\end{proof}

\subsubsection{Structure Learning}

In the gradient heuristic of structure learning, one selects the statistic to the maximal coordinate of the energy tensor of the proposal distribution.
This tensor coincides with the mean parameter of a markov logic network and has thus a fluctuation by the noise tensor.
We now use these insights to show a guarantee, that the formula chosen by grafting with respect to the empirical proposal distribution coincides with the formula chosen with respect to the expected proposal distribution.
To this end, we need to define the max gap, which is the difference between the maximal coordinate of a tensor to the second maximal coordinate.

\begin{definition}
    The max gap of a tensor $\hypercoreat{\shortcatvariables}$ is the quantity
    \begin{align*}
        \maxgapof{\hypercore} =
        \left(\max_{\shortcatindices} \hypercoreat{\indexedshortcatvariables}\right) -
        \left(\max_{\shortcatindices\notin\argmax_{\shortcatindices}\hypercoreat{\indexedshortcatvariables}}
        \hypercoreat{\indexedshortcatvariables}\right) \, .
    \end{align*}
\end{definition}

When comparing the gap with the noise width, we get the following guarantee.

\begin{theorem}
    \label{the:detGuaranteeProposalDist}
    Whenever
    \begin{align*}
        \maxgapof{\genmean}
        > 2 \cdot \widthwrtof{\{\onehotmapof{\shortcatindices}:\shortcatindices\in\facstates\}}{\sstatnoise} \, ,
    \end{align*}
    then any mode $\shortcatindices$ of the empirical proposal distribution is a mode of the expected proposal distribution.
\end{theorem}
\begin{proof}
    Let us assume that for a mode $\selindex^{\datamap}\in\argmax_{\selindexin}\datameanat{\indexedselvariable}$ of the empirical mean parameter we have
    \begin{align*}
        \selindex^{\datamap}\notin\argmax_{\selindexin}\genmeanat{\indexedselvariable} \, .
    \end{align*}
    For a mode $\selindex^{*}\in\argmax_{\selindexin}\genmeanat{\indexedselvariable}$ of the expected mean parameter we then have
    \begin{align*}
        \genmeanat{\selvariable=\selindex^{\datamap}} \leq \genmeanat{\selvariable=\selindex^{*}} - \maxgapof{\genmean}
    \end{align*}
    and
    \begin{align*}
        \datameanat{\selvariable=\selindex^{\datamap}} \geq \datameanat{\selvariable=\selindex^{*}} \, .
    \end{align*}
    Comparing both intequalities we get
    \begin{align*}
        \left(\datameanat{\selvariable=\selindex^{\datamap}} - \genmeanat{\selvariable=\selindex^{\datamap}}\right)
        + \left( - \datameanat{\selvariable=\selindex^{*}} + \genmeanat{\selvariable=\selindex^{*}} \right)
        \geq \maxgapof{\genmean} \, .
    \end{align*}
    Estimating the terms in the bracket by the width of the noise tensor with respect to basis vectors, we get
    \begin{align*}
        2 \cdot  \widthwrtof{\{\onehotmapof{\shortcatindices}:\shortcatindices\in\facstates\}}{\sstatnoise}
        \geq \maxgapof{\genmean} \, ,
    \end{align*}
    which is a contradiction to the assumption.
    Thus, any mode of the empirical mean parameter is also a model of the expected mean parameter.
\end{proof}


\subsection{Fluctuations in Logic Networks}

\red{In case of logical formulas being statistics, the coordiantes of the mean parameter are satisfaction rates to the formulas.}

For Logic Networks we have statistics consistent of boolean statistics $\enumformula$, which are logical formulas.
In this case the marginal distributions of the coordinates of $\sstatnoise$ are scaled and centered binomials, which we show now.

\begin{lemma}
    Let $\sstat$ be a statistic of boolean features $\sstatcoordinate$ for all $\selindexin$, i.e. let $\imageof{\sstatcoordinate}\subset\ozset$.
    Then, the marginal distribution of the coordinate $\sstatnoise[\indexedselvariable]$ is
    \[\frac{1}{\datanum}\left(\bidistof{\fprobof{\selindex},\datanum}- \fprobof{\selindex}\right)  \, , \]
    where by $\bidistof{\fprobof{\selindex},\datanum}$ we denote the binomial distribution with mean parameter
    \[ \fprobof{\selindex} = \sbcontraction{\sstatcoordinate,\gendistribution} \, . \]
\end{lemma}
\begin{proof}
    We notice that when forwarding a random sample $\datapoint$ of $\gendistribution$ is the random tensor
    \[ \onehotmapofat{\datapoint}{\shortcatvariables} \, \]
    and since $\imageof{\sstatcoordinate}\subset \{0,1\}$ the contraction
    \[ \sbcontraction{\sstatcoordinate, \onehotmapofat{\datapoint}{\shortcatvariables}} \]
    is a random variable taking values in $\{0,1\}$.
    The variable therefore follows a Bernoulli distribution with mean parameter
    \[ \fprobof{\selindex}
    = \expectationof{\sbcontraction{\sstatcoordinate, \onehotmapofat{\datapoint}{\shortcatvariables}}}
    = \sbcontraction{\sstatcoordinate, \gendistribution}  \, \qedhere\]
\end{proof}

%\subsubsection{Mean parameter in Markov Logic Networks}

The mean parameter of the M-projection of the empirical distribution on the family of Markov Logic Networks with statistic $\fselectionmap$ is the random tensor
\begin{align*}
    \datameanat{\selvariable}
    = \sbcontractionof{\sencmlnstat,\empdistribution}{\selvariable} \, .
\end{align*}

The expectation of this random tensor is
\begin{align*}
    \expectationof{\datamean}
    =  \sbcontractionof{\sencmlnstat,\expectationof{\empdistribution}}{\selvariable}
    =  \sbcontractionof{\sencmlnstat,\gendistribution}{\selvariable}
    =  \genmean \, ,
\end{align*}
where we used that the expectation and contraction operation can be commuted due to the multilinearity of contractions.

\subsubsection{Energy tensor in proposal distributions}

The fluctuation tensor appears as a fluctuation of the energy of the proposal distribution.
The expectation of the energy of the proposal distribution is
\begin{align*}
    \expectationof{\energytensorof{\proposalstat,\empdistribution-\currentdistribution}}
    = \expectationof{\sbcontractionof{\sencproposalstat,\empdistribution-\currentdistribution}{\selvariable}}
    = \sbcontractionof{\sencproposalstat,\expectationof{\empdistribution-\currentdistribution}}{\selvariable}
    = \sbcontractionof{\sencproposalstat,\gendistribution-\currentdistribution}{\selvariable}
    = \expectationof{\energytensorof{\proposalstat,\gendistribution-\currentdistribution}} \, .
\end{align*}

% Fluctuation
The fluctuation of this random tensor is
\begin{align*}
    \expectationof{\energytensorof{\proposalstat,\empdistribution-\currentdistribution}}  - \expectationof{\energytensorof{\proposalstat,\gendistribution-\currentdistribution}}
    = \expectationof{\energytensorof{\proposalstat,\empdistribution-\gendistribution}}
\end{align*}
and coincides with $\mlnnoise$.

\subsubsection{Minterm Exponential Family} % Interesting, since here is the connection with probability tensors: Forwarding of each random datapoint by the one hot encoding to get a multinomial random tensor.

In case of the minterm exponential family, we have $\sstat=\identityat{\shortcatvariables,\selvariable}$ and the fluctuation tensor is
\[ \mintermnoise = \empdistribution - \gendistribution \, .  \]

% Multinomial
This fluctuation tensor is related to tensor encodings of multinomial distributions, which we now define as multinomial random tensors.

\begin{definition}
    \label{def:mulinomialVariable}
    A multinomial random tensor is the sum of the one-hot encodings of independent identically distributed random states $x^\datindex$, drawn from a distribution $\probtensor$, that is
    \[ Z^{\probtensor, \datanum} = \sum_{\datindexin} \onehotmapofat{x^\datindex}{\shortcatvariables} \, . \]
\end{definition}

% Multinomial as a more general characterization
In the case of minterm exponential families, the fluctuation tensor is a multinomial, as we show next.
This characterization goes beyond the characterization of the marginal distributions as centered binomial variables, which holds for generic Markov Logic Networks.

\begin{lemma}
    \label{lem:multinomialEmpdistFluctuation}
    The fluctuation $\empdistribution - \gendistribution$ is a by $\frac{1}{\datanum}$ rescaled centered multinomial random tensor with parameters $\gendistribution$ and $\datanum$. % Needs some more explanation based on one-hot encodings?
\end{lemma}
\begin{proof}
    By the above construction we have
    \[  \empdistribution - \gendistribution
    = \frac{1}{\datanum}\sum_{\datindexin} \left( \onehotmapofat{\datapoint}{\shortcatvariables} - \expectationof{\onehotmapofat{\datapoint}{\shortcatvariables}} \right) \, .  \]
\end{proof}

\subsubsection{Guarantees for Mode of the Proposal Distribution}

Let us now derive probabilistic guarantees, that the mode of the proposal distribution at the empirical and the generating distribution are equal.

\begin{theorem}
    \label{the:probGuaranteeProposalDist}
    Whenever the energy tensor of the expected proposal distribution has a gap of $\maxgap$, then for every $\failprob>0$ any mode of the empirical proposal distribution coincides is also a mode of the expected proposal distribution with probability at least $1-\expof{-\frac{1}{\failprob^2}}$, provided that
    \[ \datanum > C\frac{\left(\sum_{\atomenumeratorin}\lnof{\catdimof{\atomenumerator}}\right)}{\maxgap^2} \]
    where $C$ is a universal constant.
\end{theorem}
\begin{proof}
    To proof the theorem we combine the deterministic guarantee \theref{the:detGuaranteeProposalDist} with the width bound of \theref{the:basisTensorWidthBound}.
    Given the assumed bound, the sub-gaussian norm of the width is upper bounded by $C_2\cdot \maxgap$, thus for any $\failprob>0$ we have
    \[  \widthwrtof{\{\onehotmapof{\shortcatindices} :\shortcatindices\in\facstates\}}{\mlnnoise}  < 2 \maxgap \]
    with probability at least $1-\expof{-\frac{1}{\failprob^2}}$.
    The claim thus follows with \theref{the:detGuaranteeProposalDist}.
\end{proof}


\begin{example}[Gap of a MLNs with single formulas]
    Let there be the MLN of a maxterm $\formula$ with $\atomorder$ variables, and let $\formulaset$ be the maxterm selecting tensor, then
    \[ \maxgap(
    \energytensorof{(\formulaset, \expdistof{(\{\formula\},\weightof{\formula})} - \normationof{\ones}{\shortcatvariables} )}
    ) = \frac{1}{2^{\atomorder}-1 + \expof{-\weightof{\formula}}}  \]
    If $\weightof{\formula}>0$ we have an exponentially small gap.
    Thus, for the above Lemma to apply, the width needs to be exponentially in $\atomorder$ small.


    Let there be the MLN of a minterm $\formula$ with $\atomorder$ variables, then
    \[ \maxgap(
    \energytensorof{(\formulaset, \expdistof{(\{\formula\},\weightof{\formula})} - \normationof{\ones}{\shortcatvariables} )}
    ) = \frac{1}{1+(2^{\atomorder}-1)\cdot\expof{-\weightof{\formula}}}  \]
    For large $\weightof{\formula}$ and $\atomorder$, the gap tends to $1$.
\end{example}

\subsubsection{Guarantees for Parameter Estimation}

\red{This is mean parameter fluctuation interpretation of the random tensor.}

\begin{lemma}
    \label{lem:meanParamDistance}
    For any $\mlnstat$ and $\datamap$ drawn from $\gendistribution$ we have
    \begin{align*}
        \normof{\datamean - \genmean}
        = \widthwrtof{\subsphere}{\mlnnoise} \, ,
    \end{align*}
    where $\datamean=\sbcontractionof{\sencmlnstat,\empdistribution}{\selvariable}$ and $\genmean=\sbcontractionof{\sencmlnstat,\gendistribution}{\selvariable}$.
\end{lemma}

%
We can thus apply the sphere bounds.


\begin{theorem}
    For any $\failprob\in(0,1)$ we have the following with probability at least $1-\failprob$.
    Let $\hat{\canparam}$ and $\precision>0$, then
    \[ \absof{\centropyof{\gendistribution}{\mlnexpdistof{\datacanparam}} - \centropyof{\empdistribution}{\mlnexpdistof{\datacanparam}}} \leq \tau \cdot \normof{\datacanparam} \]
    provided that
    \[ \datanum \geq \frac{\sbcontraction{\genmean}-\sbcontraction{(\genmean)^2}}{\failprob \precision^2} \, . \]
\end{theorem}
\begin{proof}
    We have by Cauchy Schwartz
    \[ \absof{\sbcontraction{\datamean - \genmean,\datacanparam}} \leq \normof{\datamean - \genmean} \cdot \normof{\datacanparam}\]
    and with \lemref{lem:meanParamDistance}
    \[ \absof{\sbcontraction{\datamean - \genmean,\datacanparam}} \leq \widthwrtof{\subsphere}{\mlnnoise} \cdot \normof{\datacanparam} \, . \]
    We show in Part III that in \theref{the:sphereBoundVariance} that
    \[  \widthwrtof{\subsphere}{\mlnnoise} \leq \tau \]
    when $\datanum$ satisfies the assumed lower bound, from which the claim follows.
\end{proof}

















