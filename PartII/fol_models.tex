\chapter{\chatextfolModels}\label{cha:folModels}

We now extend the tensor representation from to structured representations, whereas we previously focused on factored representation of systems.
%The models/events in this situation are precise relations between objects.


\red{We observe that the more expressive first-order logic bears another tensor structure:
The representation of each world is a boolean tensor.
}


% Formulas in FOL
%Formulas in first order logic can contain variables, which are placeholder for specific individuals.
%Given a model and an assignment of objects to the arguments of a formula, the truth of the formula can be interpreted.
%This truth interpretation defines thus for any model a tensor, which we call the grounding tensor.

%
\sect{World Tensors}

%\red{We introduce here object enumerating variables $\indvariable$ are valued in $[\inddim]$, with $\cardof{\worlddomain} = \inddim$ and an enumeration of $\worlddomain$ by $\exindividualof{\indindex}$.}

% Index interpretation of world domain
Since first-order logic follows structured representations of a system, a first-order logic world consists in objects and relations between them.
To each world there is a world domain $\worlddomain$ of objects, which we assume to be finite (this is a restrictive assumption).
We exploit the set-encoding formalism discussed in more detail in \charef{cha:basisCalculus} and use bijective index interpretation maps
\begin{align*}
	\indexinterpretation : [\inddim] \rightarrow \worlddomain \, .
\end{align*}
A so-called term variable $\indvariable$ takes states $\indindexin$, which represent objects
\begin{align*}
	\indexinterpretationat{\indindex} \in \worlddomain \, .
\end{align*}

%
The relations between objects are described by $\indorder$-ary predicates $\folpredicate$.
Given a specific world $\dataworld$ the truth of relations is represented by boolean tensors
\begin{align*}
	\groundingof{\folpredicate} : \symindstates\rightarrow\ozset \, .
\end{align*}
Given a tuple $\indindexlist\in\symindstates$ the boolean
\begin{align*}
	\groundingofat{\folpredicate}{\indexedindvariableof{0},\ldots,\indexedindvariableof{\indorder-1}} \in\ozset
\end{align*}
is called a grounding and encodes, whether the relation $\folpredicate$ is satisfied in the world $\dataworld$ for the objects $\invindexinterpretationat{\indindexof{0}},\ldots,\invindexinterpretationat{\indindexof{\indorder-1}}$.

% Assumptions
Let us assume, that we have a function-free theory with $\folpredicateorder$ predicates, where are predicates all of the same arity $\variableorder$.
We then formalize a world in the following based on a selection variable $\selvariable$ selecting a specific predicate and term variables $\shortindvariablelist=\indvariablelist$ representing choices of objects from a given set $\worlddomain$.

\begin{definition}[FOL World]\label{def:folWorld}
	Given a set of objects $\worlddomain$ enumerated by an index interpretation function $\indexinterpretation:[\inddim]\rightarrow \worlddomain$ and a finite set $\{\folpredicates\}$ of $\variableorder$-ary predicates a world is a boolean tensor
	\begin{align}
		\dataworldwith : [\catorder] \times \left( \symindstates\right) \rightarrow [2] \, . % ! Selvariable gets catorder !
	\end{align}
	We interpret the world tensor as encoding in the coordinate $\dataworldat{\selvariable=\catenumerator,\indexedshortindvariables}$, whether the $\catenumerator$-th predicate is satisfied on the object tuple $\invindexinterpretationat{\indindexof{0}},\ldots,\invindexinterpretationat{\indindexof{\indorder-1}}$.
\end{definition}


% Inclusion of functions, predicates of differing order
When the assumptions of function-free and constant variable order are not met, we can do the following tricks.
Functions are turned to predicates by their relation interpretation.
If there are predicates of different arity in the theory, we can trivially extend them to $\variableorder$ary predicates by tensor products with the trivial tensor $\ones$.
This can be done by a tensor product with $\onehotmapofat{0}{\indvariable}$, where we add an object $\exindividualof{0}$ as a placeholder for predicates with smaller arity.

% Finite worlds -> By database semantics?
While in first order logics, depending on the chosen semantics, worlds can have infinite sets of objects, we here only treat worlds with finite objects.

\subsect{Case of Propositional Logics}

%
Before continuing with the one-hot encoding of first-order logic worlds, let us show that the previously discussed formalism of propositional logics (see \charef{cha:logicalRepresentation}) is a special case of first-order logics, namely when demanding $\indorder=0$.
Consistent with \defref{def:folWorld} we have a propositional logic world by
\begin{align*}
	\dataworld: [\catorder] \rightarrow [2] \, ,
\end{align*}
which we have in \charef{cha:logicalRepresentation} represented by the assignments $\catindexof{\atomenumerator} = \dataworldat{\selvariable=\atomenumerator}$ to the categorical variables $\catvariableof{\atomenumerator}$.

% Comparison with PL
%Compared with propositional formulas, the grounding tensor does not take as input a specific world, but is defined on a given world.
%We show in this chapter, how both tensor interpretations can be transformed, i.e. by extracting samples from a FOL world $\dataworld$ interpretated as an empirical distribution over PL worlds, and by generating FOL worlds by a set of samples generated from a PL distribution.

% One-hot maps
To represent logical formulas as sets of possible worlds, and distributions of worlds, we applied in \parref{par:one} one-hot encodings of possible worlds.
For the case of propositional logics, this is
\begin{align*}
	\onehotmapofat{\dataworld}{\shortcatvariables} = \bigotimes_{\catenumeratorin} \onehotmapofat{\dataworldat{\selvariable=\atomenumerator}}{\catvariableof{\catenumerator}} \, .
\end{align*}

\subsect{One-hot encoding of worlds}

Let us now generalize the one-hot encodings of propositional logic worlds to worlds of first-order logic.
To encode the boolean tensors $\dataworld$ describing first order logics as basis elements of a tensor space, we take the one-hot encoding
\begin{align*}
	\onehotmap :
	\bigtimes_{\atomenumeratorin}\bigtimes_{\indindexofin{0}}\cdots\bigtimes_{\indindexofin{\indorder-1}} [2]
	\rightarrow \bigotimes_{\catenumeratorin}\bigotimes_{\indindexofin{0}}\cdots\bigotimes_{\indindexofin{\indorder-1}} \rr^2
\end{align*}
defined by
\begin{align*}
	\onehotmapofat{\dataworld}{\catvariableof{[\catorder]\times[\inddim]^{\indorder}}}
	= \bigotimes_{\catenumeratorin}\bigotimes_{\indindexofin{0}}\cdots\bigotimes_{\indindexofin{\indorder-1}}
	\onehotmapofat{\dataworldat{\selvariable=\atomenumerator,\indexedshortindvariables}}{\catvariableof{\catenumerator,\shortindindices}} \, .
\end{align*}
This is a tensor of order $\catorder\cdot\inddim^{\indorder}$, in a tensor space of dimension $2^{\left(\catorder\cdot\inddim^{\indorder}\right)}$.
Storage of such tensors in naive formats would not be possible.
However, the basis CP format discussed in \charef{cha:sparseCalculus} still provides storage with demand linear in the order $\catorder\cdot\inddim^{\indorder}$.

% Domain
Another issue when comparing different first-order logic worlds arises in potentially different world domains.
As we have explored, the cardinality of the domain influences the order of the one-hot encoding tensors.
To avoid such issues we here enumerate worlds coinciding in their domains.
This restriction is called database semantics (see e.g. Section 8.2.8 in \cite{russell_artificial_2021}), where only those worlds are considered, which domains have a one-to-one map to the constant symbols appearing in a respective knowledge base. % Unique name assumption + Domain closure!
% Factored systems
When restricting to worlds coinciding in their domain, we still have a factored representation of the system, since we can enumerate the possible worlds by a cartesian product.
However, the number of categorical variables representing the world is $\atomorder\cdot \inddim^{\indorder}$ and tensor representations, even in sparse formats, are not feasible due to the large order required.
These techniques to restrict to comparable factored representations are often refered to propositionalization of a first-order logic knowledge base.

% Propositional 
%Propositional worlds have been enumerated by indices of $\atomorder$ Booleans, that is for a world $\dataworld: [\folpredicateorder] \rightarrow [2]$ we take the index
%	\[ \atomindices \quad \text{where} \quad \atomlegindexof{\atomenumerator} = \dataworld(\atomenumerator) \, .  \]

% FOL
%When we want to enumerate the first order logic worlds to a fixed set of objects $\worlddomain$, we flatten the tensor $\dataworld$ and get indices
%	\[ \{\atomlegindexof{\atomenumerator,\indindexlist} \, : \, \atomenumeratorin, \indindexlist \in[\inddim] \} \quad
%	\text{where} \quad \atomlegindexof{\atomenumerator,\indindexlist} = \dataworld(\atomenumerator,\indindexlist) \, .  \]





\subsect{Probability distributions}

Having established the formalism of one-hot encodings also in the case of first-order logic worlds, we can now proceed with the definition of distributions and formulas, analogously to the development in \parref{par:one}.
Probability distributions over worlds coinciding on their domain are then non-negative and normed tensors
\begin{align*}
	\probat{\catvariableof{[\catorder]\times[\inddim]^{\indorder}}} \in \bigotimes_{\atomenumeratorin,\shortindindices\in[\inddim]^{\indorder}} \rr^2 \, .
\end{align*}
where each coordinate of a world $\dataworld$ is captured by a boolean random variable $\catvariableof{\atomenumerator,\shortindindices}$, indicating whether the $\atomenumerator$-th predicate holds on the object tuple indexed by $\shortindindices$.

% High-dimensional - watch out for repetitions!
We notice, that by definition these probability distributions are distributions of $\atomorder\cdot\inddim^{\indorder}$ Booleans with $2^{\left(\atomorder\cdot\inddim^{\indorder}\right)}$ many states.
% One-hot encodings minimal
Unfortunately, it is not possible to design encoding spaces of smaller dimension, when our aim is to get any distribution over possible worlds by an element in the encoding space.
This is due to the fact, that one-hot encodings provide a basis in the tensor space, as will be shown in \charef{cha:coordinateCalculus}.
The reason for the large encoding space dimension is therefore rooted in the equal number of possible worlds and not in an overhead in the dimension of the one-hot encoding space.
We will later in this chapter investigate methods to handle such high-dimensional distributions in the formalism of exponential families.

\subsect{Semantics of formulas}

Following the development of \charef{cha:logicalRepresentation}, we can choose a semantic approach to the definition of formulas, under the assumption of database semantics.
Since the semantic of a logical formula is the set of its models, we again have a one-to-one correspondence between logical formulas and the boolean tensors in the one-hot encoding space
\begin{align*}
	\bigotimes_{\atomenumeratorin,\shortindindices\in[\inddim]^{\indorder}} \rr^2 \, .
\end{align*}
This correspondence between the semantics and boolean tensor is through a subset encoding (see \defref{def:subsetEncoding}) of the respective formulas.
However, due to the large state dimensions, we will in the following sections choose a syntactical approach to the construction of formulas, which will naturally provide efficient tensor network decompositions.

\subsect{Two levels of tensor representation}

In comparison with propositional logics, first-order logic bears two levels of natural tensor representations.
In the first level, each world (see \defref{def:folWorld}) has a natural structure by a tensor, since it encodes relations between objects chosen by assignments to term variables.
This is different to the worlds of a propositional logic theory, which are represented by a boolean vector instead of a tensor.
The second level arises as in propositional logics, by understanding each world as a uncertain state and studying distributions over states, which are understood themself as a tensor (see \defref{def:probabilityDistribution}).
As argued above, the assumption of database semantics is central to exploit the tensor structure of the first level.
Under this assumption, representation of an uncertain state, or a collection of possible states, is done in the tensor space
\begin{align*}
	\bigotimes_{\atomenumeratorin,\shortindindices\in[\inddim]^{\indorder}} \rr^2 \,
\end{align*}
where the enumeration of the $2$-dimensional axes contains the tensor structure of the first level.



\sect{Formulas in a fixed first-order logic world}

Following the argumentation above, we in this section restrict to the exploitation of the first level tensor structure, namely a fixed world represented as a tensor $\dataworldwith$, see \defref{def:folWorld}.
We are specifically interested in the tensor network decomposition of first order formulas, which contain in full generality variables and therefore also have a tensor.
The evaluation of a first-order formula on a specific world is therefore different to the case in propositional logics, where the evaluation was a boolean in $\ozset$ indicating whether the world is a model.
%\red{Here we investigate grounding tensors to formulas with variables, and calculate them in a fixed world.}
% Arbitrary formulas

\subsect{Grounding tensors}

Given a first-order logic world $\dataworldwith$, arbitrary formulas are interpreted in terms of the satisfactions of their groundings.
We define their semantic first, and then relate their syntactical decomposition to tensor networks, similar to our approach to propositional logics in \charef{cha:logicalRepresentation}.

\begin{definition}[Grounding of a first-order formula given a world]
	Given a specific world $\dataworld$, with an domain $\worlddomain$ enumerated by $[\inddim]$, the grounding of a formula $\folexformula$ with variables $\indvariableof{\folexformula}$  is the tensor
	\begin{align*}
		\groundingofat{\folexformula}{\indvariableof{\folexformula}} :
		\bigtimes_{\indenumerator\in[\indvariableof{\folexformula}]} [\inddim] \rightarrow \ozset \, .
	\end{align*}
	Each coordinate represents thereby the boolean, whether the substitution of the variables in the formula is satisfied given a world $\dataworld$, that is
	\begin{align*}
		\groundingofat{\folexformula}{\indexedindvariableof{\folexformula}} =
			\begin{cases}
				1 & \quad \text{if the substitution of $\folexformula$ with the variables $\indvariableof{\folexformula}$ replaced by the objects $\indexinterpretationat{\indindexof{\indenumerator}}$ is satisfied on the world $\dataworld$.} \\
				0 &  \text{else}
			\end{cases} \, . 
	\end{align*}
\end{definition}

% Comment: Formulas as maps to
The grounding tensor formalism can be used to define formulas as a map
\begin{align*}
	\folexformula : \left(\bigotimes_{\atomenumeratorin,\shortindindices\in[\inddim]^{\indorder}}\rr^{2}\right)
	\rightarrow \left(\bigotimes_{\atomenumeratorin,\indindexof{\folexformula}\in[\inddim]^{\cardof{\indvariableof{\folexformula}}}}\rr^{2}\right)
\end{align*}
where each world $\dataworld$ is mapped to a grounding tensor
\begin{align*}
	\folexformula(\dataworld) = \groundingof{\folexformula} \, .
\end{align*}
This would involve the second level of tensor interpretation, namely

%% Basis encoding
%When interpreting this map as a basis encoding, formulas are tensors in the tensor space
%\begin{align*}
% 	\left(\bigotimes_{\atomenumeratorin, \indindexlist\in[\inddim]} \rr^{2} \right) \otimes
%	\left(  \bigotimes_{\atomenumeratorin, \indindexof{0},\ldots,\indindexof{\individualorder_{\folexformula}}\in[\inddim]} \rr^{2} \right) \, .
%\end{align*}

\subsect{Predicates}

The predicates itself are the simplest cases of first-order formulas with term variables.
% Atomic
They are stored in the slices to the first axis of $\dataworld$ and we have
\begin{align}
	\groundingof{\folpredicateof{\folpredicateenumerator}} =
	\contractionof{\dataworldat{\selvariable,\shortindvariablelist},\onehotmapofat{\folpredicateenumerator}{\selvariable}}{\shortindvariablelist} \, .
\end{align}

What is more abstract, we can understand the predicate itself as an object, then take the first-order world as a grounding tensor of a more abstract formula.
We will follow this thought in the ternary representation of Knowledge Graphs in \secref{subsec:knowledgeGraphTernaryRep}.

\subsect{Substitution by slicing}

% Slicing interpretation
Slicing the grounding tensor of a formula a first-order formula amounts to substitution of the respective variable by the constant at the enumeration index.

%\subsect{Syntactical Decomposition of quantifier-free formulas}

\subsect{Formuly synthesis by connectives}

In order to have a sound semantic, the grounding of FOL formulas is determined by the syntax of the formula, i.e. a decomposition of the formula into connectives and quantifiers acting on atomic formulas.

% Formulas as maps from worlds to groundings
Quantifier-free formulas are connectives acting on atomic formulas.
We can describe them as in the case of propositional logics in the $\rencodingof{}$-formalism.
While the atomic formulas where delta tensors copying states, they are more involved here.

\red{To Do: Head variables}

\begin{theorem}
	For any connective $\exconnective$ and formulas $\folexformula_1$ and $\folexformula_2$ we have
	\begin{align}
		\groundingofat{(\folexformula_1\exconnective\folexformula_2)}{\shortindvariablelist} =
			\contractionof{
			\rencodingofat{\groundingof{\folexformula_1}}{\headvariableof{\folexformula_1},\shortindvariablelist},
			\rencodingofat{\groundingof{\folexformula_2}}{\headvariableof{\folexformula_2},\shortindvariablelist},
			\rencodingofat{\exconnective}{\headvariableof{\folexformula_1\exconnective\folexformula_2}, \headvariableof{\folexformula_1}, \headvariableof{\folexformula_2}},
			\tbasisat{\headvariableof{\folexformula_1\exconnective\folexformula_2}}
			}
			{\shortindvariablelist} \, . 
	\end{align}
\end{theorem}
\begin{proof}
	This directly follows from \theref{the:compositionByContraction}.
%	By the semantic interpretation of the groundings, which has to be sound.
\end{proof}

% Shared variables
Here, variables can be shared by the connected formulas, therefore the variables in the combined formula are unions of the possible not disjoint variables of the connected formulas.

%% Propositional interpretation
%When we understand the head variables in the relational encoding of atoms as the categorical variables, and get a similar interpretation of the tensor network decomposition as in the propositional case.
%\subsect{Propositionalization}

When interpreting the head variables of relational encoded atomic formulas as the atoms of a propositional theory, we find a propositional formula $\exformula$ associated with any decomposable first order logic formula.

\begin{definition}\label{def:propositionalEquivalent}
	Given a formula $\folexformula$ in first order logic, we say that a propositional formula $\formulaat{\shortcatvariables}$ is the propositional equivalent to $\folexformula$ given atomic formulas $\extformulaof{\atomenumerator}$ in first order logic, when for any world $\dataworld$ we have
	\begin{align*}
		\groundingofat{\folexformula}{\indvariableof{\folexformula}}
		= \contractionof{ 
		\{\rencodingofat{\groundingof{\extformulaof{\atomenumerator}}}{\catvariableof{\atomenumerator},\indvariableof{\extformulaof{\atomenumerator}}} : \atomenumeratorin\}
		\cup \{\formulaat{\shortcatvariables}\}
		}{\indvariableof{\folexformula}} \, .
	\end{align*}
	We here denote the head variables of the relational encoding to $\rencodingof{\groundingof{\extformulaof{\atomenumerator}}}$ by $\catvariableof{\atomenumerator}$ to highlight their interpretation as propositional atoms.
\end{definition}

We depict the relation of a grounding tensor to a propositional formula as:
\begin{center}
	\input{./PartII/tikz_pics/fol_models/propositionalization.tex}
\end{center}


\subsect{Quantifiers}

Existential and universal quantifiers appear in generic first order logic and are besides substitutions further means to reduce the number of variables in a formula.
%They are not representable as linear transform of the respective quantifier-free formula.


% Definition of existential and universal quantifiction needed!
The semantics of existential quantification consists in a formula being true, if at least one state of the quantified variable is true, as we define next.

\begin{definition}
	Given a grounding tensor
	\begin{align*}
		\groundingofat{\folexformula}{\indvariableof{0},\ldots,\indvariableof{\indorder-1}} \,
	\end{align*}
	the existential and universal quantification with respect to the first variable are the tensors
	\begin{align*}
		\groundingofat{\left(\exists_{\indindexof{0}}\folexformula\right)}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}} \quad \text{and} \quad
		\groundingofat{\left(\forall_{\indindexof{0}}\folexformula\right)}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}} \,
	\end{align*}
	with coordinates as follows.
	For an assignment $\indindexof{1},\ldots,\indindex$ to the non-quantified variables we have
	\begin{align*}
		\groundingofat{\left(\exists_{\indindexof{0}}\folexformula\right)}{\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}} = 1
	\end{align*}
	if and only if there is an assignment $\indindexofin{0}$ such that
	\begin{align*}
		\groundingofat{\folexformula}{\indexedindvariableof{0},\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}} = 1 \, .
	\end{align*}
	Conversely, we have for the universal quantification that
	\begin{align*}
		\groundingofat{\left(\forall_{\indindexof{0}}\folexformula\right)}{\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}} = 1
	\end{align*}
	if and only if for any assignment $\indindexofin{0}$ we have
	\begin{align*}
		\groundingofat{\folexformula}{\indexedindvariableof{0},\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}} = 1 \, .
	\end{align*}
\end{definition}


Let us now show, that existential and universal quantification are coordinatewise transforms (see \defref{def:coordinatewiseTransform}) of contracted grounding tensors.
To this end, let us introduce the greater-$z$ indicator $\greaterthanfunction{z}$, where $z\in\rr$, as the function
\begin{align*}
	\greaterthanfunction : \rr \rightarrow \ozset
	\quad, \quad \greaterthanfunctionof{z}{x} =
	\begin{cases}
		1 & \quad  \text{if} \quad x > z\\
		0 & else
	\end{cases} \, .
\end{align*}

\begin{theorem}
	For any formula $\folexformula$ with variables $\shortindvariablelist$ we have
	\begin{align*}
			\groundingofat{\left(\exists_{\indindexof{0}}\folexformula\right)}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}} =
			\coordinatetrafowrtofat{\existquanttrafo}{\contractionof{\groundingof{\folexformula}}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}
	\end{align*}
	and
	\begin{align*}
			\groundingofat{\left(\forall_{\exindividualof{\indindexof{0}}} \folexformula\right)}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}=
			\coordinatetrafowrtofat{\universalquanttrafo}{\contractionof{\groundingof{\folexformula}}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}
%			\greaterzeroof{
%			\sbcontractionof{\groundingof{\folexformula}}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}
%			- \inddim \cdot \onesat{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}
%			} \, .
	\end{align*}
\end{theorem}
\begin{proof}
	We proof the claimed equalities to arbitrary slices of the remaining variables, which amount to arbitrary substitutions of the formulas.
	For any indices $\indindexofin{1},\ldots,\indindexofin{\indorder-1}$ we notice, that
	\begin{align*}
		\sbcontractionof{\groundingof{\folexformula}}{\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}}
		&= \sum_{\indindexofin{0}} \groundingofat{\folexformula}{\indexedindvariableof{0},\ldots,\indexedindvariableof{\indorder-1}} \\
		&= \cardof{\indindexofin{0} \, : \, \groundingofat{\folexformula}{\indexedindvariableof{0},\ldots,\indexedindvariableof{\indorder-1}}=1} \, .
	\end{align*}
	We can thus understand the contracted grounding tensor as storing in its coordinates the count of the coordinate extensions to the zeroth variable, such that the grounding tensor is satisfied.
	This is analogous to our interpretation of contracted propositional formulas as world counts.
	From this it is obvious, that the existential quantification is satisfied, if the count is different from zero, which is captured by the coordinatewise transform with $\existquanttrafo$.
	We therefore arrive at
	\begin{align*}
		\groundingofat{\left(\exists_{\indindexof{0}}\folexformula\right)}{\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}} =
		\coordinatetrafowrtofat{\existquanttrafo}{\contractionof{\groundingof{\folexformula}}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}}{\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}} \, .
	\end{align*}
	The first claim follows, since the assignment to the non-quantified variables was arbitrary.
	The universal quantification is satisfied, when all extensions are satisfied, and the count is $\inddim$.
	Since $\inddim$ is the maximal count, this is captured by the coordinatewise transform with $\universalquanttrafo$ and we get
	\begin{align*}
		\groundingofat{\left(\forall_{\indindexof{0}}\folexformula\right)}{\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}} =
		\coordinatetrafowrtofat{\universalquanttrafo}{\contractionof{\groundingof{\folexformula}}{\indvariableof{1},\ldots,\indvariableof{\indorder-1}}}{\indexedindvariableof{1},\ldots,\indexedindvariableof{\indorder-1}} \, .
	\end{align*}
	With the same argument, the second claim is established.
\end{proof}

% Customized quantifiers
We can extend this discussion towards more generic counting quantifiers, of which the existential and the universal quantifier are extreme cases.
One can define quantifiers by demanding that at least $z\in\nn$ compatible groundings are satisfied, and show that they amount to coordinatewise transforms with $\greaterthanfunction{z}$.
What is more, quantifiers demanding that at most $z\in\nn$ are satisfied would be representable by transforms with an analogously defined function $\ones_{\leq z}$.
Such customized quantifiers appear for example in the $\mathrm{OWL\,2}$ standard of description logics (see \cite{rudolph_foundations_2011} and \secref{sec:kgRepresentation}).

% Relational encodings
As will be discussed in \charef{cha:basisCalculus}, any coordinatewise transform can be performed by a contraction of a relational encoding of the tensor with a head vector prepared by the transform function (see \theref{the:tensorFunctionComposition}).
In the case here, a direct implementation would require a dimension of these head variables by $\inddim$, which can be infeasible when having large object sets.


\subsect{Storage in basis CP decomposition}\label{sec:basisCPgrounding}

In many situations, grounding cores are sparse and representations as single tensor cores comes with a drastic overhead.
We often encounter sparse grounding tensors, where the number of non-zero coordinates (to be investigated by basis CP ranks in \charef{cha:sparseCalculus}) satisfies
\begin{align*}
	\sparsityof{\groundingof{\folexformula}} << \inddim^{\cardof{\indvariableof{\folexformula}}} \, .
\end{align*}
In this case, since most coordinates vanish, the basis CP decomposition (see \secref{sec:basisCP}) enables a representation of the grounding with significantly lower storage demand, see \theref{the:sparseBasisCP}.
This is particularly useful for representing large relational databases, where each object has only a few relations with others, while the majority of possible relations remains unsatisfied.
We depict such CP decomposition of a formula grounding in \theref{fig:groundingCP}.

% Standard KB Encoding and Assumptions
Most logical syntaxes exploit $\ell_0$-sparsity, explicitly storing only known assertions.
The interpretation of unspecified assertions depends on the underlying assumptions.
Under the Closed World Assumption, for example, all unspecified assertions are assumed to be false.

\begin{figure}[h]
\begin{center}
	\input{./PartII/tikz_pics/fol_models/grounding_decomposition.tex}
\end{center}
\caption{Basis CP Decomposition of the grounding of $\folexformula$, following the scheme of \theref{the:sparseBasisCP}.
		Instead of direct storage of the grounding tensor $\groundingof{\folexformula}$, the non-zero coordinates are enumerated by a variable $\datvariable$ and the corresponding coordinates stored in leg-matrices $\legcoreof{\folexformula,\indenumerator}$.}
	\label{fig:groundingCP}
\end{figure}

\subsect{Queries}

A database is understood as a specific fist order logic world, and are operations on such a single world.
Queries are described by a formula $\impformula$, which are asked against a specific world $\dataworld$ to retrieve the grounding $\groundingof{\impformula}$.
The variables of such formulas are called projection variables.
The answer $\groundingof{\impformula}$ of a query is most conveniently represented as a list of solution mappings from the projection variables to objects in the world, such that the query formula is satisfied.
Answering a query by solution mappings corresponds with finding the basis CP Decomposition (see \secref{sec:basisCP}) of $\groundingof{\impformula}$.
We can understand these solution mappings as stored in the leg-matrices $\legcoreof{\folexformula,\indenumerator}$ (see \figref{fig:gorundingCP}).

Let us give with the outer join an example of a popular operation to define queries, which efficient execution and storage can be improved based on considerations in the tensor network formalism.

\begin{definition}[Outer join]
	Let there be a world $\dataworld$ and formulas $\extformulaof{\selindex}$ depending on variables $\indvariableof{\nodesof{\selindex}}$, which have grounding tensors by
	\begin{align*}
		\groundingofat{\extformulaof{\selindex}}{\indvariableof{\node}} \, : \,  \bigtimes_{\node\in\nodesof{\selindex}}[\inddimof{\node}] \rightarrow \ozset \, .
	\end{align*}
	Then their (outer) $\joinsymbol$ is defined as the grounding of their conjunctions, as
	\begin{align*}
		\groundingofat{\joinsymbol\left(\extformulaof{0},\ldots,\extformulaof{\seldim-1}\right)}{\bigcup_{\selindexin}\indvariableof{\nodesof{\selindex}}}
		= \contractionof{\groundingofat{\extformulaof{\selindex}}{\indvariableof{\nodesof{\selindex}}}\,:\,\selindexin}{\bigcup_{l\in[p]}\indvariableof{\nodesof{\selindex}}} \, .
	\end{align*}
\end{definition}

%Visualization and efficiency
We can understand the $\joinsymbol$ of groundings by a factor graph, where each grounding tensor decorates the hyperedge to the node set $\nodesof{\selindex}$.
The projection variable assignment to each formula combined in a $\joinsymbol$ operation provide a basic tensor network format to store the output of the operation.
There are thus situations, in which the solution map storage corresponding with a CP Decomposition comes with unnecessary overheads compared with other formats.

% Coordinatewise transform
We can also understand the $\joinsymbol$ operation as a coordinatewise transform (see \defref{def:coordinatewiseTransform}) with the product as transform function.
To make this connection solid, one would need to extend each joined formula trivially to the variables appearing in other formulas.

% Evaluation similar constraint propagation
The efficiency of evaluating the contraction to a $\joinsymbol$ operation might be improved by understanding it as an Constraint Satisfaction Problem (see \charef{cha:logicalReasoning}).
When applying efficient Message Passing algorithms such as Knowledge Propagation (see \algoref{alg:KP}), the groundings can be sparsified by local constraint propagation operations before turning to more global and more demanding contraction operations.
Here the groundings $\groundingof{\extformulaof{\selindex}}$ would be used to initialize Knowledge Cores $\kcoreof{\edge}$ and sequentially sparsified during the algorithm.

%\begin{example} % WOULD NEED OVERWORK: DRAW!
%	For example take a query with many basic graph patterns with pairwise different projection variables.
%	The global CP Decomposition would come here with an exponential storage overhead compared with storage as a tensor product of CP Decompositions to each Basic graph pattern.
%\end{example}

%% CONFUSING?
%\begin{remark}[Distinguishing from probabilistic queries]
%	Let us distinguish the discussion here from those of queries in probabilistic reasoning, which have two main differences.
%	First, we ask queries against all possible pairs of variables, instead of asking the probability of satisfaction of a specific formula.
%	Second, since we made the epistemologic assumption of knowing possibilities and not probabilities in logics, a query is answered by a truth value.
%	We then only output in the shape of solution mappings the variable assignments where the query formula is true.
% 	Thus, the queries here can be thought of as a batch of probabilistic queries with Boolean answers.
%	% Alternative -> Later?
%	Probabilistic queries can furthermore be understood in terms of the data extraction process described in this section.
%	We can ask the query in probabilistic form (decomposed into atomic formulas) on the resulting empirical distribution.
%	This results in the ratio of the worlds satisfying the query among those worlds satisfying the extraction query $\impformula$.
%\end{remark}


\sect{Representation of Knowledge Graphs}\label{sec:kgRepresentation}

Let us now represent a specific fragment of first-order logic, namely Description Logics which Knowledge Bases are often refered to as Knowledge Graphs.
We here use the $\mathrm{OWL\,2}$ standard, which encodes the syntax of the description logic $\mathcal{SROIQ(D)}$ \cite{rudolph_foundations_2011}.

\subsect{Representation as unary and binary predicates}

% Reduction to binary
Predicates in knowledge graphs are binary (owl:ObjectProperties) and unary (owl:Class).
%Larger formulas are created by logical connections of these atomic formulas using disjunctions, conjunctions etc.
We enumerate the predicates by $[\folpredicateorder]$, the objects in the domain $\worlddomain$ by $[\inddim]$, and extend the unary predicates to binaries by tensor product with $\onehotmapofat{0}{\indvariableof{1}}$.
A Knowledge Graph on the set $\worlddomain$ of constants (owl:NamedIndividuals) is then the tensor
\begin{align*}
	\kgat{\selvariable,\indvariableof{0},\indvariableof{1}} : [\folpredicateorder] \times [\inddim] \times [\inddim] \rightarrow \ozset \, .
\end{align*}


\subsect{Representation as ternary predicate}\label{subsec:knowledgeGraphTernaryRep}

It has been particulary convenient to represent a Knowledge Graph instead as a grounding of a single ternary predicate $\rdf$.
To this end, the predicates $\folpredicateof{\catenumerator}$ and another object $\mathrdftype$ are added to a domain $\worlddomain$, by extending the $\inddim$ and the index interpretation function accordingly.


% RDF triple: Alternative viewpoint to collection of unary and binary predicates!
Following our notation we understand a Knowledge Graph as a grounding of the rdf triple relation $\rdf$ (being a formula of order 3) on a specific world $\kg$ with individuals $\worlddomain$

We then construct a grounding tensor $\kggroundingof{\rdf}$ out of the world $\kgat{\selvariable,\indvariableof{0},\indvariableof{1}}$ by
\begin{align*}
	\kggroundingof{\rdf} : [\inddim] \times [\inddim] \times [\inddim] \rightarrow \ozset
\end{align*}
where
\begin{align*}
	\kggroundingofat{\rdf}{\indexedindvariableof{s}, \indexedindvariableof{p}, \indexedindvariableof{o}} =
	\begin{cases}
		\kgat{\selvariable=\indindexof{s},\indvariableof{0}=\indindexof{o},\indvariableof{1}=0}
		& \text{if} \quad \indindexof{p} = \invindexinterpretationat{\mathrdftype} \\
		\kgat{\selvariable=\indindexof{p},\indvariableof{0}=\indindexof{s},\indvariableof{1}=\indindexof{o}}
		& \text{if} \quad \indindexof{p} = \invindexinterpretationat{\folpredicateof{\catenumerator}} \quad \text{for some} \quad \catenumerator \\
		0  \quad & \text{else}
	\end{cases} \, .
\end{align*} 


Slicing the tensor $\kggroundingof{\rdf}$ along the predicate axis retrieves specific information about roles and can be efficiently be performed on these formats.
The role $\mathrdftype$ has a specific meaning, since it contains from a DL perspective classifications (memberships of named concepts).
Further slicing the tensor along object axis therefore results in membership lists for specific classes (concepts).
One can thus regard $\mathrdftype$ as a placeholder for unitary formulas in a space of binary formulas.

% Triple Stores, sparsity
Exploiting the $\ell_0$-sparsity now leads to a so-called triple store, where $\kggroundingof{\rdf}$ is stored by a listing of those triples $\indindexof{s},\indindexof{p},\indindexof{o}$ such that $\kggroundingofat{\rdf}{\indexedindvariableof{s}, \indexedindvariableof{p}, \indexedindvariableof{o}}=1$
A recent implementation of a triple store exploiting these intuitions is $\mathrm{TENTRIS}$, see \cite{bigerl_tentris_2020}.
In this work, such decompositions are generalized into more generic CP formats, see \charef{cha:sparseCalculus}.
% Approximation of KG Groundings
Approximations of grounding tensors by decompositions leads to embeddings of the individuals such as $\mathrm{Tucker}$, $\mathrm{ComplEx}$ and $\mathrm{RESCAL}$ (see \cite{nickel_review_2016}).

% Sparse representation
%Sparse representation of the grounding tensor to a knowledge graph is of central importance, as investigated in \cite{bigerl_tentris_2020}.
%We here do basis CP for sparse representation.


% Relational Encoding
For our purposes of evaluating logical formulas such as $\sparql$ queries we use the relational encoding of the groundings, which are depicted by
 \begin{center}
	\input{./PartII/tikz_pics/fol_models/kg_formula_tensor.tex}
\end{center}




\subsect{$\sparql$ Queries}

The $\sparql$ query language is a syntax to express first-order logic formulas $\folexformula$ and intended to be evaluated given a Knowledge Graph.
Given a specific knowledge graph $\kggroundingof{\rdf}$, the execution of query is the interpretation $\groundingof{\folexformula}$, typical represented in a sparse basis CP format where each slice represents a solution mapping.

\subsubsect{Triple Patterns}

\red{Central to $\sparql$ queries are triple patterns, which we understand as slicings of the tensor $\kggroundingof{\rdf}$.}
To each so-called triple pattern we define a corresponding pattern tensor.

% By
These slicing operations are performed by contractions with tensors, for unary triple patterns of the shape
\begin{align*}
	\patterncreatorofat{\kgtriple{\provariable}{\mathrdftype}{\folpredicateof{\catenumerator}}}{
		\indvariableof{s}, \indvariableof{p}, \indvariableof{o}, \provariable
	}
	= \identityat{\indvariableof{s},\provariable}
	\otimes \onehotmapofat{\invindexinterpretationat{\mathrdftype}}{\indvariableof{p}}
	\otimes \onehotmapofat{\invindexinterpretationat{\folpredicateof{\atomenumerator}}}{\indvariableof{o}}
\end{align*}

Binary triple patterns come with two projection variables.
The pattern tensor to the $\catenumerator$-th predicate is then
\begin{align*}
	\patterncreatorofat{\kgtriple{\provariableof{0}}{\folpredicateof{\catenumerator}}{\provariableof{1}}}{
		\indvariableof{s}, \indvariableof{p}, \indvariableof{o}, \provariableof{0}, \provariableof{1}
	}
	= \identityat{\indvariableof{s},\provariableof{0}}
	\otimes \onehotmapofat{\invindexinterpretationat{\folpredicateof{\atomenumerator}}}{\indvariableof{p}}
	\otimes \identityat{\indvariableof{o},\provariableof{1}} \, .
\end{align*}

Contraction with these pattern tensor evaluated the specific triple pattern, and outputs in a boolean tensor the indicator, which objects are members of a specific class (for unary patterns) or which pair of objects are related by a specific relation.
Again, the output of such contractions is a subset encodings of the set of solutions (see \defref{def:subsetEncoding}).

%%%%%%%%%%%% END OF FRIDAY
%%%%%%%%%%%%




Formulas which are of specific importance are triple patterns:
\begin{itemize}
	\item Unary triple pattern with one variable, representing a formula with a single projection variable.
	 	For the example $\exunarytriple$ see Figure~\ref{fig:triplePatterns}a.
	\item Binary triple pattern with two variables, representing a formula with two projection variables.
		For the example  $\exbinarytriple$ see Figure~\ref{fig:triplePatterns}b.
\end{itemize}

These triples are compositions of slicing operation, here seen as a map from single or two projection variables to the triples $(\exindividualof{\indindexof{s}}, \exindividualof{\indindexof{p}}, \exindividualof{\indindexof{o}})$, with the $\rdf$ formula.






%	\[ \psi : \left( [\folpredicateorder] \times [\inddim] \times [\inddim] \right) \times [\inddim] \rightarrow [2] \]
%and for binary
%	\[ \psi : \left( [\folpredicateorder] \times [\inddim] \times [\inddim] \right) \times [\inddim] \times [\inddim] \rightarrow [2] \]
%defined by identity and basis vectors depending on the type of triple.

In the example $\exunarytriple$ we have
	\[ \psi^{\exunarytriple} = \identity^{\exindividualof{\sindex},?\exindividualof{0}} \otimes \onehotmapof{0} \otimes \identity^{\exindividualof{\oindex},?\exindividualof{1}}   \]


%
The composition $\psi (\psi^T)$ of the matrification of the tensor $\psi$ is a projection.
That means that applying $\psi (\psi^T)$ is the same map as applying once.


\begin{figure}[h]
\begin{center}
		\input{./PartII/tikz_pics/fol_models/kg_triple_patterns.tex}
\end{center}
\caption{Triple patterns of $\sparql$ as tensor networks.
	a) Example of unary triple pattern $\exunarytriple$ specifying whether an individual $\exindividualof{1}$ is a member of class $C$.
	b) Example of a binary triple pattern $\exbinarytriple$ specifying whether individuals $\exindividualof{1}$ and $\exindividualof{2}$ have a relation $R$.
		By $\onehotmapof{0},\onehotmapof{C},\onehotmapof{R}$ we denote the one-hot encodings of the enumeration of the resources $rdf:type, C$ and $R$.}
\label{fig:triplePatterns}
\end{figure}




\subsubsect{Basic Graph Patterns}

Generic $\sparql$ queries are compositions of triple patterns by logical connectives. % Except for some stuff like regex
Statements in $\sparql$ can be translated into Propositional Logics combining the triple patterns:
\begin{center}
	\begin{tabular}{|c|c|}
  	\hline
 	\textbf{$\sparql$} & \textbf{Propositional Logics} \\
  	\hline
 	$\{f_1, f_2\}$ & $f_1\land f_2$ \\
  	\hline
 	$\mathrm{UNION}\{f_1, f_2\} $& $f_1\lor f_2$ \\
  	\hline
	\end{tabular}
\end{center}

% Decomposition of $\sparql$ queries
A $\sparql$ query $\impformula$ consistent of multiple triple pattern is the composition of the triple patterns groundings.
We can represent its groundings by a contraction of the relational encodings of basic graph patterns and the respective logical connectives.
Alternatively, the more direct effective calculus developed in \secref{sec:effectiveCalculus} can be applied.
The latter way is especially compelling, since large parts of typical $\sparql$ queries are mere conjunctions of the triple patterns, which is reflected in $\sparql$ syntax (default interpretation of instruction-free lists of patterns are conjunctions, while differing connectives need to be specified by additional instructions).



% Further $\sparql$ features
Further $\sparql$ features are
\begin{itemize}
	\item $\mathrm{FILTER}\{\cdot\}$ does not depend on triple patterns (e.g. numeric inequalities, regex functions on strings). 
		We can regard it as another basic formula, which does not result from a slicing of the $\rdf$ grounding tensor.
		Besides that, we can understand it as formulas and include it in compositions.
	\item $\mathrm{OPTIONAL}\{\cdot\}$ would result in $\ones$ leg vectors, when there is a missing variable assignment resulting.
\end{itemize}







\sect{Probabilistic Relational Models}

% MLN in FOL and PL
So far we have studied Markov Logic Networks in Propositional Logics as probability distributions over worlds.
In FOL they define probability distributions over relations in worlds with a fixed set of objects.
More generally, such models are probabilistic relational models (see for an overview \cite{getoor_introduction_2019}.

%
We in this section show, when and how we can interpret likelihoods of Markov Logic Networks in First Order Logic in terms of samples of a Markov Logic Network in Propositional Logics.

\subsect{Markov Logic Networks in FOL}

% Templates
Following \cite{richardson_markov_2006} we define Markov Logic Networks as templates for distributions, which instantiate random worlds when choosing a set of objects $\worlddomain$.
Given a fixed set of constants, they then define a distribution over the worlds, which objects correspond with the constants. % this is database semantics!
This applies database semantics, where only those worlds are considered, where the unique name and domain closure assumptions given a set of constants are satisfied.


\begin{definition}[Markov Logic Networks in FOL]
	A Markov Logic Network is a template of probability distributions defined by a set $\folformulaset$ of FOL formulas with maximal arity $\individualorder$, which is weighted by a function $\weight:\folformulaset \rightarrow \rr$.
	The Markov Logic Network instantiated for a given set of objects $\worlddomain$ and a base measure $\basemeasure$ is the random world, which is a member of the exponential family with sufficient statistics
	\begin{align*}
		\sstat_{\selindex}(\dataworld)  = \sbcontraction{\groundingof{\folexformula_\selindex}} % Formulas can have different
	\end{align*}
	and canonical parameters $\weight$.
\end{definition}

% Interpretation
The statistics
	\begin{align*}
		\sbcontraction{\groundingof{\folexformula_\selindex}} % Formulas can have different
	\end{align*}
can be interpreted as the number of substitutions to a formula, such that the formula ist satisfied.
Each substitution satisfying a formula adds a factor of $\expof{\canparam_\selindex}$ to the probability of the respective world before normalization.


%
When constructing a world tensor to a theory with predicates of different order, we already argued that we extend the arity of predicates by tensor products with $\onehotmapof{0}$.
To define random world tensors, we then restrict the corresponding base measure to be supported only on those worlds where the extended predicates hold only at the individual $\exindividualof{0}$ at the extended axis.



% Comparison with PL MLN
We choose extraction formulas $\extformulaof{\atomenumerator}$ such that any formula in the FOL MLN has a propositional equivalent (see \defref{def:propositionalEquivalent}).
The statistic map is then a formula selecting tensor as in the propositional logic case contracted with the groundings of $\extformulaof{\atomenumerator}$.




	
	
\subsect{Importance Formula}

\red{Analogous to a guard formula in \cite[Definition 6.11]{koller_probabilistic_2009}!}

We want to reduce the number of object tuples influencing the probability distribution in order to arrive at an interpretation of FOL MLNs as likelihoods to datasets of propositional MLNs.

To this end, we mark pairs of objects relevant to the distributions by an index $\datindexin$.
Given a set $\{\indindexof{[\indorder]}^{\datindex} \, : \, \datindexin \}$ of indices to the important tuples we build a set encoding (see \defref{def:subsetEncoding})
\begin{align*}
	\fixedimpformula = \sum_{\datindexin} \left(
		 \bigotimes_{\indenumeratorin} \onehotmapofat{\indindexof{\indenumerator}^{\datindex}}{\indvariableof{\indenumerator}}
	 \right) \, . 
\end{align*}

% Interpretation as grounding
We interpret the tensor $\fixedimpformula$ as the grounding of a formula, which we call the importance formula.

% Restricting to worlds with identical grounding
To have a constant importance formula we define a syntactic representation and restrict the support of the MLN to those world coinciding with groundings of the importance formula coinciding with $\fixedimpformula$ by designing a base measure
\begin{align*}
	\fixedimpbm= \begin{cases}
		1 & \text{if} \quad \groundingof{\impformula} = \fixedimpformula \\
		0 & \text{else}
	\end{cases} \, . 
\end{align*}

% Conditioning on exquery
The base measure restricts the Markov Logic Network to be those worlds, where $\groundingof{\impformula}$ is given by a fixed tensor $\fixedimpformula$.
This amounts to assuming that $\groundingof{\impformula}$ represents certain information about a FOL world, where other formulas are uncertain.



% Extraction query
To reduce the likelihood of a world to we make the assumption that all formulas in a Markov Logic Network are of the form
\begin{align}\label{eq:folImplicationForm}
	\folexformula_{\selindex}(\individuals) =
	\left( \impformula(\individuals) \Rightarrow \headfolexformula(\individuals) \right) 
\end{align}
that is a rule with the importance formula being the premise.
When this assumption holds, we can think of the importance formula as a conditions on individuals to satisfy a statistical relation given by $\headfolexformula$.

We depict the assumption, that any formula is of the form \eqref{eq:folImplicationForm} in the diagram
\begin{center}
	\input{./PartII/tikz_pics/fol_models/implication_propositionalization.tex}
\end{center}
where the second summand depends only on the query $\impformula$ and therefore does not appear in the likelihood.


%\begin{example}[Trivial importance formula]
%	When the importance formula is always satisfied, any tuple of objects contributes to the likelihood. 
%	This original approach to Markov Logic Networks \cite{domingos_markov_2006} however leads to many datapoints which are also dependent on each other.
%\end{example}





	
\subsect{Decomposition of the log likelihood}





%% NOT TRUE!
%Further, we restrict to worlds, where $\groundingof{\folpredicateof{\folpredicateenumerator}}(\individuals)=0$ for pairs with $\fixedimpformula(\individuals)=0$.
%This condition is equal to the partial ordering
%	\[ \groundingof{\folpredicateof{\folpredicateenumerator}} \prec \fixedimpformula \, . \]


% Define probability
Given a FOL MLN, the probability of a world $\dataworld$ with $\groundingof{\impformula}=\fixedimpformula$ is % and $\groundingof{\folpredicateof{\folpredicateenumerator}} \prec \fixedimpformula$ as
\begin{align*}
	\probtensorof{(\folmlnparameters)} [\dataworld]
	= \frac{1}{\partitionfunctionof{\folmlnparameters}} 
		\expof{\sum_{\folexformulain}\weightof{\folexformula}\contraction{\groundingof{(\impformula\Rightarrow\headfolexformula)}}} 
\end{align*}
where the partition function is 
\begin{align*}
	\partitionfunctionof{\folmlnparameters} = 
	\sum_{\supportedworlds}
			\expof{\sum_{\folexformulain}\weightof{\folexformula}\contraction{\groundingof{(\impformula\Rightarrow\headfolexformula)}}} 
		%\expof{\sum_{\folexformulain}  \weightof{\folexformula}  \sum_{\indindexlist\in[\inddim]} \groundingofat{(\impformula\Rightarrow\headfolexformula)}{\indexedshortindvariables} } }
	%\prod_{\individuals\in\worlddomain} \left(\prod_{\folexformulain} \expof{\weightof{\folexformula}(\impformula\Rightarrow\folexformula)(\individuals)} \right)\, . 
\end{align*}


Let us now decompose the statistics into constant and varying terms.
We have
\begin{align*}
	\contraction{\groundingof{(\impformula\Rightarrow\headfolexformula)}} = 
		\contraction{\groundingof{\impformula\land\headfolexformula}} + \contraction{\groundingof{\lnot\impformula}} \, ,
\end{align*}
where the the second term is constant among the supported worlds and the first can be enumerated by the satisfied substitutions of $\impformula$, that is
\begin{align*}
	\contraction{\groundingof{\impformula\land\headfolexformula}} 
	= \sum_{\datindexin}\groundingofat{\headfolexformula}{\indvariableof{[\indorder]} = \indindexof{[\indorder]}^{\datindex}} \, .
\end{align*}


Using these insights we decompose a normalized log likelihood as
\begin{align}\label{eq:dataworldLogProb}
	\frac{1}{\datanum} \lnof{\probtensorof{(\folmlnparameters)} [\dataworld]}
	= & \frac{1}{\datanum} \sum_{\datindexin} \groundingofat{\headfolexformula}{\indvariableof{[\indorder]} = \indindexof{[\indorder]}^{\datindex}}
	- \frac{1}{\datanum} \lnof{
		\frac{\partitionfunctionof{\folmlnparameters}}{
			\expof{\contraction{\weight} \cdot \contraction{\groundingof{\lnot\impformula}}}
		}
	}
%	\sum_{\individuals\in\worlddomain \, : \, \impformula(\individuals)=1} \left(\sum_{\exformula\in\formulaset} \weightof{\exformula} \exformula(\individuals)\right) -  \frac{1}{\datanum}\lnof{\secpartitionfunctionof{\folformulaset,\weight,\worlddomain,\fixedimpformula}} \, . 
\end{align} 

% Data identification
We notice a similarity with the likelihood in the case of MLNs in propositional logic.
When we interpret each pair $\individuals\in\worlddomain$ satisfying $\impformula(\individuals)=1$ as a datapoint, and choose the formulas $\formulaset$ from atomic formulas connected by propositional connectives, both approaches are equivalent.
However, the partition function couples multiple samples, and prevents a straight forward interpretation as an empirical dataset.
We in the next section present assumptions on the tuples satisfying $\impformula$, which lead to a factorization of the partition function.

% Partition function simplification

















\subsect{Interpretation as Likelihood of Propositional Dataset}


\begin{theorem}\label{the:FOLworldToPLdataset}
	Let $\fixedimpbm$ be a base measure of worlds such that the vectors
	\begin{align} \label{eq:data}
		\left(  \groundingofatwrt{\extformulaof{0}}{\indvariableof{0}=\indindexof{0}^\datindex,\ldots,\indvariableof{\indorder-1}=\indindexof{\indorder-1}^\datindex}{\tilde{\dataworld}}, \ldots,
			\groundingofatwrt{\extformulaof{\atomorder-1}}{\indvariableof{0}=\indindexof{0}^\datindex,\ldots,\indvariableof{\indorder-1}=\indindexof{\indorder-1}^\datindex}{\tilde{\dataworld}}
		\right) 
	\end{align}
	for $\datindexin$ are independent and identical distributed by a distribution $\atombasemeasure$, when distributing $\tilde{\dataworld}$ by $\fixedimpbm$.

	Let there further be a set of formulas $\folformulaset$, which formulas $\folexformula\in\folformulaset$ are representable by 
		\[ \folexformula = \impformula \Rightarrow \headfolexformula \]
	and we find a propositional formula $\exformula$ with
		\[ \groundingof{\headfolexformula} = \contractionof{\{\rencodingof{\groundingof{\extformulaof{\atomenumerator}}} \, : \, \atomenumeratorin \} \cup \{\exformula \}}{\shortindvariablelist} \, . \]
		
		
	We then have for the likelihood of any by $\fixedimpbm$ supported world $\dataworld$ that
		\[ 	\frac{1}{\datanum} \lnof{\probtensorof{(\folmlnparameters)} [\dataworld]} = 	\frac{1}{\datanum} \lnof{\probtensorof{(\mlnparameters,\atombasemeasure)} [\datamap]} - \frac{1}{\datanum} \sum_{\datindexin} \lnof{\atombasemeasure[\shortcatvariables = \datamap({\datindex})]} \]
	where $\atombasemeasure$ is the distribution of the random vectors \eqref{eq:data} and by $\datamap$ we denote the dataset
	\begin{align*}
		\datamap = \big\{\big( \groundingofat{\extformulaof{0}}{\indvariableof{0}=\indindexof{0}^\datindex,\ldots,\indvariableof{\indorder-1}=\indindexof{\indorder-1}^\datindex}, \ldots ,
				 \groundingofat{\extformulaof{\atomorder-1}}{\indvariableof{0}=\indindexof{0}^\datindex,\ldots,\indvariableof{\indorder-1}=\indindexof{\indorder-1}^\datindex} \big) \, : \datindexin \big\}
	\end{align*}
	and $\formulaset$ is the set of propositional formulas to $\folexformula\in\folformulaset$.
\end{theorem}

To show the theorem, we show first in the following lemma the factorization of the partition function of the FOL MLN.

\begin{lemma}\label{lem:FOLpartitionfunctionfactorization}
	Given the assumptions of \theref{the:FOLworldToPLdataset}, we have
	\begin{align*}
		\frac{\partitionfunctionof{\folmlnparameters}}{\expof{\contraction{\weight} \cdot \contraction{\groundingof{\lnot\impformula}}}} = \left(\partitionfunctionof{\mlnparameters,\atombasemeasure}\right)^\datanum \, .
	\end{align*}
\end{lemma}
\begin{proof}
	We have
	\begin{align*}
		\partitionfunctionof{\folmlnparameters} 
		&= \expectationofwrt{
			 \expof{\sum_{\folexformulain}\weightof{\folexformula}\contraction{\groundingof{(\impformula\Rightarrow\headfolexformula)}}} 
		}{\dataworld\sim\fixedimpbm} \\
		&= \expof{\contraction{\weight} \cdot \contraction{\groundingof{\lnot\impformula}}} \cdot 
		\expectationofwrt{
			 \expof{\sum_{\folexformulain}\weightof{\folexformula}  \sum_{\datindexin} \groundingofat{\headfolexformula}{\indvariableof{[\indorder]} = \indindexof{[\indorder]}^{\datindex}} }
		}{\dataworld\sim\fixedimpbm} \\
		&= \expof{\contraction{\weight} \cdot \contraction{\groundingof{\lnot\impformula}}} \cdot 
		\expectationofwrt{
			\prod_{\datindexin} \expof{ \sum_{\folexformulain} \weightof{\folexformula} \cdot \groundingofat{\headfolexformula}{\indvariableof{[\indorder]} = \indindexof{[\indorder]}^{\datindex}} }
		}{\dataworld\sim\fixedimpbm} \\
		&= \expof{\contraction{\weight} \cdot \contraction{\groundingof{\lnot\impformula}}} \cdot 
			\prod_{\datindexin}
			 \expectationofwrt{
			 \expof{ \sum_{\folexformulain} \weightof{\folexformula} \cdot \groundingofat{\headfolexformula}{\indvariableof{[\indorder]} = \indindexof{[\indorder]}^{\datindex}} }
		}{\dataworld\sim\fixedimpbm} \\
	\end{align*}
	Here we used, that since the substitutions of the atom formulas at the respective object tuples are independent, also the variables
		\[ \expof{\weightof{\folexformula}  \cdot \groundingofat{\headfolexformula}{\indvariableof{[\indorder]} = \indindexof{[\indorder]}^{\datindex}}  } \]
	for $\datindexin$ are independent.
	
	Since each $\groundingofat{\headfolexformula}{\indvariableof{[\indorder]}}$ depends only on the random value $\catindexof{\atomenumerator}^\datindex=\extformulaof{\atomenumerator}[\indvariableof{[\indorder]}]$ we have
	\begin{align*}
		\expectationofwrt{
			\expof{ \sum_{\folexformulain} \weightof{\folexformula} \cdot \groundingofat{\headfolexformula}{\indvariableof{[\indorder]} = \indindexof{[\indorder]}^{\datindex}} }
		}{\dataworld\sim\fixedimpbm} 
		& = \sum_{\catindexof{[\atomorder]} \in [2]^\atomorder} 
		\expectationofwrt{\forall {\atomenumeratorin} \, : \, \extformulaof{\atomenumerator}[\indvariableof{[\indorder]}]=\catindexof{\atomenumerator}}{\dataworld\sim\fixedimpbm} \\
		& \quad \quad \quad  \cdot 
		\expof{\sum_{\exformulain} \weightof{\exformula} \cdot \formulaat{\indexedcatvariableof{[\atomorder]}}}
		 \\
		& = \sum_{\catindexof{[\atomorder]} \in [2]^\atomorder} \atombasemeasure[\indexedcatvariableof{[\atomorder]}] \cdot 
		\expof{\sum_{\exformulain} \weightof{\exformula} \cdot \formulaat{\indexedcatvariableof{[\atomorder]}}}
		 \\
%		& = \sbcontraction{\expof{\sum_{\exformulain} \weightof{\exformula} \cdot \exformula}} \\
		& = \partitionfunctionof{\mlnparameters, \atombasemeasure} \, . 
	\end{align*}
	%Here we used again the assumption of independent atom query substitutions which states
	%\begin{align*}
	%	\expectationofwrt{\forall {\atomenumeratorin} \, : \, \extformulaof{\atomenumerator}[\indvariableof{[\indorder]}]=\catindexof{\atomenumerator}}{\dataworld\sim\fixedimpbm}
	%	= \prod_{\atomenumeratorin} \expectationofwrt{\extformulaof{\atomenumerator}[\indvariableof{[\indorder]}]=\catindexof{\atomenumerator}}{\dataworld\sim\fixedimpbm} = \frac{1}{2^\atomorder} \, . 
	%\end{align*}
	Combining the above, we arrive at the claim.
\end{proof}

\begin{proof}[Proof of \theref{the:FOLworldToPLdataset}]
	Use Equation \ref{eq:dataworldLogProb} and the \lemref{lem:FOLpartitionfunctionfactorization}.
	Note that we need to correct the likelihood by the averalge log basemeasure on the data, since that term is appearing in the likelihood of a MLN.
\end{proof}


% Independent data investigation
Let us now investigate, when the assumptions of independent data can be matched.

\begin{lemma}
	Let $\impformula$ and $\extformulas$ be quantor and constant free and let the index tuples of the support of $\fixedimpformula$ be pairwise disjoint.
	Then the vectors \eqref{eq:data} are pairwise independent.
\end{lemma}
\begin{proof}
	Then we can reduce each sample as dependent only on an independent random world with domain by the respective objects.
	Quantor and constant-free is needed that this reductions is possible.
\end{proof}



Situations where atom base $\atombasemeasure$ measures are not uniform:
\begin{itemize}
	\item extraction formula being a) conjunctions of predicates: Probability that they are satisfied decreases
							b) disjunctions of predicates: Probability that they are satisfied increases
	\item extraction formula coinciding with importance formula: Always satisfied
	 \item extraction formulas contradicting each other, more general not independent from each other
\end{itemize}








\subsect{Approximation by Independent Samples}

In general, we cannot assume that the $\impformula,\extformulas$ do not influence each other.

We approximate the partition function into factors to each solution map of $\impformula$.
This amounts to the assumption, that the atoms created to each tuple are independent.

%
For example, when solution maps share the resources, which form the arguments of an atom extraction query, they coincide on this atom and are thus dependent.

% 
If the expectations of each sample with respect to the marginalized distributions coincide, the average of empirical distribution also coincides with these (by linearity).
When the creation of samples has sufficient mixing properties, the empirical distribution converges to this expectation in the asymptotic case of large numbers of samples.

% 
%In that case, the probability of a world $\dataworld$ corresponds with the likelihood of a dataset $\datamap$ where each sample corresponds with a solution of the query $\impformula$.









\subsect{Extraction of Samples from FOL Knowledge Bases}

The decomposition of the likelihood suggests the following approach to generate samples from groundings:
%We propose the following approach to generate datacores from groundings:
\begin{itemize}
	\item Define for $\atomenumeratorin$ queries $\extformulaof{\atomenumerator}$ generating the the atoms $\atomicformulaof{\atomenumerator}$: 
	Predicates along with assignment of variables / constants to its positions.
	\item Define a query formula $\impformula$, which we decompose in the basis CP decomposition for later interpretation of datapoints
	\item Contract the groundings of each formula $\exformula^{\atomenumerator}$ with the grounding of $\impformula$ to build a data core
\end{itemize}









\subsect{Representation by Tensor Networks}

Let us now understand the extraction process as a relation between a tuple of individuals and the extracted world in the factored system of atoms $\atomicformulaof{\atomenumerator}$.
	\[ \extractionrelation
	= \{ (\individuals, \enumeratedatoms) 
	\, : \,  \impformula(\individuals) = 1 \, , \, \forall_{\atomenumeratorin} : \,  \atomicformulaof{\atomenumerator} = \extformulaof{\atomenumerator}(\individuals) \}\]
	
Towards constructing the encoding of this we enumerate the individuals in the set $\worlddomain$ and use in the following the respective one-hot encoding 
	\[ \onehotmap : \worlddomain \rightarrow \rr^{\cardof{\worlddomain}} \, . \]
The combination of the queries $\impformula$ and $\{\extformulas\}$ by the relational encoding
	\[\rencodingof{\extractionrelation} \subset \left(\indspace\right) \otimes \left(\atomspace\right) \]
defined by
	\[ \rencodingof{\extractionrelation} 
	= \left\{ (\onehotmapof{\individuals}, \enumeratedatoms) 
	\, : \, \impformula(\individuals) = 1 \, , \, \forall_{\atomenumeratorin} : \,  \atomicformulaof{\atomenumerator} = \extformulaof{\atomenumerator}(\individuals) \right\} \, . \]

%
We can represent the encoding $\rencodingof{\extractionrelation}$ by the diagram
\begin{center}
	\input{./PartII/tikz_pics/fol_models/extraction_relation.tex}
\end{center}
Here the contraction of $\rencodingof{\impformula}$ with the truth vector $\tbasis$ represents the matching condition posed by $\impformula$ when extracting pairs of individuals.

%% Empirical Distribution
The empirical distribution is then the normalized contraction leaving only the legs to the extracted atomic formulas open, that is
\begin{align*}
 	\empdistribution 
	= \frac{
	\sbcontractionof{\rencodingof{\extractionrelation}}{\shortcatvariables}
	}{
	\sbcontraction{\rencodingof{\extractionrelation}}
	}  \, . 
\end{align*}
Here the number of extracted data is the denominator
	\[ \datanum = \contraction{\rencodingof{\extractionrelation}} \, .  \]
	
We depict this by
\begin{center}
	\input{./PartII/tikz_pics/fol_models/empirical_generation.tex}
\end{center}




\subsect{Basis CP Decomposition of extracted data}

To connect with the empirical distribution introduced in \secref{sec:empDistribution} we now show how the dataset $\datamap$ extracted from the interpretations of the formulas $\impformula,\extformulas$ on a FOL world $\dataworld$ can be represented by tensor networks.



Each datacore is then a contraction with the grounding of a formula, which is contracted with the grounding of the extraction query in the basis CP decomposition, 
\begin{align*}
	\datacoreof{\atomenumerator} = \sbcontractionof{\rencodingof{\groundingof{\extformulaof{\atomenumerator}}},\groundingof{\impformula}}{\datvariable,\catvariableof{\atomenumerator}}
\end{align*}
see Figure~\ref{fig:datacoreGeneration}.

\begin{theorem}	
	We have
		\[ 	\sbcontractionof{\rencodingof{\extractionrelation}}{\shortcatvariables} 
		= \contractionof{\{\datacoreofat{\atomenumerator}{\datvariable,\catvariableof{\atomenumerator}} \, : \, \atomenumeratorin\}}{\shortcatvariables} \]
	and thus 
		\[  \empdistribution = \normationof{\{\datacoreofat{\atomenumerator}{\datvariable,\catvariableof{\atomenumerator}}  \, : \, \atomenumeratorin\}}{\shortcatvariables} \, . \]
\end{theorem}
\begin{proof}
	Using that $\groundingof{\impformula}$ is binary and the core can be copied (ref to theorem in binary calculus).
\end{proof}

% Efficient contraction: Do also basis decomposition of the extraction query and use efficient contraction!
Towards efficient calculation of the data cores, we build a basis CP decomposition of $\groundingof{\impformula}$, where we further demand $\scalarcore=\ones$.
This is a collection of basis leg cores $\legcoreof{\fixedimpformula,\indenumerator}$ such that
\begin{align*}
	\fixedimpformula[\shortindvariablelist] = \contractionof{ \left\{ \legcoreofat{\fixedimpformula,\indenumerator}{\datvariable,\indvariableof{\indenumerator}} \, : \, \indenumeratorin \right\} }{\shortindvariablelist} \, . 
\end{align*}

% Data enumeration -> To representation
%We can further utilize any decomposition of $\impformula$ into a directed and binary CP Format to enumerate the datapoints by the slice index $\datindex$. % Approaches like SPARQL directly give us these by solution mappings.
%Understanding $\impformula$ as a query on the world being the database, such decomposition is given by the set of solution mappings.


\begin{figure}[h]
\begin{center}
	\input{./PartII/tikz_pics/fol_models/datacore_generation.tex}
\end{center}
\caption{Generation of Datacores given a formula $\exformula$, whose variables (here $\exindividualof{1},\exindividualof{2},\exindividualof{3}$) are selected from an extraction query $\impformula$.
	Variables, which are not appearing in the formula $\exformula$ are trivialized over (here $\exindividualof{4},\ldots,\exindividualof{\variableorder})$.
	For consistency we denoted the index $\atomlegindexof{\exformulaof{\atomenumerator}}$ by $\atomlegindexof{\atomenumerator}$.}
	\label{fig:datacoreGeneration}
\end{figure}

% Log-likelihood
The log-likelihood of a MLN is then the contraction of the data cores with the representation of the MLN wrt the atomic formulas $\extformulaof{\atomenumerator}$.



\subsect{Representation with auxiliary term variables}


\red{
When many atom extraction formulas differ only by a constant, we can replace the constant by an auxiliary term variable.
The atoms are then the atomizations of this variable (see \secref{sec:categoricalTN}), treated as a categorical variable, with respect to the constant in the extraction query.
The advantages are that we can avoid the $\rencodingof{}$-formalims and directly model the categorical distributions.
}




\subsect{Generic Tensor Network Decomposition of Extracted Data}

\red{
More naturally: Formulas are compositions of predicates. 
When quantor free then a tensor network.
}


% 
\begin{remark}[Alternative Representation of empirical distributions]
	In many applications such as the computation of log-likelihoods we can use any representation of the empirical distribution by tensor networks. 
	It is thus not necessary to compute the data cores as above, unless one requires a list of the extracted samples.
\end{remark}





\subsect{Design of the Formulas}

Most intuitive when labeling individuals by classes.
Extraction formulas $\extformulas$ can then be defined by subclasses of the member of a class and relations between objects of different classes. % Koller calls atomic formulas the template attributes
We then choose $\formulaset$ as more involved formulas decomposed into connectives acting on these atoms.
The extraction query $\impformula$ is then designed based on class memberships to ensure, that the arguments of the formulas are always of specific classes. % Koller specifies to each argument of the attributes a class 

% Approach
We propose to
\begin{itemize}
	\item Execute an extraction query to get pairs of individuals (the pairDf).
	\item Propositionalize the FOL Formulas independently on each tuple taking the individuals as a set of constant and filtering on the possible properties of each individuals.
		(Can understand as adding knowledge that most of the relations do not hold)
	\item Understand each such generated knowledge base as datapoint and average over them to get the empirical distribution to be fit. 
	\item Fit a MLN describing the statistical relations of unseen results of the extraction query, based on likelihood maximation.
\end{itemize}




\sect{Generation of FOL worlds}


\begin{definition}[Reproduction of Empirical Distributions]
	Given an empirical distribution $\empdistribution\in\atomspace$, we say that a triple $(\dataworld,\impformula,\extformulaof{[\atomorder]})$ of a FOL world $\dataworld$ an importance formula $\impformula$ and extraction formulas $\extformulaof{\atomorder}$ reproduces $\empdistribution$, when 
		\[\empdistribution = \normationof{\{\groundingof{\impformula}\}\cup\{\rencodingof{\kggroundingof{\extformulaof{\atomenumerator}}\, : \, \atomenumeratorin}\}}{\shortcatvariables} \, .  \]
\end{definition}


\subsect{Samples by single objects}

The first reproduction scheme identifies the datapoints with objects $\worlddomain=[\datdim]$

\begin{theorem}
	Given a dataset $\datamap$ the world $\dataworld[\selvariable,\indvariable]$ 
	\begin{align*}
		\dataworld[\selvariable,\indvariable] = \sum_{\atomenumeratorin} \sum_{\datindexin \, : \datamap_{\atomenumerator}(\datindex)=1} \onehotmapofat{\atomenumerator}{\selvariable} \otimes \onehotmapofat{\datindex}{\indvariable}
	\end{align*}
	reproduces with the trivial importance query and extraction queries coinciding with the predicates the dataset $\datamap$.
\end{theorem}



\sect{Samples by pairs of objects}

We instantiate multiple objects for each datapoint, one for each variable of the importance formula, i.e. $\worlddomain=[\datdim]\times[\indorder]$



\sect{Example: Generation of Knowledge Graphs} % To generation of FOL worlds?

So far we have discussed, how MLNs for FOL Knowledge Bases such as Knowledge Graphs can be built by extracting data.
Conversely, any binary tensor can be interpreted as a Knowledge Graph.
To be more precise, we follow the intuition that the ones coordinates mark possible worlds compatible with the knowledge about a factored system.
Each possible world can then be encoded in a subgraph of the Knowledge Graph representing the world.

%
This amounts to an "inversion" of the data generation process described in the subsection above.

%
% Having a directed and binary CP decomposition of $\exformula$, each possible world is encoded by a slice.


% Formalization
\begin{definition}[Reproduction of Empirical Distributions]
	Given an empirical distribution $\empdistribution\in\bigotimes_{\atomenumeratorin}\rr^2$, we say that a tuple $(\kg,\impformula,\{\extformulas\})$ of a Knowledge Graph $\kg$ and queries $\impformula,\extformulaof{\atomenumerator}$ reproduces $\empdistribution$, when
		\[\empdistribution = \normationof{\{\kggroundingof{\impformula}\}\cup\{\rencodingof{\kggroundingof{\extformulaof{\atomenumerator}}\, : \, \atomenumeratorin}\}}{\shortcatvariables} \, .  \]
\end{definition}

%
Any distribution with rational coordinates is an empirical distribution, since each coordinate can be interpreted as the frequency of the respective world in the data $\datamap$.
%In a frequentist interpretation we instantiate each world according to the rate $\probtensor(\atomindices)$.
%This interpretation requires a rounding of the real probabilities by rational numbers.


\subsect{Samples by single resources}

\paragraph{TBox:} The categorical variables of the factored system are the coarsest classes.
We define atomic formulas by the state indicators of each categorical variable as in \secref{sec:categoricalTN}.
Each such atomic formula corresponds with a sub-class of the classes.
By definition, each collection of state indicators define thus pairwise disjoint subclasses.

\paragraph{ABox:} The samples are represented by single individuals in the Knowledge Graph.
Their sub-class memberships corresponding with the categorical variables of the system are instantiated whenever the atom is true in the sample.


\begin{theorem}
	Let there any empirical distribution $\empdistribution\in\bigotimes_{\atomenumeratorin}\rr^2$ and $\datanum\in\nn$ such that $\imageof{\datanum\cdot\empdistribution}\subset\nn$.
	Then the tuple $(\kg,\impformula,\{\extformulas\})$ defined by
	\begin{align}
		\kg =
		& \bigcup_{\atomindicesin}  \{(
			s_{j, \atomindices} \quad \mathrm{rdf:type} \quad C ) : j \in [\datanum\cdot\empdistribution(\atomindices)] \}  \\
		&\bigcup_{\atomindicesin}  \{(
			s_{j, \atomindices} \quad \mathrm{rdf:type} \quad C_\atomenumerator
		) :  j \in [\datanum\cdot\empdistribution(\atomindices)], \atomenumeratorin , \atomlegindexof{\atomenumerator}=1\} 
	\end{align}
	\begin{centeredcode}
		\impformula = SELECT \{ ?x \} WHERE \{ ?x \quad \rdftype\quad C \, .\}
	\end{centeredcode}
	\begin{centeredcode}
		$\extformulaof{\atomenumerator}$ = SELECT \{ ?x \} WHERE \{ ?x \quad \rdftype \quad $C_\atomenumerator$ \, .\}
	\end{centeredcode}
	reproduces $\empdistribution$.
\end{theorem}
\begin{proof}
	With respect to any enumeration of the resources of $\kg$ we have
	\begin{align}
		\kggroundingof{\impformula} 
		= \sum_{\atomindicesin} \sum_{j \in [\datanum\cdot\empdistribution(\atomindices)]} \onehotmapof{s_{j, \atomindices} } 
	\end{align}
	and
	\begin{align}
		\kggroundingof{\extformulaof{\atomenumerator}} 
		= \sum_{\atomindicesin \, : \, \atomlegindexof{\atomenumerator} = 1} \sum_{j \in [\datanum\cdot\empdistribution(\atomindices)]} \onehotmapof{s_{j, \atomindices} } \, . 
	\end{align}
	Summing over the resource variables of these tensors in a contraction we get
	\begin{align}
		\contractionof{\{\kggroundingof{\impformula}\}\cup\{\rencodingof{\kggroundingof{\extformulaof{\atomenumerator}}\, : \, \atomenumeratorin}\}}{\shortcatvariables}
		& = \sum_{\atomenumeratorin}  \datanum\cdot\empdistribution(\atomindices) \cdot \onehotmapof{\atomindices} = \datanum \cdot \empdistribution 
	\end{align}
	and therefore
	\begin{align}
		\normationof{\{\kggroundingof{\impformula}\}\cup\{\rencodingof{\kggroundingof{\extformulaof{\atomenumerator}}}\, : \, \atomenumeratorin\}}{\shortcatvariables} = \empdistribution \, . 
	\end{align}
\end{proof}

% 
In this simple Knowledge Graph, Description Logic is expressive enough to represent any formula $\folexformula$ composed of the formulas $\extformulas$.






\subsect{Samples by pairs of resources}




\begin{remark}[Refinement of the Samples]
	We can split each sample node into a pair of individuals.
	For this we need to specify, which each class membership will be encoded in a unary or binary attribute of the splitted individuals.
	This specification is possible based on the extraction query and the atomic formulas.
\end{remark}

% 
Taking any importance query $\impformula$, which has no permutation symmetries, we can instantiate each projection variable for each sample and prepare the links according to the triple patterns.
When the atom queries $\extformulas$ have different triple patterns compared with $\impformula$, we instantiate those in cases where $\atomlegindexof{\atomenumerator}=1$.


%
Label individuals $a_{\datindex,\indenumerator}$ by dataindex and variable index and

\begin{theorem}
	Let there be queries $\impformula,\extformulas$, a Knowledge Graph $\kg$ containing individuals $a_{\datindex,\indenumerator}$ and a data map $\datamap$.
	If
		\[ \kggroundingof{\impformula} = \sum_{\datindex} \bigotimes_{\indenumeratorin} \onehotmapof{\datindex,\indenumerator} \]
	and for any $\atomenumeratorin$
		\[ \kggroundingof{\extformulaof{\atomenumerator}} 
		= \sum_{\datindex : \datamap^{\atomenumerator}(\datindex)=1} \bigotimes_{\indenumerator \in \extformulaof{\atomenumerator}} \onehotmapof{\datindex,\indenumerator} \, . \]
	Then the tuple $(\kg,\impformula,\{\extformulas\})$ reproduces $\empdistribution$.
\end{theorem}
\begin{proof}
	It can be shown that the contraction is
		\[ \sum_{\datindex} \bigotimes_{\atomenumeratorin} \onehotmapof{\datamap^{\atomenumerator}(\datindex)} \, . \]
\end{proof}

\begin{theorem}
	The Knowledge Graph
		\[ \kggroundingof{\rdf} = \sum_{t \in \impformula} \sum_{\datindex} t(a_{\datindex,\indenumerator})
		+ \sum_{\atomenumeratorin} \sum_{t \in \extformulaof{\atomenumeratorin}} \sum_{\datindex : \datamap^{\atomenumerator}(\datindex)=1} t(a_{\datindex,\indenumerator})\]
	reproduces the data, if the summed tensors are pairwise orthogonal.
	\red{Here we denote by t(..) the one-hot encoding of the tuple when mapping the corresponding individuals on the projection variables of the triple pattern t.}
\end{theorem}

\subsect{General orthogonal projection approach}

Given any triples, such that their composed projections to pairwise different triple patterns vanish. 
One reproducing KG is then the sum of the projection of any reproducing KG.



\begin{theorem}
	Let  $\impformula,\extformulas$ contain of triple patterns such that for $t\neq \tilde{t}$
		\[ P_t \circ P_{\tilde{t}} = 0 \, . \]
	For any KG reproducing the data $\datamap$ also the KG 
		\[ \sum_{t} P_t \kggroundingof{\rdf} \]
	is a reproducing KG.
\end{theorem}
\begin{proof}
	By coinciding on all triple patterns $t$ since
		\[ P_t \left( \sum_{\tilde{t}} P_{\tilde{t}} \kggroundingof{\rdf}  \right)  
		= P_t P_t \kggroundingof{\rdf}  
		= P_t \kggroundingof{\rdf}  \, .   \]
	Then the extraction contraction on $\kg$ and $ \sum_{t} P_t\kggroundingof{\rdf} $ coincides.
\end{proof}

From this perspective, it is obvious that any triple in the orthogonal complement of all projections can be added and the KG stays reproducing.






\sect{Discussion}

Models of FOL are called Probabilistic Relational Models. % (RUSSELL - Chapter Probabilistic Programming).
Extensions are models that also handle structural uncertainty, i.e. distributions of worlds with varying $\worlddomain$.

%\subsect{Network Science}

\red{Statistical Models of Knowledge Graphs going beyond the typical single edge type perspective of network science \cite{barabasi_network_2016, giovanni_russo_vito_latora_complex_2017}.}

