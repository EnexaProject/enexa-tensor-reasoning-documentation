\section{Logic Network Inference}\label{cha:networkLearning}

In this chapter we first investigate unconstrained parameter estimated for Markov Logic Networks and Hybrid Logic Networks, which are special cases of the backward maps introduced in Chapter~\ref{cha:probDecomposition}.
We then motivate structure learning based on sparsity constraints on the parameters on the minterm family and present heuristic strategies leading to efficient structure learning algorithms.

\subsection{Mean parameters of Hybrid Logic Networks}

The convex polytope of realizable mean parameters (see Definition~\ref{def:meanForwardBackward}) is for a statistic $\formulaset$ of propositional formulas
	\[ \meansetof{\formulaset} = \left\{ \sbcontractionof{\probtensor,\sencodingof{\formulaset}}{\selvariable} \, : \, \probtensor\in\probtensorset \right\} \, ,\]
where by $\probtensorset$ we denote the set of all probability distributions.

The convex polytope has a characterization as a convex hull
\begin{align}\label{eq:hlnMeansetConvCharacterization}
	\meansetof{\formulaset} = \convhullof{\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable} \, : \, \shortcatindices\in\atomstates} \, . 
\end{align}

We exploit this characterization to show, that the extreme points of $\meansetof{\formulaset}$ are exactly those realizable by Hard Logic Networks.

\begin{theorem}\label{the:extremeCharacterizationHLN}
	Any parameter $\meanparamat{\selvariable}\in\meansetof{\formulaset}$ is an extreme point of $\meansetof{\formulaset}$, if and only $\meanparamat{\selvariable}$ is boolean and the formula
			\[ \basemeasureofat{\formulaset,\meanparam}{\shortcatvariables} \coloneqq \bigwedge_{\selindexin \, : \, \meanparamat{\indexedselvariable}\in\{0,1\}}
		\lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables} \]
	is satisfiable.
	In that case, $\meanparam$ is the mean parameter of the Hard Logic Network with 
		\[ \kb=\{\lnot^{(1-\meanparamat{\indexedselvariable})} \enumformula \, : \, \selindexin \} \, . \]
\end{theorem}
\begin{proof}
	\proofrightsymbol: Let $\meanparam$ be an extreme point of $\meansetof{\formulaset}$. 
		Since by \eqref{eq:hlnMeansetConvCharacterization} $\meansetof{\formulaset}$ is a convex hull of vectors, there exists a $\shortcatindices\in\atomstates$ such that
			\[ \meanparamat{\selvariable} = \sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}  \, . \]
		By definition of $\sencodingof{\formulaset}$, $\meanparamat{\selvariable}$ is a boolean vector and for any $\selindexin$ we have
			\[ \enumformulaat{\indexedshortcatvariables} = \meanparamat{\indexedselvariable} \]
		and thus
			\[ \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\indexedshortcatvariables} = 1 \, .  \]
		It follows that $\shortcatindices$ is also a model of $\basemeasureof{\formulaset,\meanparam}$ and $\basemeasureof{\formulaset,\meanparam}$ is satisfiable.
			
	\proofleftsymbol: To show the converse direction, let $\meanparamat{\selvariable}$ be a boolean vector such that $\basemeasureof{\formulaset,\meanparam}$ is satisfiable.
		Then there exists a model $\shortcatindices$ of $\basemeasureof{\formulaset,\meanparam}$.
		We have for any $\selindexin$ 
			\[ \sencodingofat{\formulaset}{\indexedshortcatvariables,\indexedselvariable} =  \enumformulaat{\indexedshortcatvariables} = \meanparamat{\indexedselvariable} \]
		and thus $\meanparamat{\selvariable}=\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}$.
		With the characterization \eqref{eq:hlnMeansetConvCharacterization} this establishes in particular $\meanparamat{\selvariable}\in\meansetof{\formulaset}$.		
		Since $\meanparamat{\selvariable}$ is boolean and therefore an extreme point of the cube $\cubeof{\seldim}$, it is also an extreme point of the subset $\meansetof{\formulaset}\subset\cubeof{\seldim}$.
\end{proof}


We now show, that any element of the mean parameter set can be realized by a hybrid logic network.

\begin{theorem}
	To any $\meanparamat{\selvariable}\in\meansetof{\formulaset}$ we build the set $\formulaset^{\meanparam}=\{\enumformula\in\formulaset : \meanparamat{\indexedselvariable}\notin\{0,1\}\}$ and the base measure
			\[ \basemeasureofat{\formulaset,\meanparam}{\shortcatvariables} \coloneqq \bigwedge_{\selindexin \, : \, \meanparamat{\indexedselvariable}\in\{0,1\}}
		\lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables} \, . \]
	Then, if $\meanparamat{\selvariable}$ is realizable by a positive distribution wrt $\basemeasureofat{\formulaset,\meanparam}{\shortcatvariables}$, it is realizable by an element of $\expfamilyof{\formulaset^{\meanparam},\basemeasureof{\formulaset,\meanparam}}$, that is there is a $\probtensor\in\expfamilyof{\formulaset^{\meanparam},\basemeasureof{\formulaset,\meanparam}}$ with
		\[ \sbcontractionof{\probtensor,\sencodingof{\formulaset}}{\selvariable} = \meanparamat{\selvariable} \, .  \] 
\end{theorem}
\begin{proof}
	Since $\meanparamat{\selvariable}\in\meansetof{\formulaset}$ there is a $\secprobat{\shortcatvariables}$ with 
		\[ \contractionof{\secprobtensor,\sencodingof{\formulaset}}{\selvariable} = \meanparamat{\selvariable} \, . \]
	From this it follows, that $\secprobtensor$ has a representation with respect to the base measure $\basemeasureof{\formulaset,\meanparam}$ since
		\[ \secprobat{\shortcatvariables} = \contractionof{\secprobtensor,\basemeasureof{\formulaset,\meanparam}}{\shortcatvariables}  \, . \]
	Thus, we have for the reduced mean parameter $\tilde{\meanparam}$, which results from dropping the coordinates in $\{0,1\}$, that
		\[ \tilde{\meanparam} \in \meansetof{\formulaset^{\meanparam},\basemeasureof{\formulaset,\meanparam}} \, . \]
	%% Using the positive distribution assumption -> Need to incorporate by a lemma!
	\red{Use here the below Lemma!}
	If $\formulaset^{\meanparam}$ is a minimal representation for the family $\expfamilyof{\formulaset^{\meanparam},\basemeasureof{\formulaset,\meanparam}}$,
	then $\tilde{\meanparam}$ is an interior point of $\meansetof{\formulaset^{\meanparam},\basemeasureof{\formulaset,\meanparam}}$ any by Theorem~3.3 in \cite{wainwright_graphical_2008}, there is a $\probtensor$ with $\sbcontractionof{\probtensor,\sencodingof{\formulaset}}{\selvariable} = \meanparamat{\selvariable}$.
	If $\formulaset^{\meanparam}$ is not a minimal representation, we can choose a subset of formulas providing a minimal representation with same expressivity and find $\probtensor$ with the same argument.
\end{proof}


\begin{lemma}
	If $\meanparamat{\selvariable}$ is in an exponential family with boolean base measure $\basemeasure$ and minimal representation, then it is in the interior of $\meanset$ if and only if it is representable by a positive distribution with respect to $\basemeasure$.
\end{lemma}
\begin{proof} 
	\proofrightsymbol: % Orient on proof of Theorem~3.3 
		We show, that for any test vector $\vectorat{\selvariable}$ different from the origin, there is a $\tilde{\meanparam}[\selvariable]$ with
			\[ \contraction{\vectorat{\selvariable},\meanparamat{\selvariable}} <  \contraction{\vectorat{\selvariable},\tilde{\meanparam}[\selvariable]} \, . \]
		Then, by [Rockafellar], since the exponential family is minimal and thus $\meanset$ full-dimensional, we have $\meanparamat{\selvariable}$ in the interior of $\meanset$.
	
	\proofleftsymbol: Theorem~3.3 in \cite{wainwright_graphical_2008}, we can construct the positive distribution by a member of the exponential family using that the gradient of the cumulant function is onto.
\end{proof}


\begin{lemma}
	If $\meanparamat{\selvariable}$ is in an exponential family with boolean base measure $\basemeasure$ and minimal representation, and not representable by a positive distribution with respect to $\basemeasure$, then there is $\canparamat{\selvariable}$ such that
		\[ \meanparamat{\selvariable} \in \argmax_{\meanparam\in\meanset} \contraction{\canparam,\meanparam} \, . \]
	Moreover, $\meanparamat{\selvariable}$ is representable with respect to the basemeasure
		\[ \basemeasure \cup  \indicatorofat{\arbset}{\shortcatvariables} \, , \]
	where the indicator is on the set
		\[ \arbset = \argmax_{\shortcatindices} \contraction{\canparam,\sstat(\shortcatindices)}  \, . \]
\end{lemma}
\begin{proof}
	We use the halfspace representation of the bounded polyhedron $\meanset$ to show the first claim.
	By the above lemma, $\meanparamat{\selvariable}$ is not in the interior, therefore on a facet and thus the $\canparam$ is found by the normal equation of that facet.
	
	This facet is the convex hull of the extreme points of $\meanset$ corresponding with the elements of $\arbset$.
	There exists hence a convex combination in the form of a distribution $\probtensor$ supported on $\arbset$, such that 
		\[ \meanparamat{\selvariable} = \sum_{\shortcatindices\in\arbset} \probat{\indexedshortcatvariables} \sencsstatat{\indexedshortcatvariables,\selvariable} \]
	and $\meanparamat{\selvariable}$ is reproduced by $\probtensor$.
\end{proof}

Each facet of $\meanset$ thus defines a possible base measure, which is sufficient to reproduce the mean parameters on that facet.

\begin{definition}
	The facet basemeasure to the facet of $\meanset$ with normal $\canparam$ is
		\[ \basemeasureof{\formulaset,\canparam} = \indicatorofat{\argmax_{\shortcatindices} \contraction{\canparam,\sstat(\shortcatindices)}}{\shortcatvariables} \, . \]
\end{definition}


\begin{lemma}
	Let $\canparamat{\indexedselvariable}$ be a normal to a facet of $\meanset$.
	If and only if the formula
		\[ \formulaof{\formulaset,\canparam} = \bigwedge_{\selindexin \, : \, \canparamat{\indexedselvariable}\neq0} \lnot^{(1-\greaterzeroof{\canparamat{\indexedselvariable}})} \enumformula  \]
	is satisfiable, then it is the base measure to the facet with normal $\canparam$.
	Here $\greaterzeroof{z}$ denotes the indicator of $z>0$.
	
	If the above is not satisfiable, then the base measure is
		\[ \bigvee_{v[\selvariable] \, : \, \contraction{\canparam,v} \in \max_{\meanparam} \contraction{\canparam,\meanparam} }  \formulaof{\formulaset,\contractionof{\canparam,v}{\selvariable}}  \]
	where $v$ are boolean vectors.
\end{lemma}


% Reduction to Hybrid Logic Networks
We can thus reduce the set of probability distributions in the definition of the convex polytope of mean parameters to the set $\hlnsetof{\formulaset}$ (see Definition~\ref{def:hln}), that is
\begin{align*}
	\meansetof{\formulaset} = \left\{ \sbcontractionof{\probtensor,\sencodingof{\formulaset}}{\selvariable} \, : \, \probtensor\in\hlnsetof{\formulaset} \right\} \, .
\end{align*}

% Mean parameter characterization
To summarize, for any $\meanparam\in\meansetof{\formulaset}$ we have one of the following (see also Figure~\ref{fig:meansetSketch}):
\begin{itemize}
	\item All $\meanparamat{\indexedselvariable}\notin\{0,1\}$: Then reproducible by a Markov Logic Network, $\meanparam\in\meanset$.
		This follows from the classical result of Theorem~3.3 in \cite{wainwright_graphical_2008}.
	\item At least one $\meanparamat{\indexedselvariable}\notin\{0,1\}$ and one $\meanparamat{\indexedselvariable}\in\{0,1\}$: Then reproducible by a Hybrid Logic Network.
	\item All $\meanparamat{\indexedselvariable}\in\{0,1\}$: Then  an extreme point and by Theorem~\ref{the:extremeCharacterizationHLN} reproducible by a Hard Logic Network.
\end{itemize}



\begin{figure}[h]\label{fig:meansetSketch}
\begin{center}
	\input{PartII/tikz_pics/parameter_estimation/meanset_sketch.tex}
\end{center}
\caption{Sketch of the convex polytope ${\meansetof{\formulaset}}$ as a subset of the $\seldim$-dimensional cube $[0,1]^\seldim$ (here as a 2-dimensional projection) with example mean 	parameters $\meanparam_1,\meanparam_2,\meanparam_3$.
	The boundary points $\meanparam_1,\meanparam_2\in\closureof{\meanset}_{\formulaset}/\interiorof{\meanset}_{\formulaset}$ are examples of mean parameters, which can be realized by a Hard Logic Networks (respectively Hybrid Logic Network). 
	Any extreme point $\meanparam_1\in\meansetof{\formulaset}\cup\{0,1\}^{\seldim}$ is realizable by a Hard Logic Network, while a non-extreme boundary point $\meanparam_2\in\meansetof{\formulaset}/\{0,1\}^{\seldim}$ is realizable by a Hybrid Logic Network.
	Any interior point $\meanparam_3\in\interiorof{\meansetof{\formulaset}}$ is realizable by a Markov Logic Network.
} 
\end{figure}




\subsection{Unconstrained Parameter Estimation} \label{sec:parameterEstimation} % Check for redundancy with the mln introduction chapter!

% Repetition and result transfering
Markov Logic Networks are exponential families with statistics by a set $\formulaset$ of propositional formulas.
We furthermore allow for propositional formulas as base measures, to also include the discussion of Hybrid Logic Networks.
Based on this, we apply the theory of probabilistic inference, developed in Chapter~\ref{cha:probReasoning} for the generic exponential families.

% Special example: MLE
The Maximum Likelihood Problem on Markov Logic Networks is the M-projection
\begin{align*}
	\argmax_{\canparamat{\selvariable}\in\rr^{\seldim}} \quad 
	\centropyof{\probtensor}{\expdistof{(\sstat,\canparam,\basemeasure)}}	
\end{align*}
in the case $\probtensor=\empdistribution$ for a data map $\datamap$.

% Backward map
The M-projection coincides, after dropping constant terms in case of non-trivial base measure, with the backward map
\begin{align*}
	\argmax_{\canparamat{\selvariable}\in\rr^{\seldim}} \quad 
	\sbcontraction{\canparamat{\selvariable},\meanparamat{\selvariable}} - \cumfunctionof{\canparamat{\selvariable}} 
\end{align*}
where
\begin{align*}
	\meanparam{\selvariable} 
	= \sbcontractionof{\sencodingof{\formulaset},\probtensor}{\selvariable} 
	\quad \text{and} \quad
	\cumfunctionof{\canparamat{\selvariable}} 
	= \sbcontraction{\expof{ \sbcontractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables} }, \basemeasure} \, . 
\end{align*}

\red{Theory for MLN a special case of the generic case of exponential families, here corollaries?.}

% Extension to HLN
We now extend to Hybrid Logic Networks
\begin{align*}
	\argmax_{\secprobtensor\in\hlnsetof{\formulaset}} \quad 
	\centropyof{\probtensor}{\secprobtensor}	
\end{align*}

\begin{theorem}
	Let $\meanparamat{\selvariable} = \contractionof{\probtensor,\sencfset}{\selvariable}$ and
		\[ \secformulaset = \{\enumformula \, : \, \meanparamat{\indexedselvariable} \in \{0,1\}\} \quad , \quad 
		\basemeasureofat{\secformulaset,\meanparam}{\shortcatvariables} 
		= \bigwedge_{\enumformula\in\secformulaset} \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables}
		\, . \]
	The solution of the M-projection of $\probtensor$ onto the set of hybrid logic networks representable by $\formulaset$ then coincides with the projection of $\probtensor$ onto $\expfamilyof{\formulaset/\secformulaset,\basemeasureof{\secformulaset,\meanparam}}$.
\end{theorem}
\begin{proof}
	Work by weight cutoff, in the limit of hard logics?
\end{proof}

\subsubsection{Maximum Entropy in Hybrid Networks}


% Special example: MaxEnt
The Maximum Entropy Problem for Markov Logic Networks is
\begin{align}
	\argmax_{\probtensor} \quad \sentropyof{\probtensor} 
	\quad \text{subject to} \quad  
	\sbcontractionof{\probtensor,\sencodingof{\formulaset}}{\selvariable}
	 =  \meanparamat{\selvariable} 
\end{align}



\begin{corollary}[of Theorem~\ref{the:maxEntMaxLikeDuality}]
	\red{Works only for $\meanparamat{\indexedselvariable}\in(0,1)$}
	Among all distributions $\probtensor$ of $\atomstates$ satisfying $\sbcontractionof{\probtensor,\sencodingof{\formulaset}}{\selvariable}
	 = \sbcontractionof{\empdistribution,\sencodingof{\formulaset}}{\selvariable}$ the Markov Logic Network with formulas $\formulaset$ and weights $\canparam$ being the solution of the maximum likelihood problem has minimal entropy.
\end{corollary}

% Unique property of MLN
We notice, that the solution of the maximum entropy problem is thus a Markov Logic Network.
This is remarkable, because this motivates our restriction to Markov Logic Networks as those distributions with maximal entropy given satisfaction rates of formulas in $\formulaset$.


% Extension to HLN
When now extend to the situations $\meanparamat{\indexedselvariable}\in\{0,1\}$ can appear.
It that case the formula is entailed or contradicted by the facts, and dropping should be considered in both cases.

The max entropy - max likelihood duality still holds for hybrid logic networks as we show in the next theorem.

\begin{theorem}
	Given a set of formulas $\tilde{\formulaset}$ and $\tilde{\meanparam}$, with coordinates $\tilde{\meanparam}_\selindex\in[0,1]$ in the closed interval $[0,1]$.
	If the corresponding maximum entropy problem is feasible, its solution is a hybrid logic network with 
	\begin{itemize}
		\item $\hardformulaset= \{\enumformula : \selindexin, \meanparamat{\indexedselvariable} = 1\} \cup  \{\lnot\enumformula : \selindexin, \meanparamat{\indexedselvariable} = 0\} $
		\item $\softformulaset = \{\enumformula : \selindexin, \meanparamat{\indexedselvariable} \in (0,1)\}$
		\item $\canparam$ being the backward map evaluated at the vector $\meanparam$ consisting of the coordinates of $\tilde{\meanparam}$ not in $\{0,1\}$
	\end{itemize}
\end{theorem}
\begin{proof}
	Feasible distributions have a density with base measure by $\hardformulaset$, we therefore reduce the set of distributions in the argmax to those with density to the base measure.
	The max entropy is a max entropy problem with respect to that base measure, where we only keep the constraints to the mean parameters different from $\{0,1\}$ (those are trivially satisfied).
	The statement then follows from the generic property (see Sec3.1 in \cite{wainwright_graphical_2008}).
\end{proof}






\subsection{Alternating Algorithms to Approximate the Backward Map}\label{sec:alternatingParEstMLN}

Let us now introduce an implementation of the Alternating Moment Matching Algorithm~\ref{alg:AMM} in case of Markov Logic Networks.
To solve the moment matching condition at a formula $\enumformula$ we refine Lemma~\ref{lem:mmContractionEquation} in the following.

\begin{lemma}\label{ref:lemMMinMLN}
	Let there be a base measure $\basemeasure$, a formula selecting map $\formulaset=\{\enumformula \, : \, \selindexin\}$ and weights $\canparam$, and choose $\selindexin$ such that $\enumformula  \notin \{\onesat{\shortcatvariables},\zerosat{\shortcatvariables}\}$.	
	The moment matching condition relative to $\canparam$, $\selindexin$ and $\datameanat{\indexedselvariable}\in(0,1)$ is then satisfied, if
	\begin{align} \label{sol:momentMatchingExformula}
	 	\weightat{\indexedselvariable} = \lnof{
		\frac{\datameanat{\indexedselvariable}}{(1-\datameanat{\indexedselvariable})} 
		\cdot \frac{\hypercoreat{\catvariableof{\enumformula }=0}}{\hypercoreat{\catvariableof{\enumformula }=1}} 
		} 
	\end{align}
	where by $\hypercoreat{\catvariableof{\enumformula }}$ we denote the contraction 
	\begin{align*}
	 	\hypercoreat{\catvariableof{\enumformula}} 
		= \contractionof{\{\rencodingof{\enumformula} \, : \, \selindexin\}
		\cup\{\headcoreof{\tilde{\selindex}} : \tilde{\selindex} \in [\seldim], \tilde{\selindex}\neq\selindex\}
		\cup\{\basemeasure\}}{\catvariableof{\enumformula}} \, . 
	\end{align*}
\end{lemma}
\begin{proof}
	Since $\imageof{\enumformula}\subset[2]$ we have
	\begin{align*}
		\idrestrictedto{\imageof{\enumformula}} = \onehotmapofat{1}{\catvariableof{\enumformula}}
	\end{align*}
	and the moment matching condition is by Lemma~\ref{lem:mmContractionEquation} satisfied if
	\begin{align*}
		\sbcontraction{\headcoreof{\selindex}, \onehotmapof{1}, \hypercore}
			= \sbcontraction{\headcoreof{\selindex},\hypercore} \cdot \datameanat{\indexedselvariable} \, . 
	\end{align*}
	This is equal to 
	\begin{align*}
		\expof{\canparamat{\indexedselvariable}} \cdot \hypercoreat{\catvariableof{\enumformula}=1}
		= \left( \expof{\canparamat{\indexedselvariable}} \cdot \hypercoreat{\catvariableof{\enumformula}=1} + \hypercoreat{\catvariableof{\enumformula}=0} \right) \cdot \datameanat{\indexedselvariable} \, . 
	\end{align*}
	Rearranging the equations this is equal to 
	\begin{align*}
	 	\hypercoreat{\catvariableof{\enumformula}} 
		= \contractionof{\{\rencodingof{\enumformula}\}
		\cup\{\headcoreof{\tilde{\selindex}} : \tilde{\selindex} \in [\seldim], \tilde{\selindex}\neq\selindex\}
		\cup\{\basemeasure\}}{\selvariable} \, . 
	\end{align*}
	We notice that the right side is well defined, since we have by assumption $\datameanat{\indexedselvariable}, (1- \datameanat{\indexedselvariable}) \neq 0$ and $\hypercoreat{\catvariableof{\enumformula}=0}, \hypercoreat{\catvariableof{\enumformula}=1} \neq 0$ since Markov Logic networks are positive distributions and $\enumformula \notin \{\onesat{\shortcatvariables},\zerosat{\shortcatvariables}\}$.
\end{proof}


%% Hard network reference!
In the case $\datameanat{\indexedselvariable}\in\{0,1\}$ the moment matching conditions are not satisfiable for $\canparamat{\indexedselvariable}\in\rr$.
But, we notice, that in the limit $\canparamat{\indexedselvariable}\rightarrow \infty $ (respectively $-\infty$) we have
	\[ \meanparamat{\indexedselvariable} \rightarrow  1 \quad \text{(respectively $0$)}\, ,  \]
and the moment matching can be satisfied up to arbitrary precision.
In Section~\ref{sec:hardNetworks} we will allow infinite weights and interpret the corresponding factors by logical formulas.
As a consequence, we will able to fit graphical models, which we will call hybrid networks on arbitrary satisfiable mean parameters.

%
The cases $\hypercoreat{\catvariableof{\enumformula}=1}=0$, respectively $\hypercoreat{\catvariableof{\enumformula}=1}=0$ only appear for nontrivial formulas when the distribution is not positive. 
This is not the case for Markov Logic Networks, but will happen when formulas are added as cores of a Markov Network.
This situation will has been investigated in Section~\ref{sec:hardNetworks}.


% Concave likelihood 
Since the likelihood is concave (see \cite{koller_probabilistic_2009}), there are not local maxima the coordinate descent could run into and coordinate descent will give a monotonic improvement of the likelihood. 

We suggest an alternating optimization by Algorithm~\ref{alg:AWO}, solving the moment matching equation iteratively for all formulas $\exformulain$ and repeat the optimization until a convergence criterion is met.
This is an coordinate ascent algorithm, when interpreted the loss $\lossof{\expdist}$ as an objective depending on the vector $\canparam$.

\begin{algorithm}[hbt!]
\caption{Alternating Weight Optimization (AWO)}\label{alg:AWO}
\begin{algorithmic}
%% INPUT: Numerated formula set, mean parameter $\datameanat{\selvariable}$
%\For{$\exformula\in\formulaset$}
\State $\kb = \ones$, $\secnodes=\varnothing$
\For{$\selindexin$}
	\If{$\meanparamat{\indexedselvariable}=1$}
		\[ \hardformulaset \algdefsymbol \hardformulaset \cup \{\enumformula\}\]
	%\EndIf
	\ElsIf{$\meanparamat{\indexedselvariable}=0$}
		\[ \hardformulaset \algdefsymbol \hardformulaset \cup \{\lnot\enumformula\}\]
	\Else
		\State $\secnodes\algdefsymbol \secnodes \cup \{\selindex\}$
	\EndIf
\EndFor
\For{$\selindex\in\secnodes$}	
		\State Compute
		\[ \hypercoreat{\catvariableof{\enumformula}}
		\algdefsymbol \sbcontractionof{\rencodingof{\enumformula}}{\catvariableof{\enumformula}} \]
		\State Set 
		\begin{align*}
	 		\canparamat{\indexedselvariable} 
			\algdefsymbol \lnof{
			\frac{\datameanat{\indexedselvariable}}{(1-\datameanat{\indexedselvariable})} 
			\cdot \frac{\hypercoreat{\catvariableof{\enumformula}=0}}{\hypercoreat{\catvariableof{\enumformula}=1}} 
			} 
		\end{align*}
\EndFor
\If {$\sbcontraction{\kb}=0$}
	 \State \textbf{raise} "Inconsistent Knowledge Base"
\EndIf
\While{Convergence criterion is not met}
\For{$\selindex\in\secnodes$}
	\State Compute
	\begin{align*}
	 	\hypercoreat{\catvariableof{\enumformula}} 
		= \contractionof{\{\rencodingof{\enumformula} \, : \, \selindexin\}
		\cup\{\headcoreof{\tilde{\selindex}} : \tilde{\selindex} \in [\seldim], \tilde{\selindex}\neq\selindex\}
		\cup\{\basemeasure\}}{\catvariableof{\enumformula}}
	\end{align*}
	\State Set 
	\begin{align*}
	 	\canparamat{\indexedselvariable} = \lnof{
		\frac{\datameanat{\indexedselvariable}}{(1-\datameanat{\indexedselvariable})} 
		\cdot \frac{\hypercoreat{\catvariableof{\enumformula}=0}}{\hypercoreat{\catvariableof{\enumformula}=1}} 
		} 
	\end{align*}
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}


% Independent formulas
In the initialization phase of Algorithm~\ref{alg:AWO}, each parameters is initialized relative to a uniform distribution. 
The algorithm would be finished, if the variables $\catvariableof{\exformula}$ are independent.
This would be the case, if the Markov Logic Network consists of atomic formulas only.
When they fail to be independent, the adjustment of the weights influence the marginal distribution of other formulas and we need an alternating optimization.
% 
This situation corresponds with couplings of the weights by a partition contraction, which does not factorize into terms to each formula.


% Inference
Solving Equation~\ref{sol:momentMatchingExformula} requires inference of a current model by answering the query to the formula $\texformula$.
This can be a bottleneck and circumvented by approximative inference, see e.g. CAMEL \cite{ganapathi_constrained_2008}.



\begin{remark}[Grouping of coordinates with trivial sum]
	When having a set of coordinates, such that the coordinate functions are binary and sum to the trivial tensor, one can find simultaneous updates to the canonical parameters, such that the partition function is staying invariant.
	Given a parameter $\canparam^t$ we compute
		\[ \meanparam^t = \contractionof{\expdistof{(\sstat,\canparam^t)}, \sstat}{\selvariable} \]
	and build the update
		\[ \canparam^{t+1} = \canparam^t + \lnof{\meanparam^{\datamap}}{\meanparam^t} \, . \]
	Then, $\canparam^{t+1}$ satisfies the moment matching equations for all coordinates in the set.
	
	
	The assumptions are met when taking all features to any hyperedge in a Markov Network seen as an exponential family.
	In that case, the update algorithm is refered to as  Iterative Proportional Fitting \cite{wainwright_graphical_2008}.
	Further, when activating both $\exformula$ and $\lnot\exformula$.
\end{remark}


\subsection{Forward and backward mappings in closed form}

% Closed form availability
We recall from Chapter~\ref{cha:probReasoning}, that while forward mappings are always in closed form by contractions, backward mapping in general do not have a closed form representation.
Instead, the backward map is in general implicitly characterized by a maximum entropy problem constrained to matching expected sufficient statistics.
We investigate in this section specific examples, where closed forms are available for both.
In these cases, parameter estimation can thus be solved by application of the inverse on the expected sufficient statistics with respect to the empirical distribution, and iterative algorithms can be avoided.

% Usage
%When the backward map $\backwardmap$ is available in closed form, we directly get optimal parameters by the inversion acting on the satisfaction rate and can avoid iterative algorithms of parameter estimation.

\subsubsection{Maxterms and Minterms}

Minterms (respectively maxterms) are ways in propositional logics to get a syntactical formula representation based on a formula to each world which is a model (respectively fails to be a model).
We have already studied in Section~\ref{sec:MLNMaxMintermRep} how to represent any distribution as a MLN of maxterms (respectively minterms), see Theorem~\ref{the:maximalClausesRepresentation}.

We use the tuple enumeration of the maxterms and minterms by $\atomstates$ introduced in Section~\ref{sec:termClauseDecomposition}.
With respect to this enumeration the canonical parameters and mean parameters are tensors in $\bigotimes_{\atomenumeratorin}\rr^2$. 
%% Interpretation of the mean parameters
Since the statistic of the minterm family is the identity, the mean parameters for the minterm family are
	\[ \meanparamat{\selvariableof{[\atomorder]}=\catindexof{[\atomorder]}} 
	= \probat{\catindexof{[\atomorder]}} 
	\]
and therefore after a relabeling of categorical variables to selection variables $\meanparam=\probtensor$.
For maxterms we have analogously
	\[ \meanparamat{\selvariableof{[\atomorder]}=\catindexof{[\atomorder]}} 
	= 1-\probat{\catindexof{[\atomorder]}} 
	\]
and $\meanparam = \onesat{}-\probtensor$.
We can use these insights to provide a characterization of the forward and backward maps of the minterm and maxterm family.

\begin{theorem}
	Given the Markov Logic Networks to the formula sets
		\[ \mintermformulaset := \{ \mintermof{\atomindices} \, : \, \atomindicesin\} \quad \text{and} \quad 
		\maxtermformulaset := \{ \maxtermof{\atomindices} \, : \, \atomindicesin\}  \]
	of all minterms, respectively of all mapterms, the forward mapping are
		%\[ \forwardmapwrt{\mintermformulaset}: \bigotimes_{\atomenumeratorin}\rr^{2} \rightarrow \bigotimes_{\atomenumeratorin}\rr^{2} \]
		\[ \forwardmapwrt{\mlnmintermsymbol}(\canparam) = \normationofwrt{\expof{\canparam}}{\shortcatvariables}{\varnothing}  
		\quad \text{and} \quad 
		 \forwardmapwrt{\mlnmaxtermsymbol}(\canparam) = \normationofwrt{\expof{-\canparam}}{\shortcatvariables}{\varnothing} \, , \]
	where in a slight abuse of notation we assigned the variables $\shortcatvariables$ to the canonical parameters $\canparam$.

	Possible choices of the backward mappings are
		%\[ \backwardmapwrt{\mlnmintermsymbol}: \bigotimes_{\atomenumeratorin}\rr^{2} \rightarrow \bigotimes_{\atomenumeratorin}\rr^{2} \]
		\[ \backwardmapwrt{\mlnmintermsymbol}(\meanparam) = \lnof{\meanparam} 
			\quad \text{and} \quad 
			\backwardmapwrt{\maxtermformulaset}(\meanparam) = -\lnof{\meanparam} \, .
		 \]
\end{theorem}
\begin{proof}
	For the minterms we use that
		\[ \mintermformulaset[\shortcatvariables,\catvariableof{\mintermformulaset}]  = \identityat{\shortcatvariables,\catvariableof{\maxtermformulaset}}\] 
	and get
		\[ \forwardmapwrt{\mlnmintermsymbol}(\canparam) 
		= \normationof{
		\expof{\contractionof{\{\mintermformulaset, \canparam\}}{\shortcatvariables}}
		}{\shortcatvariables}
		= 
		\normationof{\expof{\canparam}}{\shortcatvariables} \, . 
		\]
	
	We notice that for any $\meanparam$ in the image of the forward map we have
		\[ \forwardmapwrt{\mlnmintermsymbol}(\backwardmapwrt{\mlnmintermsymbol}(\meanparam)) = \meanparam \]
	Therefore, $\backwardmapwrt{\mintermformulaset}$ is indeed a backward mapping to the exponential family of minterms.
	
	For the maxterms we use that
		\[ \maxtermformulaset[\shortcatvariables,\catvariableof{\maxtermformulaset}] = \onesat{\shortcatvariables,\catvariableof{\maxtermformulaset}}-\identityat{\shortcatvariables,\catvariableof{\maxtermformulaset}} \]
	and get
	\begin{align*}
		\forwardmapwrt{\mlnmaxtermsymbol}(\canparam) 
		& = \normationof{
		\expof{\contractionof{\{\mintermformulaset, \canparam\}}{\shortcatvariables}}
		}{\shortcatvariables} \\
		& = \normationof{\{
		\expof{\contractionof{\{\ones, \canparam\}}{\shortcatvariables}}, 
		\expof{-\contractionof{\canparam}{\shortcatvariables}} \}
		}{\shortcatvariables} \\
		& = \normationof{
		\expof{-\canparam}
		}{\shortcatvariables}
	\end{align*}
	where we used, that $\expof{\contractionof{\{\ones, \canparam\}}{\shortcatvariables}}$ is a multiple of $\onesat{\shortcatvariables}$ and is thus eliminated in the normation.
	For any $\meanparam\in\imageof{\forwardmapwrt{\mlnmaxtermsymbol}}$ we have
		\[ \forwardmapwrt{\mlnmaxtermsymbol}(\backwardmapwrt{\mlnmaxtermsymbol}(\meanparam) ) 
		= \meanparam
		%-\lnof{\expof{-\canparam}} + \contractionof{\expof{-\canparam}}{\varnothing} \cdot \onesat{\shortcatvariables}
		%= \canparam + \contractionof{\expof{-\canparam}}{\varnothing} \cdot \onesat{\shortcatvariables}
		\]
	and $\backwardmapwrt{\mlnmintermsymbol}$ is thus a backward map for the exponential family of maxterms.
\end{proof}

% Fitting arbitrary distributions
Any positive probability distribution can thus be fitted by minterms when we choose $\canparam=\lnof{\probtensor}$, respectively by maxterms when we choose $\canparam=\ones-\lnof{\probtensor}$.
Thus, we have identified a subset of $2^{\atomorder}$ formulas, which is rich enough to fit any distribution.





\subsubsection{Atomic formulas}

% Repeat atomic formulas
Let us now derive a closed form backward mapping for the statistic
	\[ \atomformulaset := \{\atomicformulaof{\atomenumerator}: \atomenumeratorin\} \, . \]

The mean parameters coincide with the queries on the atomic formulas, that is the marginal 
	\[ \meanparamat{\selvariable=\atomenumerator} = \probat{\catvariableof{\atomenumerator}=1}  \, . \]

\begin{theorem}
	Given a Markov Logic Network with the statistic $\atomformulaset$ of atomic formulas, the forward mapping from canonical parameters to mean parameters is the coordinatewise sigmoid, that is
		\[ \forwardmapwrtof{\mlnatomsymbol}{\canparamat{\selvariable}} = \frac{\expof{\canparamat{\selvariable}}}{\onesat{\selvariable}+\expof{\canparamat{\selvariable}}}   \]
	where the quotient is performed coordinatewise.

	A backward mapping is the coordinatewise logit, that is
		\[ \backwardmapwrt{\mlnatomsymbol}(\meanparamat{\selvariable}) 
		= \lnof{\frac{
			\meanparamat{\selvariable}
			}{
			\onesat{\selvariable}-\meanparamat{\selvariable}
			}}  \, . \]
\end{theorem}
\begin{proof}
	We have for any $\canparamat{\selvariable}\in\rr^{\atomorder}$
		\[ \probofat{(\atomformulaset,\canparam)}{\shortcatvariables} 
		= \bigotimes_{\atomenumeratorin} \normationof{\expof{\canparamat{\selvariable=\atomenumerator}\cdot \atomicformulaof{\atomenumerator}}}{\catvariableof{\atomenumerator}}  \, . \]

	
	For any $\atomenumeratorin$ it therefore holds, that
	\begin{align*}
		\forwardmapwrtof{\mlnatomsymbol}{\canparamat{\selvariable}}[\selvariable=\atomenumerator] 
		&=\sbcontraction{\atomicformulaof{\atomenumerator},  \probofat{(\atomformulaset,\canparam)}{\shortcatvariables}} \\
		&=\sbcontraction{\atomicformulaof{\atomenumerator},  \normationof{\expof{\canparamat{\selvariable=\atomenumerator}\cdot \atomicformulaof{\atomenumerator}}}{\catvariableof{\atomenumerator}}} \\
		& = \frac{\expof{\canparamat{\selvariable=\atomenumerator}}}{1+\expof{\canparamat{\selvariable=\atomenumerator}}} \, .
	\end{align*}

	Since the coordinatewise logit is the inverse function of the coordinatewise sigmoid the map
	\begin{align*}
		\backwardmapwrtof{\mlnatomsymbol}{\meanparamat{\selvariable}}[\selvariable=\atomenumerator] 
		& = \lnof{\frac{\meanparamat{\selvariable=\atomenumerator}}{1- \meanparamat{\selvariable=\atomenumerator}}}
	\end{align*}
	satisfies for any $\meanparam$ in the image of the forward map
	\begin{align*}
		\forwardmapwrt{\mlnatomsymbol}(\backwardmapwrt{\mlnatomsymbol}(\meanparam)) = \meanparam 
	\end{align*}
	and is therefore a backward map.
\end{proof}


% Representation by selection tensor networks
In a selection tensor networks they are represented by a single neuron with identity connective and variable selection to all atoms.
We will investigate such examples in more detail in Chapter~\ref{cha:sparseTC}, where atomic formulas Markov Logic Networks are specific cases of monomial decomposition of order 1.
	
% Interpretation of the result as independence approximation
The maximum likelihood estimator of a positive probability distribution by the MLN of atomic formulas is therefore the tensor product of the marginal distributions.
The Kullback-Leibler divergence between the distribution and its projection is the mutual information of the atoms, see for example Chapter~8 in \cite{mackay_information_2003}.

\begin{remark}[Decomposition into systems of atomic networks]
	\red{By Independence Decomposition we reduce to a system of atomic MLN.
	The minterms of such MLNs are the literals.
	By redundancy (literals sum up to $\ones$), it suffices to take only the positive or the negative literal.
	}
%	We set the weights of $\weightof{\lnot\atomicformulaof{\atomenumerator}}=0$ (corresponding with a gauge normation of the energy offset symmetry). % Not needed!
\end{remark}
















