\chapter{Introduction into \parref{par:two}}

After having explored tensor approach to the classical logical and probabilistic reasoning in Part I, we now turn to neuro-symbolic applications of tensor networks in artificial intelligence.

We start in \charef{cha:formulaSelection} with the design of explainable and efficient learning architectures, which we frame formula selecting networks.
These architectures will be utilized in the representation and reasoning on graphical models in \charef{cha:networkRepresentation} and \charef{cha:networkReasoning}.


\sect{Hybrid Logical Networks}

We unify the discussion on exponential families and propositional formulas, by constructing sufficient statistics and base measures based on propositional formulas.
This restricts the sufficient statistics to boolean valued functions, which can be decomposed into connectives.
This restriction brings the following advantages:
\begin{itemize}
	\item \textbf{Computation Advantage:} The sufficient statistics is decomposable into logical connectives.
	If the formulas are sparse (in the sense of limited number of connectives necessary in their representation), this gives rise to efficient tensor network decompositions of the basis encoding.
    As a result, the features can be efficiently computed based on tensor networks.
    \item \textbf{Statistical Advantage:} Since each formula is Boolean valued, the coordinates of the sufficient statistic are Bernoulli variables.
	Due to their boundedness, they and their averages (by Hoeffdings inequality) are sub-Gaussian variables with favorable concentration properties (absence of heavy tails).
\end{itemize}

We combine graphical models with logical formulas, by combining
\begin{itemize}
    \item Hard activation cores: Boolean activation cores, as discussed in \charef{cha:logicalRepresentation}
    \item Soft activation cores: As for the representation of exponential families, as discussed in \charef{cha:probRepresentation}
\end{itemize}
The resulting networks are called hybrid, since they unify logical and probabilitic models.
In \charef{cha:networkRepresentation} we focus on the representation properties of these networks and in \charef{cha:networkReasoning} on reasoning.

In a more technical perspective, Hybrid Logic Networks are those distributions, which are computable with elementary activation tensors (see \figref{fig:elementaryComputableSketch}):
\begin{itemize}
    \item Markov Logic Network: Demanding positive and elementary activation tensors.
    \item Hard Logic Network: Demanding boolean activation tensors.
    \item Hybrid Logic Networks: Unifying both by only demanding elementary activation tensors.
\end{itemize}

\begin{figure}[h]
    \begin{center}
        \input{PartII/tikz_pics/pII_intro/elementary_computable_sketch.tex}
    \end{center}
    \caption{Sketch of distributions with sufficient statistics by a set of propositional formulas $\formulaset$.}
    \label{fig:elementaryComputableSketch}
\end{figure}


\sect{Neuro-symbolic Artificial Intelligence}

We introduce in \charef{cha:formulaSelection} parametrizations of logical formulas following neural decompositions.
These will be central in the efficient representation and reasoning of hybrid logic networks.

Hybrid Logic Networks as neuro-symbolic architectures:
\begin{itemize}
    \item \textbf{Neural Paradigm} here by decompositions of logical formulas into their connectives.
    In more generality by decompositions of sufficient statistics into composed functions, using Basis Calculus.
    Deeper nodes as carrying correlations of lower nodes.
    \item \textbf{Symbolic Paradigm} by interpretability of propositional logics.
\end{itemize}


Hybrid Logic Networks as trainable Machine Learning models:
\begin{itemize}
    \item Expressivity: Can represent any positive distribution, as shown by Theorem~\ref{the:maximalClausesRepresentation}, with $2^d$ formulas.
    \item Efficiency: Can only handle small subsets of possible formulas, since their possible number is huge.
    Tensor networks provide means to efficiently represent formulas depending on many variables and reason based on contractions.
    \item Differentiability: Distributions are differentiable functions of their weights, see Parameter Estimation Chapter.
    The log-likelihood of data is therefore also differentiable function of their weights and we can exploit first-order methods in their optimization.
    \item Structure Learning: We need to find differentiable parametrizations of logical formulas respecting a chosen architecture.
    In \charef{cha:formulaSelection} such representations are described based on Selector Tensor Networks.
\end{itemize}
Differentiability and structure learning will be investigated in more detail in the next chapter.

\red{When understanding atoms as observed variables, and the computed as hidden, Hybrid Logic Networks are deep higher-order boltzmann machines:
More generic correlations can be captured by a logical connective, calculated by a basis encoding and activated by an activation core.}


\sect{Extensions towards First-Order Logics}

While so-far the focus had been on propositional logic as an explainable framework in machine learning, we show in \charef{cha:folModels} extenstions towards more expressive first-order logics.
We therein encounter two mechanisms in logic introducing tensor structures:
\begin{itemize}
    \item \textbf{Substitution mechanisms:} Assigning objects to variables results in a vector of possible substitutions. When there are formulas with multiple variables, these lists of substitutions carry a tensor structure.
    \item \textbf{World mechanism:} Mapping all possible worlds defined by $\atomorder$ atoms requires $2^{\atomorder}$ dimensions, which are at best represented by a space of order $\atomorder$ tensors.
\end{itemize}

\sect{Probabilistic guarantees}

Besides that, probabilistic guarantees on the success of the learning problems are derived in \charef{cha:concentration}.