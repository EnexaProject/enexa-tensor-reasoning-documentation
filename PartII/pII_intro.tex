\chapter{Introduction into \parref{par:two}}

After having explored the tensor approach in the logical and probabilistic foundations of artificial intelligence in \parref{par:one}, we now investigate hybrid logic networks, which unify the two approaches.

\sect{Computation Activation Networks}

%We unify the discussion on exponential families and propositional formulas, by constructing sufficient statistics and base measures based on propositional formulas.
%This restricts the sufficient statistics to boolean valued functions, which can be decomposed into connectives.

% Computation Activation Networks in part \parref{par:one}:
The unification is along the framework of Computation Activation Networks (see \defref{def:realizableStatDistributions}), which is common to the tensor network representations in probabilistic and logical models:
\begin{itemize}
    \item Probability distributions, which have a sufficient statistics can be represented by a computation network of the statistic and an activation tensor.
    We are especially interested in distributions with elementary activation tensor, which is the case for exponential families.
    \item Propositional formulas can be computed based on their syntactical decompositions (see \charef{cha:logicalRepresentation}).
    The uniform distributions over their models has the formula itself as a sufficient statistic and can be instantiated by a boolean activation cores.
\end{itemize}
By allowing for arbitrary elementary activation tensors, we can unify both approaches, when focusing on boolean valued statistics.
The resulting computation activation networks are called hybrid logic networks, since they unify logical and probabilistic models.
In \charef{cha:networkRepresentation} we focus on the representation of such networks and introduce the following (see \figref{fig:elementaryComputableSketch}):
\begin{itemize}
    \item \textbf{Markov Logic Network:} Demanding positive and elementary activation tensors.
    \item \textbf{Hard Logic Network:} Demanding boolean activation tensors.
    \item \textbf{Hybrid Logic Networks:} Unifying both by admitting generic elementary activation tensors.
\end{itemize}
In \charef{cha:networkReasoning} we then focus on the inference properties of such networks.
We further characterize them by maximum entropy distributions.

Regarding the computation network of hybrid logic networks, we investigate sparsity mechanisms to result in efficient tensor network representations:
\begin{itemize}
	\item \textbf{Decomposition sparsity:} The sufficient statistics is decomposable into logical connectives.
	If the formulas are sparse (in the sense of limited number of connectives necessary in their representation), this gives rise to efficient tensor network decompositions of the basis encoding.
    As a result, the features can be efficiently computed based on tensor networks.
    \item \textbf{Structure sparsity:} To capture sparse mechanisms we define formula selecting networks in \charef{cha:formulaSelection}.
    This allows for sparse representation of weighted sums of formulas, which appear in energy tensors.
\end{itemize}

\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/pII_intro/elementary_computable_sketch.tex}
    \end{center}
    \caption{Sketch of distributions with sufficient statistics by a set of propositional formulas $\mlnstat$.
    In most generality, any distribution with a sufficient statistics can be represented by an arbitrary activation tensor and is therefore in $\realizabledistsof{\mlnstat,\maxgraph}$.
    The Hybrid Logic Networks are those with elementary activation tensor, that is the elements of $\realizabledistsof{\mlnstat,\elgraph}$.
    By further demanding positive activation tensors we characterize the Markov Logic Networks $\mlnstat$ and by demanding boolean activation tensors we the Hard Logic Networks $\mlnstat$.
    }
    \label{fig:elementaryComputableSketch}
\end{figure}

\sect{Extensions towards First-Order Logics}

While so-far the focus had been on propositional logic as an explainable framework in machine learning, we show in \charef{cha:folModels} extenstions towards more expressive first-order logics.
We therein encounter two mechanisms in logic introducing tensor structures:
\begin{itemize}
    \item \textbf{Substitution structure:} Assigning objects to variables results in a vector of possible substitutions. When there are formulas with multiple variables, these lists of substitutions carry a tensor structure.
    \item \textbf{World structure:} Mapping all possible worlds defined by $\atomorder$ atoms requires $2^{\atomorder}$ dimensions, which are at best represented by a space of order $\atomorder$ tensors.
    This is analogous to the factored representations of a system.
\end{itemize}

\sect{Probabilistic guarantees}

Besides that, probabilistic guarantees on the success of the learning problems are derived in \charef{cha:concentration}.
Here we also focus on boolean statistics, which coordinates are Bernoulli variables.
Due to their boundedness, they and their averages are sub-Gaussian variables with favorable concentration properties.