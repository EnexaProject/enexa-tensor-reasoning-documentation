\chapter{Introduction into \parref{par:two}}

After having explored tensor approach to the classical logical and probabilistic reasoning in Part I, we now turn to neuro-symbolic applications of tensor networks in artificial intelligence.

We start in \charef{cha:formulaSelection} with the design of explainable and efficient learning architectures, which we frame formula selecting networks.
These architectures will be utilized in the representation and reasoning on graphical models in \charef{cha:networkRepresentation} and \charef{cha:networkReasoning}.

\sect{Neuro-symbolic Learning Architectures}

We introduce in \charef{cha:formulaSelection} parametrizations of logical formulas following neural decompositions.
These will be central in the efficient representation and reasoning of hybrid logic networks.

\sect{Hybrid Logical Networks}

We unify the discussion on exponential families and propositional formulas, by constructing sufficient statistics and base measures based on propositional formulas.
This restricts the sufficient statistics to boolean valued functions, which can be decomposed into connectives.
This restriction brings the following advantages:
\begin{itemize}
	\item \textbf{Computation Advantage:} The sufficient statistics is decomposable into logical connectives.
	If the formulas are sparse (in the sense of limited number of connectives necessary in their representation), this gives rise to efficient tensor network decompositions of the basis encoding.
    As a result, the features can be efficiently computed based on tensor networks.
    \item \textbf{Statistical Advantage:} Since each formula is Boolean valued, the coordinates of the sufficient statistic are Bernoulli variables.
	Due to their boundedness, they and their averages (by Hoeffdings inequality) are sub-Gaussian variables with favorable concentration properties (absence of heavy tails).
\end{itemize}

We combine graphical models with logical formulas, by combining
\begin{itemize}
    \item Hard activation cores: Boolean activation cores, as discussed in \charef{cha:logicalRepresentation}
    \item Soft activation cores: As for the representation of exponential families, as discussed in \charef{cha:probRepresentation}
\end{itemize}
The resulting networks are called hybrid, since they unify logical and probabilitic models.
In \charef{cha:networkRepresentation} we focus on the representation properties of these networks and in \charef{cha:networkReasoning} on reasoning.



\sect{Extensions towards First-Order Logics}

While so-far the focus had been on propositional logic as an explainable framework in machine learning, we show in \charef{cha:folModels} extenstions towards more expressive first-order logics.
We therein encounter two mechanisms in logic introducing tensor structures:
\begin{itemize}
    \item \textbf{Substitution mechanisms:} Assigning objects to variables results in a vector of possible substitutions. When there are formulas with multiple variables, these lists of substitutions carry a tensor structure.
    \item \textbf{World mechanism:} Mapping all possible worlds defined by $\atomorder$ atoms requires $2^{\atomorder}$ dimensions, which are at best represented by a space of order $\atomorder$ tensors.
\end{itemize}

\sect{Probabilistic guarantees}

Besides that, probabilistic guarantees on the success of the learning problems are derived in \charef{cha:concentration}.