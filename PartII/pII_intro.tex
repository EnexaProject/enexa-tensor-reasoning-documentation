\chapter{Introduction into Part II}

After having explored tensor approach to the classical logical and probabilistic reasoning in Part I, we now turn to neuro-symbolic applications of tensor networks in artificial intelligence.

We start in \charef{cha:formulaSelection} with the design of explainable and efficient learning architectures, which we frame formula selecting networks.
These architectures will be utilized in the representation and reasoning on graphical models in \charef{cha:networkRepresentation} and \charef{cha:networkReasoning}.

While the focus is on propositional logic as an explainable framework in machine learning, we show extenstions towards more expressive first-order logics in \charef{cha:folModels}.

Besides that, probabilistic guarantees on the success of the learning problems are derived in \charef{cha:concentration}.