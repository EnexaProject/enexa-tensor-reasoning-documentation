\subsection{Constrained parameter estimation in the minterm family}

% Naive exponential family
We approach structure learning as constrained parameter estimation in the naive exponential family (see Example~\ref{exa:naiveExpFamily}), which coincides with the minterm family $\formulasetof{\mlnmintermsymbol}$.
The minterm family is defined by the statistic $\sstat = \identityat{\shortcatvariables, \selvariableof{[\catorder]}}$ and has energy tensors coinciding with the canonical parameters.

% Convex polytope characterization
\red{For the minterm family, we have as mean parameter set the convex hull of one-hot encodings. 
Each basis vector is an extreme point is an extreme point.
}


By Theorem~\ref{the:mintermExpressivityMLN} all positive distributions are member of the minterm markov logic network family.
This expressivity result was generalized to arbitrary distributions, when allowing for formulas as basemeasures by Theorem~\ref{the:mintermExpressivityHLN}.

Finding the distribution maximizing the likelihood of data would then be the empirical distribution.
In this case we would have $\datameanat{\selvariableof{[\catorder]}=\shortcatindices} = \empdistributionat{\shortcatvariables=\shortcatindices}$ and the maximum likelihood distribution is found by the problem
\begin{align*}
	\argmax_{\canparam\in\facspace}  \sbcontraction{\canparam,\empdistribution} - \cumfunctionof{\canparam} \, 
\end{align*}
which is solved at $\canparam=\lnof{\empdistribution}$ with $\probtensorof{(\identity,\lnof{\empdistribution})}= \empdistribution$.
This follows from $\lossof{\probtensorof{(\identity,\canparam)}}=\kldivof{\empdistribution}{\probtensorof{(\identity,\canparam)}}$, which is by Gibbs inequality minimized at $\probtensorof{(\identity,\canparam)}=\empdistribution$, which is the case for $\canparam = \lnof{\empdistribution}$.

We here allow for $\lnof{0}=-\infty$, with the convention of $\expof{-\infty}=0$, to handle datasets where specific worlds are not represented. 
\red{Better: Use Theorem~\ref{the:mintermExpressivityHLN} with basemeasure dropping non appearing data.}


% Regularization
To avoid this overfitting situation, we regularize by restricting the parameter to be a set $\energyhypothesis\subset\facspace$ and state
\begin{align}\tag{$\mathrm{P}_{\energyhypothesis, \empdistribution}$}\label{prob:restrictedNaiveMLE}
	\argmax_{\canparam\in\energyhypothesis}  \sbcontraction{\canparam,\empdistribution} - \cumfunctionof{\canparam} \, . 
\end{align}

Problem~\ref{prob:restricedNaiveMLE} has two important types of instantiation, which we discuss in the next sections.

\subsubsection{Parameter Estimation}

% Parameter Estimation
\red{Projecting onto the markov logic family to the statistic $\formulaset$ is the instance of Problem~\ref{prob:restricedNaiveMLE} with the hypothesis choice}
%When the $\formulaset$ is known we take $\energyhypothesis$ as the linear hull 
	\[ \energyhypothesisof{\formulaset} = \spanof{\{\formula : \formula\in\formulaset \}} \, . \]
Then, the problem is the parameter estimation problem studied in Section~\ref{sec:parameterEstimation}.
To see this, we reparametrize by the coefficient vectors of the elements in the span, which are then understood as the canonical parameter of the respective distribution in the markov logic family to $\formulaset$.


\begin{remark}[Overparametrization]
	Taking $\formulaset$ to consist of all propositional formulas, we get a massive overparametrization: 
	The essential statistics maps to a $2^{\left(2^\atomorder \right)}$ dimensional real vector space.
	All possible distributions of the $\atomorder$ atomic variables are mapped to an $2^\atomorder-1$ dimensional submanifold, where also the essential statistics maps to.

	Thus, to identify probabilistic knowledge bases, we need to drastically restrict the shape of formulas allowed.
	It is in principle impossible to decide which formulas to be activated, based only on statistics and not on prior assumptions.

	%The nodes of a Markov Propositional Network are all formulas in a propositional theory and the hyperedges all possible decompositons.
	When having $\atomorder$ atoms, there are $2^{\atomorder}$ states in the factored system.
	Since each state can either be a model of a formula or not, there are
		\[ \cardof{\formulaset} = 2^{\big(2^\atomorder \big)} \]
	formulas.
	Having, for example, $\atomorder=10$, then $\cardof{\formulaset}>10^{308}$.


	% Regularization by sparsity
	One regularization is by allowing only a small number of formulas to be active.
	This corresponds with regularization with $\sparsityof{\canparam}$.
	The problem is then non-convex.


	% Regularization by formula size
	A further regularization strategy is the restriction of the size of the possible formulas to maintain interpretability. 
	Thus, we choose small formula selection networks.
\end{remark}




\subsubsection{Structure Learning}

% Structure Learning
The problem of structure learning arises, when the set of parameters in Problem~\ref{prob:restricedNaiveMLE} is choosen as 
	\[ \energyhypothesisof{\formulasuperset}= \bigcup_{\formulaset\in\formulasuperset} \spanof{\formulaset} \, .  \] %\energyhypothesisof{\formulaset}\, . 
In this case, the problem in general fails to be convex.

% Subspace instuition
Each formula set $\formulaset$ represents a subspace in the parameters of the minterm family, which is spanned by the propositional formulas $\exformula\in\formulaset$.

%\red{Intuition by subspaces in the minterm parameters, which are selected by a nonlinear objective, to distinguish from compressed sensing.}







\subsection{Greedy Structure Learning}


%Motivation 
It can be impracticle to learn all formulas at once, since the set $\formulasuperset$ often grows combinatorically, for example when choosing as a powerset of formulas.
\red{Further, we need to avoid overfitting and carefully choose a hypothesis.}
To avoid intractabilities and overfitting, one can choose a greedy approach and learn in addition formulas $\exformula$ when already having learned a set $\formulaset$ of formulas.
We in this section assume a current model $\currentdistribution$, which is a generic positive distribution not necessarily a Markov Logic Network. % or Hybrid Logic Network.

% 
We will use the effective selection tensor network representation of exponentially many formulas described in Chapter~\ref{cha:formulaBatches} and select from them a small subset.

%\red{Alternative discussion: Can use current distribution as base measure and apply moment matching as first order condition.}


\subsubsection{Greedy formula inclusions}

Having a current set of formulas $\formulaset$ we want to choose the best $\formula\in\fselectionmap$ to extend the set of formulas to $\formulaset\cup\{\formula\}$ in a way minimizing the cross entropy.
Given this, add each step we solve the greedy cross entropy minimization
\begin{align}\label{prob:perfectGreedy}\tag{$\mathrm{P}_{\datamap,\formulaset,\fselectionmap}$}
	\argmin_{\formula\in\fselectionmap} \argmin_{\canparam\in\rr^{\cardof{\formulaset}+1}} 
	\centropyof{\empdistribution}{\expdistof{(\formulaset\cup\{\formula\},\canparam,\basemeasure)}} \, . 
\end{align}


A brute force solution would require parameter estimation for each candidate in $\fselectionmap$.
We provide two more efficient approximative heuristics in the following (see Chapter~20 in \cite{koller_probabilistic_2009}).



\subsubsection{Gradient heuristic and the proposal distribution}

\red{Advantage: Might avoid formulawise calculus, when sampling from proposal distribution. 
Brute force solution of gain heuristic require formulawise approach.}

We now derive a heuristic of choosing features based on the maximal coordinate of the gradient when differentiating the canonical parameter in the minterm family.
To prepare for this, we build the gradient of the loss
%For the naive exponential family 
\begin{align*}
	\lossof{\expdistof{(\naivestat, \naivecanparam)}} 
	%= \frac{1}{\datanum} \sum_{\dataindexin}\lnof{\expdistofat{(\naivestat, \naivecanparam)}{\shortcatvariables=\datamapof{\dataindex}}}
	= \contraction{\empdistribution, \sencodingof{\naivestat}, \naivecanparam} - \lnof{\contraction{\expof{\contractionof{\sencodingof{\naivestat}, \naivecanparam}{\shortcatvariables}}}} 
\end{align*}
as
\begin{align*}
	\gradwrt{\naivecanparamat{\selvariable}} \lossof{\expdistof{(\naivestat, \naivecanparam)}}
	&= \contractionof{\sencodingof{\naivestat},\empdistribution}{\selvariable} - \contractionof{\sencodingof{\naivestat},\expdistof{(\naivestat, \naivecanparam)}}{\selvariable} \\
	&= \empdistribution - \expdistof{(\naivestat, \naivecanparam)} \, . 
\end{align*}

%% Single feature
%Given a feature $\exfunction[\shortcatvariables]$ we vary the naive parameters by a function on $\canparam\in\rr$ by
%\begin{align*}
%	 \naivestat(\canparam) %=  \mlntensor + \weight_{\parindices} \ftensorof{\exformula_{\parindices}}
%	= \naivestat(0) + \canparam\cdot\exfunction
%\end{align*}
%and get a likelihood gradient of
%\begin{align*}
%	 \frac{\partial \lossof{\expdistof{(\naivestat(\canparam), \naivecanparam)}}}{\partial\canparam} 
%	 &= \sbcontraction{
%	 	\frac{\partial\lossof{\expdistof{(\naivestat, \naivecanparam)}}}{\partial\naivecanparam}|_{\naivecanparam(0)},
%		\frac{\partial\naivecanparam(\canparam)}{\partial\canparam} 
%	 }  \\
%	 &= \contraction{\empdistribution,\exfunction} -   \contraction{\expdistof{(\naivestat, \naivecanparam)},\exfunction} \, .
%\end{align*}


%% Positive and Negative Search
The gradient shows the typical decomposition into a positive and a negative phase.
While the positive phase comes from the data term and prefers directions of large data support, the negative phase originates in the partition function and draws the gradient away from directions already supported by the current model $\expdistof{(\naivestat, \naivecanparam)}$.
%% Regularization functionality
The negative phase is a regularization, by comparing with what has already been learned.
When nothing has been learned so far, we can take the current model to be the uniform distribution, which is the naive exponential family with vanishing canonical parameters. 



%% Collection of features by selection
Given a set $\fselectionmap$ of features we vary $\naivecanparam$ by the function
\begin{align*}
	 \exfunction(\canparam) = \naivecanparam + \sbcontractionof{\canparam,\sencodingof{\fselectionmap}}{\shortcatvariables} \, . 
\end{align*}
At $\canparam=0$ we have the gradient of the loss of the parametrized formula by
\begin{align*}
	 \gradwrtat{\canparam}{0} 
	 \lossof{\expdistof{(\naivestat,\exfunction(\canparam),\basemeasure)}}
	 &= \sbcontraction{
	 	 \gradwrtat{\exfunction(\canparam)}{\naivecanparam}  \lossof{\expdistof{(\naivestat,\exfunction(\canparam),\basemeasure)}},
		 \gradwrtat{\canparam}{0}  \exfunction(\canparam)
	 }  \\
	 &= \sbcontractionof{\empdistribution,\sencodingof{\sstat}}{\selvariable} -   \sbcontractionof{\expdistof{(\naivestat, \naivecanparam, \basemeasure)},\sencodingof{\sstat}}{\selvariable} \, . 
\end{align*}


%% Grafting
We want to choose the formula, which is best aligned with the gradient of the log-likelihood, that is using a formula selecting map $\fselectionmap$
\begin{align} \label{prob:greedyGrad} \tag{$\mathrm{P}^{\mathrm{grad}}_{\datamap,\formulaset,\fselectionmap}$} 
	\argmax_{\selindex\in[\seldim]} \sbcontractionof{\empdistribution,\fselectionmap}{\indexedselvariable} 
	- \sbcontractionof{\expdistof{(\naivestat, \naivecanparam, \basemeasure)},\fselectionmap}{\indexedselvariable} \, . 
\end{align}
This method is known as the gradient heuristic or grafting.
% Mean parameter interpretation
The objective of Problem~\eqref{prob:greedyGrad} has another interpretation by the difference of the mean parameter $\datamean$ and $\currentmean$ of the projections of the empirical and current distributions on the family to $\fselectionmap$. % ! NOT the proposal family, those have transposed statistic

%% Formula alignment perspective
Problem~\eqref{prob:greedyGrad} is further equivalent to the formula alignment
\begin{align*}
	\argmax_{\formula\in\fselectionmap} \sbcontraction{\formula,\empdistribution-\currentdistribution} \, . 
\end{align*}
The objective can be interpreted as the difference of the satisfaction probability of the formula with respect to the empirical distribution and the current distribution.
%We can choose selection architectures to efficiently parametrize the formulas in the hypothesis $\fselectionmap$ and rewrite the problem as
%\begin{align*}
%	\argmax_{\selindexin} \contractionof{ \gradwrtat{\canparam}{\canparam=0} \lossof{\expdist}}{\indexedselvariable}
%\end{align*}
%This is thus equivalent to the problem \ref{prob:greedyGrad}, when taking all formulas selectable by $\formulaset$ as the hypothesis $\Gamma$.


%\subsection{Proposal distribution}

% Proposal distribution
Let us now understand the likelihood gradient as the energy tensor of a probability distribution, which we call the proposal distribution.

\begin{definition}[Proposal Distribution]
	Let there be a base distribution $\currentdistribution$, a targeted distribution $\empdistribution$ and a formula selecting map $\fselectionmap[\shortcatvariables, \selvariable]$.
	The proposal distribution at inverse temperature $\invtemp>0$ is the distribution of $\selvariable$ defined by
	\begin{align*}
		\normationof{\expof{\sbcontractionof{\invtemp\cdot(\empdistribution-\currentdistribution),\fselectionmap}{\selvariable}} }{\selvariable} \, . 
	\end{align*}
	The proposal distribution is the member of the exponential family with statistics $\fselectionmap$ and parameter $\invtemp\cdot(\empdistribution-\currentdistribution)$.
\end{definition}


%. Exponential family
The proposal distribution is in the exponential family with sufficient statistic by the formula selecting map $\fselectionmap$, namely the member with the canonical parameters $\canparam=\empdistribution-\currentdistribution$.
Of further interest are tempered proposal distributions, which are in the same exponential family with canonical parameters $\invtemp\cdot(\empdistribution-\currentdistribution)$ where $\invtemp>0$ is the inverse temperature parameter.

% MLN
As Markov Logic Networks, the proposal distributions are in exponential families with the sufficient statistic defined in terms of formula selecting maps.
While Markov Logic Networks contract the maps on the selection variables $\selvariable$, the proposal distributions contract them along the categorical variables $\catvariable$ to define energy tensors.

% Methods to solve mode search
The grafting Problem~\eqref{prob:greedyGrad} is the search for the mode of the proposal distribution.
To solve grafting, we thus need to answer a MAP query, for which we can apply the methods introduced in Chapter~\ref{cha:probReasoning}, such as Gibbs Sampling or Mean Field Approximations in combination with annealing.





\subsubsection{Gain Heuristic}

In the gain heuristic, only the parameters of the new formula are optimized and the others left unchanged.
This amounts to 
\begin{align}\label{prob:greedyGain}\tag{$\mathrm{P}^{\mathrm{gain}}_{\datamap,\formulaset,\fselectionmap}$}
	\argmin_{\formula\in\fselectionmap} \left ( \min_{\canparamat{\cardof{\formulaset}}\in\rr} 
	\centropyof{\empdistribution}{\expdistof{(\formulaset\cup\{\formula\},\canparam,\basemeasure)}} \right) \, . 
\end{align}
Here we denote by $\canparam$ the first $\cardof{\formulaset}$ coordinates of the M-projection $\currentdistribution$  of $\empdistribution$ onto $\formulaset$ and the variable new coordinate at position $\canparamat{\cardof{\formulaset}}$.

\begin{lemma}
	The gain heuristic objective is an upper bound on the true greedy objective. 
\end{lemma}
\begin{proof}
Since
\begin{align*}
	\argmin_{\formula\in\fselectionmap} \left( \argmin_{\canparam\in\rr^{\cardof{\formulaset}+1}} 
	\centropyof{\empdistribution}{\expdistof{(\formulaset\cup\{\formula\},\canparam,\basemeasure)}} \right)
	\leq 	\argmin_{\formula\in\fselectionmap} \left ( \argmin_{\canparamat{\cardof{\formulaset}}\in\rr} 
	\centropyof{\empdistribution}{\expdistof{(\formulaset\cup\{\formula\},\canparam,\basemeasure)}} \right) \, . 
\end{align*}
\end{proof}


% Minterm family interpretation
Further, this is Problem~\eqref{prob:restrictedNaiveMLE} in the case
\begin{align*}
	\energyhypothesis = \lnof{\currentdistribution} + \cup_{\formula\in\formulaset} \spanof{\formula} \, .
\end{align*}



% For single formula
Let us choose a formula $\formula\in\formulaset$ and consider Problem~\ref{prob:restrictedNaiveMLE}  in the case
\begin{align*}
	\energyhypothesisof{\formula} = \lnof{\currentdistribution} + \spanof{\formula} \, . 
\end{align*}
This is parameter estimation on the exponential family with the single feature $\formula$ and the base measure $\currentdistribution$.
Therefore we can apply the theory of Chapter~\ref{cha:probReasoning} and characterize the solution by the $\weight$ satisfying the moment matching condition
\begin{align*}
	\contraction{\currentdistribution, \normationof{\expof{\weight}}{\shortcatvariables} } = \contraction{\empdistribution, \formula} \, . 
\end{align*}
We state the solution of this condition in the next theorem.

\begin{theorem}
	Problem~\eqref{prob:greedyGain} is solved at any
	\begin{align*}
		\hat{\canparam} = \weightof{\hat{\formula}} \cdot \hat{\formula}
	\end{align*}
	where the formula $\hat{\formula}$ is in
	\begin{align*}
		\hat{\formula} \in \argmax_{\formula\in\formulaset} \kldivof{\sbcontraction{\empdistribution,\formula}}{\sbcontraction{\currentdistribution,\formula}}
	\end{align*}
	and $\weightof{\hat{\formula}}$ is the weight of $\hat{\formula}$ in the solution of Problem~\ref{prob:restrictedNaiveMLE} with $\Gamma = \currentdistribution + \mathrm{span}(\exformula)$.
	Here we denote by $\kldivof{p_1}{p_2}$ the Kullback-Leibler divergence between Bernoulli distributions with parameters $p_1,p_2\in[0,1]$, that is
		\[ \kldivof{p_1}{p_2} = p_1 \cdot \lnof{\frac{p_1}{p_2}} + (1-p_1) \cdot \lnof{\frac{(1-p_1)}{(1-p_2)}}  \]
\end{theorem}	
\begin{proof}
	% Solution of the problem restricted to 
	For any formula $\formula$, the inner minimum of Problem~\eqref{prob:greedyGain} is by Lemma~\ref{ref:lemMMinMLN} taken at 
		\[ \weightof{\formula} = \lnof{\frac{\datamean}{(1-\datamean)}\cdot \frac{(1-\currentmean)}{\currentmean}}  \]
	where
		\[ \currentmean = \sbcontraction{\currentdistribution,\formula} \]
	and
		\[ \datamean = \sbcontraction{\empdistribution,\formula} \, . \]
	
	The difference of the likelihood at the current distribution and the optimum is
	\begin{align*}
		\centropyof{\empdistribution}{\currentdistribution}
		- \centropyof{\empdistribution}{\expdistof{(\extendedformulaset,\extendedcanparam,\basemeasure)}}
		= \datamean \cdot \weightof{\formula} - \cumfunctionwrtof{\extendedformulaset,\basemeasure}{\extendedcanparam} \, .
	\end{align*}
	
	% Loss gain at optimum
	We use the representation scheme of Theorem~\ref{the:hybridNetworkRepresentation} and get
	\begin{align*}
		\sbcontraction{\currentdistribution, \expof{\weightof{\formula} \cdot \formula}}
		& = \sbcontraction{\currentdistribution, \rencodingofat{\formula}{\catvariableof{\formula}}, \headcoreofat{\formula}{\catvariableof{\formula}}} \\
		& = (1-\currentmean) + \currentmean\cdot \expof{\weightof{\formula}} \\
		& = (1 - \currentmean) + \frac{\datamean \cdot (1-\currentmean)}{(1-\datamean)} \\
		& = (1-\currentmean) \cdot \frac{1}{(1-\datamean)} \, . 
	\end{align*}
	% Refining the cumulant term
	It follows, that
	\begin{align*}
		\cumfunctionwrtof{\extendedformulaset,\basemeasure}{\extendedcanparam}
		& = \lnof{\sbcontraction{\currentdistribution, \expof{\weightof{\formula} \cdot \formula}}} \\
		& = \lnof{1-\currentmean} - \lnof{1-\datamean} \, . 
	\end{align*}
	% Refining the mean product term
	We further have
	\begin{align*}
		\datamean \cdot \weightof{\formula}
		= \datamean \cdot \left[ \lnof{\frac{\datamean}{(1-\datamean)}\cdot \frac{(1-\currentmean)}{\currentmean}}  \right]	
		= \datamean \lnof{\datamean} - \datamean \lnof{1-\datamean} + \datamean \lnof{1-\currentmean} - \datamean \lnof{\currentmean}
	\end{align*}
	and arrive at
	\begin{align*}
		\centropyof{\empdistribution}{\currentdistribution}
		- \centropyof{\empdistribution}{\expdistof{(\exformula,\weightof{\formula},\currentdistribution)}}
		& =  \datamean \lnof{\datamean} - \datamean \lnof{1-\datamean} + \datamean \lnof{1-\currentmean} - \datamean \lnof{\currentmean}
		-  \lnof{1-\currentmean} - \lnof{1-\datamean} \\
		& = \left( -\datamean \lnof{\currentmean} - (1-\datamean) \lnof{1-\currentmean} \right)  - \left( -\datamean \lnof{\datamean} - (1-\datamean) \lnof{1-\datamean} \right) \, . 
	\end{align*}
	By definition, this is the Kullback-Leibler divergence between Bernoulli distributions with parameters $\datamean$ and $\currentmean$.
	%
	Since the gain in the likelihood loss when restricting to $\energyhypothesis = \spanof{\formula}$ is thus given by $\kldivof{\sbcontraction{\empdistribution,\formula}}{\sbcontraction{\currentdistribution,\formula}}$, we have that Problem~\ref{prob:restrictedNaiveCE}  in the case $\energyhypothesis = \bigcup_{\formula\in\formulaset}\spanof{\formula}$ is solved at $\estcanparam = \weightof{\hat{\formula}}\cdot \hat{\formula}$ where
		\[ \hat{\formula} = \kldivof{\sbcontraction{\empdistribution,\formula}}{\sbcontraction{\currentdistribution,\formula}} \, . \]
\end{proof}

\red{Thus, we solve the grain heuristic with a coordinatewise transform of the mean parameter tensors to $\empdistribution$ and $\currentdistribution$, using the Bernoulli Kullback-Leibler divergence as transform function.}


% Interpretation
One therefore takes the formula, which marginal distribution in the current model and the targeted distribution are differing at most, measured in the KL divergence.

% Optimization method
One optimization method would thus be the computation of the mean parameters to both distribution, building the coordinatewise KL divergence and choosing the maximum. 
Since we need to evaluate each coordinate, this can be intractable for large sets of formulas.


% Further weight optimization
Further improvement of the model can be achieved by iteratively optimizing the other weights as well, since their corresponding moment matching conditions might be violated after the integration of a new formula.
This would require the computation of backward mappings for each candidate formula, for which we only have an alternating approach in general.


\subsubsection{Iterations}

Let us now iterate the search for a best formula at a current model with the optimization of weights after each step.
The result is Algorithm~\ref{alg:greedyStructureLearning}, which is a greedy algorithm adding iteratively the currently best feature.

\begin{algorithm}[hbt!]
\caption{Greedy Structure Learning}\label{alg:greedyStructureLearning}
\begin{algorithmic}
	\State Initialize
		\[ \currentdistribution \algdefsymbol \frac{1}{\prod_{\catenumeratorin}\catdimof{\atomenumerator}} \cdot \onesat{\shortcatvariables} \quad, \quad \formulaset = \varnothing \]
	\While{Stopping criterion is not met}
		\State Structure Learning: Compute a (approximative) solution $\hat{\formula}$ to Problem~\ref{prob:restrictedNaiveMLE} and add the formula to $\formulaset$, i.e.
				\[ \formulaset \algdefsymbol \formulaset \cup \{\hat{\formula}\} \]
			Extend dimension of $\selvariable$ by one, by $\formulaof{\seldim}=\hat{\formula}$ and $\canparamat{\seldim}=0$
		\State Weight Estimation: Estimate the best weights for the added formula and recalibrate the weights of the previous formulas, by calling Algorithm~\ref{alg:AWO}.
				\[ \currentdistribution \algdefsymbol \expdistof{\formulaset, \canparam} \]
\EndWhile
\end{algorithmic}
\end{algorithm}



%% Energy Storage -> Useful after learning for energy-based inference
When having used the same learning architecture multiple times, the energy of the corresponding formulas are all representable by a formula selecting architecture.
Their energy term is therefore a contraction of the selecting tensor with a parameter tensor $\canparam$ in a basis CP decomposition with rank by the number of learned formulas.
When mutiple selection architectures have been used, the energy is a sum of such contractions.
% 
Let us note, that this representation is useful after learning, when performing energy-based inference algorithms on the result.
During learning, one needs to instantiate the proposal distribution, which requires instantiation of the probability tensor.
\red{However, one could alternate data energy-based and use this as a particle-based proxy for the probability tensor.}


\begin{remark}[Sparsification by Thresholding]
	To maintain a small set of active formulas, one could combine greedy learning approaches with thresholding on the coordinates of $\canparam$.
	This is a standard procedure in Iterative Hard Thresholding algorithms of Compressed Sensing, but note that here we do not have a linear in $\canparam$ objective.
\end{remark}


\subsection{Discussion}

\begin{remark}[Bayesian approach]
	We only treated the estimation of a single resulting distribution by the data, while in a Bayesian approach one typically considers an uncertainty over possible distributions.
	% MAP
	\red{When treating $\canparam$ as a random tensor, which prior distribution is given and posteriori distribution wanted, we have a more involved Bayesian approach.}
	When having a prior $\probof{\mlnparameters}$ over the Markov Logic Networks we alternatively want to find the parameters $\mlnparameters$ solving the maximum a posteriori problem
	\begin{align}
		\argmax_{\mlnparameters} \mlnprobat{\data}\cdot \probof{\mlnparameters}\, . 
	\end{align}
\end{remark}


