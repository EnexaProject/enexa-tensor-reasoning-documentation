\chapter{\chatextnetworkRepresentation}\label{cha:networkRepresentation}

Logic networks are graphical models with an interpretation by propositional logics.
We first distinguish between Markov Logic Networks, which are an approach to soft logics in the framework of exponential families, and Hard Logic Networks, which correspond with propositional knowledge bases.
Then we exploit non-trivial boolean base measures to unify both approaches by Hybrid Logic Networks, which are itself in exponential families.




%Markov Logic Networks are probability functions of truth assignments to logical functions.
%They respect propositional logic as hard constraints, but have beyond that freedom to shape probability distributions on possible situations.
%To capture these properties, we define them as graphical models with structure cores representing propositional logics and activation cores representing the specification of probability distributions.
% We in this part employ them to combine the probabilistic and the logical paradigm.


\sect{Markov Logic Networks}

Markov Logic Networks exploit the efficiency and interpretability of logical calculus as well as the expressivity of graphical models. 

\subsect{Markov Logic Networks as Exponential Families}

We introduce Markov Logic Networks in the formalism of exponential families (see \secref{sec:exponentialFamilies}).

\begin{definition}[Markov Logic Networks]
	Markov Logic Networks are exponential families $\mlnexpfamily$ with sufficient statistics by functions
		\[ \mlnstat : \atomstates \rightarrow \bigtimes_{\exformulain}[2] \subset \rr^{\cardof{\formulaset}} \]
	defined coordinatewise by propositional formulas $\exformulain$.
\end{definition}

% Binary Statistics as propositional formula


% Characterization of MLNs among exponential families: When choosing binary features
Since the image of each coordinate $\sstatcoordinateof{\selindex}$ is contained in $\ozset$, each is a propositional formulas (see \defref{def:formulas}).
%Conversely, any boolean feature $\sstatcoordinateof{\selindex}$ of an exponential family defines a propositional formula (see \defref{def:formulas}).
Thus, any exponential family of distributions of $\atomstates$, such that $\imageof{\sstatcoordinateof{\selindex}}\subset\ozset$ for all $\selindexin$ is a set of Markov Logic Networks with fixed formulas.

The sufficient statistics consistent in a map $\formulaset$ of formulas brings the following advantages:
\begin{itemize}
	\item Numerical Advantage: The sufficient statistics is decomposable into logical connectives. 
	If the formulas are sparse (in the sense of limited number of connectives necessary in their representation), this gives rise to efficient tensor network decompositions of the relational encoding.
	\item Statistical Advantage: Since each formula is Boolean valued, the coordinates of the sufficient statistic are Bernoulli variables. 
	Due to their boundedness, they and their averages (by Hoeffdings inequality) are sub-Gaussian variables with favorable concentration properties (absence of heavy tails).
\end{itemize}

\begin{remark}[Alternative Definitions]
	We here defined MLNs on propositional logic, while originally they are defined in FOL.
	The relation of both frameworks will be discussed further in \charef{cha:folModels}.
\end{remark}



\subsect{Tensor Network Representation}

Based on the previous discussion on the representation of exponential families by tensor networks in \secref{sec:exponentialFamilies} we now derive a representation for Markov Logic Networks.

\subsubsect{Relational encodings for distributions}

\begin{theorem}[Relational Encodings for Markov Logic Networks]\label{the:mlnTensorRep}
	A Markov Logic Network to a set of formulas $\formulaset = \{\enumformula \, : \, \selindexin\}$ is represented as
	\begin{align*}
		\mlnprobat{\shortcatvariables} = 
		\normationof{\{\enumformulaccwith : \selindexin \} \cup \{\enumformulaacwith : \selindexin \}
		}{\shortcatvariables}
	\end{align*}
	where we denote for each $\selindexin$ an activation core
	\begin{align*}
		\enumformulaac\left[\indexedheadvariableof{\selindex}\right]
		= \begin{cases}
			1 & \text{for} \quad \headindexof{\selindex} = 0 \\
			\expof{\canparamat{\indexedselvariable}} & \text{for} \quad \headindexof{\selindex}  = 1
		\end{cases}  \, .
	\end{align*}
%	\begin{align*}
%		\enumformulaacwith
%		= \begin{bmatrix} 1 \\
%		 \expof{\canparamat{\indexedselvariable}}
%		 \end{bmatrix}[\enumformulavar] \, .
%	\end{align*}
\end{theorem}
\begin{proof}
	Markov Logic Networks are exponential families, which base measure is trivial and which statistic consist of boolean features.
	We apply the tensor network decomposition of more generic exponential families \theref{the:expFamilyTensorRep} to this case and get
	\begin{align*}
        \mlnprobat{\shortcatvariables} =
        \normationof{\{\onesat{\shortcatvariables}\}
		\cup \{\rencodingofat{\sstatcoordinateof{\selindex}}{\sstatcatof{\selindex},\shortcatvariables} \, : \, \selindexin\}
		\cup\{\enumformulaacwith\, : \, \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
	While the base measure tensor is trivial, it can be ignored in the contraction.
	Since the image of each feature $\enumformula$ is in $[2]$, we choose the index interpretation function by the identity $\indexinterpretation : [2] \rightarrow \ozset$ and get
%	We further choose the standard index interpretation for booleans (see \secref{sec:booleanEncoding}) and get
	\begin{align*}
		\enumformulaac\left[\indexedheadvariableof{\selindex}\right]
		&= \expof{\canparamat{\indexedselvariable} \cdot \indexinterpretationofat{\selindex}{\headindexof{\selindex}} }
		= \expof{\canparamat{\indexedselvariable} \cdot \headindexof{\selindex}} \\
		&= \begin{cases}
			1 & \text{for} \quad \headindexof{\selindex} = 0 \\
			\expof{\canparamat{\indexedselvariable}} & \text{for} \quad \headindexof{\selindex}  = 1
		\end{cases}
	\end{align*}
\end{proof}

\begin{figure}[t!]
\begin{center}
	\input{PartII/tikz_pics/network_representation/factor.tex}
\end{center}
\caption{Factor of a Markov Logic Network to a formula $\enumformula$, represented as the contraction of a computation core $\enumformulacc$ and an activation core $\enumformulaac$.
	While the computation core $\enumformulacc$ prepares based on basis calculus a categorical variable representing the value of the statistic formula $\enumformula$ dependent on assignments to the distributed variables, the activation core multiplies an exponential weight to coordinates satisfying the formula.
}
\label{fig:mlnFactor}
\end{figure}

% Graphical model representation
\theref{the:mlnTensorRep} provides a decomposition of markov logic networks by a tensor network of computation cores $\rencodingof{\enumformula}$ and accompanying activation cores $\enumformulaac$.
Since the head variable $\headvariableof{\selindex}$ appears exclusively in these pairs, we can contract each computation core with the corresponding activation core to get a factor, see \figref{fig:mlnFactor}.
With this we get the decomposition
\begin{align*}
	\mlnprobat{\shortcatvariables}
	= \normationof{\{\expof{\canparamat{\indexedselvariable}\cdot\enumformulaat{\shortcatvariables}} \, : \, \selindexin\}}{\shortcatvariables} \, .
\end{align*}
More precisely, this transformation of the decomposition holds by \theref{the:splittingContractions} to be shown in \charef{cha:messagePassing}, stating that the contraction of computation and activation cores can be performed before the global contraction of the result. 

% Sparsification by trivial variables
While in the decomposition of \theref{the:mlnTensorRep} the relational encodings of the features carry all distributed variables $\shortcatvariables$, we now seek towards sparser decompositions.
To each $\selindexin$ we denote by $\nodesof{\selindex}$ the maximal subset of $[\catorder]$ such that there is a reduced function
$\tilde{\formula}_{\selindex} : \bigtimes_{} \rightarrow [2]$
with
\begin{align*}
	\enumformulaat{\shortcatvariables}
	= \contractionof{\tilde{\formula}_{\selindex}[\catvariableof{\nodesof{\selindex}}]}{\shortcatvariables} \, .
\end{align*}
We often account for such situations of sparse formulas, when $\enumformula$ has a syntactical decomposition involving only the atomic variables $\nodesof{\selindex}$.
As a consequence we have
\begin{align*}
	\rencodingofat{\enumformula}{\shortcatvariables}
	= \rencodingofat{\tilde{\formula}_{\selindex}}{\catvariableof{\nodesof{\selindex}}}
\end{align*}
and
\begin{align*}
	\mlnprobat{\shortcatvariables}
	= \normationof{\{\expof{\canparamat{\indexedselvariable}\cdot\tilde{\formula}_{\selindex}[\catvariableof{\nodesof{\selindex}}]} \, : \, \selindexin\}}{\shortcatvariables} \, .
\end{align*}
Thus, any markov logic network has a sparse representation by a markov network on the graph
\begin{align*}
	\graphof{\formulaset} = ([\catorder],\{\nodesof{\selindex} \, : \, \selindexin\}) \, .
\end{align*}
This sparsity inducing mechanism is analogous to the decomposition of probability distributions based on conditional independence assumptions, when understanding each formula in the markov logic network as an introduced dependency among the affected variables $\nodesof{\selindex}$.


\begin{figure}[t]
\begin{center}
	\input{PartII/tikz_pics/network_representation/decomposed_representation.tex}
\end{center}
\caption{Example of a decomposed Markov Network representation of a Markov Logic Network with formulas $\{\formulaof{0} = a\lor b, \formulaof{1} = a \lor b \lor \lnot c\}$.
	Since both formulas share the subformula $a\lor b$, their contracted factors have a representation by a connected tensor network.}
% Where $\actcoreofat{\enumformula}{\enumformulavar} =\begin{bmatrix} 1 & \expof{\weightof{\exformula}} \end{bmatrix}[\formulavar] $}
\label{fig:mlnDecRep}
\end{figure}


% Sparsity by decomposition
A further sparsity introducing mechanism is through exploiting redundancy in the computation of $\enumformula$, when a decomposition of the feature is known.
For the propositional formulas $\enumformula$ this amounts to a syntactic representation of the formula as a composition of logical connectives is available (see \figref{fig:mlnDecRep}). % Make a precise definition in logicRepresentation and link it!
In this case, we exploit the representation by tensor networks of the relational encodings (shown as \theref{the:compositionByContraction} in \charef{cha:basisCalculus})
Note, that this decomposition scheme introduces further auxiliary variables $\headvariable$ with deterministic dependence on the distributed variables $\shortcatvariables$.
Such variables are often refered to as hidden.

% Shared formulas
We can further exploit common syntactical structure in the formulas $\enumformula\in\formulaset$ to reduce the number of relational encodings of connectives.
This is the case, when the syntax graph of two or more formulas share a subgraph.
In that case, the respective syntax graph needs to be represented only once an can be incorporated into the decomposition of all formulas, which share this subgraph.
For an example see \figref{fig:mlnDecRep}, where the syntactical representation of the formula $\formulaof{0}$ is a subgraph of the syntactical representation of $\formulaof{1}$.


%Since any member of an exponential family is a Markov Network with tensors to each coordinate of the statistic, also Markov Logic Networks are Markov Networks.

%\begin{corollary}\label{cor:MLNasMN}
%	Given a set $\formulaset$ of formulas on atomic variables $\catvariableof{\nodes}$, we construct a $\graph=(\nodes,\edges)$, where $\nodes$ are decorated by the atoms and
%		\[ \edges = \{ \nodesof{\formula}: \formula\in\formulaset \} \, , \]
%	where by $\nodesof{\formula}$ we denote the minimal set such that there exists a tensor $\hypercoreat{\catvariableof{\nodesof{\formula}}}$ with
%		\[ \formulaat{\catvariableof{\nodes}} = \hypercoreat{\catvariableof{\nodesof{\formula}}} \otimes \onesat{\catvariableof{\nodes/\nodesof{\formula}}} \, . \]
%	Any Markov Logic Network $\mlnprobat{\shortcatvariables}$ is then a Markov Network given the graph $\graphof{\formulaset}$
%	$\{\expof{\canparamat{\indexedselvariable}\cdot\enumformula}
%\, :\,\selindexin\}$.
%\end{corollary}


% MLN as graphical models
%Markov Logic Networks are Markov Networks with the factors given in a restricted form from the weighted truth of a formula.
%Each formula is seen as a factor of the graphical model.

To summarize, there are two sparsity mechanisms, originating from graphical models and propositional syntax, providing sparse representations of markov logic network:
\begin{itemize}
	\item \textbf{Dependence Sparsity:} Formulas depend only on subsets of atoms.
		This exploits the main sparsity mechanism in graphical models, where factors in sparse representations depend only on a subset of variables.
%		The underlying assumptions of conditional independence loss generality.
	\item \textbf{Computation Sparsity:}
		When the features of an exponential family are compositions of smaller formulas, the computation core is decomposed into a tensor network of their relational encodings.
		This can be regarded as the main sparsity mechanism of propositional logics, where syntactical decompositions of formulas are exploited.
		Further, when the structure of the smaller formulas is shared among different features, the respective relational encodings need to be instantiated only once.
\end{itemize}


%%
%\red{
%	We can extend the set of variables, by including the hidden formulas, and get a Markov Network of the relational encodings of connectives and headcores.
%Here hidden variables are additional variables facilitating the decomposition, but not appearing in open variables of contractions when doing reasoning.
%One can then exploit redundancies and make sure that every subresult is computed just once, by dropping relational encodings with identical head functions.
%}




%\begin{theorem}[Selection encodings for Energy representation]
%	\red{More the definition of exponential families.}
%	The energy of Markov Logic Networks is the contraction
%		\[ \mlnenergy = \sbcontractionof{\sencodingof{\formulaset},\canparam}{\shortcatvariables} \, . \]
%\end{theorem}


\subsubsect{Selection encodings for energy tensors}

%% Tensor Representation of MLN

As for generic exponential families, we can represent markov logic networks in terms of their energy tensors
%The energy tensor of an markov logic network is the contraction
\begin{align}
	\mlnenergy\left[\shortcatvariables\right]
	= \sum_{\selindexin} \canparamat{\indexedselvariable} \cdot \enumformulaat{\shortcatvariables} 
	= \sbcontractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables} \, .
\end{align}
The energy tensor provides an exponential representation of the distribution by
\begin{align}
	\mlnprobat{\shortcatvariables} = \normationof{\expof{\mlnenergy}}{\shortcatvariables} \, . 
\end{align}

In case of a common structure of the formulas in a Markov Logic Network, formula selecting networks (see \charef{cha:formulaSelection}) can be applied to represent their energies.
% Energy representation
%The weighted sum of formulas is then the energy of the Markov Logic Network.
We represent the superposition of formulas as a contraction with a parameter tensor.
Given a factored parametrization of formulas $\exformula_{\selindices}$ with indices $\selindexof{\selenumerator}$ we have the superposition by the network representation:
\begin{center}
	\input{PartII/tikz_pics/network_representation/energy_contraction.tex}
\end{center}


% Representation 
If the number of atoms and parameters gets large, it is important to represent the tensor ${\exformula_{\selindices}}$ efficiently in tensor network format and avoid contractions.
To avoid inefficiency issues, we also have to represent the parameter tensor $\canparam$ in a tensor network format to improve the variance of estimations (see \charef{cha:concentration}) and provide efficient numerical algorithms.

% Fail of full probability representation
However, when required to instantiate the probability distribution of a Markov Logic Network as a tensor network, we need to exponentiate and normate the energy tensor, a task for which relational encodings are required.
For such tasks, contractions of formula selecting networks are not sufficient and each formula with a nonvanishing weight needs to be instantiated as a factor tensor of a Markov Network. 






\subsect{Expressivity}\label{sec:MLNMaxMintermRep}

Based on Markov Logic Networks containing only maxterms and minterms (see \defref{def:clauses}), we now show that any positive probability distribution has a representation by a markov logic network.

\begin{theorem}\label{the:maximalClausesRepresentation}\label{the:mintermExpressivityMLN}
	Let there be a positive probability distribution
		 \[ \probat{\shortcatvariables} \in \bigotimes_{\atomenumeratorin}\rr^2 \, . \]
	Then the Markov Logic Network of minterms (see \defref{def:clauses})
		\[ \mintermformulaset = \{\mintermof{\atomindices} \, : \, \atomindices\in\atomstates \}\]
	with parameters %with nonzero weights at the maxterms indexed by $\atomindicesin$
		\[ \canparamat{\selvariableof{0}=\catindexof{0},\ldots,\selvariableof{\atomorder-1}=\catindexof{\atomorder-1}}% \weightof{\mintermof{\atomindices}} 
		= \ln \probat{\indexedcatvariables} \]
	coincides with $\probat{\shortcatvariables}$.

	Further, the Markov Logic Network of maxterms
		\[ \maxtermformulaset = \{\maxtermof{\atomindices} \, : \, \atomindices\in\atomstates \}\]
	with wparameters
		\[ \canparamat{\selvariableof{0}=\catindexof{0},\ldots,\selvariableof{\atomorder-1}=\catindexof{\atomorder-1}} %\weightof{\maxtermof{\atomindices}} 
		= - \ln\probat{\indexedcatvariables} \]
	coincides with $\probat{\shortcatvariables}$.
\end{theorem}
\begin{proof}
	It suffices to show, that in both cases of choosing $\formulaset$ by minterms or maxterms with the respective parameters
		\[ \mlnenergy =  \ln\probat{\shortcatvariables} \]
	and therefore
		\[ \mlnprobat{\shortcatvariables} 
		= \sbnormationof{\expof{\mlnenergy}}{\shortcatvariables} 
		=  \sbcontractionof{\expof{\mlnenergy}}{\shortcatvariables} 
		= \probat{\shortcatvariables}\, . \]
	
	In the case of minterms, we notice that for any $\atomindicesin$
		\[ \mintermof{\atomindices}[\shortcatvariables] = \onehotmapofat{\atomindices}{\shortcatvariables} \]
	and thus with the weights in the claim
		\[ \sum_{\atomindicesin} 
		\left( \ln \probat{\indexedcatvariables} \right) \cdot \mintermof{\atomindices}[\shortcatvariables]
		= \ln\probat{\shortcatvariables} \, .
		 \]

	For the maxterms we have analogously
		\[ \maxtermof{\atomindices}[\shortcatvariables] = \onesat{\shortcatvariables} - \onehotmapofat{\catindices}{\shortcatvariables} \]
	and thus that the maximal clauses coincide with the one-hot encodings of respective states.
	We thus have
	\begin{align*}
		& \sum_{\atomindicesin} 
		\left( - \ln \probat{\indexedcatvariables} \right) \cdot \maxtermof{\atomindices}[\shortcatvariables] \\
		& =
		\left(  \sum_{\nodes_0\subset [\atomorder]} 
		\left( - \ln \probat{\indexedcatvariables} \right) \cdot \onesat{\shortcatvariables} \right) \\
		& \quad + 
		\left(  \sum_{\nodes_0\subset [\atomorder]} 
		\left(  \ln \probat{\indexedcatvariables} \right) \cdot
		\onehotmapofat{\catindices}{\shortcatvariables} 
		\right) 
		 \\
		 & = \ln\probat{\shortcatvariables} + \lambda \cdot  \onesat{\shortcatvariables}\,,
	\end{align*}
	where $\lambda$ is a constant.
\end{proof}

We note, that there are $2^{\atomorder}$ maxterms and $2^{\atomorder}$ minterms, which would have to be instantiated by relational encodings to get a tensor network decomposition.
This large number of features originates from the generality of the representation scheme.
As a fundamental tradeoff, efficient representations come at the expense of a smaller expressivity of the representation scheme.


% Redundant parametrization
%In general, this representation is redundant, since any offset of the weight by $\lambda\cdot\ones$ results in the same distribution.
%However, the only $\bar{\canparam}$ are multiples of $\onesat{\shortcatvariables}$.

% Comparison with previous schemes
Theorem~\ref{the:maximalClausesRepresentation} is the analogue in Markov Logic to Theorem~\ref{the:tensorToMaxMinTerms}, which shows that any binary tensor has a representation by a logical formula, to probability tensors.
Here we require positive distributions for well-defined energy tensors.

% Markov Networks
Sparser representation formats based on the same principle as used in \theref{the:maximalClausesRepresentation} can be constructed to represent markov networks by markov logic networks.
Here, we can separately instantiate the factors by combinations of terms and clauses only involving the variables containted in the factor.

%\begin{remark}[Representation of Markov Networks]
% Composition of Markov Networks
%	If a probability distribution is representable as a Markov Network, we only need to activate clauses and terms, which variables are contained in factors.%
%	\red{Make a theorem out of that?}
%\end{remark}
%\subsect{Examples}

\subsect{Examples}

Let us now provide examples of markov logic networks.

\subsubsect{Distribution of independent variables}

We show next, the independent positive distributions are representable by tuning the $\atomorder$ weights of the atomic formulas and keeping all other weights zero.

\begin{theorem}\label{the:independentAtomicMLN}
	Let $\probat{\shortcatvariables}$ be a positive probability distribution, such that atomic formulas are independent from each other.
	Then $\probat{\shortcatvariables}$ is the Markov Logic Network of atomic formulas
		\[ \atomformulaset = \{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin \} \]
	and parameters
		\[ \canparamat{\selvariable=\catenumerator} 
		= \lnof{\frac{
		\contractionof{\probtensor}{\catvariableof{\catenumerator}=1}
		}{
		\contractionof{\probtensor}{\catvariableof{\catenumerator}=0}
		}} \]
%	Any distribution such that the atom satisfaction is independent from each other is reproducable by a MLN with nonzero weights only for the atomic formulas.
\end{theorem}
\begin{proof}
%	Using the independent assumptions, the probability tensor factorizes into normed vectors to each atom, with are transformed atomic formulas (leaving out the neutral ones tensors).
%	We then find a weight to each atom such that the vector is reproduced by the contraction with the activation core.
	
	By Theorem~\ref{the:independenceProductCriterion} we get a decomposition 
		\[ \probat{\shortcatvariables} = \bigotimes_{\catenumeratorin} \probofat{\catenumerator}{\catvariableof{\catenumerator}} \,  \]
	where 
		\[ \probofat{\catenumerator}{\catvariableof{\catenumerator}} = \sbcontractionof{\probtensor}{\catvariableof{\catenumerator}} \, . \]
	
	By assumption of positivity, the vector $\probofat{\catenumerator}{\catvariableof{\catenumerator}}$ is positive for each $\catenumeratorin$ and the parameter
		\[ \canparamat{\selvariable=\catenumerator} 
		= \lnof{\frac{
		\probofat{\catenumerator}{\catvariableof{\catenumerator}=1}
		}{
		\probofat{\catenumerator}{\catvariableof{\catenumerator}=0}
		}} \]
	well-defined.
	
	We then notice, that 
		\[ \expdistofat{(\{\atomicformulaof{\catenumerator}\},\canparamat{\selvariable=\catenumerator})}{\catvariableof{\catenumerator}} 
		= \probofat{\catenumerator}{\catvariableof{\catenumerator}}\]
	and therefore with the parameter vector of dimension $\seldim=\catorder$ defined as
		\[ \canparamat{\selvariable} = \sum_{\catenumeratorin} \canparamat{\selvariable=\catenumerator} \cdot \onehotmapofat{\catenumerator}{\selvariable}  \]
	we have
	\begin{align*}
	 	 \expdistofat{(\{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin\},\canparam)}{\shortcatvariables} 
		& = \bigotimes_{\catenumeratorin} \expdistofat{(\{\atomicformulaof{\catenumerator}\},\canparamat{\selvariable=\catenumerator})}{\catvariableof{\catenumerator}} \\
		& = \bigotimes_{\catenumeratorin} \probofat{\catenumerator}{\catvariableof{\catenumerator}} \\
		& = \probat{\shortcatvariables} \, . 
	\end{align*}
\end{proof}

%In general, the statistic to an atomic formula measures the marginal distribution. -> To Parameter Estimation

% Failing to be positive -> Hybrid networks
In Theorem~\ref{the:independentAtomicMLN} we made the assumption of positive distributions.
If the distribution fails to be positive, we still get a decomposition into distributions of each variable, but there is at least one factor failing to be positive.
Such factors need to be treated by hybrid logic networks, that is they are base measure for an exponential family coinciding with a logical literal (see \secref{sec:hardNetworks}).

% Energy representation
All atomic formulas can be selected by a single variable selecting tensor, that is
	\[ \energytensorofat{(\{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin\},\canparam)}{\shortcatvariables}
	= \sbcontractionof{\vselectionmapat{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables} \, . 
	\]
	
% Holds also more generally for any formula! -> Place it earlier?
In case of negative coordiantes $\canparamat{\selvariable=\catenumerator}$ it is convenient to replace $\atomicformulaof{\catenumerator}$ by $\lnot\atomicformulaof{\catenumerator}$, in order to facilitate the interpretation.
The probability distribution is left invariant, when also replacing $\canparamat{\selvariable=\catenumerator}$ by $-\canparamat{\selvariable=\catenumerator}$.



\subsubsect{Boltzmann machines}

A Boltzmann machine is a distribution of boolean variables $\shortcatvariables$ depending on weight tensors %(see e.g. Chapter 43 in \cite{mackay_information_2003})
\begin{align*}
	W[\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}] \in\rr^{\catorder}\otimes\rr^{\catorder} \quad \text{and} \quad b[\selvariableof{\vselectionsymbol,0}] \in\rr^{\catorder} \, .
\end{align*}
Its distribution is
\begin{align*}
	\probofat{W,b}{\shortcatvariables} = \normationof{\expof{\energytensorofat{W,b}{\shortcatvariables}}}{\shortcatvariables}
\end{align*}
where its energy tensor is
	\[ \energytensorofat{W,b}{\indexedshortcatvariables} =
	\sum_{\atomenumerator,\secatomenumerator \in [\atomorder]} 
		W[\selvariableof{\vselectionsymbol,0}=\atomenumerator, \selvariableof{\vselectionsymbol,1}=\secatomenumerator] \cdot \catindexof{\atomenumerator} \cdot \catindexof{\secatomenumerator} 
	+ \sum_{\atomenumerator\in[\atomorder]} b[\selvariableof{\vselectionsymbol,0}=\atomenumerator] \cdot \catindexof{\atomenumerator}\, . \]
We notice, that this tensor coincides with the energy tensor of a markov logic network with formula set
	\[ \formulaset = \{ \catvariableof{\atomenumerator} \Leftrightarrow \catvariableof{\secatomenumerator} \, : \, \atomenumerator,\secatomenumerator \in[\atomorder] \} 
	\cup \{ \catvariableof{\atomenumerator}\, : \, \atomenumeratorin \} \, \]
with cardinality $\atomorder^2+\atomorder$.
Each formula is in the expressivity of an architecture consisting of a single binary logical neuron selecting any variable of $\shortcatvariables$ in each argument and selecting connectives $\{\eqbincon,\lpasbincon\}$, where by $\lpasbincon$ we refer to a connective passing the first argument, defined for $\catindexofin{0}, \catindexofin{1}$ as 
	\[ \lpasbincon[\indexedcatvariableof{0},\indexedcatvariableof{1}] = \vselectionmapat{\indexedcatvariableof{0},\indexedcatvariableof{1},\selvariableof{\vselectionsymbol}=0} \, . \]
When we choose the canonical parameter as
	\[ \canparamat{\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}}
	= \onehotmapofat{0}{\selvariableof{\cselectionsymbol}} \otimes W[\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}]
	+ \onehotmapofat{1}{\selvariableof{\cselectionsymbol}} \otimes b[\selvariableof{\vselectionsymbol,0}] \otimes  \onehotmapofat{0}{\selvariableof{\vselectionsymbol,1}} \, .
	\]
we have (see \figref{fig:boltzmannEnergy})
	\[ \energytensorofat{W,b}{\shortcatvariables} = 
	\sbcontractionof{\fsnnat{\shortcatvariables,\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}},
		\canparamat{\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}}}{\shortcatvariables} \, . \]

Therefore, Boltzmann machines are specific markov logic networks with the statistic being biimplications between atoms and atoms itself.
Generic markov logic networks are more expressive than Boltzmann machines, by the flexibility to create further features by propositional formulas.
%We can use any binary logical connective and have an associated formula where we can put a weight on.

\begin{figure}[t]
\begin{center}
	\input{PartII/tikz_pics/network_representation/boltzmann_energy.tex}
\end{center}
\caption{Tensor network representation of the energy of a Boltzmann machine}
\label{fig:boltzmannEnergy}
\end{figure}


%where by $(\cdot,\cdot)|_{0}$
%To connect with the formalism of Boltzmann machines, let us identify the visible units of a Boltzmann machines with the atoms in a propositional theory.

%Boltzmann machines are then reproduced by taking $\atomorder^2+\atomorder$ many formulas, namely those measuring the correlations and the marginal distributions.
%To be more precise, the correlation between atom $\atomicformulaof{\atomenumerator}$ and $\atomicformulaof{\secatomenumerator}$ is measured by the satisfaction rate of the formula 
%	\[ \exformula_{\atomenumerator,\secatomenumerator} = \atomicformulaof{\atomenumerator} \leftrightarrow \atomicformulaof{\secatomenumerator}\]

%\begin{theorem}
%	Any Boltzmann machine over $\atomorder$ units with interaction matrix $U\in\rr^{\atomorder\times\atomorder}$ and potential term $b\in\rr^{\atomorder}$ (MacKay Book notation) is a MLN where the only nonzero weights are 
%		\[ \weightof{\atomicformulaof{\atomenumerator} } = b_{\atomenumerator} \quad, \quad \atomenumeratorin \]
%	and 
%		\[ \weightof{ \exformula_{\atomenumerator,\secatomenumerator} } = U_{\atomenumerator, \secatomenumerator} \quad , \quad \atomenumerator,\secatomenumerator \in [\atomorder]\] 
%\end{theorem}



















\sect{Hard Logic Networks}\label{sec:hardNetworks} % To be dropped in the unification with the MLN chapter

% Hard logic vs markov logic
While exponential families are positive distributions, in logics probability distributions can assign states zero probability.
As a consequence, Markov Logic Networks have a soft logic interpretation in the sense that violation of activated formulas have nonzero probability.
We here discuss their hard logic counterparts, where worlds not satisfying activated formulas have zero probability.

\subsect{The limit of hard logic}\label{sec:hardLogicLimit} % To be merged with the above

The probability function of Markov Logic Networks with positive weights mimiks the tensor network representation of the knowledge base, which is the conjunction of the formulas. 
The maxima of the probability function coincide with the models of the corresponding knowledge base, if the latter is satisfiable.
However, since the Markov Logic Network is defined as a normed exponentiation of the weighted formula sum, it is a positive distribution whereas uniform distributions among the models of a knowledge base assign zero probability to world failing to be a model.
Since both distributions are tensors in the same space to a factored system, we can take the limits of large weights and observe, that Markov Logic Networks indeed converge to normed knowledge bases.


\begin{lemma}
	For any satisfiable formula $\formulaat{\shortcatvariables}$ and a variable weight $\canparam\in\rr$, we have for $\canparam\rightarrow\infty$
	\begin{align*}
		\normationof{\expof{\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables} \rightarrow \normationof{\exformula}{\shortcatvariables}
	\end{align*}
	and for $\canparam\rightarrow-\infty$
	\begin{align*}
		\normationof{\expof{\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables} \rightarrow \normationof{\lnot\exformula}{\shortcatvariables} \, .
	\end{align*}
	Here we denote the understand the convergence of tensors as a convergence of each coordinate.
\end{lemma}
\begin{proof}
	We have 
	\begin{align*}
		\partitionfunctionof{\mlnparameters} = \left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator}\right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\canparam}
	\end{align*}
	and therefore for any $\shortcatindices\in\atomstates$ with $\formulaat{\indexedshortcatvariables}=1$
	\begin{align*}
		\normationof{\expof{\canparam\cdot \exformula}}{\indexedshortcatvariables}
		&= \frac{
			\expof{\canparam}
			}{
			\left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator} \right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\canparam}
			} \\
		& \rightarrow \frac{1}{\contraction{\exformula}} 
		= \normationof{\exformula}{\indexedcatvariables} \, . 
	\end{align*}
	For any $\atomindices\in\atomstates$ with $\formulaat{\indexedshortcatvariables}=0$ we have on the other side
	\begin{align*}
		\normationof{\expof{\canparam\cdot \exformula}}{\indexedcatvariables}
		&= \frac{
			1
			}{
			\left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator}\right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\canparam}
			} \\
		& \rightarrow 0
		= \normationof{\exformula}{\indexedcatvariables} \, . 
	\end{align*}
\end{proof}



% Limit on the activation core
We can by the above Lemma represent both the situation of non-asymptotic weights and the limit for diverging weights by the same computation core $\formulaccwith$, with different activation cores, since
\begin{align*}
	\normationof{\expof{\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables} 
	= \contractionof{\formulaccwith,\actcoreof{\formula,\canparam}}{\shortcatvariables}
\end{align*}
and 
\begin{align*}
	\normationof{\formula}{\shortcatvariables}
	= \contractionof{\formulaccwith,\tbasisat{\formulavar}}{\shortcatvariables} \quad \text{respectively} \quad
	\normationof{\lnot\formula}{\shortcatvariables}
	= \contractionof{\formulaccwith,\fbasisat{\formulavar}}{\shortcatvariables} \, . 
\end{align*}



\begin{theorem}
	Let $\formulaset$ be a formula set and $\canparam$ a positive parameter vector.
	If the formula
		\[ \kb = \bigwedge_{\exformulain} \exformula \]
	is satisfiable we have in the limit $\invtemp\rightarrow\infty$ the coordinatewise convergence
		\[ \expdistofat{(\formulaset,\invtemp\cdot\canparam)}{\shortcatvariables} \rightarrow \normationofwrt{\kb}{\shortcatvariables} \, . \]
\end{theorem}
\begin{proof}
	Since $\kb$ is satisfiable we find $\catindices\in\atomstates$ with
		\[  \contractionof{\expof{\sum_{\exformulain}\invtemp\cdot \weightof{\exformula} \cdot \exformula}}{\indexedcatvariables} = \expof{\invtemp \cdot \sum_{\exformulain}\weightof{\exformula}}  \]
	and the partition function obeys
		\[ \contractionof{\expof{\sum_{\exformulain}\invtemp\cdot \weightof{\exformula} \cdot \exformula}}{\varnothing} \geq  \expof{\invtemp \cdot \sum_{\exformulain}\weightof{\exformula}}  \, . \]
	For any state $\catindices\in\atomstates$ with $\kb(\catindices)=0$ we find $\secexformula\in\formulaset$ with $\secexformula(\catindices)=0$ and have
	\begin{align*}
	 	\frac{
		\contractionof{\expof{\sum_{\exformulain}\invtemp\cdot \weightof{\exformula} \cdot \exformula}}{\indexedcatvariables}
		}{
		\contractionof{\expof{\sum_{\exformulain}\invtemp\cdot \weightof{\exformula} \cdot \exformula}}{\varnothing}
		} 
		\leq  
	 	\frac{
		\expof{\invtemp\cdot \sum_{\exformulain : \exformula\neq \secexformula}\weightof{\exformula}}
		}{
		\expof{\invtemp\cdot \sum_{\exformulain}\weightof{\exformula}}
		} 
		= \expof{\invtemp \cdot \weightof{\secexformula}} \rightarrow 0 \, . 
	\end{align*}
	The limit of the distribution has thus support only on the models of $\kb$. 
	Since any model of $\kb$ has same energy at any $\invtemp$ the limit is a uniform distribution and coincides therefor with
		\[ \normationof{\kb}{\shortcatvariables} \, . \]
\end{proof}

% Generalization to face base measures
We here assumed, that the conjunction $\kb$ of the formulas in $\formulaset$ is satisfiable, and showed, that the markov logic network converges in the limit of large weights to the uniform distribution of the models of $\kb$.
In \charef{cha:networkReasoning} we will drop this assumption and show that the limit is the face base measure associated with the corresponding face of the mean parameter polytope.
The face base measure coincides thereby with $\kb$, if $\kb$ is satisfiable.

%\begin{remark}[More generic situation of simulated annealing]
%	The process of taking $\invtemp\rightarrow\infty$ is known as simulated annealing, see \charef{cha:probReasoning}.
%	From the discussion there we have the more general statement, that the limiting distribution is the uniform distribution among the maxima of $\expdistofat{(\formulaset,\canparam)}{\shortcatvariables}$.
%	If the formula $\kb$ is not satisfiable the normation $\normationofwrt{\kb}{\shortcatvariables}{\varnothing}$ does not exist and the limit distribution has another syntactical representation, to be gained e.g. by minterm or maxterm representation (see Theorem~\ref{the:tensorToMaxMinTerms}).
%\end{remark}


\subsect{Tensor Network Representation}

Hard Logic Network coincide with Knowledge Bases and are thus representable by contractions of formulas (which can be interpreted as an effective calculus scheme, see \secref{sec:effectiveCalculus}).


\begin{theorem}[Conjunction Decomposition of Knowledge Bases]\label{the:conDecKB}
	For a Knowledge Base
		\[ \kb = \bigwedge_{\exformula\in\formulaset} \exformula \]
	we have
		\[ \kbat{\shortcatvariables} = \contractionof{\formulaat{\shortcatvariables}}{\shortcatvariables}   \]
	and
		\[ \kbat{\shortcatvariables} = \contractionof{\{\formulaccwith \, : \, \formula\in\formulaset\} \cup \{\tbasisat{\formulavar} \, : \, \formula\in\formulaset\} }{\shortcatvariables} \, .  \]
\end{theorem}
\begin{proof}
	This follows from the representation of conjunctions by contraction (see \secref{sec:effectiveCalculus}) and
%	By the $\land$-symmetry, see effective calculus and
		\[ \formulaat{\shortcatvariables} =  \contractionof{\formulaccwith,\tbasisat{\formulavar}}{\shortcatvariables} \, .\]
\end{proof}

We call this representation scheme the $\land$-symmetry, since we can either represent $\kb$ by instantiation of $\rencodingof{\kb}$, which involves a relational encoding of the conjunction $\land$, or by instantiations of a collection of $\rencodingof{\formula}$.
We use the $\land$ symmetry to represent them as a contraction of the formulas building the Knowledge Base as conjunction.

\begin{remark}{$\land$ symmetry does not generalize to Markov Logic Networks} % Strange -> Drop?
	% Comparison to Markov Logic
	In Markov Logic Networks, similar decompositions are not possible.
	For example, consider a MLN with a single formula $\atomicformulaof{0}\land\atomicformulaof{1}$ and nonvanishing weight $\canparam$.
	This does not coincide with the distribution of a MLN of two formulas $\atomicformulaof{0}$ and $\atomicformulaof{1}$.
	To see this, we notice that with respect to the distribution of the first MLN, both variables are not independent, while for any MLN constructed by the two atomic formulas they are.
	% Can also be understood based on non-elementary contraction of $\land$ !
\end{remark}


%\begin{theorem}[$\land$-symmetry]\label{the:landSymmetry}
%	We observe that the contraction of an $\land$ core with $\tbasis$  is equivalent with $\tbasis$ cores on all the connected subformulas.
%\end{theorem}
%\begin{proof}
%	By equality of the Knowledge Base contraction in both ways: The missing subformulas behave the same if they are activated, since they then are contrained to the same subnetworks somewhere else. 
%	%\red{Find better arguments for missing subformulas when having the larger core.}
%\end{proof}
%
%\begin{theorem}[$\lnot$-symmetry]
%	Similarly the contraction of an $\lnot$ core with $\tbasis$ or $\tbasis$ has the same result as with $\tbasis$ or $\tbasis$ on the subformula.
%\end{theorem}
%
%We call the application of these in changing the Knowledge Cores without changing the contracted network as the representation symmetry.


%\subsect{Conjunctive Normal Representation}

%One tensor representation of a Knowledge Base is the association of the Knowledge Core $\tbasis$ at the formula being the Knowledge Base itself.
%We can use the $\land$ symmetry (Theorem~\ref{the:landSymmetry}) to propagate $\tbasis$ to all clause cores and get an alternative representation.
%Those are especially interesting when using Modus Ponens/Resolution as local sub-KB reasoners (see \secref{subsec:LocalEntailment}).

\sect{Hybrid Logic Network}

Markov Logic Networks are by definition positive distributions.
In contrary, Hard Logic Networks model uniform distributions over model sets of the respective Knowledge Base and therefore have vanishing coordinates.
We now show how to combine both approaches by defining Hybrid Logic Networks, when understanding Hard Logic Networks as base measures.
This trick is known to the field of variational inference, see for Example~3.6 in \cite{wainwright_graphical_2008}. 

\begin{definition}\label{def:hln}
	Given a set of formulas $\softformulaset$ with weights $\canparam$ and set $\hardformulaset$ of formulas, which conjunction is satisfiable, the hybrid logic network is the probability distribution
	\begin{align*}
		\probtensorof{(\softformulaset,\canparam,\hfbasemeasure)}[\shortcatvariables] 
		= \normationof{
		\{\exformula : \exformula\in\hardformulaset\} \cup \{\expof{\weightof{\exformula}\cdot\exformula} : \exformula\in\softformulaset\}
		}{\shortcatvariables} \, ,
	\end{align*}
	which is the member of the exponential family with statistic by $\softformulaset$ and the base measure
		\[ \hfbasemeasure[\shortcatvariables] = \contractionof{\{\formula : \formula \in \hardformulaset\}}{\shortcatvariables} \, .\]
		
	Given a set of formulas $\formulaset$, we define the set of hybrid logic networks realizable with $\formulaset$ and elementary activation cores as
	\begin{align*}
		\hlnsetof{\formulaset} 
		= \bigcup_{\secformulaset\subset\formulaset \, , \, \meanparam\in\{0,1\}^{\cardof{\formulaset}}}
		\expfamilyof{\formulaset/\secformulaset,\basemeasureof{\secformulaset,\meanparam}}
	\end{align*}
	where we denote base measures
	\begin{align*}
		\basemeasureofat{\secformulaset,\meanparam}{\shortcatvariables}
		= \bigwedge_{\enumformula\in\secformulaset} \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables} \, . 
	\end{align*}
\end{definition}

The assumption of a satisfiable set $\hardformulaset$ is necessary, as we show next.

\begin{theorem}
	If any only if $\bigwedge_{\formula\in\hardformulaset}\formula$ is satisfiable, the tensor 
		\[  \contractionof{
		\{\exformula : \exformula\in\hardformulaset\} \cup \{\expof{\weightof{\exformula}\cdot\exformula} : \exformula\in\softformulaset\}
		}{\shortcatvariables} \]
	is normable.
\end{theorem}
\begin{proof}
	We need to show that
	\begin{align}\label{eq:tbsWellDefinedHLN}
		\contraction{\{\exformula : \exformula\in\hardformulaset\} \cup \{\expof{\weightof{\exformula}\cdot\exformula} : \exformula\in\softformulaset\}} > 0 \, . 
	\end{align}
	Since the conjunction of $\hardformulaset$ is satisfiable we find a $\shortcatindices$ with $\formulaat{\indexedcatvariableof{[\catorder]}}=1$ for all $\exformula\in\hardformulaset$.
	Then 
	\begin{align*}
		 \contractionof{\{\exformula : \exformula\in\hardformulaset\} \cup \{\expof{\weightof{\exformula}\cdot\exformula} : \exformula\in\softformulaset\}}{\indexedcatvariableof{[\catorder]}}  
		 & = \left( \prod_{\exformula\in\hardformulaset}\formulaat{\indexedcatvariableof{[\catorder]}} \right) 
		 \cdot \left( \prod_{\exformula\in\softformulaset}\expof{\weightof{\exformula}\cdot\exformula}[\indexedcatvariableof{[\catorder]}] \right) \\
		 & =  \left( \prod_{\exformula\in\softformulaset}\expof{\weightof{\exformula}\cdot\exformula}[\indexedcatvariableof{[\catorder]}] \right) \\
		 & > 0 \, . 
	\end{align*}
	Condition \eqref{eq:tbsWellDefinedHLN} follows from this and the Hybrid Logic Network is well-defined.
\end{proof}


\subsect{Tensor Network Representation}

We can employ the formula decompositions to represent both probabilistic facts of the MLN and hard facts (seen as the limit of large weights).

\begin{theorem}\label{the:hybridNetworkRepresentation}
	For any hybrid logic network we have
	\begin{align*}
		\probtensorof{(\softformulaset,\canparam,\hardformulaset)}[\shortcatvariables] 
		= \normationof{
		\{\formulaccwith : \exformula\in\softformulaset\cup\hardformulaset \}
		\cup \{\tbasisat{\formulavar} : \exformula\in\hardformulaset \}
		\cup \{\actcoreofat{\exformula}{\formulavar} : \exformula\in\softformulaset \}
		}{\shortcatvariables} \, . 
	\end{align*}
\end{theorem}
\begin{proof}
	By \lemref{lem:formulaEncodingDecomposition}.
\end{proof}

% Discussion
While the statistics computing cores in the relational encoding are shared to compute the soft and the hard logic formulas, their activation cores differ.
While probabilistic soft formulas get activation cores (see Theorem~\ref{the:mlnTensorRep})
\begin{align*}
	\actcoreofat{\exformula}{\formulavar} 
	= \begin{bmatrix} 1 \\
		 \expof{\canparamat{\indexedselvariableof{}}} 
		 \end{bmatrix}[\enumformulavar] \,
\end{align*}
the hard formulas get activation cores by unit vectors
\begin{align*}
	\tbasisat{\formulavar} 
	= \begin{bmatrix} 0 \\
		 1
	 \end{bmatrix}[\enumformulavar] Â \, .
\end{align*}
As shown in \secref{sec:hardLogicLimit}, the soft activation cores converge to these hard activation cores in the limit of large parameters, when imposing a local normation.
%
We further notice, that the probabilistic activation cores are trivial tensors if and only if the corresponding parameter coordinate vanishes.


%The reason for this is the Slicing Theorem, enabling the operations by both (exponentiation and selection of one slice) by the activation cores.
For an example see Figure~\ref{fig:ActivatedHeads}.

\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/network_representation/activated_heads.tex}
\end{center}
\caption{Diagram of a formula tensor with activated heads, containing \textcolor{\concolor}{hard constraint cores} and \textcolor{\probcolor}{probabilistic weight cores} .} %along \textcolor{\inactivecolor}{inactive cores}.}
\label{fig:ActivatedHeads} 
\end{figure}



\begin{remark}{Probability interpretation using the Partition function}
	The tensor networks here represent unnormalized probability distributions.
	The probability distribution can be normed by the quotient with the naive contraction of the network, the partition function.
\end{remark}


\subsect{Reasoning Properties}

Deciding probabilistic entailment (see \defref{def:probEntailment}) with respect to Hybrid Logic Networks can be reduced to the hard logic parts of the network.

\begin{theorem}\label{the:hlnEntailmentReduction}
	Let $(\softformulaset,\canparam,\hardformulaset)$ define a Hybrid Logic Network.
	Given a query formula $\exformula$ we have that 
		\[ \probtensorof{(\softformulaset,\canparam,\hardformulaset)} \models \exformula \]
	if and only if
		\[ \hardformulaset \models \exformula \, . \]
\end{theorem}
\begin{proof}
	This follows from Theorem~\ref{the:factorReduction} on the representation of Hybrid Logic Networks as Markov Networks in Theorem~\ref{the:hybridNetworkRepresentation}.
\end{proof}


Formulas in $\softformulaset$, which are entailed or contradicted by $\hardformulaset$ are redundant, as we show next.

\begin{theorem}%\label{the:hlnRepRedundancy}
	If for a formula $\exformula$ and $\hardformulaset$ we have 
		\[ \hardformulaset \models \exformula \, \quad \text{or} \quad \hardformulaset \models \lnot\exformula \]
	then for any $(\softformulaset,\canparam,\hardformulaset)$
		\[ \probofat{(\softformulaset/\{\exformula\},\tilde{\canparam},\hardformulaset)}{\shortcatvariables} =  \probofat{(\softformulaset,\canparam,\hardformulaset)}{\shortcatvariables}  \, , \]
	where $\tilde{\canparam}$ denotes the tensor $\canparam$, where the coordinate to $\exformula$ is dropped, if $\exformula\in\softformulaset$.
\end{theorem}
\begin{proof}
	Isolate the factor to the hard formula, which is constant for all situations.
\end{proof}

%% Now in the 
A similar statement holds for the hard formulas itself, as shown in Theorem~\ref{the:ReduncancyOfEntailed}.
However, notice that if $\hardformulaset/\{\exformula\}\models\lnot\exformula$, then $\hardformulaset\cup\{\exformula\}$ is not satisfiable and a hybrid logic network cannot be defined for $\hardformulaset\cup\{\exformula\}$ as hard logic formulas.

%If the conjunction of $\hardformulaset/\{\exformula\}$ entails $\exformula$, we can erase $\exformula$ from $\hardformulaset$ without changing the contraction, therefore without changing the base measure of the Hybrid Logic Network.

% Utility in Contraction KB implementation
These results are especially interesting for the efficient implementation of \algoref{alg:contractionKB}, which has been introduced in \charef{cha:logicalReasoning}.
By Theorem~\ref{the:hlnEntailmentReduction} only the hard logic parts of a Hybrid Logic Network are required in the ASK operation.
%Theorem~\ref{the:hlnRepRedundancy} for the TELL operation, but already discussed in more generality

\subsect{Expressivity}

Hybrid Logic Networks extend the expressivity result of Theorem~\ref{the:mintermExpressivityMLN} to arbitrary probability tensors, dropping the positivity constraints for Markov Logic Networks.

\begin{theorem}\label{the:mintermExpressivityHLN}
	Let $\probat{\shortcatvariables}$ a possibly not positive probability tensor we build a base measure
		\[ \hfbasemeasure = \nonzeroof{\probat{\shortcatvariables}} \]
	and a parameter tensor
	\begin{align*}
		\canparamat{\selvariableof{[\catorder]}=\shortcatindices}
		= \begin{cases}
			0 & \text{if} \quad \probat{\indexedshortcatvariables} = 0  \\
			\lnof{\probat{\indexedshortcatvariables}} & \text{else} 
		\end{cases} \, . 
	\end{align*}
	Then the probability tensor is the member of the minterm exponential family with base measure $\hardformulaset$ and parameter $\canparam$, that is
		\[ \probat{(\mintermformulaset,\canparam,\hfbasemeasure)}\]
\end{theorem}
\begin{proof}
	It suffices to show that 
		\[ \sbcontractionof{\hfbasemeasure, \expof{\contractionof{
		\sencodingof{\mintermformulaset}\canparam
		}{
		\shortcatvariables
		}}}{\shortcatvariables} = \probat{\shortcatvariables} \, . \]
	For indices $\shortcatindices$ with $\probat{\indexedshortcatvariables}=0$ we have $\hfbasemeasureat{\indexedshortcatvariables}=0$ and thus also 
		\[ \sbcontractionof{\hfbasemeasure, \expof{\contractionof{
		\sencodingof{\mintermformulaset}\canparam
		}{
		\shortcatvariables
		}}}{\indexedshortcatvariables} = 0 \, . \]
	For indices $\shortcatindices$ with $\probat{\indexedshortcatvariables}>0$ we have $\hfbasemeasureat{\indexedshortcatvariables}=1$ and
	\begin{align*}
		 \sbcontractionof{\hfbasemeasure, \expof{\contractionof{
		\sencodingof{\mintermformulaset},\canparam
		}{
		\shortcatvariables
		}}}{\indexedshortcatvariables} 
		&= \prod_{\selindexof{[\catorder]}} \expof{\canparamat{\selvariableof{[\catorder]}=\selindexof{[\catorder]}} \cdot \mintermofat{\selindexof{[\catorder]}}{\indexedshortcatvariables}} \\
		&=  \expof{\canparamat{\selvariableof{[\catorder]}=\shortcatindices}} \\
		&=  \probat{\indexedshortcatvariables} \, .
	\end{align*}
\end{proof}



\sect{Polynomial Representation}

%We now apply the representation symmetries to represent a propositional Knowledge Base in conjunctive normal form.
We now sparse representation formats for the introduced logic networks, namely the basis+ $\cpformat$ format introduced in \charef{cha:sparseCalculus} which are understood as polynomial decompositions.
First of all, we establish a sparsity result for terms and clauses (see \defref{def:clauses}).

\begin{lemma}\label{lem:clauseTermBasisPlus}
	Any term is representable by a single monomial and any clause is representable by a sum of at most two monomials. %, any term of basis+ with rank 1. %Use also \baspluscprankof{}
\end{lemma}
\begin{proof}
	Let $\nodes_0$ and $\nodes_1$ be disjoint subsets of $\nodes$, then we have
	\begin{align*}
		\termof{\nodes_0}{\nodes_1} = \onehotmapofat{
			\{\catindexof{\atomenumerator} = 0 : \atomenumerator\in\nodes_0\} \cup \{\catindexof{\atomenumerator} = 1 : \atomenumerator\in\nodes_1\}
		}{\catvariableof{\nodes_0\cup\nodes_1}} \otimes \onesat{\catvariableof{\nodes/(\nodes_0\cup\nodes_1)}}
	\end{align*}
	and
	\begin{align*}
		\clauseof{\nodes_0}{\nodes_1} = \onesat{\catvariableof{\nodes}} - \onehotmapofat{
			\{\catindexof{\atomenumerator} = 0 : \atomenumerator\in\nodes_0\} \cup \{\catindexof{\atomenumerator} = 1 : \atomenumerator\in\nodes_1\}
		}{\catvariableof{\nodes_0\cup\nodes_1}}
		\otimes \onesat{\catvariableof{\nodes/(\nodes_0\cup\nodes_1)}} \, .
	\end{align*}
	We notice, that any tensors $\ones$ and $\onehotmapof{\catindex}\otimes \ones$ habe basis+-rank of $1$ and therefore $\termof{\nodes_0}{\nodes_1}$ of $1$ and $\clauseof{\nodes_0}{\nodes_1}$ of at most $2$.
\end{proof}

A formula in conjunctive normal form is a conjunction of clauses, where clauses are disjunctions of literals being atoms (positive literals) or negated atoms (negative literals).
Based on these normal forms, we show representations of formulas as sparse polynomials. %, which will be discussed in more detail in \charef{cha:sparseCalculus} (see \defref{def:polynomialSparsity}).
We apply \lemref{lem:clauseTermBasisPlus} to show the following sparsity bound. % on the energy tensor of Markov Logic Networks.

\begin{theorem}\label{the:formulaSlicePolynomialDecomposition}
	Any formula $\exformula$ with a conjunctive normal form of $\clausedim$ clauses satisfies
		\[ \slicesparsityof{\exformula} \leq 2^{\clausedim} \, . \]
\end{theorem}
\begin{proof}
	Let $\exformula$ have a conjunctive normal form with clauses indexed by $\clauseenumeratorin$ and each clause represented by subsets $\nodes_0^\clauseenumerator, \nodes_1^\clauseenumerator$, that is
		\[ \exformula = \bigwedge_{\clauseenumeratorin} \clauseof{\nodes_0^\clauseenumerator}{\nodes_1^\clauseenumerator} \, . \]
	We now use the rank bound of \theref{the:CPrankContractionBound} and \lemref{lem:clauseTermBasisPlus} to get
	\begin{align*}
		\slicesparsityof{\exformula} \leq \prod_{\clauseenumeratorin} \slicesparsityof{\clauseof{\nodes_0^\clauseenumerator}{\nodes_1^\clauseenumerator}} \leq 2^{\clausedim} \, .
	\end{align*}
\end{proof}

We apply this result on the sparse representation of a single formula to derive sparse representations for Hard Logic Networks and the energy tensor of Hybrid Logic Networks.
Both results use in addition to \theref{the:formulaSlicePolynomialDecomposition} sparsity bounds, which are shown by explicit representation construction in \charef{cha:sparseCalculus}.

\begin{corollary}
	Any Hard Logic Network $\hardformulaset$ obeys
	\begin{align*}
		\slicesparsityof{\hardformulaset} \leq \prod_{\exformula\in\hardformulaset} 2^{\clausedimof{\exformula}}
\end{align*}
\end{corollary}
\begin{proof}
	We apply the contraction bound \theref{the:CPrankContractionBound} for the decomposition
	\begin{align*}
		\kbat{\shortcatvariables} = \contractionof{\{\formulaat{\shortcatvariables} \, : \, \formula\in\hardformulaset\}}{\shortcatvariables}
	\end{align*}
	and get
	\begin{align*}
		\slicesparsityof{\kb} \leq \prod_{\formula\in\hardformulaset} \slicesparsityof{\formula} \, .
	\end{align*}
	The claimed bound follows with \theref{the:formulaSlicePolynomialDecomposition}.
\end{proof}

\begin{corollary}
	The energy tensor of a Hybrid Logic Network with statistic $\mlnstat$
	\begin{align*}
		\slicesparsityof{\contractionof{\sencmlnstatwith,\canparamwith}{\shortcatvariables}} \leq \sum_{\selindexin\,:\,\canparamat{\indexedselvariable}\neq0} 2^{\clausedimof{\enumformula}} \, .
	\end{align*}
	where $\clausedimof{\enumformula}$ denotes the number of clauses in a conjunctive normal form of $\enumformula$.
\end{corollary}
\begin{proof}
	We decompse the energy into the sum
	\begin{align*}
		\contractionof{\sencmlnstatwith,\canparamwith}{\shortcatvariables}
		= \sum_{\selindexin\,:\,\canparamat{\indexedselvariable}\neq0} \canparamat{\indexedselvariable} \cdot \enumformulawith
	\end{align*}
	and apply \theref{the:CPrankSumBound} to get
	\begin{align*}
		\slicesparsityof{\contractionof{\sencmlnstatwith,\canparamwith}{\shortcatvariables}}
		\leq \sum_{\selindexin\,:\,\canparamat{\indexedselvariable}\neq0} \slicesparsityof{\enumformulawith}
		\leq \sum_{\selindexin\,:\,\canparamat{\indexedselvariable}\neq0}2^{\clausedimof{\enumformula}} \, .
	\end{align*}
\end{proof}

\sect{Applications}

Hybrid Logic Networks as neuro-symbolic architectures:
\begin{itemize}
	\item Neural Paradigm here by decompositions of logical formulas into their connectives.
		In more generality by decompositions of sufficient statistics into composed functions, using Basis Calculus.
		Deeper nodes as carrying correlations of lower nodes.
	\item Symbolic Paradigm by interpretability of propositional logics.
\end{itemize}


Hybrid Logic Networks as trainable Machine Learning models:
\begin{itemize}
	\item Expressivity: Can represent any positive distribution, as shown by Theorem~\ref{the:maximalClausesRepresentation}, with $2^d$ formulas.
	\item Efficiency: Can only handle small subsets of possible formulas, since their possible number is huge.
		Tensor networks provide means to efficiently represent formulas depending on many variables and reason based on contractions.
	\item Differentiability: Distributions are differentiable functions of their weights, see Parameter Estimation Chapter. 
		The log-likelihood of data is therefore also differentiable function of their weights and we can exploit first-order methods in their optimization.
	\item Structure Learning: We need to find differentiable parametrizations of logical formulas respecting a chosen architecture.
		In \charef{cha:formulaSelection} such representations are described based on Selector Tensor Networks.
\end{itemize}
Differentiability and structure learning will be investigated in more detail in the next chapter.

\red{When understanding atoms as observed variables, and the computed as hidden, Hybrid Logic Networks are deep higher-order boltzmann machines: 
More generic correlations can be captured by a logical connective, calculated by a relational encoding and activated by an activation core.}

\red{Hybrid Logic Networks as bridging soft and hard logics within the formalism of exponential families.}

%\begin{remark}[Hopfield networks]
%	Also interesting for MLNs is a Hopfield perspective.
%	Having an initialization the update can be interpreted as a Gibbs sampling step at temperature $0$ (since deterministic update).
%\end{remark}


% CSP
A more general class of problems, which have natural representations by Hard Logic Networks are Constraint Satisfaction Problems (see Chapter~5 in \cite{russell_artificial_2021}).
Solving such problems is then equivalent to sampling from the worlds in a logical interpretation, and can be approached by the methods of \charef{cha:logicalReasoning}.
Among these classed, we have only discussed the Sudoku game in Example~\ref{exa:sudoku}.
Extensions by Hybrid Logic Networks can be interpreted as implementations of preferences among possible solutions by probabilities.