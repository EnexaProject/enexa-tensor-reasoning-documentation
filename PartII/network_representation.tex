\chapter{\chatextnetworkRepresentation}\label{cha:networkRepresentation}

Hybrid logic networks are graphical models with an interpretation by propositional logics.
Since they unify probalisitic and logical reasoning, we call them hybrid.
In this chapter we investigate their representation based on tensor networks, while the reasoning mechanisms are discussed in \charef{cha:networkReasoning}.
We first distinguish between Markov Logic Networks, which are an approach to soft logics in the framework of exponential families, and Hard Logic Networks, which correspond with propositional knowledge bases.
Then we exploit non-trivial boolean base measures to unify both approaches by Hybrid Logic Networks, which are itself in exponential families.
We then investigate the corresponding mean parameter polytopes of Hybrid Logic Networks and show expressivity results.

%Markov Logic Networks are probability functions of truth assignments to logical functions.
%They respect propositional logic as hard constraints, but have beyond that freedom to shape probability distributions on possible situations.
%To capture these properties, we define them as graphical models with structure cores representing propositional logics and activation cores representing the specification of probability distributions.
% We in this part employ them to combine the probabilistic and the logical paradigm.


\sect{Markov Logic Networks}

Markov Logic Networks exploit the efficiency and interpretability of logical calculus as well as the expressivity of graphical models.

\subsect{Markov Logic Networks as Exponential Families}

We introduce Markov Logic Networks in the formalism of exponential families (see \secref{sec:exponentialFamilies}).

\begin{definition}[Markov Logic Networks]
    \label{def:mln}
    Markov Logic Networks are exponential families $\mlnexpfamily$ with sufficient statistics by functions
    \begin{align*}
        \mlnstat : \atomstates \rightarrow \bigtimes_{\exformulain}[2] \subset \rr^{\cardof{\formulaset}}
    \end{align*}
    defined coordinatewise by propositional formulas $\exformulain$.
\end{definition}

% Characterization of MLNs among exponential families: When choosing binary features
Since the image of each coordinate $\sstatcoordinateof{\selindex}$ is contained in $\ozset$, each is a propositional formulas (see \defref{def:formulas}).
%Conversely, any boolean feature $\sstatcoordinateof{\selindex}$ of an exponential family defines a propositional formula (see \defref{def:formulas}).
Thus, any exponential family of distributions of $\atomstates$, such that $\imageof{\sstatcoordinateof{\selindex}}\subset\ozset$ for all $\selindexin$ is a set of Markov Logic Networks with fixed formulas.

\begin{remark}[Alternative Definitions]
    We here defined Markov Logic Networks on propositional logic, while originally they are defined in the more expressive first-order logics \cite{richardson_markov_2006}.
    The relation of both frameworks will be discussed further in \charef{cha:folModels}.
\end{remark}



\subsect{Tensor Network Representation}

Based on the previous discussion on the representation of exponential families by tensor networks in \secref{sec:exponentialFamilies} we now derive a representation for Markov Logic Networks.

\subsubsect{Basis encodings for distributions}

\begin{theorem}[Basis encodings for Markov Logic Networks]
    \label{the:mlnTensorRep}
    A Markov Logic Network to a set of formulas $\formulaset = \{\enumformula \, : \, \selindexin\}$ is represented as
    \begin{align*}
        \mlnprobat{\shortcatvariables} =
        \normalizationof{\{\enumformulaccwith : \selindexin \} \cup \{\enumformulaacwith : \selindexin \}
        }{\shortcatvariables}
    \end{align*}
    where we denote for each $\selindexin$ an activation core
    \begin{align*}
        \enumformulaac\left[\indexedheadvariableof{\selindex}\right]
        = \begin{cases}
              1 & \text{for} \quad \headindexof{\selindex} = 0 \\
              \expof{\canparamat{\indexedselvariable}} & \text{for} \quad \headindexof{\selindex}  = 1
        \end{cases}  \, .
    \end{align*}
\end{theorem}
\begin{proof}
    Markov Logic Networks are exponential families, which base measure is trivial and which statistic consist of boolean features.
    We apply the tensor network decomposition of more generic exponential families \theref{the:expFamilyTensorRep} to this case and get
    \begin{align*}
        \mlnprobat{\shortcatvariables} =
        \normalizationof{\{\onesat{\shortcatvariables}\}
        \cup \{\bencodingofat{\sstatcoordinateof{\selindex}}{\sstatcatof{\selindex},\shortcatvariables} \, : \, \selindexin\}
        \cup\{\enumformulaacwith\, : \, \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
    While the base measure tensor is trivial, it can be ignored in the contraction.
    Since the image of each feature $\enumformula$ is in $[2]$, we choose the index interpretation function by the identity $\indexinterpretation : [2] \rightarrow \ozset$ and get
%	We further choose the standard index interpretation for booleans (see \secref{sec:booleanEncoding}) and get
    \begin{align*}
        \enumformulaac\left[\indexedheadvariableof{\selindex}\right]
        &= \expof{\canparamat{\indexedselvariable} \cdot \indexinterpretationofat{\selindex}{\headindexof{\selindex}} }
        = \expof{\canparamat{\indexedselvariable} \cdot \headindexof{\selindex}} \\
        &= \begin{cases}
               1 & \text{for} \quad \headindexof{\selindex} = 0 \\
               \expof{\canparamat{\indexedselvariable}} & \text{for} \quad \headindexof{\selindex}  = 1
        \end{cases} \qedhere
    \end{align*}
\end{proof}

\begin{figure}[t!]
    \begin{center}
        \input{PartII/tikz_pics/network_representation/factor.tex}
    \end{center}
    \caption{Factor of a Markov Logic Network to a formula $\enumformula$, represented as the contraction of a computation core $\enumformulacc$ and an activation core $\enumformulaac$.
    While the computation core $\enumformulacc$ prepares based on basis calculus a categorical variable representing the value of the statistic formula $\enumformula$ dependent on assignments to the distributed variables, the activation core multiplies an exponential weight to coordinates satisfying the formula.
    }
    \label{fig:mlnFactor}
\end{figure}

% Graphical model representation
\theref{the:mlnTensorRep} provides a decomposition of markov logic networks by a tensor network of computation cores $\bencodingof{\enumformula}$ and accompanying activation cores $\enumformulaac$.
Since the head variable $\headvariableof{\selindex}$ appears exclusively in these pairs, we can contract each computation core with the corresponding activation core to get a factor, see \figref{fig:mlnFactor}.
With this we get the decomposition
\begin{align*}
    \mlnprobat{\shortcatvariables}
    = \normalizationof{\{\expof{\canparamat{\indexedselvariable}\cdot\enumformulaat{\shortcatvariables}} \, : \, \selindexin\}}{\shortcatvariables} \, .
\end{align*}
More precisely, this transformation of the decomposition holds by \theref{the:splittingContractions} to be shown in \charef{cha:messagePassing}, stating that the contraction of computation and activation cores can be performed before the global contraction of the result.

% Sparsification by trivial variables
While in the decomposition of \theref{the:mlnTensorRep} the basis encodings of the features carry all distributed variables $\shortcatvariables$, we now seek towards sparser decompositions.
To each $\selindexin$ we denote by $\nodesof{\selindex}$ the maximal subset of $[\catorder]$ such that there is a reduced function
$\tilde{\formula}_{\selindex} : \bigtimes_{} \rightarrow [2]$
with
\begin{align*}
    \enumformulaat{\shortcatvariables}
    = \contractionof{\tilde{\formula}_{\selindex}[\catvariableof{\nodesof{\selindex}}]}{\shortcatvariables} \, .
\end{align*}
We often account for such situations of sparse formulas, when $\enumformula$ has a syntactical decomposition involving only the atomic variables $\nodesof{\selindex}$.
As a consequence we have
\begin{align*}
    \bencodingofat{\enumformula}{\shortcatvariables}
    = \bencodingofat{\tilde{\formula}_{\selindex}}{\catvariableof{\nodesof{\selindex}}}
\end{align*}
and
\begin{align*}
    \mlnprobat{\shortcatvariables}
    = \normalizationof{\{\expof{\canparamat{\indexedselvariable}\cdot\tilde{\formula}_{\selindex}[\catvariableof{\nodesof{\selindex}}]} \, : \, \selindexin\}}{\shortcatvariables} \, .
\end{align*}
Thus, any markov logic network has a sparse representation by a markov network on the graph
\begin{align*}
    \graphof{\formulaset} = ([\catorder],\{\nodesof{\selindex} \, : \, \selindexin\}) \, .
\end{align*}
This sparsity inducing mechanism is analogous to the decomposition of probability distributions based on conditional independence assumptions, when understanding each formula in the markov logic network as an introduced dependency among the affected variables $\nodesof{\selindex}$.


\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/network_representation/decomposed_representation.tex}
    \end{center}
    \caption{Example of a decomposed Markov Network representation of a Markov Logic Network with formulas $\{\formulaof{0} = a\lor b, \formulaof{1} = a \lor b \lor \lnot c\}$.
    Since both formulas share the subformula $a\lor b$, their contracted factors have a representation by a connected tensor network.}
% Where $\actcoreofat{\enumformula}{\enumformulavar} =\begin{bmatrix} 1 & \expof{\weightof{\exformula}} \end{bmatrix}[\formulavar] $}
    \label{fig:mlnDecRep}
\end{figure}


% Sparsity by decomposition
A further sparsity introducing mechanism is through exploiting redundancy in the computation of $\enumformula$, when a decomposition of the feature is known.
For the propositional formulas $\enumformula$ this amounts to a syntactic representation of the formula as a composition of logical connectives is available (see \figref{fig:mlnDecRep}). % Make a precise definition in logicRepresentation and link it!
In this case, we exploit the representation by tensor networks of the basis encodings (shown as \theref{the:compositionByContraction} in \charef{cha:basisCalculus})
Note, that this decomposition scheme introduces further auxiliary variables $\headvariable$ with deterministic dependence on the distributed variables $\shortcatvariables$.
Such variables are often refered to as hidden.

% Shared formulas
We can further exploit common syntactical structure in the formulas $\enumformula\in\formulaset$ to reduce the number of basis encodings of connectives.
This is the case, when the syntax graph of two or more formulas share a subgraph.
In that case, the respective syntax graph needs to be represented only once an can be incorporated into the decomposition of all formulas, which share this subgraph.
For an example see \figref{fig:mlnDecRep}, where the syntactical representation of the formula $\formulaof{0}$ is a subgraph of the syntactical representation of $\formulaof{1}$.


%Since any member of an exponential family is a Markov Network with tensors to each coordinate of the statistic, also Markov Logic Networks are Markov Networks.

%\begin{corollary}\label{cor:MLNasMN}
%	Given a set $\formulaset$ of formulas on atomic variables $\catvariableof{\nodes}$, we construct a $\graph=(\nodes,\edges)$, where $\nodes$ are decorated by the atoms and
%		\[ \edges = \{ \nodesof{\formula}: \formula\in\formulaset \} \, , \]
%	where by $\nodesof{\formula}$ we denote the minimal set such that there exists a tensor $\hypercoreat{\catvariableof{\nodesof{\formula}}}$ with
%		\[ \formulaat{\catvariableof{\nodes}} = \hypercoreat{\catvariableof{\nodesof{\formula}}} \otimes \onesat{\catvariableof{\nodes/\nodesof{\formula}}} \, . \]
%	Any Markov Logic Network $\mlnprobat{\shortcatvariables}$ is then a Markov Network given the graph $\graphof{\formulaset}$
%	$\{\expof{\canparamat{\indexedselvariable}\cdot\enumformula}
%\, :\,\selindexin\}$.
%\end{corollary}


% MLN as graphical models
%Markov Logic Networks are Markov Networks with the factors given in a restricted form from the weighted truth of a formula.
%Each formula is seen as a factor of the graphical model.

To summarize, there are two sparsity mechanisms, originating from graphical models and propositional syntax, providing sparse representations of markov logic network:
\begin{itemize}
    \item \textbf{Dependence Sparsity:} Formulas depend only on subsets of atoms.
    This exploits the main sparsity mechanism in graphical models, where factors in sparse representations depend only on a subset of variables.
%		The underlying assumptions of conditional independence loss generality.
    \item \textbf{Computation Sparsity:}
    When the features of an exponential family are compositions of smaller formulas, the computation core is decomposed into a tensor network of their basis encodings.
    This can be regarded as the main sparsity mechanism of propositional logics, where syntactical decompositions of formulas are exploited.
    Further, when the structure of the smaller formulas is shared among different features, the respective basis encodings need to be instantiated only once.
\end{itemize}

%\begin{theorem}[Selection encodings for Energy representation]
%	\red{More the definition of exponential families.}
%	The energy of Markov Logic Networks is the contraction
%		\[ \mlnenergy = \contractionof{\sencodingof{\formulaset},\canparam}{\shortcatvariables} \, . \]
%\end{theorem}


\subsubsect{Selection encodings for energy tensors}

%% Tensor Representation of MLN

As for generic exponential families, we can represent markov logic networks in terms of their energy tensors
%The energy tensor of an markov logic network is the contraction
\begin{align}
    \mlnenergy\left[\shortcatvariables\right]
    = \sum_{\selindexin} \canparamat{\indexedselvariable} \cdot \enumformulaat{\shortcatvariables}
    = \contractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables} \, .
\end{align}
The energy tensor provides an exponential representation of the distribution by
\begin{align}
    \mlnprobat{\shortcatvariables} = \normalizationof{\expof{\mlnenergy}}{\shortcatvariables} \, .
\end{align}

In case of a common structure of the formulas in a Markov Logic Network, formula selecting networks (see \charef{cha:formulaSelection}) can be applied to represent their energies.
% Energy representation
%The weighted sum of formulas is then the energy of the Markov Logic Network.
We represent the superposition of formulas as a contraction with a parameter tensor.
Given a factored parametrization of formulas $\exformula_{\selindices}$ with indices $\selindexof{\selenumerator}$ we have the superposition by the network representation:
\begin{center}
    \input{PartII/tikz_pics/network_representation/energy_contraction.tex}
\end{center}


% Representation 
If the number of atoms and parameters gets large, it is important to represent the tensor ${\exformula_{\selindices}}$ efficiently in tensor network format and avoid contractions.
To avoid inefficiency issues, we also have to represent the parameter tensor $\canparam$ in a tensor network format to improve the variance of estimations (see \charef{cha:concentration}) and provide efficient numerical algorithms.

% Fail of full probability representation
However, when required to instantiate the probability distribution of a Markov Logic Network as a tensor network, we need to exponentiate and normate the energy tensor, a task for which basis encodings are required.
For such tasks, contractions of formula selecting networks are not sufficient and each formula with a nonvanishing weight needs to be instantiated as a factor tensor of a Markov Network.






\subsect{Expressivity}\label{sec:MLNMaxMintermRep}

Based on Markov Logic Networks containing only maxterms and minterms (see \defref{def:clauses}), we now show that any positive probability distribution has a representation by a markov logic network.

\begin{theorem}
    \label{the:maximalClausesRepresentation}\label{the:mintermExpressivityMLN}
    Let there be a positive probability distribution
    \[ \probat{\shortcatvariables} \in \bigotimes_{\atomenumeratorin}\rr^2 \, . \]
    Then the Markov Logic Network of minterms (see \defref{def:clauses})
    \[ \mintermformulaset = \{\mintermof{\atomindices} \, : \, \atomindices\in\atomstates \}\]
    with parameters %with nonzero weights at the maxterms indexed by $\atomindicesin$
    \[ \canparamat{\selvariableof{0}=\catindexof{0},\ldots,\selvariableof{\atomorder-1}=\catindexof{\atomorder-1}}% \weightof{\mintermof{\atomindices}}
    = \ln \probat{\indexedcatvariables} \]
    coincides with $\probat{\shortcatvariables}$.

    Further, the Markov Logic Network of maxterms
    \[ \maxtermformulaset = \{\maxtermof{\atomindices} \, : \, \atomindices\in\atomstates \}\]
    with parameters
    \[ \canparamat{\selvariableof{0}=\catindexof{0},\ldots,\selvariableof{\atomorder-1}=\catindexof{\atomorder-1}} %\weightof{\maxtermof{\atomindices}}
    = - \ln\probat{\indexedcatvariables} \]
    coincides with $\probat{\shortcatvariables}$.
\end{theorem}
\begin{proof}
    It suffices to show, that in both cases of choosing $\formulaset$ by minterms or maxterms with the respective parameters
    \[ \mlnenergy =  \ln\probat{\shortcatvariables} \]
    and therefore
    \[ \mlnprobat{\shortcatvariables}
    = \normalizationof{\expof{\mlnenergy}}{\shortcatvariables}
    =  \contractionof{\expof{\mlnenergy}}{\shortcatvariables}
    = \probat{\shortcatvariables}\, . \]

    In the case of minterms, we notice that for any $\atomindicesin$
    \[ \mintermof{\atomindices}[\shortcatvariables] = \onehotmapofat{\atomindices}{\shortcatvariables} \]
    and thus with the weights in the claim
    \[ \sum_{\atomindicesin}
    \left( \ln \probat{\indexedcatvariables} \right) \cdot \mintermof{\atomindices}[\shortcatvariables]
    = \ln\probat{\shortcatvariables} \, .
    \]

    For the maxterms we have analogously
    \[ \maxtermof{\atomindices}[\shortcatvariables] = \onesat{\shortcatvariables} - \onehotmapofat{\catindices}{\shortcatvariables} \]
    and thus that the maximal clauses coincide with the one-hot encodings of respective states.
    We thus have
    \begin{align*}
        & \sum_{\atomindicesin}
        \left( - \ln \probat{\indexedcatvariables} \right) \cdot \maxtermof{\atomindices}[\shortcatvariables] \\
        & =
        \left(  \sum_{\nodes_0\subset [\atomorder]}
        \left( - \ln \probat{\indexedcatvariables} \right) \cdot \onesat{\shortcatvariables} \right) \\
        & \quad +
        \left(  \sum_{\nodes_0\subset [\atomorder]}
        \left(  \ln \probat{\indexedcatvariables} \right) \cdot
        \onehotmapofat{\catindices}{\shortcatvariables}
        \right)
        \\
        & = \ln\probat{\shortcatvariables} + \lambda \cdot  \onesat{\shortcatvariables}\,,
    \end{align*}
    where $\lambda$ is a constant.
\end{proof}

We note, that there are $2^{\atomorder}$ maxterms and $2^{\atomorder}$ minterms, which would have to be instantiated by basis encodings to get a tensor network decomposition.
This large number of features originates from the generality of the representation scheme.
As a fundamental tradeoff, efficient representations come at the expense of a smaller expressivity of the representation scheme.


% Redundant parametrization
%In general, this representation is redundant, since any offset of the weight by $\lambda\cdot\ones$ results in the same distribution.
%However, the only $\bar{\canparam}$ are multiples of $\onesat{\shortcatvariables}$.

% Comparison with previous schemes
Theorem~\ref{the:maximalClausesRepresentation} is the analogue in Markov Logic to Theorem~\ref{the:tensorToMaxMinTerms}, which shows that any binary tensor has a representation by a logical formula, to probability tensors.
Here we require positive distributions for well-defined energy tensors.

% Markov Networks
Sparser representation formats based on the same principle as used in \theref{the:maximalClausesRepresentation} can be constructed to represent markov networks by markov logic networks.
Here, we can separately instantiate the factors by combinations of terms and clauses only involving the variables containted in the factor.
\red{The reason for this is, that minterms form a partition statistics, which will be discussed in more detail in \charef{cha:basisCalculus}, see \defref{def:partitionStatistic}.}

\subsect{Examples}

Let us now provide examples of markov logic networks.

\subsubsect{Distribution of independent variables}

We show next, the independent positive distributions are representable by tuning the $\atomorder$ weights of the atomic formulas and keeping all other weights zero.

\begin{theorem}
    \label{the:independentAtomicMLN}
    Let $\probat{\shortcatvariables}$ be a positive probability distribution, such that atomic formulas are independent from each other.
    Then $\probat{\shortcatvariables}$ is the Markov Logic Network of atomic formulas
    \[ \atomformulaset = \{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin \} \]
    and parameters
    \[ \canparamat{\selvariable=\catenumerator}
    = \lnof{\frac{
        \contractionof{\probtensor}{\catvariableof{\catenumerator}=1}
    }{
        \contractionof{\probtensor}{\catvariableof{\catenumerator}=0}
    }} \]
%	Any distribution such that the atom satisfaction is independent from each other is reproducable by a MLN with nonzero weights only for the atomic formulas.
\end{theorem}
\begin{proof}
%	Using the independent assumptions, the probability tensor factorizes into normed vectors to each atom, with are transformed atomic formulas (leaving out the neutral ones tensors).
%	We then find a weight to each atom such that the vector is reproduced by the contraction with the activation core.

    By Theorem~\ref{the:independenceProductCriterion} we get a decomposition
    \[ \probat{\shortcatvariables} = \bigotimes_{\catenumeratorin} \probofat{\catenumerator}{\catvariableof{\catenumerator}} \,  \]
    where
    \[ \probofat{\catenumerator}{\catvariableof{\catenumerator}} = \contractionof{\probtensor}{\catvariableof{\catenumerator}} \, . \]

    By assumption of positivity, the vector $\probofat{\catenumerator}{\catvariableof{\catenumerator}}$ is positive for each $\catenumeratorin$ and the parameter
    \[ \canparamat{\selvariable=\catenumerator}
    = \lnof{\frac{
        \probofat{\catenumerator}{\catvariableof{\catenumerator}=1}
    }{
        \probofat{\catenumerator}{\catvariableof{\catenumerator}=0}
    }} \]
    well-defined.

    We then notice, that
    \[ \expdistofat{(\{\atomicformulaof{\catenumerator}\},\canparamat{\selvariable=\catenumerator})}{\catvariableof{\catenumerator}}
    = \probofat{\catenumerator}{\catvariableof{\catenumerator}}\]
    and therefore with the parameter vector of dimension $\seldim=\catorder$ defined as
    \[ \canparamat{\selvariable} = \sum_{\catenumeratorin} \canparamat{\selvariable=\catenumerator} \cdot \onehotmapofat{\catenumerator}{\selvariable}  \]
    we have
    \begin{align*}
        \expdistofat{(\{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin\},\canparam)}{\shortcatvariables}
        & = \bigotimes_{\catenumeratorin} \expdistofat{(\{\atomicformulaof{\catenumerator}\},\canparamat{\selvariable=\catenumerator})}{\catvariableof{\catenumerator}} \\
        & = \bigotimes_{\catenumeratorin} \probofat{\catenumerator}{\catvariableof{\catenumerator}} \\
        & = \probat{\shortcatvariables} \, . \qedhere
    \end{align*}
\end{proof}

%In general, the statistic to an atomic formula measures the marginal distribution. -> To Parameter Estimation

% Failing to be positive -> Hybrid networks
In Theorem~\ref{the:independentAtomicMLN} we made the assumption of positive distributions.
If the distribution fails to be positive, we still get a decomposition into distributions of each variable, but there is at least one factor failing to be positive.
Such factors need to be treated by hybrid logic networks, that is they are base measure for an exponential family coinciding with a logical literal (see \secref{sec:hardNetworks}).

% Energy representation
All atomic formulas can be selected by a single variable selecting tensor, that is
\[ \energytensorofat{(\{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin\},\canparam)}{\shortcatvariables}
= \contractionof{\vselectionmapat{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables} \, .
\]

% Holds also more generally for any formula! -> Place it earlier?
In case of negative coordiantes $\canparamat{\selvariable=\catenumerator}$ it is convenient to replace $\atomicformulaof{\catenumerator}$ by $\lnot\atomicformulaof{\catenumerator}$, in order to facilitate the interpretation.
The probability distribution is left invariant, when also replacing $\canparamat{\selvariable=\catenumerator}$ by $-\canparamat{\selvariable=\catenumerator}$.



\subsubsect{Boltzmann machines}

A Boltzmann machine is a distribution of boolean variables $\shortcatvariables$ depending on weight tensors %(see e.g. Chapter 43 in \cite{mackay_information_2003})
\begin{align*}
    W[\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}] \in\rr^{\catorder}\otimes\rr^{\catorder} \quad \text{and} \quad b[\selvariableof{\vselectionsymbol,0}] \in\rr^{\catorder} \, .
\end{align*}
Its distribution is
\begin{align*}
    \probofat{W,b}{\shortcatvariables} = \normalizationof{\expof{\energytensorofat{W,b}{\shortcatvariables}}}{\shortcatvariables}
\end{align*}
where its energy tensor is
\[ \energytensorofat{W,b}{\indexedshortcatvariables} =
\sum_{\atomenumerator,\secatomenumerator \in [\atomorder]}
W[\selvariableof{\vselectionsymbol,0}=\atomenumerator, \selvariableof{\vselectionsymbol,1}=\secatomenumerator] \cdot \catindexof{\atomenumerator} \cdot \catindexof{\secatomenumerator}
+ \sum_{\atomenumerator\in[\atomorder]} b[\selvariableof{\vselectionsymbol,0}=\atomenumerator] \cdot \catindexof{\atomenumerator}\, . \]
We notice, that this tensor coincides with the energy tensor of a markov logic network with formula set
\[ \formulaset = \{ \catvariableof{\atomenumerator} \Leftrightarrow \catvariableof{\secatomenumerator} \, : \, \atomenumerator,\secatomenumerator \in[\atomorder] \}
\cup \{ \catvariableof{\atomenumerator}\, : \, \atomenumeratorin \} \, \]
with cardinality $\atomorder^2+\atomorder$.
Each formula is in the expressivity of an architecture consisting of a single binary logical neuron selecting any variable of $\shortcatvariables$ in each argument and selecting connectives $\{\eqbincon,\lpasbincon\}$, where by $\lpasbincon$ we refer to a connective passing the first argument, defined for $\catindexofin{0}, \catindexofin{1}$ as
\[ \lpasbincon[\indexedcatvariableof{0},\indexedcatvariableof{1}] = \vselectionmapat{\indexedcatvariableof{0},\indexedcatvariableof{1},\selvariableof{\vselectionsymbol}=0} \, . \]
When we choose the canonical parameter as
\[ \canparamat{\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}}
= \onehotmapofat{0}{\selvariableof{\cselectionsymbol}} \otimes W[\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}]
+ \onehotmapofat{1}{\selvariableof{\cselectionsymbol}} \otimes b[\selvariableof{\vselectionsymbol,0}] \otimes  \onehotmapofat{0}{\selvariableof{\vselectionsymbol,1}} \, .
\]
we have (see \figref{fig:boltzmannEnergy})
\[ \energytensorofat{W,b}{\shortcatvariables} =
\contractionof{\fsnnat{\shortcatvariables,\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}},
    \canparamat{\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}}}{\shortcatvariables} \, . \]

Therefore, Boltzmann machines are specific markov logic networks with the statistic being biimplications between atoms and atoms itself.
Generic markov logic networks are more expressive than Boltzmann machines, by the flexibility to create further features by propositional formulas.
%We can use any binary logical connective and have an associated formula where we can put a weight on.

\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/network_representation/boltzmann_energy.tex}
    \end{center}
    \caption{Tensor network representation of the energy of a Boltzmann machine}
    \label{fig:boltzmannEnergy}
\end{figure}


%where by $(\cdot,\cdot)|_{0}$
%To connect with the formalism of Boltzmann machines, let us identify the visible units of a Boltzmann machines with the atoms in a propositional theory.

%Boltzmann machines are then reproduced by taking $\atomorder^2+\atomorder$ many formulas, namely those measuring the correlations and the marginal distributions.
%To be more precise, the correlation between atom $\atomicformulaof{\atomenumerator}$ and $\atomicformulaof{\secatomenumerator}$ is measured by the satisfaction rate of the formula 
%	\[ \exformula_{\atomenumerator,\secatomenumerator} = \atomicformulaof{\atomenumerator} \leftrightarrow \atomicformulaof{\secatomenumerator}\]

%\begin{theorem}
%	Any Boltzmann machine over $\atomorder$ units with interaction matrix $U\in\rr^{\atomorder\times\atomorder}$ and potential term $b\in\rr^{\atomorder}$ (MacKay Book notation) is a MLN where the only nonzero weights are 
%		\[ \weightof{\atomicformulaof{\atomenumerator} } = b_{\atomenumerator} \quad, \quad \atomenumeratorin \]
%	and 
%		\[ \weightof{ \exformula_{\atomenumerator,\secatomenumerator} } = U_{\atomenumerator, \secatomenumerator} \quad , \quad \atomenumerator,\secatomenumerator \in [\atomorder]\] 
%\end{theorem}



















\sect{Hard Logic Networks}\label{sec:hardNetworks} % To be dropped in the unification with the MLN chapter

% Hard logic vs markov logic
While exponential families are positive distributions, in logics probability distributions can assign states zero probability.
As a consequence, Markov Logic Networks have a soft logic interpretation in the sense that violation of activated formulas have nonzero probability.
We now discuss their hard logic counterparts, where worlds not satisfying activated formulas have zero probability.

%\subsect{The limit of hard logic}\label{sec:hardLogicLimit} % To be merged with the above

The probability function of Markov Logic Networks with positive weights mimiks the tensor network representation of the knowledge base, which is the conjunction of the formulas.
The maxima of the probability function coincide with the models of the corresponding knowledge base, if the latter is satisfiable.
However, since the Markov Logic Network is defined as a normed exponentiation of the weighted formula sum, it is a positive distribution whereas uniform distributions among the models of a knowledge base assign zero probability to world failing to be a model.
Since both distributions are tensors in the same space to a factored system, we can take the limits of large weights and observe, that Markov Logic Networks indeed converge to normed knowledge bases.

\begin{lemma}
    \label{lem:localHardLimit}
    Given a formula $\exformula$ we have in the limit of $\invtemp\rightarrow\infty$ the coordinatewise convergence
    \begin{align*}
        \normalizationof{\expof{\invtemp\cdot\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables} \rightarrow
        \begin{cases}
            \normalizationof{\exformula}{\shortcatvariables} & \text{if $\exformula$ satisfiable and } \canparam>0\\
            \normalizationof{\ones}{\shortcatvariables} & \text{if $\exformula$ or $\lnot\exformula$ not satisfiable or } \canparam=0 \\
            \normalizationof{\lnot\exformula}{\shortcatvariables} & \text{if $\lnot\exformula$ satisfiable and } \canparam<0\\
        \end{cases} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    The statement is trivial for the case of $\exformula$ or $\lnot\exformula$ being satisfiable, or $\canparam=0$, since then for all $\invtemp\in\rr$
    \begin{align*}
        \normalizationof{\expof{\invtemp\cdot\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables} = \normalizationof{\ones}{\shortcatvariables} \, .
    \end{align*}
    In the case of $\exformula$ being satisfiable and $\canparam>0$ we have
    \begin{align*}
        %\partitionfunctionof{\mlnparameters}
        \contraction{\expof{\invtemp\cdot\canparam\cdot\formulaat{\shortcatvariables}}}
        = \left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator}\right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\invtemp\cdot\canparam}
    \end{align*}
    and therefore for any $\shortcatindices\in\atomstates$ with $\formulaat{\indexedshortcatvariables}=1$
    \begin{align*}
        \normalizationof{\expof{\invtemp\cdot\canparam\cdot\exformula}}{\indexedshortcatvariables}
        &= \frac{
            \expof{\invtemp\cdot\canparam}
        }{
            \left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator} \right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\invtemp\cdot\canparam}
        } \\
        & \rightarrow \frac{1}{\contraction{\exformula}}
        = \normalizationof{\exformula}{\indexedcatvariables} \, .
    \end{align*}
    For any $\atomindices\in\atomstates$ with $\formulaat{\indexedshortcatvariables}=0$ we have on the other side
    \begin{align*}
        \normalizationof{\expof{\invtemp\cdot\canparam\cdot\exformula}}{\indexedcatvariables}
        &= \frac{
            1
        }{
            \left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator}\right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\invtemp\cdot\canparam}
        } \\
        & \rightarrow 0
        = \normalizationof{\exformula}{\indexedcatvariables} \, .
    \end{align*}
    This shows the statement in the case of $\exformula$ being satisfiable and $\canparam>0$.
    The case of $\lnot\exformula$ being satisfiable and $\canparam<0$ follows from a similar argument.
\end{proof}

\lemref{lem:localHardLimit} only characterizes the annealing limits in exponential families with statistics by single formulas.
If there are multiple formulas the limiting distribution gets more involved.
We will characterize the generic limiting distributions later in \secref{sec:hardLogicLimit} based on face measures in corresponding mean parameter polytopes.

%% Generalization to face base measures
%We here assumed, that the conjunction $\kb$ of the formulas in $\formulaset$ is satisfiable, and showed, that the markov logic network converges in the limit of large weights to the uniform distribution of the models of $\kb$.
%In \charef{cha:networkReasoning} we will drop this assumption and show that the limit is the face base measure associated with the corresponding face of the mean parameter polytope.
%The face base measure coincides thereby with $\kb$, if $\kb$ is satisfiable.

% Representation by activation cores
We can represent the local limiting distribution in the three cases by an activation core contracted to the computation core $\bencodingofat{\exformula}{\formulavar,\shortcatvariables}$.
When choosing
\begin{align*}
    \kcoreat{\headvariableof{\exformula}} =
    \begin{cases}
        \tbasisat{\headvariableof{\exformula}} & \text{if $\exformula$ satisfiable and } \canparam>0\\
        \onesat{\headvariableof{\exformula}} & \text{if $\exformula$ or $\lnot\exformula$ not satisfiable or } \canparam=0 \\
        \fbasisat{\headvariableof{\exformula}} & \text{if $\lnot\exformula$ satisfiable and } \canparam<0\\
    \end{cases}
\end{align*}
we have
\begin{align*}
    \normalizationof{\expof{\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables} \rightarrow
    \normalizationof{\bencodingofat{\exformula}{\headvariableof{\exformula},\shortcatvariables},\kcoreat{\headvariableof{\exformula}}}{\shortcatvariables} \, .
\end{align*}

We notice, that these three activation cores are all non-vanishing boolean cores in dimension $2$.
This motivates the definition of Hard Logic Networks as follows.

\begin{definition}
    \label{def:hardLogicNetwork}
    Given a boolean statistic $\mlnstat$ and for $\selindexin$ boolean vectors $\kcoreofat{\selindex}{\headvariableof{\selindex}}$ the Hard Logic Network is the distribution
    \begin{align*}
        \probofat{\mlnstat,\{\kcoreofat{\selindex}\wcols\selindexin\}}{\shortcatvariables} =
        \normalizationof{
            \{\mlnstatccwith\} \cup
            \{\kcorewith\wcols\selindexin\}
        }{\shortcatvariables} \, .
    \end{align*}
    provided that the normalization exists.
    %We denote the distribution by $\probofat{\formulaset,\{\kcoreofat{\selindex}\wcols\selindexin\}}{\shortcatvariables}$
%	The set of Hard Logic Networks is defined as
%	\begin{align*}
%		\left\{\probofat{\{\kcoreofat{\selindex}\wcols\selindexin\}}{\shortcatvariables} \wcols \kcoreofat{\selindex}{\headvariableof{\selindex}}\in\{\tbasisat{\headvariableof{\selindex}},\fbasisat{\headvariableof{\selindex}},\onesat{\headvariableof{\selindex}}\} \right\}
%	\end{align*}
\end{definition}

We notice, that from the assumption that the normation exists, each activation core needs to be non-vanishing and thus obeys
\begin{align*}
    \kcoreofat{\selindex}{\headvariableof{\selindex}}\in\{\tbasisat{\headvariableof{\selindex}},\fbasisat{\headvariableof{\selindex}},\onesat{\headvariableof{\selindex}}\}    \, .
\end{align*}
Any core $\onesat{\headvariableof{\selindex}}$ furthermore results in a trivial contraction with the corresponding computation core.
Based on these insights we can characterize Hard Logic Networks by propositional formulas.

\begin{theorem}
    Given a Hard Logic Network
    \begin{align*}
        \probofat{\formulaset,\{\kcoreof{\selindex}\wcols\selindexin\}}{\shortcatvariables}
    \end{align*}
    we build the set
    \begin{align*}
        \variableset = \{\selindex\wcols\selindexin\ncond\kcorewith\in\{\fbasisat{\headvariableof{\selindex}},\tbasisat{\headvariableof{\selindex}}\}\}
    \end{align*}
    and the tuple $\headindexof{\variableset}\in\bigtimes_{\selindex\in\variableset}[2]$ by coordinates
    \begin{align*}
        \headindexof{\selindex}
        = \begin{cases}
              0 & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}} = \fbasisat{\headvariableof{\selindex}} \\
              1 & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}} = \tbasisat{\headvariableof{\selindex}} \\
        \end{cases} \, .
%        = \{\selindex\wcols\selindexin\ncond\kcorewith\in\{\fbasisat{\headvariableof{\selindex}},\tbasisat{\headvariableof{\selindex}}\}\}
    \end{align*}
    Then we have for the formula
    \begin{align*}
        \hlnformula =
        \left(\bigwedge_{\selindex\in\variableset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
        \land
        \left(\bigwedge_{\selindex\in\variableset\wcols\headindexof{\selindex}=0} \lnot\enumformulaat{\shortcatvariables}\right)
    \end{align*}
    that
    \begin{align*}
        \probofat{\formulaset,\{\kcoreof{\selindex}\wcols\selindexin\}}{\shortcatvariables}
        = \normalizationof{\hlnformula}{\shortcatvariables} \, .
    \end{align*}
%
%
%    coincides with the uniform distribution % $\normalizationof{\hlnformula}{\shortcatvariables}$, where
%%    \begin{align*}
%%        \mpfo
%%    \end{align*}
%    $\normalizationof{\formulaofat{\{\kcoreofat{\selindex}{\headvariableof{\selindex}}\wcols\selindexin\}}{\shortcatvariables}}{\shortcatvariables}$, where
%    \begin{align*}
%        \formulaofat{\{\kcoreofat{\selindex}{\headvariableof{\selindex}}\wcols\selindexin\}}{\shortcatvariables} =
%        \left(\bigwedge_{\selindexin\wcols\kcoreofat{\selindex}{\headvariableof{\selindex}}=\tbasisat{\headvariableof{\selindex}}} \enumformulaat{\shortcatvariables}\right)
%        \land
%        \left(\bigwedge_{\selindexin\wcols\kcoreofat{\selindex}{\headvariableof{\selindex}}=\fbasisat{\headvariableof{\selindex}}} \lnot\enumformulaat{\shortcatvariables}\right) \, .
%    \end{align*}
\end{theorem}
\begin{proof}
    The claim is shown by performing the contraction
    \begin{align*}
        \contractionof{
            \bigcup_{\selindexin}\{\bencodingofat{\enumformula}{\headvariableof{\selindex},\shortcatvariables},\kcoreofat{\selindex}{\headvariableof{\selindex}}\}
        }{\shortcatvariables}
    \end{align*}
    defining the Hard Logic Network.
    We notice, that for each $\selindexin$ the variables $\headvariableof{\selindex}$ appear only in the cores $\bencodingofat{\shortcatvariables}{\headvariableof{\selindex},\shortcatvariables}$ and $\kcoreofat{\selindex}{\headvariableof{\selindex}}$ and thus
    \begin{align*}
        & \contractionof{
            \bigcup_{\selindexin}\{\bencodingofat{\enumformula}{\headvariableof{\selindex},\shortcatvariables},\kcoreofat{\selindex}{\headvariableof{\selindex}}\}
        }{\shortcatvariables} \\
        & \quad =    \contractionof{
            \left\{
            \contractionof{\bencodingofat{\enumformula}{\headvariableof{\selindex},\shortcatvariables},\kcoreofat{\selindex}{\headvariableof{\selindex}}}{\shortcatvariables}
            \wcols\selindexin\right\}
        }{\shortcatvariables} \\
        & \quad = \contractionof{
            \{
            \enumformulaat{\shortcatvariables}
            \wcols\kcoreofat{\selindex}{\headvariableof{\selindex}} = \tbasisat{\headvariableof{\selindex}}
            \}
            \cup
            \{
            \enumformulaat{\shortcatvariables}
            \wcols\kcoreofat{\selindex}{\headvariableof{\selindex}} = \fbasisat{\headvariableof{\selindex}}
            \}
        }{\shortcatvariables} \\
        & = \hlnformulawith \, .
    \end{align*}
    Here we used in the second equation \lemref{lem:formulaEncodingDecomposition} and \corref{cor:onesHead} to simplify the contraction of the basis encodings with the activation cores.
    In the third equation we use the representation of conjunctions by contraction to be shown in \theref{the:effectiveConjunction}.
\end{proof}

%
%% OLD Local limits
%\begin{lemma}
%	For any satisfiable formula $\formulaat{\shortcatvariables}$ and a variable weight $\canparam\in\rr$, we have for $\canparam\rightarrow\infty$
%	\begin{align*}
%		\normalizationof{\expof{\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables} \rightarrow \normalizationof{\exformula}{\shortcatvariables}
%	\end{align*}
%	and for $\canparam\rightarrow-\infty$, provided that $\lnot\formula$ is satisfiable,
%	\begin{align*}
%		\normalizationof{\expof{\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables} \rightarrow \normalizationof{\lnot\exformula}{\shortcatvariables} \, .
%	\end{align*}
%	Here we denote the understand the convergence of tensors as a convergence of each coordinate.
%\end{lemma}
%\begin{proof}
%	We have
%	\begin{align*}
%		\partitionfunctionof{\mlnparameters} = \left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator}\right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\canparam}
%	\end{align*}
%	and therefore for any $\shortcatindices\in\atomstates$ with $\formulaat{\indexedshortcatvariables}=1$
%	\begin{align*}
%		\normalizationof{\expof{\canparam\cdot \exformula}}{\indexedshortcatvariables}
%		&= \frac{
%			\expof{\canparam}
%			}{
%			\left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator} \right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\canparam}
%			} \\
%		& \rightarrow \frac{1}{\contraction{\exformula}}
%		= \normalizationof{\exformula}{\indexedcatvariables} \, .
%	\end{align*}
%	For any $\atomindices\in\atomstates$ with $\formulaat{\indexedshortcatvariables}=0$ we have on the other side
%	\begin{align*}
%		\normalizationof{\expof{\canparam\cdot \exformula}}{\indexedcatvariables}
%		&= \frac{
%			1
%			}{
%			\left(\prod_{\atomenumeratorin} \catdimof{\atomenumerator}\right) - \contraction{\exformula} + \contraction{\exformula} \cdot \expof{\canparam}
%			} \\
%		& \rightarrow 0
%		= \normalizationof{\exformula}{\indexedcatvariables} \, . \qedhere
%	\end{align*}
%\end{proof}

%% Limit on the activation core
%We can by the above Lemma represent both the situation of non-asymptotic weights and the limit for diverging weights by the same computation core $\formulaccwith$, with different activation cores, since
%\begin{align*}
%	\normalizationof{\expof{\canparam\cdot\formulaat{\shortcatvariables}}}{\shortcatvariables}
%	= \contractionof{\formulaccwith,\actcoreof{\formula,\canparam}}{\shortcatvariables}
%\end{align*}
%and
%\begin{align*}
%	\normalizationof{\formula}{\shortcatvariables}
%	= \contractionof{\formulaccwith,\tbasisat{\formulavar}}{\shortcatvariables}
%\end{align*}
%respectively
%\begin{align*}
%	\normalizationof{\lnot\formula}{\shortcatvariables}
%	= \contractionof{\formulaccwith,\fbasisat{\formulavar}}{\shortcatvariables} \, .
%\end{align*}


% Global limits


%\begin{theorem}
%	Let $\formulaset$ be a formula set and $\canparam$ a positive parameter vector.
%	If the formula
%		\[ \kb = \bigwedge_{\exformulain} \exformula \]
%	is satisfiable we have in the limit $\invtemp\rightarrow\infty$ the coordinatewise convergence
%		\[ \expdistofat{(\formulaset,\invtemp\cdot\canparam)}{\shortcatvariables} \rightarrow \normalizationofwrt{\kb}{\shortcatvariables} \, . \]
%\end{theorem}
%\begin{proof}
%	\red{This follows from the face measure coinciding with the vertex $\onesat{\selvariable}$.}
%	Since $\kb$ is satisfiable we find $\catindices\in\atomstates$ with
%		\[  \contractionof{\expof{\sum_{\exformulain}\invtemp\cdot \weightof{\exformula} \cdot \exformula}}{\indexedcatvariables} = \expof{\invtemp \cdot \sum_{\exformulain}\weightof{\exformula}}  \]
%	and the partition function obeys
%		\[ \contractionof{\expof{\sum_{\exformulain}\invtemp\cdot \weightof{\exformula} \cdot \exformula}}{\varnothing} \geq  \expof{\invtemp \cdot \sum_{\exformulain}\weightof{\exformula}}  \, . \]
%	For any state $\catindices\in\atomstates$ with $\kb(\catindices)=0$ we find $\secexformula\in\formulaset$ with
%	\begin{align*}
%		\secexformula(\catindices)=0
%	\end{align*}
%	and have
%	\begin{align*}
%	 	\frac{
%		\contractionof{\expof{\sum_{\exformulain}\invtemp\cdot \weightof{\exformula} \cdot \exformula}}{\indexedcatvariables}
%		}{
%		\contractionof{\expof{\sum_{\exformulain}\invtemp\cdot \weightof{\exformula} \cdot \exformula}}{\varnothing}
%		}
%		\leq
%	 	\frac{
%		\expof{\invtemp\cdot \sum_{\exformulain : \exformula\neq \secexformula}\weightof{\exformula}}
%		}{
%		\expof{\invtemp\cdot \sum_{\exformulain}\weightof{\exformula}}
%		}
%		= \expof{\invtemp \cdot \weightof{\secexformula}} \rightarrow 0 \, .
%	\end{align*}
%	The limit of the distribution has thus support only on the models of $\kb$.
%	Since any model of $\kb$ has same energy at any $\invtemp$ the limit is a uniform distribution and coincides therefor with
%		\[ \normalizationof{\kb}{\shortcatvariables} \, . \]
%\end{proof}


%\begin{remark}[More generic situation of simulated annealing]
%	The process of taking $\invtemp\rightarrow\infty$ is known as simulated annealing, see \charef{cha:probReasoning}.
%	From the discussion there we have the more general statement, that the limiting distribution is the uniform distribution among the maxima of $\expdistofat{(\formulaset,\canparam)}{\shortcatvariables}$.
%	If the formula $\kb$ is not satisfiable the normalization $\normalizationofwrt{\kb}{\shortcatvariables}{\varnothing}$ does not exist and the limit distribution has another syntactical representation, to be gained e.g. by minterm or maxterm representation (see Theorem~\ref{the:tensorToMaxMinTerms}).
%\end{remark}
%
%
%\subsect{Tensor Network Representation}
%
%Hard Logic Network coincide with Knowledge Bases and are thus representable by contractions of formulas (which can be interpreted as a hybrid calculus scheme, see \secref{sec:hybridCalculus}).
%
%
%\begin{theorem}[Conjunction Decomposition of Knowledge Bases]\label{the:conDecKB}
%	For a Knowledge Base
%		\[ \kb = \bigwedge_{\exformula\in\formulaset} \exformula \]
%	we have
%		\[ \kbat{\shortcatvariables} = \contractionof{\{\formulaat{\shortcatvariables}\wcols\exformula\in\formulaset\}}{\shortcatvariables}   \]
%	and
%		\[ \kbat{\shortcatvariables} = \contractionof{\{\formulaccwith \, : \, \formula\in\formulaset\} \cup \{\tbasisat{\formulavar} \, : \, \formula\in\formulaset\} }{\shortcatvariables} \, .  \]
%\end{theorem}
%\begin{proof}
%	This follows from the representation of conjunctions by contraction (see \secref{sec:hybridCalculus}) and
%%	By the $\land$-symmetry, see effective calculus and
%	\begin{align*}
%		\formulaat{\shortcatvariables} =  \contractionof{\formulaccwith,\tbasisat{\formulavar}}{\shortcatvariables} \, .
%	\end{align*}
%\end{proof}
%
%We call this representation scheme the $\land$-symmetry, since we can either represent $\kb$ by instantiation of $\bencodingof{\kb}$, which involves a basis encoding of the conjunction $\land$, or by instantiations of a collection of $\bencodingof{\formula}$.
%We use the $\land$ symmetry to represent them as a contraction of the formulas building the Knowledge Base as conjunction.
%
%\begin{remark}{$\land$ symmetry does not generalize to Markov Logic Networks} % Strange -> Drop?
%	% Comparison to Markov Logic
%	In Markov Logic Networks, similar decompositions are not possible.
%	For example, consider a MLN with a single formula $\atomicformulaof{0}\land\atomicformulaof{1}$ and nonvanishing weight $\canparam$.
%	This does not coincide with the distribution of a MLN of two formulas $\atomicformulaof{0}$ and $\atomicformulaof{1}$.
%	To see this, we notice that with respect to the distribution of the first MLN, both variables are not independent, while for any MLN constructed by the two atomic formulas they are.
%	% Can also be understood based on non-elementary contraction of $\land$ !
%\end{remark}


%\begin{theorem}[$\land$-symmetry]\label{the:landSymmetry}
%	We observe that the contraction of an $\land$ core with $\tbasis$  is equivalent with $\tbasis$ cores on all the connected subformulas.
%\end{theorem}
%\begin{proof}
%	By equality of the Knowledge Base contraction in both ways: The missing subformulas behave the same if they are activated, since they then are contrained to the same subnetworks somewhere else. 
%	%\red{Find better arguments for missing subformulas when having the larger core.}
%\end{proof}
%
%\begin{theorem}[$\lnot$-symmetry]
%	Similarly the contraction of an $\lnot$ core with $\tbasis$ or $\tbasis$ has the same result as with $\tbasis$ or $\tbasis$ on the subformula.
%\end{theorem}
%
%We call the application of these in changing the Knowledge Cores without changing the contracted network as the representation symmetry.


%\subsect{Conjunctive Normal Representation}

%One tensor representation of a Knowledge Base is the association of the Knowledge Core $\tbasis$ at the formula being the Knowledge Base itself.
%We can use the $\land$ symmetry (Theorem~\ref{the:landSymmetry}) to propagate $\tbasis$ to all clause cores and get an alternative representation.
%Those are especially interesting when using Modus Ponens/Resolution as local sub-KB reasoners (see \secref{subsec:LocalEntailment}).

\sect{Hybrid Logic Network}\label{sec:hybridNetworks}

Markov Logic Networks are by definition positive distributions and are represented by positive activation tensors in $\elrealizabledistsof{\mlnstat}$ (see \figref{fig:elementaryComputableSketch}).
In contrary, Hard Logic Networks model uniform distributions over model sets of the respective Knowledge Base and therefore have vanishing coordinates.
They are represented by boolean activation tensors in $\elrealizabledistsof{\mlnstat}$.
Having discussed these representation approaches, let us now investigate the general case of distributions in $\elrealizabledistsof{\mlnstat}$, which we call Hybrid Logic Networks.

\subsect{Definition}

\begin{definition}
    \label{def:hln}
    Given a boolean statistic $\mlnstat$ %of propositional formulas
    %\begin{align*}
    %    \mlnstat : \atomstates \rightarrow \bigtimes_{\selindexin}[2]
    %\end{align*}
    any element of $\elrealizabledistsof{\mlnstat}$ is called a Hybrid Logic Network (HLN).
\end{definition}

We now extract from a Hybrid Logic Network, defined as a distribution computable by $\mlnstat$ and $\elgraph$, a Hard Logic Network and a Markov Logic Network, which contraction builds the Hybrid Logic Network.

\begin{theorem}
    \label{the:hybridNetworkRepresentation}
    Any Hybrid Logic Network $\probwith\in\elrealizabledistsof{\mlnstat}$ is a normalized contraction of a Hard Logic Network and a Markov Logic Network, that is there are for $\selindexin$ boolean cores $\kcoreofat{\selindex}{\headvariableof{\selindex}}$ and a weight vector $\canparamat{\selvariable}$ such that
    \begin{align*}
        \probwith =
        \normalizationof{
            \{\bencodingofat{\mlnstat}{\sstatheadvariables,\shortcatvariables}\}
            \cup \left(\bigcup_{\selindexin}\{\kcoreofat{\selindex}{\headvariableof{\selindex}},\actcoreofat{\selindex,\canparamat{\indexedselvariable}}{\headvariableof{\selindex}}\}\right)
        }{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    We show the claim by splitting the activation cores into a contraction of hard (i.e. logical) and soft (i.e. probabilistic) activation cores.
    Since $\probwith\in\elrealizabledistsof{\mlnstat}$ we find for $\selindexin$ non-negative vectors $\hypercoreofat{\selindex}{\headvariableof{\selindex}}$ such that
    \begin{align*}
        \probwith =
        \normalizationof{
            \{\bencodingofat{\mlnstat}{\sstatheadvariables,\shortcatvariables}\}
            \cup \{\hypercoreofat{\selindex}{\headvariableof{\selindex}}\wcols\selindexin\}
        }{\shortcatvariables} \, .
    \end{align*}
    We now define for $\selindexin$ boolean cores by
    \begin{align*}
        \kcoreofat{\selindex}{\headvariableof{\selindex}} =
        \greaterzeroof{\hypercoreofat{\selindex}{\headvariableof{\selindex}}} \, .
    \end{align*}
    And a canonical parameter vector $\canparamwith$ by
    \begin{align*}
        \canparamat{\indexedselvariable} =
        \begin{cases}
            0 & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}}\neq\onesat{\headvariableof{\selindex}} \\
            \lnof{\frac{\hypercoreofat{\selindex}{\headvariableof{\selindex}=1}}{\hypercoreofat{\selindex}{\headvariableof{\selindex}=0}}} & \text{else} \\
        \end{cases} \, .
    \end{align*}
    For all $\selindexin$ we have that $\canparamat{\indexedselvariable}$ is well-defined, since by assumption $\hypercoreofat{\selindex}{\headvariableof{\selindex}}$ is positive, if $\kcoreofat{\selindex}{\headvariableof{\selindex}}=\onesat{\headvariableof{\selindex}}$.
    If for $\selindexin$ we have $\kcoreofat{\selindex}{\headvariableof{\selindex}}\neq\onesat{\headvariableof{\selindex}}$, then $\hypercoreofat{\selindex}{\headvariableof{\selindex}}\in\{\tbasisat{\headvariableof{\selindex}},\fbasisat{\headvariableof{\selindex}}\}$, since $\hypercoreofat{\selindex}{\headvariableof{\selindex}}=\zerosat{\headvariableof{\selindex}}$ would lead to a vanishing contraction with the basis encoding of $\mlnstat$.
    Then is a real number $\lambda_{\selindex}>0$ such that
    \begin{align*}
        \hypercoreofat{\selindex}{\headvariableof{\selindex}}
        = \lambda_{\selindex} \cdot \kcoreofat{\selindex}{\headvariableof{\selindex}}
    \end{align*}
    and, since $\actcoreofat{\selindex,0}{\headvariableof{\selindex}} = \onesat{\headvariableof{\selindex}}$ also
    \begin{align*}
        \hypercoreofat{\selindex}{\headvariableof{\selindex}}
        = \lambda_{\selindex} \cdot \contractionof{\kcoreofat{\selindex}{\headvariableof{\selindex}},\actcoreofat{\selindex,\indexedcanparam}{\headvariableof{\selindex}}}{\headvariableof{\selindex}} \, .
    \end{align*}
    Conversely, if for $\selindexin$ we have $\kcoreofat{\selindex}{\headvariableof{\selindex}}=\onesat{\headvariableof{\selindex}}$, then there is a scalar $\lambda_{\selindex}>0$ such that
    \begin{align*}
        \hypercoreofat{\selindex}{\headvariableof{\selindex}}
        &= \lambda_{\selindex} \cdot \actcoreofat{\selindex,\indexedcanparam}{\headvariableof{\selindex}} \\
        &= \lambda_{\selindex} \cdot \contractionof{\kcoreofat{\selindex}{\headvariableof{\selindex}},\actcoreofat{\selindex,\indexedcanparam}{\headvariableof{\selindex}}}{\headvariableof{\selindex}} \, . \\
    \end{align*}
    We therefore have
    \begin{align*}
        &\contractionof{
            \{\bencodingofat{\mlnstat}{\sstatheadvariables,\shortcatvariables}\}
            \cup \{\hypercoreofat{\selindex}{\headvariableof{\selindex}}\wcols\selindexin\}
        }{\shortcatvariables}\\
        &\quad =\left(\prod_{\selindexin}\lambda_{\selindex}\right)\cdot\contractionof{
            \{\bencodingofat{\mlnstat}{\sstatheadvariables,\shortcatvariables}\}
            \cup \left(\bigcup_{\selindexin}\{\kcoreofat{\selindex}{\headvariableof{\selindex}},\actcoreofat{\selindex,\canparamat{\indexedselvariable}}{\headvariableof{\selindex}}\}\right)
        }{\shortcatvariables}
    \end{align*}
    and the claim follows, since the normalization of a tensor is invariant under multiplication with the positive scalar $\left(\prod_{\selindexin}\lambda_{\selindex}\right)$.
\end{proof}

We now relate with the formalism of exponential families and determine the base measures which are used to build Hybrid Logic Networks.


%\begin{lemma}
%    To any Hybrid Logic Network $\probwith\in\elrealizabledistsof{\mlnstat}$ we find $\variableset\subset[\seldim]$ and $\headindexof{\variableset}\in\bigtimes_{\selindex\in\variableset}[2]$ such that
%    \begin{align*}
%        \probwith \in \expfamilyof{\mlnstat,\hlnformula} \, ,
%    \end{align*}
%    where
%    \begin{align*}
%        \hlnformula \coloneqq
%        \left(\bigwedge_{\selindex\in\variableset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
%        \land
%        \left(\bigwedge_{\selindex\in\variableset\wcols\headindexof{\selindex}=1} \lnot\enumformulaat{\shortcatvariables}\right) \, .
%    \end{align*}
%\end{lemma}


\begin{theorem}
    Given a boolean statistic $\mlnstat$, we build for any $\variableset\subset[\seldim]$ and $\headindexof{\variableset}\in\bigtimes_{\selindex\in\variableset}[2]$ the formula
%    %for any $\meanparamwith\in\{0,0.5,1\}^{\seldim}$ the formula
    \begin{align*}
        \hlnformula \coloneqq
        \left(\bigwedge_{\selindex\in\variableset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
        \land
        \left(\bigwedge_{\selindex\in\variableset\wcols\headindexof{\selindex}=1} \lnot\enumformulaat{\shortcatvariables}\right) \, .
    \end{align*}
    Then we have
    \begin{align*}
        \elrealizabledistsof{\mlnstat}
        = \bigcup_{\variableset\subset[\seldim]} \bigcup_{\headindexof{\variableset}\in\bigtimes_{\selindex\in\variableset}[2]\wcols\contraction{\hlnformula}>0} \expfamilyof{\mlnstat,\hlnformula} \, ,
       % = \bigcup_{\meanparamwith\in\{0,0.5,1\}^{\seldim}\wcols\contraction{\formulaof{\mlnstat,\meanparamwith}}>0} \expfamilyof{\mlnstat,\hlnformula} \, ,
    \end{align*}
    where the union is over the satisfiable formulas $\formulaof{\mlnstat,\meanparamwith}$.
\end{theorem}
\begin{proof}
    % Base Measure as formula
    "$\subset$":\\
    To any Hybrid Logic Network $\probwith$ in $\elrealizabledistsof{\mlnstat}$ we find by \theref{the:hybridNetworkRepresentation} hard activation cores $\{\kcoreofat{\selindex}{\headvariableof{\selindex}}\wcols\selindexin\}$.
    We build a set
    \begin{align*}
        \variableset =
        \left\{\selindex\wcols\kcoreofat{\selindex}{\headvariableof{\selindex}}\in\{\fbasisat{\headvariableof{\selindex}},\tbasisat{\headvariableof{\selindex}}\}\right\}
    \end{align*}
    and a boolean tuple $\headindexof{\variableset}$ by
    \begin{align*}
        \headindexof{\selindex} =
        \begin{cases}
            0 & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}}=\fbasisat{\headvariableof{\selindex}} \\
            1 & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}}=\tbasisat{\headvariableof{\selindex}} \\
        \end{cases} \, .
    \end{align*}
%    We build a vector $\meanparamwith\in\{0,0.5,1\}^{\seldim}$ with coordinates
%    \begin{align*}
%        \indexedmeanparam =
%        \begin{cases}
%            0 & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}}=\fbasisat{\headvariableof{\selindex}} \\
%            0.5 & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}}=\onesat{\headvariableof{\selindex}} \\
%            1 & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}}=\tbasisat{\headvariableof{\selindex}} \\
%        \end{cases}
%    \end{align*}
    For the constructed pair $(\variableset,\headindexof{\variableset})$ it holds that
    \begin{align*}
        \contractionof{\{\bencodingofat{\mlnstat}{\sstatheadvariables,\shortcatvariables}\}
        \cup \{\kcoreofat{\selindex}{\headvariableof{\selindex}} \wcols \selindexin\}
        }{\shortcatvariables}
        &=
        \left(\bigwedge_{\selindex\in\variableset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
        \land
        \left(\bigwedge_{\selindex\in\variableset\wcols\headindexof{\selindex}=1} \lnot\enumformulaat{\shortcatvariables}\right) \\
        &= \hlnformulawith \, .
    \end{align*}
    We notice, that $\hlnformulawith$ is satisfiable, since $\probwith$ is representable with respect to the base measure $\hlnformulawith$.
    It follows that $\probwith$ is an element of the exponential family $\expfamilyof{\mlnstat,\hlnformula}$.
    Since $\probwith$ was an arbitary element of $\elrealizabledistsof{\mlnstat}$, we have shown the inclusion
    \begin{align*}
        \elrealizabledistsof{\mlnstat}
        \subset \bigcup_{\variableset\subset[\seldim]} \bigcup_{\headindexof{\variableset}\in\bigtimes_{\selindex\in\variableset}[2]\wcols\contraction{\hlnformula}>0} \expfamilyof{\mlnstat,\hlnformula} \, ,
    \end{align*}

    "$\supset$":\\
    Conversely, given an arbitrary pair $(\variableset,\headindexof{\variableset})$ we choose an arbitary element
    \begin{align*}
        \probofat{(\mlnstat,\canparam,\hlnformula)}{\shortcatvariables} \in \expfamilyof{\mlnstat,\hlnformula} \, .
    \end{align*}
    We build for $\selindexin$ the knowledge cores
    \begin{align*}
        \kcorewith =
        \begin{cases}
            \onesat{\headvariableof{\selindex}} & \text{if} \quad \selindex\notin\variableset \\
            \tbasisat{\headvariableof{\selindex}} & \text{if} \quad \selindex\in\variableset \quad \text{and} \quad \headindexof{\selindex}=1 \\
            \fbasisat{\headvariableof{\selindex}} & \text{if} \quad \selindex\in\variableset \quad \text{and} \quad \headindexof{\selindex}=0 \\
        \end{cases}
    \end{align*}
    Then we have $\probofat{(\mlnstat,\canparam,\hlnformula)}{\shortcatvariables}\in\elrealizabledistsof{\mlnstat}$ since
    \begin{align*}
        \probwith =
        \contractionof{\{\mlnstatccwith\}\cup\left(\bigcup_{\selindexin}\{\kcorewith,\actcorewith\}\right)}{\shortcatvariables} \, .
    \end{align*}
    This shows that
    \begin{align*}
        \bigcup_{\variableset\subset[\seldim]} \bigcup_{\headindexof{\variableset}\in\bigtimes_{\selindex\in\variableset}[2]\wcols\contraction{\hlnformula}>0} \expfamilyof{\mlnstat,\hlnformula}
        \subset \elrealizabledistsof{\mlnstat} \, . & \qedhere
    \end{align*}
\end{proof}

% COMMENT: 0.5 a dummy variable
%We strengthen, that $\meanparamwith$ does not coincide with the mean parameter of the Hybrid Logic Network, since here $0.5$ has been chosen as a dummy value whereas the index is in the open interval $(0,1)$.

% Base Measure trick
We can thus understand Hybrid Logic Networks, as a union of exponential families, where the union is over all boolean activation cores understood as generating a base measure.
This trick is known to the field of variational inference, see for Example~3.6 in \cite{wainwright_graphical_2008}.
For an example of a tensor network representation of a hybrid logic network see \figref{fig:ActivatedHeads}.






\begin{figure}[h]
    \begin{center}
        \input{PartII/tikz_pics/network_representation/activated_heads.tex}
    \end{center}
    \caption{Tensor network representation of a Hybrid Logic Network with statistics $\mlnstat=(\catvariableof{a}\lor\catvariableof{b},\catvariableof{a}\lor\catvariableof{b}\lor \lnot c)$, following \theref{the:hybridNetworkRepresentation}.
    Besides the computation cores required to compute the statistics, we have one non-trivial \textcolor{\probcolor}{probabilistic soft activation core $\actcoreofat{0,\canparamat{\selvariable=0}}{\headvariableof{a\lor b}}$} and one non-trivial \textcolor{\concolor}{constraint hard activation core $\kcoreofat{1}{\headvariableof{a\lor b \lor \lnot c}}$}.
    The trivial activation cores $\kcoreofat{0}{\headvariableof{a\lor b}}$ and $\actcoreofat{1,0}{\headvariableof{a\lor b \lor \lnot c}}$ are omitted from the diagram, since leaving the contraction invariant.}
    \label{fig:ActivatedHeads}
\end{figure}

%\begin{remark}{Probability interpretation using the Partition function}
%    The tensor networks here represent unnormalized probability distributions.
%    The probability distribution can be normed by the quotient with the naive contraction of the network, the partition function.
%\end{remark}


\subsect{Logical Reasoning Properties}

Deciding probabilistic entailment (see \defref{def:probEntailment}) with respect to Hybrid Logic Networks can be reduced to the hard logic parts of the network.

\begin{theorem}
    \label{the:hlnEntailmentReduction}
    Let $\probofat{(\mlnstat,\canparam,\hlnformula)}{\shortcatvariables}$ be a Hybrid Logic Network.
    Given a query formula $\exformula$ we have that
    \begin{align*}
        \probtensorof{\mlnstat,\canparam,\hlnformula} \models \exformula
    \end{align*}
    if and only if
    \[ \hlnformula \models \exformula \, . \]
\end{theorem}
\begin{proof}
    This follows from Theorem~\ref{the:factorReduction} on the representation of Hybrid Logic Networks as exponential distributions in \theref{the:hybridNetworkRepresentation}.
\end{proof}


Formulas in $\softformulaset$, which are entailed or contradicted by $\hardformulaset$ are redundant, as we show next.

\begin{theorem}%\label{the:hlnRepRedundancy}
    If for a Hybrid Logic Network $\probof{(\formulaset,\canparam,\hlnformula)}$ and any formula $\exformula\in\formulaset$ we have
    \[ \hlnformula \models \exformula \, \quad \text{or} \quad \hlnformula \models \lnot\exformula \]
    then
    \[ \probofat{(\formulaset/\{\exformula\},\tilde{\canparam},\hlnformula)}{\shortcatvariables} =  \probofat{(\formulaset,\canparam,\hlnformula)}{\shortcatvariables}  \, , \]
    where $\tilde{\canparam}$ denotes the tensor $\canparam$, where the coordinate to $\exformula$ is dropped. %, if $\exformula\in\softformulaset$.
\end{theorem}
\begin{proof}
    Isolate the factor to the hard formula, which is constant for all situations.
\end{proof}

%% Now in the 
A similar statement holds for the hard formulas itself, as shown in Theorem~\ref{the:ReduncancyOfEntailed}.
%However, notice that if $\hardformulaset/\{\exformula\}\models\lnot\exformula$, then $\hardformulaset\cup\{\exformula\}$ is not satisfiable and a hybrid logic network cannot be defined for $\hardformulaset\cup\{\exformula\}$ as hard logic formulas.

%If the conjunction of $\hardformulaset/\{\exformula\}$ entails $\exformula$, we can erase $\exformula$ from $\hardformulaset$ without changing the contraction, therefore without changing the base measure of the Hybrid Logic Network.

% Utility in Contraction KB implementation
These results are especially interesting for the efficient implementation of \algoref{alg:contractionKB}, which has been introduced in \charef{cha:logicalReasoning}.
By Theorem~\ref{the:hlnEntailmentReduction} only the hard logic parts of a Hybrid Logic Network are required in the ASK operation.
%Theorem~\ref{the:hlnRepRedundancy} for the TELL operation, but already discussed in more generality

\subsect{Expressivity}

Hybrid Logic Networks extend the expressivity result of Theorem~\ref{the:mintermExpressivityMLN} to arbitrary probability tensors, dropping the positivity constraints for Markov Logic Networks.

\begin{theorem}
    \label{the:mintermExpressivityHLN}
    Let $\probat{\shortcatvariables}$ a (possibly not positive) probability tensor.
    We build a base measure
    \[ \basemeasure = \nonzeroof{\probat{\shortcatvariables}} \]
    and a parameter tensor
    \begin{align*}
        \canparamat{\selvariableof{[\catorder]}=\shortcatindices}
        = \begin{cases}
              0 & \text{if} \quad \probat{\indexedshortcatvariables} = 0  \\
              \lnof{\probat{\indexedshortcatvariables}} & \text{else}
        \end{cases} \, .
    \end{align*}
    Then the probability tensor is the member of the minterm exponential family with base measure $\hardformulaset$ and parameter $\canparam$, that is
    \[ \probwith = \probofat{(\mintermformulaset,\canparam,\hfbasemeasure)}{\shortcatvariables} \, . \]
\end{theorem}
\begin{proof}
    It suffices to show that
    \[ \contractionof{\hfbasemeasure, \expof{\contractionof{
        \sencodingof{\mintermformulaset}\canparam
    }{
        \shortcatvariables
    }}}{\shortcatvariables} = \probat{\shortcatvariables} \, . \]
    For indices $\shortcatindices$ with $\probat{\indexedshortcatvariables}=0$ we have $\hfbasemeasureat{\indexedshortcatvariables}=0$ and thus also
    \[ \contractionof{\hfbasemeasure, \expof{\contractionof{
        \sencodingof{\mintermformulaset}\canparam
    }{
        \shortcatvariables
    }}}{\indexedshortcatvariables} = 0 \, . \]
    For indices $\shortcatindices$ with $\probat{\indexedshortcatvariables}>0$ we have $\hfbasemeasureat{\indexedshortcatvariables}=1$ and
    \begin{align*}
        \contractionof{\hfbasemeasure, \expof{\contractionof{
            \sencodingof{\mintermformulaset},\canparam
        }{
            \shortcatvariables
        }}}{\indexedshortcatvariables}
        &= \prod_{\selindexof{[\catorder]}} \expof{\canparamat{\selvariableof{[\catorder]}=\selindexof{[\catorder]}} \cdot \mintermofat{\selindexof{[\catorder]}}{\indexedshortcatvariables}} \\
        &=  \expof{\canparamat{\selvariableof{[\catorder]}=\shortcatindices}} \\
        &=  \probat{\indexedshortcatvariables} \, .
    \end{align*}
\end{proof}



\sect{Polynomial Representation}

%We now apply the representation symmetries to represent a propositional Knowledge Base in conjunctive normal form.
We now sparse representation formats for the introduced logic networks, namely the basis+ $\cpformat$ format introduced in \charef{cha:sparseRepresentation} which are understood as polynomial decompositions.
First of all, we establish a sparsity result for terms and clauses (see \defref{def:clauses}).

\begin{lemma}
    \label{lem:clauseTermBasisPlus}
    Any term is representable by a single monomial and any clause is representable by a sum of at most two monomials. %, any term of basis+ with rank 1. %Use also \baspluscprankof{}
\end{lemma}
\begin{proof}
    Let $\nodes_0$ and $\nodes_1$ be disjoint subsets of $\nodes$, then we have
    \begin{align*}
        \termof{\nodes_0}{\nodes_1} = \onehotmapofat{
            \{\catindexof{\atomenumerator} = 0 : \atomenumerator\in\nodes_0\} \cup \{\catindexof{\atomenumerator} = 1 : \atomenumerator\in\nodes_1\}
        }{\catvariableof{\nodes_0\cup\nodes_1}} \otimes \onesat{\catvariableof{\nodes/(\nodes_0\cup\nodes_1)}}
    \end{align*}
    and
    \begin{align*}
        \clauseof{\nodes_0}{\nodes_1} = \onesat{\catvariableof{\nodes}} - \onehotmapofat{
            \{\catindexof{\atomenumerator} = 0 : \atomenumerator\in\nodes_0\} \cup \{\catindexof{\atomenumerator} = 1 : \atomenumerator\in\nodes_1\}
        }{\catvariableof{\nodes_0\cup\nodes_1}}
        \otimes \onesat{\catvariableof{\nodes/(\nodes_0\cup\nodes_1)}} \, .
    \end{align*}
    We notice, that any tensors $\ones$ and $\onehotmapof{\catindex}\otimes \ones$ habe basis+-rank of $1$ and therefore $\termof{\nodes_0}{\nodes_1}$ of $1$ and $\clauseof{\nodes_0}{\nodes_1}$ of at most $2$.
\end{proof}

A formula in conjunctive normal form is a conjunction of clauses, where clauses are disjunctions of literals being atoms (positive literals) or negated atoms (negative literals).
Based on these normal forms, we show representations of formulas as sparse polynomials. %, which will be discussed in more detail in \charef{cha:sparseRepresentation} (see \defref{def:polynomialSparsity}).
We apply \lemref{lem:clauseTermBasisPlus} to show the following sparsity bound. % on the energy tensor of Markov Logic Networks.

\begin{theorem}
    \label{the:formulaSlicePolynomialDecomposition}
    Any formula $\exformula$ with a conjunctive normal form of $\clausedim$ clauses satisfies
    \[ \slicesparsityof{\exformula} \leq 2^{\clausedim} \, . \]
\end{theorem}
\begin{proof}
    Let $\exformula$ have a conjunctive normal form with clauses indexed by $\clauseenumeratorin$ and each clause represented by subsets $\nodes_0^\clauseenumerator, \nodes_1^\clauseenumerator$, that is
    \[ \exformula = \bigwedge_{\clauseenumeratorin} \clauseof{\nodes_0^\clauseenumerator}{\nodes_1^\clauseenumerator} \, . \]
    We now use the rank bound of \theref{the:CPrankContractionBound} and \lemref{lem:clauseTermBasisPlus} to get
    \begin{align*}
        \slicesparsityof{\exformula} \leq \prod_{\clauseenumeratorin} \slicesparsityof{\clauseof{\nodes_0^\clauseenumerator}{\nodes_1^\clauseenumerator}} \leq 2^{\clausedim} \, .
    \end{align*}
\end{proof}

We apply this result on the sparse representation of a single formula to derive sparse representations for Hard Logic Networks and the energy tensor of Hybrid Logic Networks.
Both results use in addition to \theref{the:formulaSlicePolynomialDecomposition} sparsity bounds, which are shown by explicit representation construction in \charef{cha:sparseRepresentation}.

\begin{corollary}
    Any Hard Logic Network $\hardformulaset$ obeys
    \begin{align*}
        \slicesparsityof{\hardformulaset} \leq \prod_{\exformula\in\hardformulaset} 2^{\clausedimof{\exformula}}
    \end{align*}
\end{corollary}
\begin{proof}
    We apply the contraction bound \theref{the:CPrankContractionBound} for the decomposition
    \begin{align*}
        \kbat{\shortcatvariables} = \contractionof{\{\formulaat{\shortcatvariables} \, : \, \formula\in\hardformulaset\}}{\shortcatvariables}
    \end{align*}
    and get
    \begin{align*}
        \slicesparsityof{\kb} \leq \prod_{\formula\in\hardformulaset} \slicesparsityof{\formula} \, .
    \end{align*}
    The claimed bound follows with \theref{the:formulaSlicePolynomialDecomposition}.
\end{proof}

\begin{corollary}
    The energy tensor of a Hybrid Logic Network with statistic $\mlnstat$
    \begin{align*}
        \slicesparsityof{\contractionof{\sencmlnstatwith,\canparamwith}{\shortcatvariables}} \leq \sum_{\selindexin\,:\,\canparamat{\indexedselvariable}\neq0} 2^{\clausedimof{\enumformula}} \, .
    \end{align*}
    where $\clausedimof{\enumformula}$ denotes the number of clauses in a conjunctive normal form of $\enumformula$.
\end{corollary}
\begin{proof}
    We decompse the energy into the sum
    \begin{align*}
        \contractionof{\sencmlnstatwith,\canparamwith}{\shortcatvariables}
        = \sum_{\selindexin\,:\,\canparamat{\indexedselvariable}\neq0} \canparamat{\indexedselvariable} \cdot \enumformulawith
    \end{align*}
    and apply \theref{the:CPrankSumBound} to get
    \begin{align*}
        \slicesparsityof{\contractionof{\sencmlnstatwith,\canparamwith}{\shortcatvariables}}
        \leq \sum_{\selindexin\,:\,\canparamat{\indexedselvariable}\neq0} \slicesparsityof{\enumformulawith}
        \leq \sum_{\selindexin\,:\,\canparamat{\indexedselvariable}\neq0}2^{\clausedimof{\enumformula}} \, .
    \end{align*}
\end{proof}


\sect{Mean parameters of Hybrid Logic Networks}

% Polytope
While mean parameter polytopes $\genmeanset$ to generic exponential families have been subject to \charef{cha:probReasoning}, we in this section restrict to the mean polytopes of hybrid logic networks, which we characterize using propositional logics.
Hybrid Logic Networks are exponential families, which statistic $\sstat$ consists of coordinates with $\imageof{\sstatcoordinateof{\selindex}}\subset\ozset$ and are therefore propositional formulas.
The convex polytope of mean parameters (see \defref{def:meanPolytope}) is for a statistic $\mlnstat$ of propositional formulas and a base measure $\basemeasure$ the set
\begin{align*}
    \hlnmeanset = \left\{ \contractionof{\probwith,\sencmlnstatwith}{\selvariable} \wcols \probwith\in\bmrealprobof{\basemeasure} \right\} \, ,
\end{align*}
where by $\bmrealprobof{\basemeasure}$ we denote the set of all by $\basemeasure$ representable probability distributions.
By \theref{the:meanPolytopeConvHull} the convex polytope has a characterization as a convex hull
\begin{align}
    \label{eq:hlnMeansetConvCharacterization}
    \hlnmeanset
    = \convhullof{\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable} \wcols \shortcatindices\in\atomstates, \, \basemeasureat{\indexedshortcatvariables}=1} \, .
\end{align}

% 0/1 Polytopes
We notice, that for boolean statistics $\formulaset$ all $\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}$ are boolean vectors in $\parspace$.
The mean parameter polytopes are thus $0/1$-polytopes \cite{ziegler_lectures_2000,gillmann_01-polytopes_2007}, from which a few obvious properties follow.
Since those are convex subsets of the cube $[0,1]^\seldim$, which vertices are all boolean vectors in $\parspace$, also each $\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}$ (with $\basemeasureat{\indexedshortcatvariables}=1$) is a vertex.
Further, if for any $\selindexin$ we have $\meanparamat{\indexedselvariable}\in\ozset$, then $\meanparamwith$ is on a proper face of the cube and thus also of $\hlnmeanset$.
\red{In the following, we characterize the faces of the mean parameter polytope.}

\subsect{Vertices by hard logic networks} % -> More general: Face measures by hard logic networks

First of all, we show that the vertices of $\hlnmeanset$ are the boolean vectors contained in $\hlnmeanset$.

\begin{theorem}
    \label{the:vertexByHardLogicNetworks}
    Given a boolean statistic $\formulaset$, a base measure $\basemeasurewith$ and let $\meanparamwith\in\hlnmeanset$.
    Then the following are equivalent:
    \begin{itemize}
        \item[(i)] The set $\{\meanparamwith\}$ is a vertex of $\hlnmeanset$
        \item[(ii)] $\meanparamwith$ is boolean
    \end{itemize}
\end{theorem}
\begin{proof}
(i)
    $\Rightarrow$(ii):
    We use that any set of vectors, which convex hull forms a polytope, contains all vertices of that polytope (see Proposition 2.2 in \cite{ziegler_lectures_2013}).
    %For any polytope, and any set of vectors spanning the polytope as convex hull, all vertices are contained in all sets, which convex hull is $\hlnmeanset$,
    Since by definition $\hlnmeanset$ is the convex hull of boolean vectors $\sencmlnstatat{\indexedshortcatvariables,\selvariable}$, all vertices are boolean vectors.

    (ii)$\Rightarrow$(i):
    Since $\hlnmeanset$ is contained in $\fullparcube$ and all boolean vectors in $\fullparcube$ are vertices, whenever a boolean vector is contained in $\hlnmeanset$ it is a vertex.
\end{proof}

We can further provide an equivalent criterion for a vertex by the satisfaction of a corresponding formula.

\begin{theorem}
    Given a boolean statistic $\formulaset$, a base measure $\basemeasurewith$ and let $\meanparamwith$ be a boolean vector.
    Then the following are equivalent:
    \begin{itemize}
        \item[(i)] The set $\{\meanparamwith\}$ is a vertex of $\hlnmeanset$.
        \item[(ii)] The formula
        \begin{align*}
            \vertexformula = \bigwedge_{\selindexin} \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables}
        \end{align*}
        is satisfiable.
        \item[(iii)] There is a Hard Logic Network $\probwith\in\hlnsetof{\mlnstat}$ such that
        \begin{align*}
            \meanparamwith = \contractionof{\probwith,\sencmlnstatwith}{\selvariable} \, .
        \end{align*}
    \end{itemize}
    Here we denote by $\lnot^0$ the identity connective and by $\lnot^1=\lnot$ the logical negation.
\end{theorem}
\begin{proof}
    (i)$\Rightarrow$(ii):
    If $\{\meanparamwith\}$ is a vertex of $\hlnmeanset$, then there is $\shortcatindices\in\atomstates$ such that % in particular $\meanparamwith\in$.
    \begin{align*}
        \meanparamwith = \sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable} \, .
    \end{align*}
    For all $\selindexin$ we have
    \begin{align*}
        \lnot^{(1-\meanparamat{\indexedselvariable})}\enumformulaat{\indexedshortcatvariables} \, .
    \end{align*}
    The index $\shortcatindices$ is therefore a model of $\vertexformula$ and $\vertexformula$ is satisfiable.

    (ii)$\Rightarrow$(iii):
    We choose for $\selindexin$
    \begin{align*}
        \kcorewith = \begin{cases}
                         \fbasisat{\headvariableof{\selindex}} & \text{if} \quad \meanparamat{\indexedselvariable}=0 \\
                         \tbasisat{\headvariableof{\selindex}} & \text{if} \quad \meanparamat{\indexedselvariable}=1 \\
        \end{cases}
    \end{align*}
    and notice that
    \begin{align*}
        \normalizationof{\{\bencodingofat{\mlnstat}{\sstatheadvariables,\shortcatvariables}\}
        \cup \{\kcoreofat{\selindex}{\headvariableof{\selindex}} \wcols \selindexin\}
        }{\shortcatvariables} = \normalizationof{\vertexformula}{\shortcatvariables} \, .
    \end{align*}
    Thus, the normalized formula $\vertexformula$ is a Hard Logic Network.
    We then have for any $\selindexin$ that
    \begin{align*}
        \meanparamat{\indexedselvariable} = \contraction{\normalizationof{\vertexformula}{\shortcatvariables},\enumformulawith}\, .
    \end{align*}

    (iii)$\Rightarrow$(i):
    Since $\probwith$ reproduces $\meanparamwith$ we have $\meanparamwith\in\hlnmeanset$ and since $\meanparamwith$ is boolean by \theref{the:vertexByHardLogicNetworks} $\{\meanparamwith\}$ is a vertex.
\end{proof}

% Hard Logic Network representation -> Another theorem?
%Let us notice, that the normalization of $\vertexformula$ is a Hard Logic Network using the statistic $\formulaset$ and the base measure $\basemeasure$.
%Any model of this formula is furthermore mapped by the statistics encoding onto the corresponding vertex $\{\meanparamwith\}$ and the mean parameter of the Hard Logic Network is the unique vertex element $\meanparamwith$.
%We will in \charef{cha:networkReasoning} show, that among the distributions having the vertex as mean parameter, the corresponding Hard Logic Network is the one with maximal entropy.




\subsect{Faces represented by Hard Logic Networks}

With \theref{the:vertexByHardLogicNetworks} we showed that all vertices are reproduced by Hard Logic Networks.
In general, as we show next, Hard Logic Networks are face measures to a set of faces which we characterize in the next theorem.

\begin{theorem}
    \label{the:faceMeasureHardLogicNetworks}
    Let $\genfaceset$ be a face of $\hlnmeanset$, then the following are equivalent:
    \begin{itemize}
        \item[(i)] There is a face $\cubeface$ of $\fullparcube$ such that $\genfaceset=\hlnmeanset\cap\cubeface$.
        \item[(ii)] There is a Hard Logic Network which coincides with the normalized face measure $\normalizationof{\basemeasureof{\formulaset,\facecondset}}{\shortcatvariables}$.
        \item[(iii)] The normalized face measure $\normalizationof{\basemeasureof{\formulaset,\facecondset}}{\shortcatvariables}$ is in $\realizabledistsof{\sstat,\elgraph,\basemeasure}$.
    \end{itemize}
\end{theorem}
\begin{proof}
(i)
    $\Rightarrow$(ii):
    Given the face $\cubeface$ such that $\genfaceset=\hlnmeanset\cap\cubeface$, we construct a boolean and basis+ elementary tensor (see \charef{cha:sparseRepresentation}), such that
    \begin{align*}
        \basemeasureofat{\sstat,\facecondset}{\shortcatvariables}
        = \contractionof{\bencodingofat{\formulaset}{\sstatheadvariables,\shortcatvariables},\kcoreofat{\facecondset}{\sstatheadvariables}}{\shortcatvariables} \, .
    \end{align*}
    For each $\selindexin$ we build the vectors
    \begin{align*}
        \kcoreofat{\selindex}{\headvariableof{\selindex}}
        = \begin{cases}
              \tbasisat{\headvariableof{\selindex}} & \text{if} \quad \uniquantwrtof{\meanparam\in\cubeface}{\meanparamat{\indexedselvariable}=1} \\
              \fbasisat{\headvariableof{\selindex}} & \text{if} \quad \uniquantwrtof{\meanparam\in\cubeface}{\meanparamat{\indexedselvariable}=0} \\
              \onesat{\headvariableof{\selindex}} & \text{else}
        \end{cases}
    \end{align*}
    and get the activation tensor as
    \begin{align*}
        \kcoreofat{\facecondset}{\sstatheadvariables}
        = \bigotimes_{\selindexin} \kcoreofat{\selindex}{\headvariableof{\selindex}} \, .
    \end{align*}

    (ii)$\Rightarrow$(iii):
    By definition any Hard Logic Network is in $\realizabledistsof{\sstat,\elgraph,\basemeasure}$.

    (iii)$\Rightarrow$(i):
    If the normalized face measure $\normalizationof{\basemeasureof{\formulaset,\facecondset}}{\shortcatvariables}$ is in $\realizabledistsof{\sstat,\elgraph,\basemeasure}$,
    Then there is a boolean and elementary tensor $\kcoreofat{\facecondset}{\sstatheadvariables}$ such that the face measure is
    \begin{align*}
        \basemeasureofat{\sstat,\facecondset}{\shortcatvariables}
        = \contractionof{\bencodingofat{\formulaset}{\sstatheadvariables,\shortcatvariables},\kcoreofat{\facecondset}{\sstatheadvariables}}{\shortcatvariables} \, .
    \end{align*}
    Boolean and elementary tensors in leg-dimension two are basis+ elementary tensors.
    Given a basis+ elementary tensor decomposition of $\kcoreof{\facecondset}$ we construct a face of the cube $\fullparcube$ by sets
    \begin{align*}
        \arbsetof{\selindex} = \begin{cases}
                                   \{0\} & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}} = \fbasisat{\headvariableof{\selindex}} \\
                                   \{1\} & \text{if} \quad \kcoreofat{\selindex}{\headvariableof{\selindex}} = \tbasisat{\headvariableof{\selindex}} \\
                                   [0,1] & \text{else}
        \end{cases}
    \end{align*}
    and define a face by the cartesian product of these sets as
    \begin{align*}
        \cubeface = \bigtimes_{\selindexin} \arbsetof{\selindex} \, .
    \end{align*}
    We then notice, that
    \begin{align*}
        \contractionof{\bencodingofat{\formulaset}{\sstatheadvariables,\shortcatvariables},\kcoreofat{\facecondset}{\sstatheadvariables}}{\indexedshortcatvariables} = 1
    \end{align*}
    if and only if $\sencmlnstatat{\indexedshortcatvariables,\selvariable}\in\cubeface$.
    Therefore, the parametrized face is the intersection of $\hlnmeanset$ with $\cubeface$.
\end{proof}

% Hard Logic Network
\theref{the:faceMeasureHardLogicNetworks} characterizes the faces, which measures are reproducable by Hard Logic Networks.
The corresponding mean parameters are the face centers.


% Generalization to non-boolean features
\begin{remark}
    For non-boolean statistics $\sstat$, we can derive limited versions of \theref{the:faceMeasureHardLogicNetworks}.
    Any face, which is the intersection with faces of the distorted cube
    \begin{align*}
        \bigtimes_{\selindexin} [\min_{\catindex}\sstatcoordinateofat{\selindex}{\indexedcatvariable},\max_{\catindex}\sstatcoordinateofat{\selindex}{\indexedcatvariable}]
    \end{align*}
    has a representation of its encoded pre-image with an activation tensor of basis+ rank 1.
    Since for leg dimensions larger than 2 the boolean tensors with elementary decomposition contain more tensors than the basis+ tensors of rank 1, we can represent further faces.
\end{remark}

\subsect{Generic faces}

\red{When the normalized face measure to a face is not reproduceable by a Hard Logic Network, it does not lie in $\realizabledistsof{\sstat,\elgraph,\basemeasure}$.
On the other side, \theref{the:faceMeasureCharacterization} implies, that in general $\sstat$ is a sufficient statistic and it thus lies in $\realizabledistsof{\sstat,\maxgraph,\basemeasure}$.
The core $\kcoreofat{\facecondset}{\headvariables}$ has then $\cpformat$ rank of at least two.}

%
%Since the by inclusion partially ordered set of faces is a graded lattice (see Theorem~2.7 in \cite{ziegler_lectures_2013}), the faces are ranked by the affine dimension of the convex hull. % ! This is not the number of included vertices.
%However, the number of vertices in each face is not determined by the rank.

In general, we can express any face measure by a propositional formula.

\begin{theorem}
    \label{the:faceMeasureCharacterizationHLN}
    Let $\genfaceset$ be a face of $\hlnmeanset$.
    Then the face measure is given by the formula
    \begin{align*}
        \hlnfacemeasure
        = \bigvee_{\meanparam\in\genfaceset\cap\boundaryparcube} \vertexformula
        = \bigvee_{\meanparam\in\genfaceset\cap\boundaryparcube}
        \bigwedge_{\selindexin} \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    By \theref{the:faceMeasureCharacterization} we have
    \begin{align*}
         \hlnfacemeasureat{\shortcatvariables}
         =\contractionof{\mlnstatccwith,\kcoreofat{\facecondset}{\headvariables}}{\shortcatvariables}
    \end{align*}
    where
    \begin{align*}
        \kcoreofat{\facecondset}{\headvariables}
        = \sum_{\meanparam\in\genfacesetof{\facecondset}\cap\imageof{\sstatencoding}} \onehotmapofat{\headindex^{\meanparam}_{[\seldim]}}{\headvariables} \, .
    \end{align*}
    We notice, that for each mean parameter $\meanparam\in\genfacesetof{\facecondset}\cap\imageof{\sstatencoding}$ the boolean tensors
    \begin{align*}
        \contractionof{\mlnstatccwith,\onehotmapofat{\headindex^{\meanparam}_{[\seldim]}}{\headvariables}}{\shortcatvariables}
    \end{align*}
    coincide with $\vertexformula$.
    Further, the formulas $\vertexformula$ have disjoint model sets and their disjunction is therefore represented by their sum.
    We therefore have
    \begin{align*}
         \hlnfacemeasureat{\shortcatvariables}
            &=\sum_{\meanparam\in\genfacesetof{\facecondset}\cap\imageof{\sstatencoding}} \vertexformulaat{\shortcatvariables} \\
            &=\bigvee_{\meanparam\in\genfaceset\cap\boundaryparcube}
            \bigwedge_{\selindexin} \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables} \, . \qedhere
    \end{align*}
%    By representing the contraction with $\kcoreofat{\facecondset}{\sstatheadvariables}$.
 %   Any basis tensors is a disjoint propositional formulas, the vertex formula $\hlnformula$ and their sum is thus represented by the disjunction.
\end{proof}


\subsect{Mean parameters reproducable by Hybrid Logic Networks}\label{sec:HLNrepMean} % Generalize to any mean parameter reproduction by hybrid logic networks

% Check: Where to define cube face and effective interior
Based on the representation results for face measures, we now study the mean parameters of Hybrid Logic Networks.

\begin{definition}\label{def:HLNrepMean}
    We say that a mean parameter $\meanparamwith\in\hlnfaceset$ is reproducable by a Hybrid Logic Network in $\elrealizabledistsof{\mlnstat}$, if the minimal face of $\hlnfaceset$, which includes $\meanparamwith$ is an intersection of $\genmeanset$ and a face $\cubeface$ of the cube $\fullparcube$.
\end{definition}

Let us show, that the condition of reproduceability in \defref{def:HLNrepMean} is equivalent to the existence of a Hybrid Logic Network reproducing $\meanparamwith$.

\begin{theorem}
    \label{the:hlnInteriorCharacterization}
    Hybrid Logic Networks reproduce exactly those mean parameters $\meanparamwith$, which are in the effective interior of cube faces $\fullparcube$, i.e. for which there is a face $\genfaceset$ of $\hlnmeanset$ and a face $\cubeface$ of the cube $\fullparcube$ such that
    \begin{align*}
        \meanparamwith\in\sbinteriorof{\genfaceset} \quad\text{and}\quad \genfaceset = \hlnmeanset\cap\cubeface \, .
    \end{align*}
\end{theorem}
\begin{proof}
    \theref{the:faceMeasureHardLogicNetworks} implies that the normalization of the face measure $\basemeasureof{\sstat,\facecondset}$ to $\genfaceset$ is a Hard Logic Network and in $\elrealizabledistsof{\formulaset}$.
    By assumption, $\meanparamwith$ is in the effective interior of $\meansetof{\formulaset,\basemeasureof{\sstat,\facecondset}}$ and therefore reproduced by a distribution $\probwith$ in $\expfamilyof{\formulaset,\basemeasureof{\sstat,\facecondset}}$.
    Since $\probwith\in\elrealizabledistsof{\formulaset}$ we found a Hybrid Logic Network reproducing $\meanparamwith$.
\end{proof}

We now provide a procedure to investigate, whether a mean parameter $\meanparamwith\in\hlnmeanset$ is reproduceable by a Hybrid Logic Network in $\elrealizabledistsof{\formulaset}$.
To this end, we first determine the minimal face with respect to face inclusion (the partial order of the face lattice, see \cite{ziegler_lectures_2013}), which includes $\meanparamwith$.

\begin{lemma}
    \label{lem:minimalContainingFace}
    For each mean parameter $\meanparamwith\in\hlnmeanset$ the minimal face of the cube $\fullparcube$ containing $\meanparamwith$ is
    \begin{align*}
        \cubeface^{\meanparam}
        = \bigtimes_{\selindexin} \arbsetof{\selindex} \, .
    \end{align*}
    where for each $\selindexin$
    \begin{align*}
        \arbsetof{\selindex} = \begin{cases}
                                   \{\meanparamat{\indexedselvariable}\} & \text{if} \quad \meanparamat{\indexedselvariable} \in \ozset \\
                                   [0,1] & \text{else}
        \end{cases} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    Since $\hlnmeanset\subset\fullparcube$, $\meanparamwith$ is always contained in the maximal face $\fullparcube=\bigtimes_{\selindexin}[0,1]$ of the cube, and the smallest face containing $\meanparamwith$ exists.
    Any face of the cube has a representation as a cartesian product of $\arbsetof{\selindex}\in\{\{0\},\{1\},[0,1]\}$ and contains $\meanparamwith$ if and only if
    \begin{align*}
        \uniquantwrtof{\selindexin}{\left(\arbsetof{\selindex}=\{0\} \Rightarrow \meanparamat{\indexedselvariable}=0 \right) \land \left(\arbsetof{\selindex}=\{1\} \Rightarrow \meanparamat{\indexedselvariable}=1 \right)} \, .
    \end{align*}
    The minimal face containing $\meanparamwith$ is therefore $\cubeface^{\meanparam}$.
\end{proof}

% Determine whether reproducable
\begin{theorem}
    $\meanparamwith$ is reproducable by a Hybrid Logic Network, if and only if it is in the effective interior of the set
    \begin{align*}
        \hlnmeanset\cap \cubeface^{\meanparam} \, .
    \end{align*}
    Here $\cubeface^{\meanparam}$ is the minimal face of the cube containing $\meanparamwith$.
\end{theorem}
\begin{proof}
    This follows from \theref{the:hlnInteriorCharacterization} and \lemref{lem:minimalContainingFace}.
\end{proof}

% Extension towards arbitrary hypergraphs
We can extend the discussion to $\realizabledistsof{\formulaset,\graph}$, where $\graph$ is an arbitrary hypergraph.
Whenever a normalized face measure is in $\realizabledistsof{\formulaset,\graph}$, then the all mean parameters on the interior of the face can be reproduced by $\realizabledistsof{\formulaset,\graph}$.

%By \theref{the:meanPolytopeInterior} the interior points are those realizable by a Hybrid Logic Network with statistics $\mlnstat$ and base measure $\basemeasure$, as we state in the following Corollary.
%
%\begin{corollary}
%    \label{cor:interiorCharacterizationHLN}
%    If $\meanparamwith\in\interiorof{\hlnmeanset}$, or equivalently the statistic is minimal and $\meanparamwith$ is reproduceable by a distribution positive with respect to $\basemeasure$, then there is $\canparamat{\selvariable}$ such that $\expdistof{\formulaset,\canparam,\basemeasure}$ reproduces $\meanparamwith$.
%\end{corollary}

\subsect{The limit of hard logic}\label{sec:hardLogicLimit}

While \lemref{lem:localHardLimit} describes the limiting distributions of a statistic consistent of a single formula under annealing, we now turn towards statistics of multiple formulas.

\begin{theorem}
    \label{the:limitingDistribution}
    Let $\mlnstat$ be a boolean statistics, $\canparamwithin$ and $\hlnfaceset$ the face such that $\canparamwith\in\hlnmaxcone$.
    In the limit $\invtemp\rightarrow\infty$ the distribution $\probofat{\formulaset,\invtemp\cdot\canparam}{\shortcatvariables}$ converges coordinatewise to the normalized face measure $\hlnfacemeasure$, that is for any $\shortcatindices\in\facstates$ we have
    \begin{align*}
        \probofat{\formulaset,\invtemp\cdot\canparam}{\indexedshortcatvariables}
        \rightarrow \frac{1}{\contraction{\hlnfacemeasureat{\shortcatvariables}}} \cdot \hlnfacemeasureat{\indexedshortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    The limit is supported on the set of states, for which $\contraction{\canparamat{\selvariable},\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}}$ is maximal.
    This set has an basis encoding by the face measure (see \defref{def:faceMeasure}) and the limiting distribution is thus the normalized face measure.
\end{proof}

We can now use characterizations of the face measure in case of satisfiable formulas to get a more explicit representation of the limiting distribution.

\begin{theorem}
    If for a $\canparamwithin$ the formula
    \begin{align*}
        \exformula =
        \bigwedge_{\selindex\wcols\canparamat{\indexedselvariable}\neq0} \lnot^{(1-\greaterzeroof{\canparamat{\indexedselvariable}})} \enumformula
    \end{align*}
    is satisfiable, then $\probofat{\formulaset,\invtemp\cdot\canparam}{\shortcatvariables}$ converges for $\invtemp\rightarrow\infty$ coordinatewise to the normalized uniform distribution among the models of $\exformula$.
\end{theorem}
\begin{proof}
    If $\exformula$ is satisfiable, then the face measure $\hlnfacemeasure$ to the face $\hlnfaceset$ with $\canparamwith\in\hlnmaxcone$ is the uniform distribution among the models of $\exformula$.
    The claim follows from \theref{the:limitingDistribution}.
\end{proof}

We notice, that $\exformula$ is computable with elementary activation cores, that is for
\begin{align*}
    \kcoreat{\headvariables}
    = \left(\bigotimes_{\selindex\wcols\canparamat{\indexedselvariable}=0} \onesat{\headvariableof{\selindex}}\right)
    \otimes \left(\bigotimes_{\selindex\wcols\canparamat{\indexedselvariable}>0} \tbasisat{\headvariableof{\selindex}}\right)
    \otimes \left(\bigotimes_{\selindex\wcols\canparamat{\indexedselvariable}<0} \fbasisat{\headvariableof{\selindex}}\right)
\end{align*}
we have
\begin{align*}
    \formulaat{\shortcatvariables} =
    \normalizationof{\bencodingofat{\mlnstat}{\sstatheadvariables,\shortcatvariables}\kcoreat{\headvariables}}{\shortcatvariables} \, .
\end{align*}


\subsect{Expressivity of Hybrid Logic Networks}

\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/network_representation/reproducable_sketch.tex}
    \end{center}
    \caption{Sketch of distributions and their mean parameters with respect to .
    The hybrid logic networks are also maximum entropy distribution, as we will show in \charef{cha:networkReasoning}.}
    \label{fig:reproducableSketch}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/network_representation/meanset_sketch_hln.tex}
    \end{center}
    \caption{Sketch of the mean polytope ${\hlnmeanset}$ to a statistic $\mlnstat$ which is minimal with respect to $\basemeasure$, as a special case of the more generic sketch \figref{fig:meansetSketchGeneric}.
    The mean polytope is a subset of the $\seldim$-dimensional cube $\fullparcube$ (dashed in the sketch), where each mean parameter in one of the three cases $\meanparamof{1},\meanparamof{2}$ or $\meanparamof{3}$.
    \textcolor{\concolor}{Face centers} $\meanparamof{1}$ (in particular the vertices $\hlnmeanset\cap\boundaryparcube$) are reproducable by a Hard Logic Network given $\mlnstat$.
    Mean points on non-vertex faces outside the interior $\meanparamof{2}\in\hlnmeanset/(\interiorof{\hlnmeanset}\cap\boundaryparcube)$ are reproducable by Hybrid Logic Networks with statistic $\mlnstat$ and refined base measure $\secbasemeasure$.
    \textcolor{\probcolor}{Interior points} $\meanparamof{3}\in\interiorof{\hlnmeanset}$ are reproducable by a Markov Logic Network with statistic $\mlnstat$.
    }\label{fig:meansetSketch}
\end{figure}

In \figref{fig:reproducableSketch} we classify the distributions by their mean parameters in $\hlnmeanset$ and draw a corresponding sketch of the mean parameter polytope in \figref{fig:meansetSketch}.
We have already shown, that the distributions in $\realizabledistsof{\formulaset,\maxgraph}$ suffice to reproduce all mean parameters in $\hlnmeanset$.
The Hybrid Logic Networks are the subset $\hlnsetof{\formulaset}\subset\realizabledistsof{\formulaset,\maxgraph}$, which can be computed with elementary activation cores.
% Reduction to Hybrid Logic Networks, question of sufficient expressivity
%Let us recall, that the set $\hlnsetof{\formulaset}$ contains all Hybrid Logic Networks, which can be realized as tensor networks with the same structure of computation and activation cores.
We now investigate, in which cases also $\hlnsetof{\formulaset}$ suffices to reproduce all mean parameters in $\hlnmeanset$, that is in which cases $\hlnmeanset$ coincides with
%whether we can reduce the set of probability distributions in the definition of the convex polytope of mean parameters to the set $\hlnsetof{\formulaset}$ , that is
\begin{align*}
    \hlnmeanset|_{\hlnsetof{\formulaset}}
    = \left\{ \contractionof{\probtensor,\sencmlnstat}{\selvariable} \wcols \probtensor\in\hlnsetof{\formulaset} \right\} \, .
\end{align*}
While $\hlnmeanset|_{\hlnsetof{\formulaset}}\subset\hlnmeanset$ is obvious, we investigate, for which $\formulaset$ there is an equivalence.
We will refer to the equality of both sets as sufficient expressivity of $\hlnsetof{\formulaset}$.
In the next example we provide a class of formulas, for which $\hlnsetof{\formulaset}$ does not have sufficient expressivity.

% Insufficient Expressivity
\begin{example}[Insufficient expressivity of $\hlnsetof{\formulaset}$ in cases of disjoint models]
    \label{exa:insufficentHLNsetExpressivity}
    To provide an example, where the set of hybrid logic networks does not suffice to reproduce all possible mean parameters, consider the formulas
    \[ \formulaof{0} = \exrandom \land \secexrandom \quad, \quad \formulaof{1} = \lnot\exrandom \land \lnot\secexrandom \, . \]
    The probability distributions on the facet with normal $\canparam=[1\,\, 1]$ are those with support on the models of $\formulaof{0}\lor\formulaof{1}$.
    The Hybrid Logic Networks can only reproduce those with are supported on the model of $\formulaof{0}$ or the model of $\formulaof{1}$, but not their convex combinations.

    %% EXTENSION
    %More generally, one can construct similar examples by arbitrary sets of formulas with pairwise disjoint model sets.
    %If they do not sum to $\ones$, i.e. there is a world which is not a model to any formula, the statistic is minimal.
    %The vector $\canparamat{\selvariable} = \onesat{\selvariable}$ is then the normal of the facet with
    %All probabilities supported on the models of the formulas have mean parameters on this facet.
    %Such statistics will be further investigated as partition statistics in \secref{sec:partitionStatistics}. % Most precise: Partition statistics do sum to one
\end{example}

% Sufficient Expressivity
Before presenting the example class of atomic formulas as a case, where $\hlnsetof{\formulaset}$ has sufficient expressivity, let us first proof a generic criterion.


\begin{theorem}
    \label{the:sufficientHLNExpressivity}
    The set $\hlnmeanset|_{\hlnsetof{\formulaset}}$ of mean parameters realizable by $\hlnsetof{\mlnstat}$ coincides with $\hlnmeanset$, if and only if $\hlnmeanset$ is a face of the cube $\fullparcube$.
\end{theorem}
\begin{proof}
    The mean parameters to the effective interior of any face $\genfaceset$ of $\hlnsetof{\mlnstat}$ are reproducable by $\hlnsetof{\mlnstat}$, if and only if the normalization of the corresponding face measure is in $\hlnsetof{\mlnstat}$.
    This is exactly the case, when the face is the intersection $\genfaceset=\hlnmeanset\cap\cubeface$ for a face $\cubeface$ of $\fullparcube$.
    The polytopes $\hlnmeanset\subset\fullparcube$ such that for all its faces we find such a $\cubeface$, are the faces of $\fullparcube$.
\end{proof}

% Refinements of $\hlnsetof{\formulaset}$
When the assumptions of \theref{the:sufficientHLNExpressivity} are not satisfied, there are mean parameters, which can not be reproduced by a distribution in $\hlnsetof{\formulaset}$.
In that case, we can flexibilize the distribution, to also represent the base measures used for refinement in \theref{the:hlnFaceBaseMeasureCharacterization}.
This can be done by adding activation cores with multiple variables, or further computation cores calculating the disjunctions of formulas.


%% To prabilitiy distribution
%\begin{theorem}
%	The encoded pre-image of any face of rank $r$ is represented by an activation core of basis $\cpformat$ rank $r$.
%\end{theorem}
%\begin{proof}
%	We build for each vertex $\{\meanparam\}$ a basis vector $\bigotimes_{\selindex\in\seldim}\onehotmapofat{\meanparamat{\indexedselvariable}}{\headvariableof{\selindex}}$, which contraction with $\bencodingof{\formulaset}$.
%	Note, that the vertex base measures have disjoint support and their disjunction is thus a summation.
%\end{proof}

% COMMENT: BASIS+ VECTORS
% Activation vectors of hard logic networks are on the other side basis+, so they might be more efficiently representable

%\subsect{Case of tree computation networks} % NEED TO DEFINE FIRST!
%In this case, the mean polytope can be embedded into a markov network and characterized by local consistency of the mean parameters of the markov network.


\subsect{Examples}

We can relate our two standard examples of the atomic and the minterm formula sets to well-studied polytopes, namely the $\catorder$-dimensional hypercube and the standard simplex (see Lecture~0 in \cite{ziegler_lectures_2013}).

\begin{example}[Atomic formulas]
    The assumption of \theref{the:sufficientHLNExpressivity} is satisfied in the case for atomic formulas, where the formulas  $\formulaof{\formulaset,\canparam}$ are the minterms, which are always satisfiable in exactly one situation.
    The mean polytope in this case is the $\catorder$-dimensional hypercube
    \begin{align*}
        \meansetof{\atomformulaset,\ones} = \fullparcube
    \end{align*}
    which is called a simple polytope, since each vertex is contained in the minimal number of $\catorder$ facets.
    Since the cube $\fullparcube$ is a face of itself, \theref{the:sufficientHLNExpressivity} implies $\hlnmeanset|_{\hlnsetof{\atomformulaset}}=\hlnmeanset$.
\end{example}

\begin{example}[Minterm formulas]
    The mean polytope is in the case of the minterm exponential family is the $2^\catorder-1$-dimensional standard simplex.
    In this case, $\hlnsetof{\mintermformulaset}$ contains any distribution and therefore trivially realizes any mean parameter in $\meansetof{\mintermformulaset,\ones}$.
\end{example}


\sect{Discussion}

We understand Hybrid Logic Networks can be understood as neuro-symbolic models:
\begin{itemize}
    \item \textbf{Neural Paradigm} is the representation of models as compositions of smaller models.
    For Hybrid Logic Networks, the decompositions of logical formulas into their connectives implements this neural paradigm.
    In more generality the decompositions of sufficient statistics into composed functions has a tensor network representation based on basis calculus.
    Deeper nodes as carrying correlations of lower nodes.
    \item \textbf{Symbolic Paradigm} refers to the formalization of reasoning by human-interpretable symbols.
    Since the sufficient statistics of Hybrid Logic Networks are propositional formulas, they can be verbalized and interpreted based on their syntactical decompositions.
\end{itemize}

%Hybrid Logic Networks are trainable Machine Learning models:
%\begin{itemize}
%    \item Expressivity: Can represent any positive distribution, as shown by Theorem~\ref{the:maximalClausesRepresentation}, with $2^d$ formulas.
%    %\item Efficiency: Can only handle small subsets of possible formulas, since their possible number is huge.
%    %Tensor networks provide means to efficiently represent formulas depending on many variables and reason based on contractions.
%    \item Continuous parameterization: Distributions are differentiable functions dependent on their activation cores (e.g. in exponential parametrization).
%    The log-likelihood of data is therefore also differentiable function of their weights and we can exploit first-order methods in their optimization.
%\end{itemize}