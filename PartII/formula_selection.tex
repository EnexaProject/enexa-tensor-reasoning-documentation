\section{Formula Selecting Networks}\label{sec:superposedFT}\label{cha:formulaBatches}\label{cha:architectures}

In this chapter we will investigate efficient schemes to represent collections of formulas with similar structure in one tensor network.

% Relational encoding of the selection map
\begin{definition}
	Given a set of $\parlegdim$ formulas $\{\formulaof{\selindex} : \selindexin\}$, we define the formula selecting map as
		\[  \fselectionmapat{\shortcatvariables,\selvariable} : \atomstates \times [\parlegdim] \rightarrow [2] \]
	defined for $\selindexin$ by
		\[ \fselectionmapat{\shortcatvariables=\atomindices,\selvariable=\selindex} =  \formulaofat{\selindex}{\shortcatvariables=\atomindices} \, . \]
\end{definition}

% Selection Variables
We introduce a selection variable $\selvariable$ and depict the formula selection in Figure~\ref{fig:formulaSelectionMap}.

% Depiction
\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/formula_selection/formula_selecting_map.tex}
\end{center}
\caption{Representation of the Formula Selecting map as a 
a) Graphical Model with a selection variable $\fselectionmap$.
b) Dual Tensor Core with selection variable corresponding with an additional axis.}
\label{fig:formulaSelectionMap}
\end{figure}


% Decomposition
A naive representation of the formula selecting map is as a sum
	\[ \fselectionmap = \sum_{\selindexin} \formulaofat{\selindex}{\shortcatvariables}  \otimes \onehotmapofat{\selindex}{\selvariable} \, . \]
Such a representation scheme requires linear resources in the number of formulas.
We will show in the following, that we can exploit common structure in formulas to drastically reduce this resource consumption.



\subsection{Construction schemes}

% Naturality of folding
Let us now investigate efficient schemes to define sets of formulas to be used in the definition of $\fselectionmap$.
We will motivate the folding of the selection variable into multiple selection variables by compositions of selection maps.


\subsubsection{Connective Selecting Tensors}

We represent choices over connectives with a fixed number of arguments by adding a selection variable to the cores and defining each slice by a candidate connective.

% Formal map
\begin{definition}\label{def:connectiveSelector}
	Let $\{\connectiveof{0},\ldots,\connectiveof{\parlegdimof{\cselectionsymbol}-1}\}$ be a set of connectives with $\atomorder$ arguments.
	The associated connective selection map is
		\[ \cselectionmapat{\shortcatvariables,\selvariableof{\cselectionsymbol}}
		: \atomstates \times [\parlegdimof{\cselectionsymbol}] \rightarrow [2] \]
	defined for each $\selindexofin{\cselectionsymbol}$ and $\shortcatindices\in\atomstates$ by 
		\[ \cselectionmapat{\shortcatvariables=\shortcatindices,\indexedselvariableof{\cselectionsymbol}} 
		= \connectiveofat{\selindexof{\cselectionsymbol}}{\shortcatvariables=\shortcatindices}  \, . \]
\end{definition}

We depict the relational encoding of connective selection maps in Figure~\ref{fig:connectiveSelector}.

\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/formula_selection/connective_selector.tex}
\end{center}
\caption{Connective Selector.}
\label{fig:connectiveSelector}
\end{figure}

%Following a different perspective: skeleton+atomindices at atomic expression level, atomindices at complex expression level!
%Having an parametrization of binary connectives by $\circ_{\selindex}$ we can define the corresponding connective selector tensor by
%	\[ \rencodingof{\circ}_{\selindex,:,:} = \rencodingof{\circ_{\selindex}}_{:,:} \, . \]

\begin{remark}[$\htformat$ Interpretation of Superposed Formula Tensor Networks]\label{rem:HTDecomSFT}
	Continuing Remark ~\ref{rem:HTDecomFT}: 
	Superposed Formula Tensors have a decomposition into a $\htformat$ as sketched here, where we distinguish between formula selection subspaces (indices $\selindexof{\parenumerator}$) and atomic subspaces (indices $\atomlegindexof{\atomenumerator})$.
	At each formula selection we thus have a decomposition into three subspaces, two of atomic formulas and one for the formula selection.
\end{remark}




\subsubsection{Variable Selecting Tensor Network}

%\red{Works also for categorical variables! -> Into Contraction Calculus?}

%% Definition
\begin{definition}\label{def:variableSelector}
	The selection of one out of $\seldim$ variables in a list $\catvariableof{[\seldim]}$ is done by variable selecting maps
	\begin{align}
		\vselectionmapat{\catvariableof{[\seldim]},\selvariableof{\vselectionsymbol}}:  \left(\bigtimes_{\selindex\in[\parlegdim]}[2]\right) \times [\seldim]  \rightarrow [2]
	\end{align}
	are defined coordinatewise by
	\begin{align}
		\vselectionmapat{\indexedcatvariableof{0},\ldots,\indexedcatvariableof{\seldim-1},\indexedselvariableof{\vselectionsymbol}} = \catindexof{\selindex} \, .
	\end{align}
\end{definition}
	
% Interpretation as multiplex gate
Variable selecting maps appear in the literature as multiplex gates (see Definition 5.3 in \cite{koller_probabilistic_2009}).

The relational encoding of the variable selection map has a decomposition 
\begin{align*}
	\rencodingofat{\vselectionmap}{\vselectionheadvar,\catvariableof{[\seldimof{\vselectionsymbol}]}}
	= \sum_{\selindexofin{\vselectionsymbol}} 
	\rencodingofat{\atomicformulaof{\selindexof{\vselectionsymbol}}}{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}}} \otimes  \onehotmapofat{\selindexof{\vselectionsymbol}}{\selvariableof{\vselectionsymbol}} \, . 
\end{align*}
This structure is exploited in the next theorem to derive a tensor network decomposition of $\rencodingof{\vselectionmap}$.

\begin{theorem}[Decomposition of Variable Selecting Maps]\label{the:varSelectorDecomposition}
	Given a list $\catvariableof{[\seldimof{\vselectionsymbol}]}$ of variables, we define for each $\selindexofin{\vselectionsymbol}$ the tensors
		\[ \selectorcomponentofat{\selindexof{\vselectionsymbol}}{\catvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}} 
		= \identityat{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}}} \otimes \onehotmapofat{\selindexof{\vselectionsymbol}}{\selvariableof{\vselectionsymbol}} 
		+ \onesat{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}}} \otimes \left(\onesat{\selvariableof{\vselectionsymbol}} - \onehotmapofat{\selindexof{\vselectionsymbol}}{\selvariableof{\vselectionsymbol}} \right) \, . 
		\]
	Then we have (see Figure~\ref{fig:SelectorDecomposition})
		\[ \rencodingofat{\vselectionmap}{\vselectionheadvar,\catvariableof{[\parlegdim]},\selvariableof{\vselectionsymbol}} 
		= \contractionof{
			\{\selectorcomponentofat{\selindexof{\vselectionsymbol}}{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}} \, : \, \selindexofin{\vselectionsymbol}\}
		}{\vselectionheadvar,\catvariableof{[\parlegdim]},\selvariableof{\vselectionsymbol}} \, . 
		\]
\end{theorem}
\begin{proof}
	We show the equivalence of the tensors on an arbitrary coordinates.
	For $\tilde{\selindex}_{\vselectionsymbol}\in[\seldimof{\vselectionsymbol}]$, $\vselectionheadvar\in[2]$ and $\catindexof{[\seldimof{\vselectionsymbol}]}\in\bigtimes_{\catenumerator\in[\seldimof{\vselectionsymbol}]}[2]$ we have
	\begin{align*}
		& \contractionof{
			\{\selectorcomponentofat{\selindexof{\vselectionsymbol}}{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}} \, : \, \selindexofin{\vselectionsymbol}\}
		}{\indexedheadvariableof{\vselectionsymbol},\indexedcatvariableof{[\parlegdim]},\selvariableof{\vselectionsymbol} = \tilde{\selindex}_{\vselectionsymbol}} \\
		& \quad = 
		\prod_{\selindexofin{\vselectionsymbol}} \selectorcomponentofat{\selindexof{\vselectionsymbol}}{
			\indexedheadvariableof{\vselectionsymbol},\indexedcatvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}=\tilde{\selindex}_{\vselectionsymbol}
			} \\
		& \quad = \selectorcomponentofat{\tilde{\selindex}_{\vselectionsymbol}}{
			\indexedheadvariableof{\vselectionsymbol},\indexedcatvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}=\tilde{\selindex}_{\vselectionsymbol}
		} \\
		& \quad = 
		\begin{cases}
		 	1 & \text{if} \quad \headindexof{\vselectionsymbol} = \catindexof{\selindexof{\vselectionsymbol}} \\
		 	0 & \text{else}  
		 \end{cases} \\Â 
		 & = \rencodingofat{\vselectionmap}{\indexedheadvariableof{\vselectionsymbol},\indexedcatvariableof{[\parlegdim]},\selvariableof{\vselectionsymbol}=\tilde{\selindex}_{\vselectionsymbol}}
	\end{align*}
	In the second equality, we used that the tensor $\selectorcomponentof{\selindexof{\vselectionsymbol}}$ have coordinates $1$ whenever $\tilde{\selindex}_{\vselectionsymbol}\neq\selindexof{\vselectionsymbol}$.
\end{proof}


The decomposition provided by \theref{the:varSelectorDecomposition} is in a CP format, as will be further discussed in \charef{cha:sparseCalculus}.
The introduced tensors $\selectorcomponentof{\selindexof{\vselectionsymbol}}$ are Boolean, but not directed and therefore encodings of relations but not functions (see \charef{cha:basisCalculus}).

%%% Decomposition
%% ! THIS IS NOT \theref{the:functionDecompositionBasisCP}, but works on slice sparsity!

%Using that the encoding $\rencodingof{\atomicformulaof{\selindex}}$ of atomic formulas admits and elementary decomposition (see \theref{the:AtomicFTensor}) we notice that Equation~\ref{eq:selectorDecomposition} describes a so-called monomial decomposition, which will be introduced in \defref{def:polynomialSparsity}.
%We can apply \theref{the:sliceToCP} to find a decomposition of $\selectorcore$ in a CP format consisting of cores
%\begin{align}
%	\selectorcoreof{\selindex} = \dirdeltaof{\randomxof{\selindex},\vselectionvariable} \otimes \onehotmapof{\selindex}
%	+ \sum_{\tilde{\selindex}\in[\parlegdim] \, , \, \tilde{\selindex}\neq\selindex} \onesof{\randomxof{\selindex},\vselectionvariable} \otimes \onehotmapof{\tilde{\selindex}} \, . 
%\end{align}	
%The CP decomposition is depicted in Figure~\ref{fig:SelectorDecomposition}.
%
%% Selectorcores are non-functional relational encodings
%We notice, that the selector cores $\selectorcoreof{\selindex}$ are encodings of a relation, which is not a function.
%Therefore, they are binary but not directed tensors.
%Their contraction 
%\begin{align}\label{eq:selectorDecomposition}
% 	\selectorcore = 
%	\contractionof{\{\selectorcomponentof{\selindex}\, : \, \selindexin\}}
%	{\{\randomxof{0},\ldots,\randomxof{\parlegdim-1},\vselectionvariable\}} 
%\end{align}
%is the relational encoding of the function $\vselectionmap$ and thus binary and directed.


%% Interpretation
%The selectorcores $\selectorcoreof{\selindexof{1}}$ are contracted with the parameter cores and select the respective atom when contracted with truth vector tensormultiplied by constant cores (as placeholder for the other possible atoms).
%Decomposed into disconnected strands for each atomkey, which connect on the selection axis and on the atom truth axis.


\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/formula_selection/variable_selector.tex}
\end{center}
\caption{Decomposition of the relational encoding of a variable selecting tensor into a network of tensors defined in \theref{the:varSelectorDecomposition}.
	The decomposition is in a $\cpformat$-Format (see \charef{cha:sparseCalculus}. %, when grouping the indices  $\selindexof{\parenumerator}$ and $\atomlegindexof{\atomicformulaof{\selindexof{\parenumerator}}}$).
	%To ease the notation, we here use $\rencodingof{\parenumerator}$ to denote $\rencodingof{\rencodingof{\parenumerator}}$.
}
\label{fig:SelectorDecomposition}
\end{figure}




\subsection{State Selecting Tensors}

As an alternative, one can select a state of a categorical variable $\catvariable$.

\begin{definition}
	Given a categorical variable $\catvariableof{\sselectionsymbol}$ with dimension $\catdimof{\sselectionsymbol}$ and a selection variable $\selvariableof{\sselectionsymbol}$ with dimension $\seldimof{\sselectionsymbol}=\catdimof{\sselectionsymbol}$ the state selecting tensor 
		\[ \sselectionmapat{\catvariableof{\sselectionsymbol},\selvariableof{\sselectionsymbol}} : [\catdimof{\sselectionsymbol}] \times [\seldimof{\sselectionsymbol}] \rightarrow [2] \]
	is defined on $\catindexofin{\sselectionsymbol}$ and $\selindexofin{\sselectionsymbol}$ by
	\begin{align*}
		\sselectionmapat{\indexedcatvariableof{},\indexedselvariableof{\sselectionsymbol}} = 
		\begin{cases}
			1 & \text{if} \quad \catindex = \selindexof{\sselectionsymbol} \\
			0 & \text{else}
		\end{cases} \, . 
	\end{align*}
\end{definition}

% Comment: Alternative based on categorical constraints to be introduced later
State selecting tensors can also be realized by variable selecting tensors.
In \secref{sec:categoricalTN} we will describe methods to build atomic variables indicating the states of a categorical variable.
This would, however, increase the number of variables in a tensor network and can thus lead to an exponential overhead of dimensions.
State selecting tensors can therefore be seen as a mean to avoid such dimension increases.

\red{Comment: State Selectors can be integrated in Variable Selection framework. In this perspective, Variable selection networks are the specific case to $X=1$. }


%% OLD Alternative
%Such categorical variable cores have the advantage of avoiding a full atomization of the categorical variable, which is the creation of atoms reproducing the values
%	\[ \catvariable==\catindexof{\catvariable} \, . \]
%By representing categorical variable choice, one can thus avoid an increase of the order of the encoded tensors, which avoids intractabilities.
%Categorical selection cores can further be integrated in the decomposition scheme \eqref{eq:selectorDecomposition}. 







\subsection{Composition of formula selecting maps}
%\subsection{Folding of the Selection Variable}

We will now parametrize the sets $\formulaset$ with additional indices and define formula selector maps subsuming all formulas.
To handle large sets of formulas, we further fold the selection variable into tuples of selection variables.

\begin{definition}%\label{def:formulaSelector}
	Let there be a formula $\formulaof{\selindexlist}$ for each index tuple in $\selindexlist\in\selstates$, where $\selorder,\seldimof{0},\ldots,\seldimof{\selorder-1}\in\nn$.
	The folded formula selector map (see Figure~\ref{fig:foldedSelector}) is the map 
		\[ \fselectionmapat{\shortcatvariables,\shortselvariables} : \left(\atomstates\right) \times \left(\selstates\right) \rightarrow [2] \]
	with the coordinates at the indices $\shortcatindices\in\atomstates$, $\shortselindices\in\selstates$
		\[  \fselectionmapat{\shortcatvariables=\shortcatindices,\shortselvariables=\shortselindices} 
		= \formulaofat{\shortselindices}{\shortcatvariables=\shortcatindices} \, . \]
\end{definition}

% Formula Section based on skeleton expressions
We will find formula selector maps by composition variables selector maps (\defref{def:variableSelector}) and connective selector maps (\defref{def:connectiveSelector}).
This is especially useful to provide efficient decompositions of relational encodings. 

\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/formula_selection/folded_selector.tex}
\end{center}
\caption{Relational encoding of the folded map $\fselectionmap$.}
\label{fig:foldedSelector}
\end{figure}




\subsubsection{Formula Selecting Neuron}


% Motivating foldings by composition
The folding of the selection variable is motivated by the composition of selection maps.
We call the composition of a connective selection with variable selection maps for each argument a formula selecting neuron.


\begin{definition}\label{def:fsNeuron}
	Given an order $\selorder\in\nn$ let there be a connective selector $\cselectionvariable$ selecting connectives of order $\selorder$ and let $\vselectionmapof{0},\ldots,\vselectionmapof{\selorder-1}$ be a collection of variable selectors.
	The corresponding logical neuron is the map
	\begin{align*}
		\lneuronat{\shortcatvariablelist,\shortselvariablelist} 
		: \left(\atomstates\right) \times [\seldimof{\cselectionsymbol}] \times \left( \bigtimes_{\selenumeratorin} [\seldimof{\selenumerator}]\right) \rightarrow [2] 
	\end{align*}
	defined for $\shortcatindices\in\atomstates$, $\selindexof{\cselectionsymbol}\in[\parlegdimof{\cselectionsymbol}]$ and 
	$\parindices\in \bigtimes_{\parenumeratorin} [\parlegdimof{\parenumerator}]$ by
	\begin{align*}
		\lneuron(\atomindices, \selindexof{\cselectionsymbol}, \parindices) = 
		\cselectionmap(\vselectionmapof{0}(\atomindices, \selindexof{0}),\ldots,\vselectionmapof{\parorder-1}(\atomindices,\selindexof{\parorder-1}), \selindexof{\cselectionsymbol}) \, .
	\end{align*}
\end{definition}

% Tensor Network Decomposition
Each neuron has a tensor network decomposition by a connective selector tensor and a variable selector tensor network for each argument, as we state in the next theorem.

\begin{theorem}{Decomposition of formula selecting neurons}\label{the:neuronDecomposition}
	Let $\lneuron$ a logical neuron, defined for a connective selector $\cselectionvariable$ and variable selectors $\vselectionmapof{0},\ldots,\vselectionmapof{\selorder-1}$.
	Then we have (see Figure~\ref{fig:neuronDecomposition} for the example of $\selorder=2$):
	\begin{align*}
		&\rencodingofat{\lneuron}{\headvariableof{\lneuron},\shortcatvariables,\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\ldots,\selvariableof{\vselectionsymbol,\selorder-1}} \\
		&\quad = \langle\{\rencodingofat{\cselectionmap}{
				\headvariableof{\lneuron},\headvariableof{\vselectionsymbol,0},\ldots,\headvariableof{\vselectionsymbol,\selorder-1}}, \\
		& \quad\quad\quad\rencodingofat{\vselectionmapof{0}}{
				\headvariableof{\vselectionsymbol,0},\shortcatvariables,\selvariableof{\vselectionsymbol,0}},\ldots,
				\rencodingofat{\vselectionmapof{\selorder-1}}{
					\headvariableof{\vselectionsymbol,\selorder-1},\shortcatvariables,\selvariableof{\vselectionsymbol,\selorder-1}}
				\} \rangle
		\left[\headvariableof{\lneuron},\shortcatvariables, \selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\ldots,\selvariableof{\vselectionsymbol,\selorder-1}\right] \, .
	\end{align*}
\end{theorem}
\begin{proof}
	By composition \theref{the:compositionByContraction}.
\end{proof}




\red{Example of a formula selecting neuron:}
Given a skeleton expression and a set of candidates at each placeholder, we parameterize a set of formulas by the assignment of candidate atoms to each placeholder position.
Let us denote the set of formulas, which are generated through choosing atoms from $\candidatelistof{\parenumerator}$ for the skeleton formula $\skeleton$ by
		\[ \formulasetof{\skeleton} \coloneqq 
	 \left\{ \skeletonof{\placeholderof{1},\ldots,\placeholderof{\atomorder}} \, : \, \placeholderof{\atomenumerator} \in \candidatelistof{\atomenumerator} \right\} \]

%We now enumerate at each position $\parenumerator$ the list of candidates $\candidatelistof{\parenumerator}$ using an index $\selindexof{\parenumerator}\in[\parlegdimof{\parenumerator}]$ and parametrize the choice of the $\selindexof{\parenumerator}$ for the placeholder $\placeholderof{\parenumerator}$ by unit vectors
%	\[ \unitvectoratof{\parenumerator}{\selindexof{\parenumerator}} \in \rr^{\parlegdimof{\parenumerator}} \, . \]
%We thus have a parameter space $\parameterspace$ parametrizing the possible assignments to the skeleton in its basis vectors.

\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/formula_selection/logical_neuron.tex}
\end{center}
\caption{Example of a logical neuron $\lneuron$ of order $\selorder=2$.
	a) Selection and categorical variables and their interdependencies visualized in a hypergraph.
	b) Relational encoding of the logical neuron and tensor network decomposition into variable selecting and connective selecting tensors.
}
\label{fig:neuronDecomposition}
\end{figure}


\subsubsection{Formula Selecting Neural Network}

% Enhancement of the Expressivity
Single neurons have a limited expressivity, since for each choice of the selection variables they can just express single connectives acting on atomic variables.
The expressivity is extended to all propositional formulas, when allowing for networks of neurons, which can select each others as input arguments.


\begin{definition}\label{def:fsNeuralNetwork}

%	We call a graph consistent of nodes decorated by formula selecting neurons and directed edges representing the argument dependencies of the neuron on other neurons, an architecture graph.
%	An acyclic architecture graph is called a formula selecting neural network.	
%	Formula selecting neurons, which are not included by other formula selecting neurons are called output neurons and collected in the variables $\catvariableof{\larchitecture}$. 
%	A logical neural network is a collection of logical neurons, such that the network graph (nodes: neurons, edges: directed representing argument dependencies) is acyclic (a DAG).
	
	An architecture graph $\graphof{\larchitecture}=(\nodesof{\larchitecture},\edgesof{\larchitecture})$ is an acyclic directed hypergraph with nodes appearing at most once as outgoing nodes.
	Nodes appearing only as outgoing nodes are input neurons and are labeled by $\inneuronset$ and nodes not appearing as outgoing nodes are the output neurons in the set $\outneuronset$ (see Figure~\ref{fig:architectureGraph} for an example).

	Given an architecture graph $\graphof{\larchitecture}=(\nodesof{\larchitecture},\edgesof{\larchitecture})$, a \emph{formula selecting neural network} $\fsnn$ is a tensor network of logical neurons at each $\lneuron\in\nodesof{\larchitecture}/\inneuronset$, such that each neuron depends on variables $\catvariableof{\parentsof{\lneuron}}$ and on selection variables $\selvariableof{\lneuron}$.
	The collection of all selection variable is notated by $\selvariableof{\larchitecture}$.

	The activation tensor of each neuron $\lneuron\in\nodesof{\larchitecture}/\inneuronset$ is
	\begin{align*}
		\lneuractivationat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} 
		= \contractionof{
			\{\rencodingof{\tilde{\lneuron}} \, : \, \tilde{\lneuron}\in\nodesof{\larchitecture}/\inneuronset \} \cup \{\onehotmapofat{1}{\headvariableof{\lneuron}}\}
		}{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \, . 
	\end{align*}
		
	The activation tensor of the formula selecting neural network is the contraction
	\begin{align*}
		\fsnnat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} 
		= \contractionof{
			\{\rencodingofat{\lneuractivation}{\headvariableof{\lneuron},\catvariableof{\parentsof{\lneuron}},\selvariableof{\larchitecture}} \, : \, \lneuron\in\nodesof{\larchitecture}/\inneuronset \} \cup \{\onehotmapofat{1}{\headvariableof{\lneuron}} \, : \, \lneuron\in\outneuronset\}
		}{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \, . 
	\end{align*}
	
	The expressivity of a formula selecting neural network $\fsnn$ is the formula set
	\begin{align*}
		\formulasetof{\larchitecture} = \left\{ \larchitectureat{\catvariableof{\inneuronset},\indexedselvariableof{\larchitecture}}  : \selindexof{\larchitecture}\in\selstates \right\} \, . 
	\end{align*}
	
\end{definition}

% ? Extend by activation cone stuff
The activation tensor of each neuron depends in general on the activation tensor of its ancestor neurons with respect to the directed graph $\graphof{\larchitecture}$, and thus inherits the selection variables.

% Architecture graph -> Tensor Network
We notice that the architecture graph is a scheme to construct the variable dependency graph of the tensor network $\formulasetof{\larchitecture}$.
To this end, we replace each neuron $\lneuron\in\nodesof{\larchitecture}/\inneuronset$ by an output variable $\headvariableof{\lneuron}$ and further add selection variables $\selvariableof{\lneuron}$ to the directed edges, that is to each directed hyperedge $(\{\lneuron\}, \parentsof{\lneuron})\in\edgesof{\larchitecture}$ we construct a directed hyperedge $(\{\headvariableof{\lneuron}\}, \catvariableof{\parentsof{\lneuron}}\cup\selvariableof{\lneuron})$.

\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/formula_selection/architecture_graph.tex}
\end{center}
\caption{Example of an architecture graph $\graphof{\larchitecture}$ with input neurons $\inneuronset=\{\lneuronof{0},\lneuronof{1},\lneuronof{2},\lneuronof{3}\}$ and output neurons $\outneuronset=\{\lneuronof{6},\lneuronof{7}\}$
}
\label{fig:architectureGraph}
\end{figure}


\begin{theorem}
	Given fixed selection variables $\selvariableof{\larchitecture}$, the formula selecting neural network is the conjunction of output neurons, that is
	\begin{align*}
		\fsnnat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} = \bigwedge_{\lneuron\in\outneuronset} \lneuronat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \, . 
	\end{align*}
\end{theorem}
\begin{proof}
	By effective calculus (see \theref{the:effectiveConjunction}), we have
		\[ \sbcontractionof{\rencodingofat{\land}{\catvariableof{\land},\shortcatvariables},\onehotmapofat{1}{\catvariableof{\land}}}{\shortcatvariables} = \bigotimes_{\catenumeratorin} \onehotmapofat{1}{\catvariableof{\catenumerator}} \]
	and thus
	\begin{align*}
		\fsnnat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}}
		= \contractionof{
			\{\rencodingof{\lneuron} \, : \, \lneuron\in\nodesof{\larchitecture}/\inneuronset \} \cup \{\rencodingofat{\land}{\catvariableof{\land},\headvariableof{\lneuron}  \, : \, \lneuron\in\outneuronset}, \onehotmapofat{1}{\catvariableof{\land}}\}
		}{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \, . 
	\end{align*}
\end{proof}


% Combination of decompositions
By the commutation of contractions, we can further use \theref{the:neuronDecomposition} to decompose each tensor $\rencodingof{\lneuron}$ into connective and variable selecting components to get a sparse representation of a formula selecting neural network $\fsnn$.

%% Now as the definition!
%\begin{theorem}{Decomposition of formula selecting neural networks}\label{the:architectureDecomposition}
%	We have
%		\[ \rencodingof{\larchitecture} = \contractionof{\{\rencodingof{\lneuron} \, : \, \lneuron \in \larchitecture\}}{\catvariableof{\larchitecture},\shortcatvariables,\selvariableof{\larchitecture}} \]
%\end{theorem}
%\begin{proof}
%	By composition \theref{the:compositionByContraction}.
%	%\red{In addition: $X_{\larchitecture}$ specifying the headneurons! }
%\end{proof}

%% Now as the definition!
%% Relation between $\lneuron$ and $\rencodingof{\larchitecture}$
%Another useful property of encoded formula selecting architecture, is that we can retrieve any neuron by a simple contraction, as we show next.
%
%\begin{theorem}\label{the:formulaRetrieval}
%	Any neuron $\lneuron\in\larchitecture$ is retrieved by the contraction 
%		\[ \lneuron = \contractionof{\rencodingof{\larchitecture},\onehotmapof{1}[X_{\lneuron}]}{X\cup Z} \, . \]
%\end{theorem}
%\begin{proof}
%	First use the head neutralization property (Corollary~\ref{cor:onesHead}) in a parent stripping argument.
%	Then we are left with an architecture with $\lneuron$ being the only output neuron and use Corollary~\ref{cor:rhoToNormal} (we have $\restrictionofto{\mathrm{Id}}{[2]}=\onehotmapof{1}$).
%\end{proof}

% Alternative: Headneuron retrieval
%In case of multiple output neurons, the retrieval needs to be performed separately as in \theref{the:formulaRetrieval}, since contracting basis vectors $\onehotmapof{1}$ at multiple output neurons will retrieve the conjunction of those output neurons.




%\subsubsection{Skeleton Expressions}
%
%When only allowing for argument selections at the leaf level of the network, we get a skeleton expression.
%
%\begin{definition}\label{def:skeleton}
%	A skeleton expression
%		\[ \skeleton(\placeholderof{0},\ldots,\placeholderof{\parorder-1}) \] 
%	is a composition of atom and connective selector maps, which are denoted by placeholders $\placeholderof{\parenumerator}$, where $\parenumerator\in[\parorder]$..
%	Each placeholder has a by $\selindexof{\parenumerator}$ enumerated list $\candidatelistof{\parenumerator}$ with cardinality $\parlegdimof{\parenumerator}= \cardof{\candidatelistof{\parenumerator}}$ of possible symbols denoting atoms or connectives to be placed in at this position.
%	This defines a map
%		\[ \skeleton : \left(\facstates\right) \times \left(\secfacstates\right) \rightarrow \{0,1\} \]
%	where $\skeleton(\atomindices,\parindices)$ denotes the formula given the selection of placeholders by $\parindices$, which is evaluated at the atoms $\atomindices$.
%\end{definition}

%\begin{definition}
%	A skeleton expression is a formula
%		\[ \skeleton(\placeholderof{0},\ldots,\placeholderof{\parorder-1}) \]
%	where instead of atoms and connectives there are placeholders $\placeholderof{\parenumerator}$, where $\parenumerator\in[\parorder]$.
%	Each skeleton has for each placeholder $\placeholderof{\parenumerator}$ a set $\candidatelistof{\parenumerator}$ of candidate atoms to be plugged in the placeholders.
%	We denote its cardinality to be $\parlegdimof{\parenumerator}= \cardof{\candidatelistof{\parenumerator}}$ and enumerate the elements $\placeholderof{\parenumerator}_{\selindexof{\parenumerator}}$ in each candidates list by an index $\selindexof{\parenumerator}\in[\parlegdimof{\parenumerator}]$.
%\end{definition}






%% ANOTHER EXAMPLE:





%\begin{definition}
%	Given a skeleton expression, the skeleton tensor is the map from the parameter space to the space of formula tensors, defined by
%	\begin{align}
%		 \skeletontensor : \parameterspace \rightarrow  \modelspace \quad , \quad
%		 \skeletontensor\left( \bigotimes_{\parenumeratorin}\unitvectoratof{\parenumerator}{\selindexof{\parenumerator}} \right) = \rencodingof{\skeletonof{\placeholderof{\parenumerator}_{\selindexof{\parenumerator}}\, : \, \parenumeratorin}}
%	\end{align}
%\end{definition}
%
%Using the canonical duality of tensors as maps and elements of tensor spaces, we can reinterpret is as a tensor \red{Domain representation of skeleton map}
%	\[ \skeletontensor \in \bigotimes_{\parenumerator\in[\parorder]} \rr^{\parlegdimof{\parenumerator}} \otimes  \modelspace  \, , \]
%which is the superposed formula tensor to a skeleton based parametrization.
%In the following, we investigate how to efficiently represent the skeleton tensor $\rencodingof{\skeleton}$ as a tensor network.









\subsection{Application of Formula Selecting Networks}

There are two main applications of formula selecting networks.
First, when contracting the selection variables with a weight tensor we get a weighted sum of the parametrized formulas.
Second, when contracting the categorical variables with a distribution or a knowledge base, we get a tensor storing the satisfaction rates respectively the world counts of the parametrized formulas.

\subsubsection{Representation of selection encodings}

\red{In technical perspective: FSN provide efficient representation of $\sencodingof{\formulaset}$ 
-> Use for exponential families, structure learning.}

\begin{lemma}\label{lem:relToSelFSN}
	Given a set $\{\formulaof{\selindexlist} : \selindexlist\in\selstates\}$ of propositional formulas we define the statistic
		 \[ \formulaset : \catindices \rightarrow (\formulaof{\selindexlist}(\catindices))_{\selindexlist} \, . \]
	and the formula selecting map
		\[ \fselectionmap: \catindices , \selindexlist \rightarrow \formulaof{\selindexlist} (\catindices) \, . \]
	Then 
		\[ \sencodingofat{\formulaset}{\shortcatvariablelist, \shortselvariablelist} = \fselectionmap\left[\shortcatvariablelist, \shortselvariablelist \right] \, .  \]
		%\[ \sencodingofat{\formulaset}{\shortcatvariablelist, \shortselvariablelist}
		 %= \sbcontractionof{\rencodingofat{\fselectionmap}{\headvariableof{\fselectionmap}, \shortcatvariablelist, \shortselvariablelist}
		 %, \onehotmapofat{1}{\headvariableof{\fselectionmap}}}{\shortcatvariablelist, \shortselvariablelist} \, . \]
\end{lemma}
\begin{proof}
	For any indices $\shortselindices\in\selstates$ and $\shortcatindices\in\atomstates$ we have
	\begin{align*}
		\sencodingofat{\formulaset}{\shortcatvariablelist=\shortcatindices, \shortselvariablelist=\shortselindices}
		=  \formulaof{\selindexlist}(\catindices) =  \fselectionmap\left[\shortcatvariablelist=\shortcatindices, \shortselvariablelist=\shortselindices \right] \, . 
	\end{align*}
\end{proof}

%% Reason for relational encodings and selection encodings.
Technically, relational encodings have been exploited to derive decompositions based on basis calculus.
Selection encodings on the other hand enable the application of formula selecting networks as superpositions of formulas.



\subsubsection{Efficient Representation of Formulas}

\red{Weight contracted at the selection variables is elementary, then single formula retrieved.}

% Exponentially many formulas represented by linear demand
Formula Selecting Neural Networks are means to represent exponentially many formulas with linear (in sum of candidates list lengths) storage.
Their contraction with probability tensor networks, is thus a batchwise evaluation of exponentially many formulas.
This is possible due to redundancies in logical calculus due to modular combinations of subformulas.

% Retrieve functions
We can retrieve specific formulas by slicing the selection variables, i.e. for $\parindices$ we have
	\[ \exformula_{\parindices}[\shortcatvariables] = \fselectionmapat{\shortcatvariables,\selvariable=\parindices} \, .  \]

In a tensor network diagram we depict this by
\begin{center}
	\input{PartII/tikz_pics/formula_selection/formula_retrieval.tex}
\end{center}

% Interpretation by dynamic programming
Another perspective on the efficient formula evaluation by selection tensor networks is dynamic computing.
Evaluating a formula requires evaluations of its subformulas, which are done by subcontractions and saved for different subformulas due to the additional selection legs.

% Storage problem of solutions
However, we need to avoid contracting the tensor with leaving all selection legs open, since this would require exponential storage demand.

% Sparse algorithm
We can avoid this storage bottleneck by extending the contractions by additional cores leaving less variable legs open.
This is the case when contracting gradients of the parameter tensor networks in alternating least squares approaches.
Other methods avoiding the bottleneck can be constructed by MCMC sampling, for example Gibbs Sampling.
Here we only need to vary local components of the formula reflected in keeping only single variable legs open.



\subsubsection{Batch contraction of parametrized formulas}

Given a set $\formulaset$ of formulas, we build a formula selecting network parametrizing the formulas.
The contraction 
\begin{align*}
	\contractionof{\extnet,\fselectionmap}{\shortselvariables} 
\end{align*}
is a tensor containing the contractions of the formulas $\formulaof{\shortselindices}$ with an arbitrary tensor network $\extnet$ as
\begin{align*}
	\sbcontraction{\extnet,\formulaof{\shortselindices}} = \sbcontractionof{\extnet,\fselectionmap}{\shortselvariables=\shortselindices} \, . 
\end{align*}


\subsubsection{Average contraction of parametrized formulas}

We show in the next two examples, how a full contraction of the formula selecting map with a probability distribution or a knowledge base can be interpreted.

\begin{example}[Average satisfaction of formulas]
	The average of the formula satisfactions in $\formulaset$ giben a probability tensor $\probtensor$ is 
		\[ \frac{1}{\prod_{\selenumeratorin}\seldimof{\selenumerator}} \cdot \sbcontraction{\probtensor,\sencodingof{\formulaset}} \, . \]
\end{example}


\begin{example}[Deciding whether any formula is not contradicted]
	For example: We want to decide, whether there is a formula in $\formulaset$ not contradicted by a Knowledge base $\kb$.
	This is the case if and only if 
		\[ \sbcontraction{\kb,\sencodingof{\formulaset}} = 0 \, .  \]
	We use \lemref{lem:relToSelFSN} to get that $\sencodingof{\formulaset}=\fselectionmap$.
	When the formulas are representable in a folded scheme, we find tensor network decompositions of $\fselectionmap$ and exploit them along efficient representations of $\kb$ in an efficient calculation of $\sbcontraction{\kb,\sencodingof{\formulaset}} $.
	This is further equal to 
		\[ \kb \models \lnot \left( \bigvee_{\exformula\in\formulaset} \exformula\right) \, . \]
\end{example}



%\subsubsection{Neuro-Symbolic Architectures}
%
%%% Neuro-Symbolic Architecture
%We understand selector tensor networks as a neuro-symbolic architecture, where the selector variables are understood as parameters and the processed variables as neural activation variables.
%The orientation of the tensor network organizes the variables in layers.




\subsection{Examples of formula selecting neural networks}

%Here we provide examples of exponential families
%\red{See further: \charef{cha:energyRepresentation} studying the exact representation of energy as a weighted superposition of formulas.}

%\subsubsection{Boltzmann Machines}
%The inverse is the representation of a given distribution by a Boltzmann machine?


\subsubsection{Correlation}


For example (see Figure \ref{fig:AndSupFTDecomposition}) consider the logical neuron with single activation candidate $\{\land\}$ and two variable selectors selecting $\catorder$ atomic variables $\shortcatvariables$.
The expressivity of this network is the set of all conjunctions of the atoms
	\[ \{\catvariableof{\atomenumerator} \land \catvariableof{\secatomenumerator} \, : \, \atomenumerator,\secatomenumerator\in[\atomorder] \} \]


% Covariance measure
Contracting with a probability distribution, we use the tensor
	\[ \hypercoreat{\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}} = \sbcontractionof{\fsnn}{\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}} \]
to read of covariances as
	\[ \mathrm{Cov}(\catvariableof{\atomenumerator},\catvariableof{\secatomenumerator}) = \hypercoreat{\selvariableof{\vselectionsymbol,0}=\atomenumerator,\selvariableof{\vselectionsymbol,1}=\secatomenumerator}  - 
	\hypercoreat{\selvariableof{\vselectionsymbol,0}=\atomenumerator,\selvariableof{\vselectionsymbol,1}=\atomenumerator}  \cdot \hypercoreat{\selvariableof{\vselectionsymbol,0}=\secatomenumerator,\selvariableof{\vselectionsymbol,1}=\secatomenumerator} \, .  \]
	
	
	

%	\[ \skeleton = \placeholderof{1} \land \placeholderof{2} \]
%with the candidates for each placeholder being a set of $\atomorder$ atoms.

\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/formula_selection/and_supft_decomposition.tex}
\end{center}
\caption{Superposition of the encoded formulas $\rencodingof{\atomicformulaof{\selindexof{1}} \land \atomicformulaof{\selindexof{2}}}$ with weight $\canparam_{\selindexof{1} \selindexof{2}}$}
\label{fig:AndSupFTDecomposition}
\end{figure}



\subsubsection{Conjunctive and Disjunctive Normal Forms}%\label{sec:CNFasFormulaSelection}

% Architecture
We can represent any propositional knowledge base by the following scheme:
Literal selecting neurons by connective identity/negation (selecting positive/negative literal) and selecting one of the atoms.
Single output neuron representing the disjunction combining the literal selecting neurons.
Number of neurons defined by the maximal clause size plus one.
Smaller clauses can be covered when adding False as a possible choice (The respective neuron has to choose the identity, otherwise the full clause will be trivial).

% Parameter
The parameter core is in the basis CP format and each slice selects a clause of the knowledge base.
When taking the slice values to infinity (e.g. by an annealing procedure), the represented member of the exponential family converges to the uniform distribution of the models of the knowledge base.


\red{Useful to derive basis+ CP format based on CNF!}


% Representation by selection tensor networks
\begin{remark}[Minterms and Maxterms]
	All minterms and maxterms can be represented by a two layer selection tensor networks without variable selection in two layers.
	The bottom layer has an $\lnot/\mathrm{Id}$ connective selection neuron to each atom and the upper layer consists of a single $\atomorder$ary conjunction.
\end{remark}




\subsection{Extension to variables of larger dimension}

Connective selecting tensors: Can encode arbitrary functions $h_{\selindex}$ of discrete variables, but need $\catvariableof{\cselectionmap}$ to be an enumeration of the states, in particular to be of dimension
	\[ \catdimof{\cselectionmap} = \cardof{ \cup_{\selindexin} \imageof{h_{\selindex}} } \, . \]
	
Variable selecting tensors can be understood as specific cases of connective selecting tensors and can thus also be generalized in a straight forward manner by
	\[ \catdimof{\cselectionmap} = \cardof{ \cup_{\selindexin} \imageof{h_{\selindex}} } \, .  \]
	
State selecting tensors are directly 


\begin{example}[Discretization of a continuous neuron]
	Let there be a neuron
		\[ \sigma( w, y) : \rr \times \rr^{\catorder} \rightarrow \rr \, .\]
	When $w \in \arbsetof{weight}\subset \rr^{\catorder}$ and $x \in \arbsetof{x}\subset \rr^{\catorder}$ have
		\[ \cardof{\sigma( \contractionof{w, x} )} \leq \cardof{\arbsetof{weight}} \cdot \cardof{\arbsetof{x}} \, . \] 

	To represent the discretization of the neuron, we use the subset encoding scheme of \defref{def:subsetEncoding}.
	The variables $\indvariableof{weight}$ indexing $\arbsetof{weight}$ will be understood as selection incoming variables and the variables $\indvariableof{weight}$ indexing $\arbsetof{weight}$ as categorical incoming variables.
	We further define a variable $\indvariableof{\sigma}$ indexing $\imageof{\restrictionofto{\sigma}{\arbsetof{weight}\times\arbsetof{x}}}$ and have a tensor
		\[ \rencodingofat{\sigma}{\indvariableof{\sigma},\indvariableof{\arbsetof{x}},\indvariableof{weight}} \, . \] 

	If the neuron is of the form
		\[ \sigma(w,x) = \psi(\sum_i w_i \cdot x_i)\]
	a decomposition into multiplication at each coordinate and summation of the results, with relational encodings for each, can be done.
\end{example}
