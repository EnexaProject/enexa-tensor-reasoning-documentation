\chapter{\chatextformulaSelection}\label{cha:formulaSelection}

% Purpose
In this chapter we will investigate efficient schemes to represent collections of propositional formulas with similar structure by a single tensor network.
% Technical
We introduce the mechanism of selection sparsity, which utilizes selection variables to represent common structure of formulas as a tensor network.

\sect{Formula Selecting Maps}

In \charef{cha:logicalRepresentation} we introduced with decomposition sparsity a scheme of tensor network representation of a single propositional formula.
Let us now extend this scheme to selection sparsity, which provides a sparse representation of sets of propositional formulas.
To this end, we first define formula selecting maps and then derive tensor network representation for them.
%Selection sparsity

% basis encoding of the selection map
\begin{definition}
    Given a set of $\seldim$ propositional formulas $\{\formulaof{\selindex} : \selindexin\}$, the formula selecting map is the map
    \begin{align*}
        \fselectionmap \defcols \atomstates \rightarrow \bigtimes_{\selindex\in\seldim} [2]
    \end{align*}
    defined for $\shortcatindices\in\atomstates$ as
    \begin{align*}
        \fselectionmapat{\shortcatindices}
        = \bigtimes_{\selindex\in\seldim} \formulaofat{\selindex}{\shortcatindices} \, .
    \end{align*}
\end{definition}

% Selection Variables
A tensor representation of a formula selecting map is provided by the selection encoding (see \defref{def:selectionEncoding})
\begin{align*}
    \sencfselectionmapat{\shortcatvariables,\selvariable}
\end{align*}
where the selection variable $\selvariable$ takes values in $[\seldim]$ and selects specific formulas in the set $\{\formulaof{\selindex} : \selindexin\}$.
By definition, we have for any $\shortcatindices\in\atomstates$ and $\selindexin$
\begin{align*}
    \sencfselectionmapat{\indexedshortcatvariables,\indexedselvariable}
    =  \formulaofat{\selindex}{\shortcatvariables=\atomindices} \, .
\end{align*}
% Decomposition
This selection encoding is thus the sum
\begin{align*}
    \sencfselectionmapat{\shortcatvariables,\selvariable}
    = \sum_{\selindexin} \formulaofat{\selindex}{\shortcatvariables}
    \otimes \onehotmapofat{\selindex}{\selvariable} \, .
\end{align*}
Such a representation scheme requires linear resources in the number of formulas.
We will show in the following, that we can exploit common structure in formulas to drastically reduce this resource consumption.
Central to this sparse representation scheme are basis encodings of the selection encodings $\sencodingof{\fselectionmap}$ (see \secref{sec:selectionEncodingNotation}).
To this end we understand the tensor $\sencodingof{\fselectionmap}$ as a boolean function by its coordinate evaluation map and define
\begin{align*}
    \bsencodingofat{\fselectionmap}{\headvariable,\shortcatvariables,\selvariable}
    = \sum_{\shortcatindicesin}\sum_{\selindexin} \onehotmapofat{\sencodingofat{\fselectionmap}{\indexedshortcatvariables,\indexedselvariable}}{\headvariable} \otimes \onehotmapofat{\shortcatindices}{\shortcatvariables}  \otimes \onehotmapofat{\selindex}{\selvariable} \, .
\end{align*}
These basis encodings represent the selection encoding by
\begin{align*}
    \sencfselectionmapat{\shortcatvariables,\selvariable}
    = \contractionof{\tbasisat{\headvariable},\bsencodingofat{\fselectionmap}{\headvariable,\shortcatvariables,\selvariable}}{\shortcatvariables,\selvariable} \, .
\end{align*}
The mechanism of {\em \selectionSparsity{}} now applies this equations to find tensor network decompositions of $\sencfselectionmap$ based on decompositions of $\bsencodingof{\fselectionmap}$.
%It is thus a refinement of \decompositionSparsity{}, which is used to represent $\bencodingof{\fselectionmap}$.
We depict the basis encoding $\bsencodingof{\fselectionmap}$ of formula selecting maps in \figref{fig:formulaSelectionMap}.
% Selection Sparsity




% Depiction
\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/formula_selection/formula_selecting_map.tex}
    \end{center}
    \caption{Representation of the basis encoding $\bsencodingof{\fselectionmap}$ to a selection encoded formula selecting map as an
    a) Factor of a Graphical Model with a selection variable $\selvariable$ and a computed variable $\headvariableof{\fselectionmap}$.
    b) Decorating Tensor Core with selection variable corresponding with an additional axis.}
    \label{fig:formulaSelectionMap}
\end{figure}


\sect{Construction Schemes}

% Naturality of folding
Let us now investigate efficient schemes to define sets of formulas to be used in the definition of $\fselectionmap$.
We will motivate the folding of the selection variable into multiple selection variables by compositions of selection maps.


\subsect{Connective Selecting Maps} % ! This is more or less the same as the general formula selecting map

We represent choices over connectives with a fixed number of arguments by adding a selection variable to the cores and defining each slice by a candidate connective.

% Formal map
\begin{definition}
    \label{def:connectiveSelector}
    Let $\{\connectiveof{0},\ldots,\connectiveof{\seldimof{\cselectionsymbol}-1}\}$ be a set of connectives with $\atomorder$ arguments.
    The associated connective selection map is
    \begin{align*}
        \cselectionmap\defcols \atomstates \rightarrow \bigtimes_{\selindex\in[\seldimof{\cselectionsymbol}]}[2]
    \end{align*}
    defined for $\shortcatindices\in\atomstates$ as
    \begin{align*}
        \cselectionmapat{\shortcatindices} = \bigtimes_{\selindex\in[\seldimof{\cselectionsymbol}]} \connectiveofat{\selindex}{\shortcatvariables=\shortcatindices} \, .
    \end{align*}
%    \[ \cselectionmapat{\shortcatvariables,\selvariableof{\cselectionsymbol}}
%   \defcols \atomstates \times [\seldimof{\cselectionsymbol}] \rightarrow [2] \]
%    defined for each $\selindexofin{\cselectionsymbol}$ and $\shortcatindices\in\atomstates$ by
%    \[ \cselectionmapat{\shortcatvariables=\shortcatindices,\indexedselvariableof{\cselectionsymbol}}
%    = \connectiveofat{\selindexof{\cselectionsymbol}}{\shortcatvariables=\shortcatindices}  \, . \]
\end{definition}

%We depict the basis encoding of connective selection maps in \figref{fig:connectiveSelector}.
%
%\begin{figure}[t] % Already drawn
%    \begin{center}
%        \input{PartII/tikz_pics/formula_selection/connective_selector.tex}
%    \end{center}
%    \caption{Connective Selector.}
%    \label{fig:connectiveSelector}
%\end{figure}


\subsect{Variable Selecting Maps}

Choices of connectives can be combined with selections of variables assigned building the arguments of a connective.
To this end, we introduce variable selecting maps.

%% Definition
\begin{definition}
    \label{def:variableSelector}
    The selection of one out of $\seldim$ variables in a list $\catvariableof{[\seldim]}$ is done by variable selecting maps
    \begin{align}
        \vselectionmap\defcols \bigtimes_{\selindex\in[\seldim]}[2] \rightarrow \bigtimes_{\selindex\in[\seldim]}[2] \, .
        % \vselectionmapat{\catvariableof{[\seldim]},\selvariableof{\vselectionsymbol}}:  \left(\bigtimes_{\selindex\in[\seldim]}[2]\right) \times [\seldim]  \rightarrow [2]
    \end{align}
    defined as the identity map
    \begin{align*}
        \vselectionmapat{\catvariableof{[\seldim]}} = \catvariableof{[\seldim]} \, .
    \end{align*}
%    are defined coordinatewise by
%    \begin{align}
%        \vselectionmapat{\indexedcatvariableof{0},\ldots,\indexedcatvariableof{\seldim-1},\indexedselvariableof{\vselectionsymbol}} = \catindexof{\selindex} \, .
%    \end{align}
\end{definition}

The selection encoding of the variable selecting map is the tensor $\sencvselectionmapat{\catvariableof{[\seldim]},\selvariableof{\vselectionsymbol}}$
\begin{align*}
    \sencvselectionmapat{\indexedcatvariableof{[\seldim]},\indexedselvariableof{\vselectionsymbol}} = \catindexof{\selindexof{\vselectionsymbol}} \, .
\end{align*}
% Interpretation as multiplex gate
Selection encodings of variable selecting maps appear in the literature as multiplex gates (see e.g. Definition 5.3 in \cite{koller_probabilistic_2009}).
The basis encoding of the variable selection map has a decomposition
\begin{align*}
    \bsencodingofat{\vselectionmap}{\vselectionheadvar,\catvariableof{[\seldimof{\vselectionsymbol}]},\selvariableof{\vselectionmap}}
    = \sum_{\selindexofin{\vselectionsymbol}}
    \identityat{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}}}
    \otimes \onesat{\catvariableof{[\seldimof{\vselectionsymbol}/\{\selindexof{\vselectionsymbol}\}]}}
    \otimes \onehotmapofat{\selindexof{\vselectionsymbol}}{\selvariableof{\vselectionsymbol}} \, .
\end{align*}
This structure is exploited in the next theorem to derive a tensor network decomposition of $\bsencodingof{\vselectionmap}$.

\begin{theorem}[Decomposition of Variable Selecting Maps]
    \label{the:varSelectorDecomposition}
    Given a list $\catvariableof{[\seldimof{\vselectionsymbol}]}$ of variables, we define for each $\selindexofin{\vselectionsymbol}$ the tensors
    \begin{align*}
        \selectorcomponentofat{\selindexof{\vselectionsymbol}}{\catvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}}
        = \identityat{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}}} \otimes \onehotmapofat{\selindexof{\vselectionsymbol}}{\selvariableof{\vselectionsymbol}}
        + \onesat{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}}}
        \otimes \left(\onesat{\selvariableof{\vselectionsymbol}} - \onehotmapofat{\selindexof{\vselectionsymbol}}{\selvariableof{\vselectionsymbol}} \right) \, .
    \end{align*}
    Then we have (see \figref{fig:SelectorDecomposition})
    \begin{align*}
        \bsencodingofat{\vselectionmap}{\vselectionheadvar,\catvariableof{[\seldim]},\selvariableof{\vselectionsymbol}}
    = \contractionof{
        \{\selectorcomponentofat{\selindexof{\vselectionsymbol}}{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}} \wcols \selindexofin{\vselectionsymbol}\}
    }{\vselectionheadvar,\catvariableof{[\seldim]},\selvariableof{\vselectionsymbol}} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    We show the equivalence of the tensors on an arbitrary coordinates.
    For $\tilde{\selindex}_{\vselectionsymbol}\in[\seldimof{\vselectionsymbol}]$, $\vselectionheadvar\in[2]$ and $\catindexof{[\seldimof{\vselectionsymbol}]}\in\bigtimes_{\catenumerator\in[\seldimof{\vselectionsymbol}]}[2]$ we have
    \begin{align*}
        & \contractionof{
            \{\selectorcomponentofat{\selindexof{\vselectionsymbol}}{\vselectionheadvar,\catvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}} \wcols \selindexofin{\vselectionsymbol}\}
        }{\indexedheadvariableof{\vselectionsymbol},\indexedcatvariableof{[\seldim]},\selvariableof{\vselectionsymbol} = \tilde{\selindex}_{\vselectionsymbol}} \\
        & \quad =
        \prod_{\selindexofin{\vselectionsymbol}} \selectorcomponentofat{\selindexof{\vselectionsymbol}}{
            \indexedheadvariableof{\vselectionsymbol},\indexedcatvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}=\tilde{\selindex}_{\vselectionsymbol}
        } \\
        & \quad = \selectorcomponentofat{\tilde{\selindex}_{\vselectionsymbol}}{
            \indexedheadvariableof{\vselectionsymbol},\indexedcatvariableof{\selindexof{\vselectionsymbol}},\selvariableof{\vselectionsymbol}=\tilde{\selindex}_{\vselectionsymbol}
        } \\
        & \quad =
        \begin{cases}
            1 & \ifspace \headindexof{\vselectionsymbol} = \catindexof{\selindexof{\vselectionsymbol}} \\
            0 & \text{else}
        \end{cases} \\
        & = \bsencodingofat{\vselectionmap}{\indexedheadvariableof{\vselectionsymbol},\indexedcatvariableof{[\seldim]},\selvariableof{\vselectionsymbol}=\tilde{\selindex}_{\vselectionsymbol}}
    \end{align*}
    In the second equality, we used that the tensor $\selectorcomponentof{\selindexof{\vselectionsymbol}}$ have coordinates $1$ whenever $\tilde{\selindex}_{\vselectionsymbol}\neq\selindexof{\vselectionsymbol}$.
\end{proof}

The decomposition provided by \theref{the:varSelectorDecomposition} is in a $\cpformat$ format (see \charef{cha:sparseRepresentation}).
The introduced tensors $\selectorcomponentof{\selindexof{\vselectionsymbol}}$ are boolean, but not directed and therefore basis encodings of relations but not functions (see \charef{cha:basisCalculus}).

%% Interpretation
%The selectorcores $\selectorcoreof{\selindexof{1}}$ are contracted with the parameter cores and select the respective atom when contracted with truth vector tensormultiplied by constant cores (as placeholder for the other possible atoms).
%Decomposed into disconnected strands for each atomkey, which connect on the selection axis and on the atom truth axis.

\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/formula_selection/variable_selector.tex}
    \end{center}
    \caption{Decomposition of the basis encoding of a variable selecting tensor into a network of tensors defined in \theref{the:varSelectorDecomposition}.
    The decomposition is in a $\cpformat$ format (see \charef{cha:sparseRepresentation}). %, when grouping the indices  $\selindexof{\selenumerator}$ and $\atomlegindexof{\atomicformulaof{\selindexof{\selenumerator}}}$).
    }
    \label{fig:SelectorDecomposition}
\end{figure}




\sect{State Selecting Maps}

When the variables to be selected are the atomization variables to the same categorical variable (see \secref{sec:categoricalTN}), one can avoid the instantiation of all atomization cores and instead represent the variable selecting map using the categorical variable only.
To show this we introduce the state selecting map to a categorical variable $\catvariable$.

\begin{definition}
    \label{def:stateSelector}
    Given a categorical variable $\catvariableof{\sselectionsymbol}$ with dimension $\catdimof{\sselectionsymbol}$ and a selection variable $\selvariableof{\sselectionsymbol}$ with dimension $\seldimof{\sselectionsymbol}=\catdimof{\sselectionsymbol}$ the state selecting map is the map
    \begin{align*}
        \sselectionmap\defcols[\catdimof{\sselectionsymbol}] \rightarrow \bigtimes_{\catenumerator\in[\catdimof{\sselectionsymbol}]} [2]
    \end{align*}
    defined for $\catindexofin{\sselectionsymbol}$ as
    \begin{align*}
        \sselectionmapat{\catindexof{\sselectionsymbol}} = \onehotmapofat{\catindexof{\sselectionsymbol}}{\selvariableof{\sselectionsymbol}} \, .
    \end{align*}
%    \[ \sselectionmapat{\catvariableof{\sselectionsymbol},\selvariableof{\sselectionsymbol}}\defcols[\catdimof{\sselectionsymbol}] \times [\seldimof{\sselectionsymbol}] \rightarrow [2] \]
%    is defined on $\catindexofin{\sselectionsymbol}$ and $\selindexofin{\sselectionsymbol}$ by
%    \begin{align*}
%        \sselectionmapat{\indexedcatvariableof{},\indexedselvariableof{\sselectionsymbol}} =
%        \begin{cases}
%            1 & \ifspace \catindex = \selindexof{\sselectionsymbol} \\
%            0 & \text{else}
%        \end{cases} \, .
%    \end{align*}
\end{definition}

The selection encoding of the state selecting map coincides with the dirac delta tensor, that is for $\catindexofin{\sselectionsymbol}$ and $\selindexofin{\sselectionsymbol}$ we have
\begin{align*}
    \sencsselectionmapat{\catvariableof{\sselectionsymbol},\selvariableof{\sselectionsymbol}} =
    \begin{cases}
        1 & \ifspace \catindex = \selindexof{\sselectionsymbol} \\
        0 & \text{else}
    \end{cases} \, .
\end{align*}

The relation of the variable selecting map and the state selecting map is shown in the next lemma.

\begin{lemma}
    \label{lem:stateSelectorVsVariableSelector}
    Let $\catvariableof{[\seldim]}$ be a collection of atomization variables to a categorical variable $\catvariable$ taking values in $[\seldim]$.
    Then we have for
    \begin{align*}
        \contractionof{\bencodingofat{\categoricalmap}{\catvariableof{[\seldim]},\catvariable},
            \sencvselectionmapat{\catvariableof{[\seldim]},\selvariable}
        }{\catvariable,\selvariable}
        =
        \sencsselectionmapat{\catvariable,\selvariable} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    For each $\catindex,\selindexin$ we have that
    \begin{align*}
        \contractionof{\bencodingofat{\categoricalmap}{\catvariableof{[\seldim]},\catvariable},
            \sencvselectionmapat{\catvariableof{[\seldim]},\selvariable}
        }{\indexedcatvariable,\indexedselvariable}
        &=
        \contraction{\bencodingofat{\categoricalmap}{\catvariableof{[\seldim]},\indexedcatvariable},
            \sencvselectionmapat{\catvariableof{[\seldim]},\indexedselvariable}
        } \\
        &= \begin{cases}
               1 & \ifspace \catindex = \selindex \\
               0 & \text{else}
        \end{cases}
    \end{align*}
    which is thus equal to $\sencsselectionmapat{\indexedcatvariable,\indexedselvariable}$.
\end{proof}

\lemref{lem:stateSelectorVsVariableSelector} shows that when the variables to be selected are the atomization variables to a categorical variable, the state selecting map can thus be used instead of the variable selecting map.
The state selecting map has the advantage, that the instantiation of the tensor $\bencodingofat{\categoricalmap}{\catvariableof{[\seldim]},\catvariable}$ enforcing the categorical constraint can be avoided.

% Comment: Alternative based on categorical constraints to be introduced later
%State selecting tensors can also be realized by variable selecting tensors.
%In \secref{sec:categoricalTN} we have described methods to build atomic variables indicating the states of a categorical variable.
%This would, however, increase the number of variables in a tensor network and can thus lead to an exponential overhead of dimensions.
%State selecting tensors can therefore be seen as a mean to avoid such dimension increases.

\sect{Composition of Formula Selecting Maps}

We will now parametrize the sets $\formulaset$ with additional indices and define formula selector maps subsuming all formulas.
To handle large sets of formulas, we further fold the selection variable into tuples of selection variables.

\begin{definition}
    \label{def:formulaSelector}
    Let there be a formula $\formulaof{\selindexlist}$ for each index tuple in $\selindexlist\in\selstates$, where $\selorder,\seldimof{0},\ldots,\seldimof{\selorder-1}\in\nn$.
    The folded formula selection map (see \figref{fig:foldedSelector}) is the map
    \begin{align*}
        \fselectionmap \defcols \atomstates \rightarrow \bigtimes_{\selindexofin{0}} \cdots \bigtimes_{\selindexofin{\selorder-1}} [2]
    \end{align*}
    defined as
    \begin{align*}
        \fselectionmapat{\shortcatindices} = \left( \formulaofat{\shortselindices}{\shortcatvariables=\shortcatindices} \right)_{\shortselindices\in\selstates} \, .
    \end{align*}
\end{definition}

Folding the selection variable into multiple selection variables is especially useful to find efficient decomposition schemes of the formula selecting maps.
In the reminder of this section we will provide an example, where each selection variable is constructed to parameterize a local change to the formula with the result that the basis encoding of the global formula selecting map decomposes into local formula selecting maps.


\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/formula_selection/folded_selector.tex}
    \end{center}
    \caption{Basis encoding of the folded map $\fselectionmap$.}
    \label{fig:foldedSelector}
\end{figure}




\subsect{Formula Selecting Neuron}


% Motivating foldings by composition
The folding of the selection variable is motivated by the composition of selection maps.
We call the composition of a connective selection (see \defref{def:connectiveSelector}) with variable selection maps (see \defref{def:variableSelector}) for each argument a formula selecting neuron.


\begin{definition}
    \label{def:fsNeuron}
    Given an order $\selorder\in\nn$ let there be a connective selector $\selvariable_{\exconnective}$ selecting connectives of order $\selorder$ and let $\vselectionmapof{0},\ldots,\vselectionmapof{\selorder-1}$ be a collection of variable selectors.
    The corresponding logical neuron is the map
    \begin{align*}
        \lneuron\defcols\atomstates \rightarrow \bigtimes_{\selindexofin{\cselectionsymbol}} \bigtimes_{\selindexofin{0}} \cdots \bigtimes_{\selindexofin{\selorder-1}} [2]
    \end{align*}
    defined for $\shortcatindices\in\atomstates$ by
    \begin{align*}
        \lneuronat{\shortcatindices}
        = \left( \connectiveof{\selindexof{\cselectionsymbol}(\catvariableof{\selindexof{0}},\ldots,\catvariableof{\selindexof{\selorder-1}})}\right)_{\selindexofin{\cselectionsymbol},\selindexofin{0},\ldots,\selindexofin{\selorder-1}}
    \end{align*}
%    \begin{align*}
%        \lneuronat{\shortcatvariablelist,\shortselvariablelist}
%       \defcols\left(\atomstates\right) \times [\seldimof{\cselectionsymbol}] \times \left( \bigtimes_{\selenumeratorin} [\seldimof{\selenumerator}]\right) \rightarrow [2]
%    \end{align*}
%    defined for $\shortcatindices\in\atomstates$, $\selindexof{\cselectionsymbol}\in[\seldimof{\cselectionsymbol}]$ and
%    $\selindices\in \bigtimes_{\selenumeratorin} [\seldimof{\selenumerator}]$ by
%    \begin{align*}
%        \lneuron(\atomindices, \selindexof{\cselectionsymbol}, \selindices) =
%        \cselectionmap(\vselectionmapof{0}(\atomindices, \selindexof{0}),\ldots,\vselectionmapof{\selorder-1}(\atomindices,\selindexof{\selorder-1}), \selindexof{\cselectionsymbol}) \, .
%    \end{align*}
\end{definition}

% Tensor Network Decomposition
Each neuron has a tensor network decomposition by a connective selector tensor and a variable selector tensor network for each argument, as we state in the next theorem.

\begin{theorem}{Decomposition of formula selecting neurons}
    \label{the:neuronDecomposition}
    Let $\lneuron$ a logical neuron, defined for a connective selector $\selvariable_{\exconnective}$ and variable selectors $\vselectionmapof{0},\ldots,\vselectionmapof{\selorder-1}$.
    Then we have (see \figref{fig:neuronDecomposition} for the example of $\selorder=2$):
    \begin{align*}
        &\bsencodingofat{\lneuron}{\headvariableof{\lneuron},\shortcatvariables,\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\ldots,\selvariableof{\vselectionsymbol,\selorder-1}} \\
        &\quad = \langle\{\bsencodingofat{\cselectionmap}{
            \headvariableof{\lneuron},\headvariableof{\vselectionsymbol,0},\ldots,\headvariableof{\vselectionsymbol,\selorder-1}}, \\
        & \quad\quad\quad\bsencodingofat{\vselectionmapof{0}}{
            \headvariableof{\vselectionsymbol,0},\shortcatvariables,\selvariableof{\vselectionsymbol,0}},\ldots,
        \bsencodingofat{\vselectionmapof{\selorder-1}}{
            \headvariableof{\vselectionsymbol,\selorder-1},\shortcatvariables,\selvariableof{\vselectionsymbol,\selorder-1}}
        \} \rangle
        \left[\headvariableof{\lneuron},\shortcatvariables, \selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\ldots,\selvariableof{\vselectionsymbol,\selorder-1}\right] \, .
    \end{align*}
\end{theorem}
\begin{proof}
    By composition \theref{the:compositionByContraction}.
\end{proof}


%% Example of a FSNN: A skeleton expression, where only the atoms are varied.
%Given a skeleton expression and a set of candidates at each placeholder, we parameterize a set of formulas by the assignment of candidate atoms to each placeholder position.
%Let us denote the set of formulas, which are generated through choosing atoms from $\candidatelistof{\selenumerator}$ for the skeleton formula $\skeleton$ by
%		\[ \formulasetof{\skeleton} \coloneqq
%	 \left\{ \skeletonof{\placeholderof{1},\ldots,\placeholderof{\atomorder}} \wcols \placeholderof{\atomenumerator} \in \candidatelistof{\atomenumerator} \right\} \]

%We now enumerate at each position $\selenumerator$ the list of candidates $\candidatelistof{\selenumerator}$ using an index $\selindexof{\selenumerator}\in[\seldimof{\selenumerator}]$ and parametrize the choice of the $\selindexof{\selenumerator}$ for the placeholder $\placeholderof{\selenumerator}$ by unit vectors
%	\[ \unitvectoratof{\selenumerator}{\selindexof{\selenumerator}} \in \rr^{\seldimof{\selenumerator}} \, . \]
%We thus have a parameter space $\parspace$ parametrizing the possible assignments to the skeleton in its basis vectors.

\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/formula_selection/logical_neuron.tex}
    \end{center}
    \caption{Example of a logical neuron $\lneuron$ of order $\selorder=2$.
    a) Selection and categorical variables and their interdependencies visualized in a hypergraph.
    b) Basis encoding of the logical neuron and tensor network decomposition into variable selecting and connective selecting tensors.
    }
    \label{fig:neuronDecomposition}
\end{figure}


\subsect{Formula Selecting Neural Network}

% Enhancement of the Expressivity
Single neurons have a limited expressivity, since for each choice of the selection variables they can just express single connectives acting on atomic variables.
The expressivity is extended to all propositional formulas, when allowing for networks of neurons, which can select each others as input arguments.


\begin{definition}
    \label{def:fsNeuralNetwork}

%	We call a graph consistent of nodes decorated by formula selecting neurons and directed edges representing the argument dependencies of the neuron on other neurons, an architecture graph.
%	An acyclic architecture graph is called a formula selecting neural network.	
%	Formula selecting neurons, which are not included by other formula selecting neurons are called output neurons and collected in the variables $\catvariableof{\larchitecture}$. 
%	A logical neural network is a collection of logical neurons, such that the network graph (nodes: neurons, edges: directed representing argument dependencies) is acyclic (a DAG).

    An architecture graph $\graphof{\larchitecture}=(\nodesof{\larchitecture},\edgesof{\larchitecture})$ is an acyclic directed hypergraph with nodes appearing at most once as outgoing nodes.
    Nodes appearing only as outgoing nodes are input neurons and are labeled by $\inneuronset$ and nodes not appearing as outgoing nodes are the output neurons in the set $\outneuronset$ (see \figref{fig:architectureGraph} for an example).

    Given an architecture graph $\graphof{\larchitecture}=(\nodesof{\larchitecture},\edgesof{\larchitecture})$, a \emph{formula selecting neural network} $\fsnn$ is a tensor network of logical neurons at each $\lneuron\in\nodesof{\larchitecture}/\inneuronset$, such that each neuron depends on variables $\catvariableof{\parentsof{\lneuron}}$ and on selection variables $\selvariableof{\lneuron}$.
    The collection of all selection variable is notated by $\selvariableof{\larchitecture}$.

    The activation tensor of each neuron $\lneuron\in\nodesof{\larchitecture}/\inneuronset$ is
    \begin{align*}
        \lneuractivationat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}}
        = \contractionof{
            \{\bsencodingof{\tilde{\lneuron}} \wcols \tilde{\lneuron}\in\nodesof{\larchitecture}/\inneuronset \} \cup \{\tbasisat{\headvariableof{\lneuron}}\}
        }{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \, .
    \end{align*}

    The activation tensor of the formula selecting neural network is the contraction
    \begin{align*}
        \fsnnat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}}
        = \contractionof{
            \{\bencodingofat{\lneuractivation}{\headvariableof{\lneuron},\catvariableof{\parentsof{\lneuron}},\selvariableof{\larchitecture}} \wcols \lneuron\in\nodesof{\larchitecture}/\inneuronset \} \cup \{\tbasisat{\headvariableof{\lneuron}} \wcols \lneuron\in\outneuronset\}
        }{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \, .
    \end{align*}

    The expressivity of a formula selecting neural network $\fsnn$ is the formula set
    \begin{align*}
        \formulasetof{\larchitecture} = \left\{ \fsnnat{\catvariableof{\inneuronset},\indexedselvariableof{\larchitecture}}  \wcols \selindexof{\larchitecture}\in\selstates \right\} \, .
    \end{align*}

\end{definition}

% ? Extend by activation cone stuff
The activation tensor of each neuron depends in general on the activation tensor of its ancestor neurons with respect to the directed graph $\graphof{\larchitecture}$, and thus inherits the selection variables.

% Architecture graph -> Tensor Network
We notice that the architecture graph is a scheme to construct the variable dependency graph of the tensor network $\formulasetof{\larchitecture}$.
To this end, we replace each neuron $\lneuron\in\nodesof{\larchitecture}/\inneuronset$ by an output variable $\headvariableof{\lneuron}$ and further add selection variables $\selvariableof{\lneuron}$ to the directed edges, that is to each directed hyperedge $(\{\lneuron\}, \parentsof{\lneuron})\in\edgesof{\larchitecture}$ we construct a directed hyperedge $(\{\headvariableof{\lneuron}\}, \catvariableof{\parentsof{\lneuron}}\cup\selvariableof{\lneuron})$.

\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/formula_selection/architecture_graph.tex}
    \end{center}
    \caption{Example of an architecture graph $\graphof{\larchitecture}$ with input neurons $\inneuronset=\{\lneuronof{0},\lneuronof{1},\lneuronof{2},\lneuronof{3}\}$ and output neurons $\outneuronset=\{\lneuronof{6},\lneuronof{7}\}$
    }
    \label{fig:architectureGraph}
\end{figure}


\begin{theorem}
    Given fixed selection variables $\selvariableof{\larchitecture}$, the formula selecting neural network is the conjunction of output neurons, that is
    \begin{align*}
        \fsnnat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} %= \bigwedge_{\lneuron\in\outneuronset} \lneuronat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}}
        = \contractionof{\left\{\lneuractivationat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \wcols \lneuron\in\outneuronset\right\}}{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    By effective calculus (see \theref{the:effectiveConjunction}), we have
    \[ \contractionof{\bsencodingofat{\land}{\catvariableof{\land},\shortcatvariables},\tbasisat{\catvariableof{\land}}}{\shortcatvariables} = \bigotimes_{\catenumeratorin} \tbasisat{\catvariableof{\catenumerator}} \]
    and thus
    \begin{align*}
        \fsnnat{\catvariableof{\inneuronset},\selvariableof{\larchitecture}}
        = \contractionof{
            \{\bsencodingof{\lneuron} \wcols \lneuron\in\nodesof{\larchitecture}/\inneuronset \} \cup \{\bsencodingofat{\land}{\catvariableof{\land},\headvariableof{\lneuron}  \wcols \lneuron\in\outneuronset}, \tbasisat{\catvariableof{\land}}\}
        }{\catvariableof{\inneuronset},\selvariableof{\larchitecture}} \, .
    \end{align*}
\end{proof}


% Combination of decompositions
By the commutation of contractions, we can further use \theref{the:neuronDecomposition} to decompose each tensor $\bsencodingof{\lneuron}$ into connective and variable selecting components to get a sparse representation of a formula selecting neural network $\fsnn$.


\sect{Application of Formula Selecting Networks}

Formula selecting networks are efficient in the simultaneous representation of multiple formulas with similar structure.


%There are two main applications of formula selecting networks.
%First, when contracting the selection variables with a weight tensor we get a weighted sum of the parametrized formulas.
%Second, when contracting the categorical variables with a distribution or a knowledge base, we get a tensor storing the satisfaction rates respectively the world counts of the parametrized formulas.

%\subsect{Representation of selection encodings}
%
%The main application of formula selecting networks in this work is the efficient representation of selection encodings.
%This will be exploited in the sparse representation of exponential families by energies and in structure learning.
%In the next lemma we will show the correspondence of formula selecting networks and selection encodings.
%
%\begin{lemma}
%    \label{lem:relToSelFSN}
%    Given a set $\{\formulaof{\selindexlist} : \selindexlist\in\selstates\}$ of propositional formulas we define the statistic
%    \[ \formulaset : \catindices \rightarrow (\formulaof{\selindexlist}(\catindices))_{\selindexlist} \, . \]
%    and the formula selecting map
%    \[ \fselectionmap: \catindices , \selindexlist \rightarrow \formulaof{\selindexlist} (\catindices) \, . \]
%    Then
%    \[ \sencodingofat{\formulaset}{\shortcatvariablelist, \shortselvariablelist} = \fselectionmap\left[\shortcatvariablelist, \shortselvariablelist \right] \, .  \]
%\end{lemma}
%\begin{proof}
%    For any indices $\shortselindices\in\selstates$ and $\shortcatindices\in\atomstates$ we have
%    \begin{align*}
%        \sencodingofat{\formulaset}{\shortcatvariablelist=\shortcatindices, \shortselvariablelist=\shortselindices}
%        =  \formulaof{\selindexlist}(\catindices) =  \fselectionmap\left[\shortcatvariablelist=\shortcatindices, \shortselvariablelist=\shortselindices \right] \, .
%    \end{align*}
%\end{proof}
%
%%% Reason for basis encodings and selection encodings.
%Technically, basis encodings have been exploited to derive decompositions based on basis calculus.
%Selection encodings on the other hand enable the application of formula selecting networks as superpositions of formulas.



\subsect{Efficient Representation of Formulas}

% Exponentially many formulas represented by linear demand
Formula Selecting Neural Networks are means to represent exponentially many formulas with linear (in sum of candidates list lengths) storage.
Their contraction with probability tensor networks, is thus a batchwise evaluation of exponentially many formulas.
This is possible due to redundancies in logical calculus due to modular combinations of subformulas.

% Retrieve functions
We can retrieve specific formulas by slicing the selection variables, i.e. for $\selindices$ we have
\[ \exformula_{\selindices}[\shortcatvariables] = \sencfselectionmapat{\shortcatvariables,\selvariable=\selindices} \, .  \]

In a tensor network diagram we depict this by
\begin{center}
    \input{PartII/tikz_pics/formula_selection/formula_retrieval.tex}
\end{center}

% Interpretation by dynamic programming
Another perspective on the efficient formula evaluation by selection tensor networks is dynamic computing.
Evaluating a formula requires evaluations of its subformulas, which are done by subcontractions and saved for different subformulas due to the additional selection legs.

% Storage problem of solutions
However, we need to avoid contracting the tensor with leaving all selection legs open, since this would require exponential storage demand.
% Sparse algorithm
We can avoid this storage bottleneck by contraction of parameter cores $\canparam$ with efficient network decompositions along the selection variables. %extending the contractions by additional cores leaving less variable legs open.

% Gibbs sampling
In Gibbs Sampling (\algoref{alg:Gibbs}), one can use the energy-based approach to queries \theref{the:energyContractionQueries}, and contract basis vectors on all but one selection variables.

%\red{This is the case when contracting gradients of the parameter tensor networks in alternating least squares approaches.
%Other methods avoiding the bottleneck can be constructed by MCMC sampling, for example Gibbs Sampling.
%Here we only need to vary local components of the formula reflected in keeping only single variable legs open.}



\subsect{Batch Contraction of Parametrized Formulas}

Given a set $\formulaset$ of formulas, we build a formula selecting network parametrizing the formulas.
The contraction
\begin{align*}
    \contractionof{\extnet,\fselectionmap}{\shortselvariables}
\end{align*}
is a tensor containing the contractions of the formulas $\formulaof{\shortselindices}$ with an arbitrary tensor network $\extnet$ as
\begin{align*}
    \contraction{\extnet,\formulaof{\shortselindices}} = \contractionof{\extnet,\fselectionmap}{\shortselvariables=\shortselindices} \, .
\end{align*}


\subsect{Average Contraction of Parametrized Formulas}

We show in the next two examples, how a full contraction of the formula selecting map with a probability distribution or a knowledge base can be interpreted.

\begin{example}[Average satisfaction of formulas]
    The average of the formula satisfactions in $\formulaset$ given a probability tensor $\probtensor$ is
    \[ \frac{1}{\prod_{\selenumeratorin}\seldimof{\selenumerator}} \cdot \contraction{\probtensor,\sencodingof{\formulaset}} \, . \]
\end{example}


\begin{example}[Deciding whether any formula is not contradicted]
    For example: We want to decide, whether there is a formula in $\formulaset$ not contradicted by a Knowledge base $\kb$.
    This is the case if and only if
    \[ \contraction{\kb,\sencodingof{\formulaset}} = 0 \, .  \]
%    We use \lemref{lem:relToSelFSN} to get that $\sencodingof{\formulaset}=\fselectionmap$.
    When the formulas are representable in a folded scheme, we find tensor network decompositions of $\fselectionmap$ and exploit them along efficient representations of $\kb$ in an efficient calculation of $\contraction{\kb,\sencodingof{\formulaset}} $.
    This is further equal to
    \[ \kb \models \lnot \left( \bigvee_{\exformula\in\formulaset} \exformula\right) \, . \]
\end{example}


\sect{Examples of Formula Selecting Neural Networks}

\subsect{Correlation}

For example (see \figref{fig:AndSupFTDecomposition}) consider the logical neuron with single activation candidate $\{\land\}$ and two variable selectors selecting $\catorder$ atomic variables $\shortcatvariables$.
The expressivity of this network is the set of all conjunctions of the atoms
\[ \{\catvariableof{\atomenumerator} \land \catvariableof{\secatomenumerator} \wcols \atomenumerator,\secatomenumerator\in[\atomorder] \} \]


% Covariance measure
Contracting with a probability distribution, we use the tensor
\[ \hypercoreat{\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}} = \contractionof{\fsnn}{\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}} \]
to read of covariances as
\[ \mathrm{Cov}(\catvariableof{\atomenumerator},\catvariableof{\secatomenumerator}) = \hypercoreat{\selvariableof{\vselectionsymbol,0}=\atomenumerator,\selvariableof{\vselectionsymbol,1}=\secatomenumerator}  -
\hypercoreat{\selvariableof{\vselectionsymbol,0}=\atomenumerator,\selvariableof{\vselectionsymbol,1}=\atomenumerator}  \cdot \hypercoreat{\selvariableof{\vselectionsymbol,0}=\secatomenumerator,\selvariableof{\vselectionsymbol,1}=\secatomenumerator} \, .  \]


%	\[ \skeleton = \placeholderof{1} \land \placeholderof{2} \]
%with the candidates for each placeholder being a set of $\atomorder$ atoms.

\begin{figure}[t]
    \begin{center}
        \input{PartII/tikz_pics/formula_selection/and_supft_decomposition.tex}
    \end{center}
    \caption{Superposition of the encoded formulas ${\atomicformulaof{\selindexof{1}} \land \atomicformulaof{\selindexof{2}}}$ with weight $\canparamat{\indexedselvariableof{1},\indexedselvariableof{2}}$}
    \label{fig:AndSupFTDecomposition}
\end{figure}



\subsect{Conjunctive and Disjunctive Normal Forms}%\label{sec:CNFasFormulaSelection}

% Architecture
\red{
    We can represent any propositional knowledge base by the following scheme:
    Literal selecting neurons are logical neurons with connective identity/negation (selecting positive/negative literal) and selecting neurons select for each an atom.
    The single output neuron represents the disjunction, respectively the conjunction, combining the literal selecting neurons.
    The number of neurons defined by the maximal clause size plus one.
    Smaller clauses can be covered when adding False as a possible choice (The respective neuron has to choose the identity, otherwise the full clause will be trivial).
    This architecture will be discussed in more detail in \charef{cha:approximation} as $\cpformat$ selecting networks.
% Parameter
    The parameter core is in the basis $\cpformat$ format and each slice selects a clause of the knowledge base.
    In combination with polynomial decompositions, which will be provided in \charef{cha:networkRepresentation}, one can exploit this architecture to find sparse formula decompositions.
%When taking the slice values to infinity (e.g. by an annealing procedure), the represented member of the exponential family converges to the uniform distribution of the models of the knowledge base.
}

% Representation by selection tensor networks
\begin{remark}[Minterms and Maxterms]
    All minterms and maxterms can be represented by a two layer selection tensor networks without variable selection in two layers.
    The bottom layer has an $\lnot/\mathrm{Id}$ connective selection neuron to each atom and the upper layer consists of a single $\atomorder$ary conjunction.
\end{remark}




\sect{Extension to Variables of Larger Dimension}

While we here restricted on boolean variables, formula selecting networks can be extended to variables of larger cardinality.
\begin{itemize}
    \item Connective selecting tensors: Can encode arbitrary functions $h_{\selindex}$ of discrete variables, but need $\catvariableof{\cselectionmap}$ to be an enumeration of the states, in particular to be of dimension
    \[ \catdimof{\cselectionmap} = \cardof{ \cup_{\selindexin} \imageof{h_{\selindex}} } \, . \]
    \item Variable selecting tensors can be understood as specific cases of connective selecting tensors and can thus also be generalized in a straight forward manner by
    \[ \catdimof{\cselectionmap} = \cardof{ \cup_{\selindexin} \imageof{h_{\selindex}} } \, .  \]
    \item State selecting tensors are directly defined for larger dimensions
\end{itemize}


An example of such a more generic usage is a discretization scheme for continuous neurons.

\begin{example}[Discretization of a continuous neuron]
    Let there be a neuron by a map of weight vectors and input vectors to $\rr$, that is
    \[ \lneuron( w, x)\defcols \rr^{\catorder} \times \rr^{\catorder} \rightarrow \rr \, .\]
%	When $w \in \arbsetof{weight}\subset \rr^{\catorder}$ and $x \in \arbsetof{x}\subset \rr^{\catorder}$ have
    We restrict the weights to a subset $\arbsetof{weight}\subset\rr^{\catorder}$ and the input vectors to $\arbsetof{x}\subset\rr^{\catorder}$,
    If follows that
    \[ \cardof{\imageof{\restrictionofto{\lneuron}{\arbsetof{weight}\times\arbsetof{x}}}} \leq \cardof{\arbsetof{weight}} \cdot \cardof{\arbsetof{x}} \, . \]
    To discretize the neuron, we use the subset encoding scheme of \defref{def:subsetEncoding} and define enumeration variables $\indvariableof{weight}$, $\indvariableof{x}$ and $\indvariableof{\lneuron}$ enumerating $\arbsetof{weight}$, $\arbsetof{x}$ and $\imageof{\restrictionofto{\lneuron}{\arbsetof{weight}\times\arbsetof{x}}}$, which are accompanied by respective index interpretation functions.
    Then the basis encoding of the discretized neuron is
    \begin{align*}
        \bsencodingofat{\lneuron}{\indvariableof{\lneuron},\indvariableof{weight},\indvariableof{x}} \, .
        = \sum_{\indindexofin{weight},\indindexofin{x}}
        \onehotmapof{\invindexinterpretationofat{\lneuron(\indexinterpretationofat{weight}{\indindexof{weight}},\indexinterpretationofat{x}{\indindexof{x}})}{\lneuron}}{\indvariableof{\lneuron}}
        \otimes \onehotmapofat{\indindexof{weight}}{\indvariableof{weight}}
        \otimes \onehotmapofat{\indindexof{x}}{\indvariableof{x}} \, .
    \end{align*}
    If the neuron is of the form
    \[ \lneuron(w,x) = \psi(\sum_i w_i \cdot x_i)\]
    a decomposition into multiplication at each coordinate and summation of the results, with basis encodings for each, can be done.
    \red{Here the index interpretation variables are split into a selection enumerated by $i$ and each variable gets assigned to single cores in the decomposition.}
\end{example}
