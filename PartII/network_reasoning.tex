\chapter{\chatextnetworkReasoning}\label{cha:networkReasoning}

In this chapter we investigate the inference properties of Hybrid Logic Networks starting with characterizations of its mean parameter polytopes.
We investigate unconstrained parameter estimated for Markov Logic Networks and Hybrid Logic Networks, which are special cases of the backward maps introduced in \charef{cha:probRepresentation}.
We then motivate structure learning based on sparsity constraints on the parameters on the minterm exponential family and present heuristic strategies leading to efficient structure learning algorithms.

\sect{Mean parameters of Hybrid Logic Networks}

% Polytope
While mean parameter polytopes $\genmeanset$ to generic exponential families have been subject to \charef{cha:probReasoning}, we in this section restrict to the mean polytopes of hybrid logic networks, which we characterize using propositional logics.
Hybrid Logic Networks are exponential families, which statistic $\sstat$ consists of coordinates with $\imageof{\sstatcoordinateof{\selindex}}\subset\ozset$ and are therefore propositional formulas.
The convex polytope of mean parameters (see \defref{def:meanPolytope}) is for a statistic $\mlnstat$ of propositional formulas and a base measure $\basemeasure$
\begin{align*}
	\hlnmeanset = \left\{ \sbcontractionof{\probtensor,\sencmlnstat}{\selvariable} \, : \, \probtensor\in\bmrealprobof{\basemeasure} \right\} \, ,
\end{align*}
where by $\bmrealprobof{\basemeasure}$ we denote the set of all by $\basemeasure$ representable probability distributions.
By \theref{the:meanPolytopeConvHull} the convex polytope has a characterization as a convex hull
\begin{align}\label{eq:hlnMeansetConvCharacterization}
	\hlnmeanset
	= \convhullof{\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable} \, : \, \shortcatindices\in\atomstates, \, \basemeasureat{\indexedshortcatvariables}=1} \, .
\end{align}

% 0/1 Polytopes
We notice, that all $\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}$ are boolean vectors in $\parspace$.
The mean parameter polytopes are thus of $0/1$-polytopes \cite{ziegler_lectures_2000,gillmann_01-polytopes_2007}, from which a few obvious properties follow.
Since those are convex subsets of the cube $[0,1]^\seldim$, which vertices are all binary vectors, also each $\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}$ (with $\basemeasureat{\indexedshortcatvariables}=1$) is an extreme point.
Further, if for any $\selindexin$ we have $\meanparamat{\indexedselvariable}\in\{0,1\}$, then $\meanparamwith$ is in the boundary of the cube and thus also of $\hlnmeanset$

\red{In the following, we characterize the faces of the mean parameter }


\subsect{Vertices by hard logic networks}

\red{$\basemeasureofat{\formulaset,\meanparam}{\shortcatvariables}$ are the subset encodings of preimages of the statistics encoding.}

We exploit this characterization to show, that the vertices of $\hlnmeanset$ are exactly those reproducable by Hard Logic Networks.

\begin{theorem}\label{the:extremeCharacterizationHLN}
	Any set $\{\meanparamwith\}\subset\hlnmeanset$ of cardinality $1$ is a vertex of $\hlnmeanset$, if and only if its unique element $\meanparamwith$ is boolean and the formula
	\begin{align*}
		 \basemeasureofat{\formulaset,\meanparam}{\shortcatvariables}
			\coloneqq \bigwedge_{\selindexin} \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables}
	\end{align*}
	is satisfiable.
	In that case, $\meanparam$ is the mean parameter of the Hard Logic Network with formulas
	\begin{align*}
		\kb=\{\lnot^{(1-\meanparamat{\indexedselvariable})} \enumformula \, : \, \selindexin \} \, , % call this \formula^{\meanparam}
	\end{align*}
	where we denote by $\lnot^0$ the identity connective and by $\lnot^1=\lnot$ the logical negation.
\end{theorem}
\begin{proof}
	\proofrightsymbol: Let $\meanparam$ be an extreme point of $\hlnmeanset$.
		Since by \eqref{eq:hlnMeansetConvCharacterization} $\hlnmeanset$ is a convex hull of vectors, there exists a $\shortcatindices\in\atomstates$ such that
			\[ \meanparamwith = \sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}  \, . \]
		By definition of $\sencmlnstat$, $\meanparamwith$ is a boolean vector and for any $\selindexin$ we have
			\[ \enumformulaat{\indexedshortcatvariables} = \meanparamat{\indexedselvariable} \]
		and thus
			\[ \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\indexedshortcatvariables} = 1\, .  \]
		It follows that $\shortcatindices$ is also a model of $\basemeasureof{\formulaset,\meanparam}$ and $\basemeasureof{\formulaset,\meanparam}$ is satisfiable.

	\proofleftsymbol: To show the converse direction, let $\meanparamwith$ be a boolean vector such that $\basemeasureof{\formulaset,\meanparam}$ is satisfiable.
		Then there exists a model $\shortcatindices$ of $\basemeasureof{\formulaset,\meanparam}$.
		We have for any $\selindexin$
			\[ \sencodingofat{\formulaset}{\indexedshortcatvariables,\indexedselvariable} =  \enumformulaat{\indexedshortcatvariables} = \meanparamat{\indexedselvariable} \]
		and thus $\meanparamwith=\sencodingofat{\formulaset}{\indexedshortcatvariables,\selvariable}$.
		With the characterization \eqref{eq:hlnMeansetConvCharacterization} this establishes in particular $\meanparamwith\in\hlnmeanset$.
		Since $\meanparamwith$ is boolean and therefore an extreme point of the cube $\cubeof{\seldim}$, it is also an extreme point of the subset $\hlnmeanset\subset\cubeof{\seldim}$.
\end{proof}

\subsect{Faces of larger rank}

% True?
Since the by inclusion partially ordered set of faces is a graded lattice (see Theorem~2.7 in \cite{ziegler_lectures_2013}), the faces are ranked by dimension of th% the number of included vertices.

The face base measure is thus
\begin{align*}
	\basemeasureof{\facecondset}
	= \bigcup_{\meanparam\in\genfaceset\cup\{0,1\}^{\seldim}} \formula^{\meanparam}
	= \bigcup_{\meanparam\in\genfaceset\cup\{0,1\}^{\seldim}} \bigwedge_{\selindexin} \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformula \, .
\end{align*}


\subsect{Mean parameters in the interior}

By \theref{the:meanPolytopeInterior} the interior points are those realizable by a Hybrid Logic Network with statistics $\mlnstat$ and base measure $\basemeasure$, as we state in the following Corollary.

\begin{corollary}\label{cor:interiorCharacterizationHLN}
	If $\meanparamwith\in\interiorof{\hlnmeanset}$, or equivalently the statistic is minimal and $\meanparamwith$ is reproduceable by a distribution positive with respect to $\basemeasure$, then there is $\canparamat{\selvariable}$ such that $\expdistof{\formulaset,\canparam,\basemeasure}$ reproduces $\meanparamwith$.
\end{corollary}

\subsect{Mean parameters outside the interior}

By \theref{the:meanPolytopeInteriorCharacterization} mean parameter vectors outside the interior of $\hlnmeanset$ are not realizable by distributions, which are positive with respect to the base measure $\basemeasure$.
Instead, we in this section construct refined base measures $\secbasemeasure$ (refinement in the sense of $\secbasemeasure\prec\basemeasure$), such that there are distributions, which are positive with respect to $\secbasemeasure$ and represent such mean parameters.


% Sufficient criterion for boundary: 0/1 coordinates
First of all, we can use the criterion of mean parameter coordinates in $\{0,1\}$ as a sufficient condition for $\meanparamwith\notin\interiorof{\hlnmeanset}$.
In this case, the next theorem provides us with a procedure to refine the base measure in these cases.

\begin{theorem}[Base measure refinement for mean coordinates in $\{0,1\}$]\label{the:hlnMeanPolytopeReduction}
	To any $\meanparamwith\in\hlnmeanset$ we build the base measure
			\[ \basemeasureofat{\formulaset,\meanparam}{\shortcatvariables} \coloneqq \bigwedge_{\selindexin \, : \, \meanparamat{\indexedselvariable}\in\{0,1\}}
		\lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables} \, . \]
	Then any probability distribution reproducing $\meanparamwith$ has a representation with respect to the base measure
		\[ \secbasemeasureat{\shortcatvariables} =  \contractionof{\basemeasure,\basemeasureof{\formulaset,\meanparam}}{\shortcatvariables} \, . \]
\end{theorem}
\begin{proof}
	Let $\probtensor$ be a distribution representable with respect to $\basemeasure$ and reproducing $\meanparam$, that is
		\[ \contractionof{\probtensor,\sencmlnstat}{\selvariable} = \meanparamwith \, . \]
	For any $\selindexin$, if $\meanparamat{\indexedselvariable}=1$ we have
		\[  \contraction{\probat{\shortcatvariables},\enumformulaat{\shortcatvariables}} =1 \]
	and with \theref{the:probEntailment} probabilistic entailment $\probtensor\models\enumformula$ (see \defref{def:probEntailment}).
	On the other hand for any $\selindexin$, if $\meanparamat{\indexedselvariable}=0$ we have
		\[  \contraction{\probat{\shortcatvariables},\lnot\enumformulaat{\shortcatvariables}} =1 \]
	and with the same argumentation $\probtensor\models\lnot\enumformula$.

	Together it holds that $\probtensor\models\basemeasureof{\formulaset,\meanparam}$ and
		\[  \probat{\shortcatvariables} = \contractionof{\probat{\shortcatvariables},\basemeasureofat{\formulaset,\meanparam}{\shortcatvariables}} \, . \]
	Thus, $\probtensor$ is representable with respect to the base measure $\basemeasureof{\formulaset,\meanparam}$ and also with respect to the base measure $\secbasemeasure$.
\end{proof}

% Reduction to non 0/1 coordinates, but still no guarantee to be in the interior
By \theref{the:hlnMeanPolytopeReduction} we can reduce the statistics to the set $\formulaset^{\meanparam}=\{\enumformula\in\formulaset : \meanparamat{\indexedselvariable}\notin\{0,1\}\}$ and continue searching for a reproducing distribution, now for the reduced mean parameter with coordinates not in $\{0,1\}$.
The criterion of mean parameter coordinates in $\{0,1\}$ is, however, not a necessary condition for $\meanparamwith\notin\interiorof{\hlnmeanset}$, see \exaref{exa:insufficentHLNsetExpressivity} for minimal representations where mean parameters outside the interior exist with coordinates not in $\{0,1\}$.

% Face base measure refinement as a generic approach
Following a different approach, any mean parameter outside the interior of the mean parameter polytope is on a face of the polytope.
We can use this insight, to refine base measures by face base measures (see \defref{def:faceBaseMeasure}), which has been shown in more generality in \theref{the:faceToArgmax}.
For hybrid logic network we characterize the face base measures in the next theorem.

%continue with a characterization of the face base measures (see \defref{def:faceBaseMeasure}) in the context of hybrid logic networks, which provides us with further base measure refinements in case of mean parameters outside the interior of the mean parameter polytope.

\begin{theorem}[Base measures by formula satisfaction]\label{the:hlnFaceBaseMeasureCharacterization}
	Let $\canparamat{\indexedselvariable}$ be a normal to a face of $\meanset$.
	If and only if the face formula
		\[ \formulaof{\formulaset,\canparam} = \bigwedge_{\selindexin \, : \, \canparamat{\indexedselvariable}\neq0} \lnot^{(1-\greaterzeroof{\canparamat{\indexedselvariable}})} \enumformula  \]
	is satisfiable, then it is the base measure to the face with normal $\canparam$.
	Here $\greaterzeroof{z}$ denotes the indicator of $z>0$.

	If the above is not satisfiable, then the base measure is
		\[ \bigvee_{v[\selvariable] \, : \, \contraction{\canparam,v} \in \max_{\meanparam} \contraction{\canparam,\meanparam} }  \formulaof{\formulaset,\contractionof{\canparam,v}{\selvariable}}  \]
	where $v$ are boolean vectors.
\end{theorem}
\begin{proof}
	We show this lemma based on characterizations of
		\[ \argmax_{\shortcatindices} \contraction{\canparamat{\selvariable},\sencfsetat{\indexedshortcatvariables,\selvariable}} \, . \]

	For the first claim:
	We have since $\sencfset$ is boolean that
		\[ \|\canparamat{\selvariable}\|_1 \geq \max_{\shortcatindices} \contraction{\canparamat{\selvariable},\sencfsetat{\indexedshortcatvariables,\selvariable}}  \, . \]
	If and only if the formula $\formulaof{\formulaset,\canparam}$ is satisfiable, then we find a model $\shortcatindices$ and the inequalitiy is straight.
	The maximum is taken at any $\shortcatindices$ if and only if for any $\selindex$ in the support of $\canparam$ we have $\sencfsetat{\indexedshortcatvariables,\indexedselvariable}=\greaterzeroof{\canparamat{\indexedselvariable}}$.
	This condition is equal to $\shortcatindices$ being a model of $\lnot^{(1-\greaterzeroof{\canparamat{\indexedselvariable}})} \enumformula$ for any $\selindex$ in the support of $\canparam$ and thus to being a model of $\formulaof{\formulaset,\canparam}$.

	For the second claim:
	The auxiliary boolean vector $v$ serves as an indicator fo the formulas $\lnot^{(1-\greaterzeroof{\canparamat{\indexedselvariable}})} \enumformula$ being satisfied.
	In the worlds, for which $\max_{\shortcatindices}\contraction{\canparamat{\selvariable},\sencfsetat{\indexedshortcatvariables,\selvariable}}$ is attained, we have
		\[ \contraction{\canparam,v} \in \max_{\meanparam} \contraction{\canparam,\meanparam} \, . \]
	They are the models of $\formulaof{\formulaset,\contractionof{\canparam,v}{\selvariable}}$.
	Note that then, all formulas $\enumformula$ with $v[\indexedselvariable]=0$ need to be contradicted from $\formulaof{\formulaset,\contractionof{\canparam,v}{\selvariable}}$, since otherwise the maximum would be larger.
	The mean parameters solving the optimization problem are convex combinations of these extreme points, which correspond with distributions being convex combinations of the one-hot encodings of these models.
	Such distributions are therefore representable by the base measure being the indicator of these models, which is
		\[ \bigvee_{v[\selvariable] \, : \, \contraction{\canparam,v} \in \max_{\meanparam} \contraction{\canparam,\meanparam} }  \formulaof{\formulaset,\contractionof{\canparam,v}{\selvariable}}  \, . \]
\end{proof}


\subsect{Expressivity of Hybrid Logic Networks}

% Summarize
To summarize the above, we have characterized all the mean parameter vectors in $\hlnmeanset$ by their reproducing distributions, as sketched in \figref{fig:meansetSketch}.
Extreme points in $\hlnmeanset$ are boolean mean parameter vectors and by \theref{the:extremeCharacterizationHLN} exactly those reproduced by hard logic networks.
Points, which are neither extreme nor in the interior of $\hlnmeanset$, are reproduced by hybrid logic networks with a refined base measure $\secbasemeasure$. % Need to connect with base measure refinement algo?
If the statistic $\mlnstat$ is minimal with respect to $\basemeasure$, then $\secbasemeasure$ does not coincide with $\basemeasure$.
Finally, interior points are by \corref{cor:interiorCharacterizationHLN} those reproduced by a hybrid logic network with base measure $\basemeasure$.

%% Mean parameter characterization
%To summarize, for any $\meanparam\in\hlnmeanset$ we have one of the following (see \figref{fig:meansetSketch}):
%\begin{itemize}
%	\item $\meanparamat{\indexedselvariable}\in\interiorof{\hlnmeanset}$: Then reproducible by a Hybrid Logic Network with base measure $\basemeasure$ .
%%		This follows from the classical result of Theorem~3.3 in \cite{wainwright_graphical_2008}.
%	\item  $\meanparamat{\indexedselvariable}\notin\interiorof{\hlnmeanset}$:
%		Then on a facet and reproducible by a Hybrid Logic Network with refined base measure $\secbasemeasure$.
%		Whether the base measure can be realized by the computation network of $\fselectionmap$ depends on the satisfiability of the face formula.
%\end{itemize}

\begin{figure}[t!]
\begin{center}
	\input{PartII/tikz_pics/network_reasoning/meanset_sketch_hln.tex}
\end{center}
\caption{Sketch of the mean polytope ${\hlnmeanset}$ to a statistic $\mlnstat$ which is minimal with respect to $\basemeasure$, as a special case of the more generic sketch \figref{fig:meansetSketchGeneric}.
	The mean polytope is a subset of the $\seldim$-dimensional cube $[0,1]^\seldim$ (here sketched as a 2-dimensional projection), where each mean parameter in one of the three cases $\meanparamof{1},\meanparamof{2}$ or $\meanparamof{3}$.
	Extreme points $\meanparamof{1}\in\hlnmeanset\cap\{0,1\}^{\seldim}$ are those reproduceable by a Hard Logic Network given $\mlnstat$.
	Non-extreme points outside the interior $\meanparamof{2}\in\hlnmeanset/(\interiorof{\hlnmeanset}\cup\{0,1\}^{\seldim})$ are reproducable by Hybrid Logic Networks with statistic $\mlnstat$ and refined base measure $\secbasemeasure$.
	Interior points $\meanparamof{3}\in\interiorof{\hlnmeanset}$ are reproducable by a Hybrid Logic Network with statistic $\mlnstat$ and refined base measure $\secbasemeasure$.
%	Any extreme point $\meanparamof{1}\in\hlnmeanset\cap\{0,1\}^{\seldim}$ is reproducable by a Hard Logic Network,
%	while a non-extreme boundary point $\meanparamof{2}\in\hlnmeanset/\{0,1\}^{\seldim}$ is realizable by a Hybrid Logic Network.
%	The boundary points $\meanparamof{1},\meanparamof{2}\in\hlnmeanset/\sbinteriorof{\hlnmeanset}$ are examples of mean parameters, which can be reproduced by a Hard Logic Networks (respectively Hybrid Logic Network).
}\label{fig:meansetSketch}
\end{figure}




% Reduction to Hybrid Logic Networks, question of sufficient expressivity
Let us recall, that the set $\hlnsetof{\formulaset}$ contains all Hybrid Logic Networks, which can be realized as tensor networks with the same structure of computation and activation cores.
We now investigate, whether we can reduce the set of probability distributions in the definition of the convex polytope of mean parameters to the set $\hlnsetof{\formulaset}$ (see \defref{def:hln}), that is
\begin{align*}
	\hlnmeanset|_{\hlnsetof{\formulaset}} = \left\{ \sbcontractionof{\probtensor,\sencmlnstat}{\selvariable} \, : \, \probtensor\in\hlnsetof{\formulaset} \right\} \, .
\end{align*}
While $\hlnmeanset|_{\hlnsetof{\formulaset}}\subset\hlnmeanset$ is obvious, we pose the question, for which $\formulaset$ there is an equivalence.
We will refer to the equality of both sets as sufficient expressivity of $\hlnsetof{\formulaset}$.
In the next example we provide a class of formulas, for which $\hlnsetof{\formulaset}$ does not have sufficient expressivity.

% Insufficient Expressivity
\begin{example}[Insufficient expressivity of $\hlnsetof{\formulaset}$ in cases of disjoint models]\label{exa:insufficentHLNsetExpressivity}
	To provide an example, where the set of hybrid logic networks does not suffice to reproduce all possible mean parameters, consider the formulas
		\[ \formulaof{0} = \exrandom \land \secexrandom \quad, \quad \formulaof{1} = \lnot\exrandom \land \lnot\secexrandom \, . \]
	The probability distributions on the facet with normal $\canparam=[1\,\, 1]$ are those with support on the models of $\formulaof{0}\lor\formulaof{1}$.
	The Hybrid Logic Networks can only reproduce those with are supported on the model of $\formulaof{0}$ or the model of $\formulaof{1}$, but not their convex combinations.

	More generally, we can construct similar examples by arbitrary sets of formulas with pairwise disjoint model sets.
	If they do not sum to $\ones$, i.e. there is a world which is not a model to any formula, the statistic is minimal.
	The vector $\canparamat{\selvariable} = \onesat{\selvariable}$ is then the normal of the facet with
	All probabilities supported on the models of the formulas have mean parameters on this facet.
\end{example}

% Sufficient Expressivity
Before presenting the example class of atomic formulas as a case, where $\hlnsetof{\formulaset}$ has sufficient expressivity, let us first proof a generic criterion.

% Drop!
\begin{theorem}\label{the:sufficientHLNExpressivity}
	The set of mean parameters realizable by $\hlnsetof{\formulaset}$ coincides with the set of mean parameters realizable by any distribution, if for any boolean vector $\canparamat{\selvariable}$ the formula $\formulaof{\formulaset,\canparam}$ is satisfiable.
\end{theorem}
\begin{proof}
	%\proofleftsymbol:
		If these formulas are satisfiable, the base measures to the facets coincides with those realizable by $\hlnsetof{\formulaset}$.
	%\proofrightsymbol:
	%	We show that implication by refutation.
	%	If one of these formulas are not satisfiable, we find a facet, which mean parameters optimize
	%	The uniform distribution on the worlds taking the maximum is not representable by any base measure.
\end{proof}



% Refinements of $\hlnsetof{\formulaset}$
When the assumptions of \theref{the:sufficientHLNExpressivity} are not satisfied, there are mean parameters, which can not be reproduced by a distribution in $\hlnsetof{\formulaset}$.
In that case, we can flexibilize the distribution, to also represent the base measures used for refinement in \theref{the:hlnFaceBaseMeasureCharacterization}.
This can be done by adding activation cores with multiple variables, or further computation cores calculating the disjunctions of formulas.



% New 15.5.25: Full characterization of representable pre-images
To characterize all the pre-images of faces, which are in $\hlnsetof{\formulaset}$, we exploit the faces of the $\seldim$-dimensional cube.
The cube $[0,1]^{\seldim}$ is a polytope in $\rr^{\seldim}$, namely the convex hull of all boolean vectors
\begin{align*}
	[0,1]^{\seldim} = \convhullof{\meanparamat{\selvariable}\wcols\uniquantwrtof{\selindexin}{\meanparamat{\indexedselvariable}\in\{0,1\}}} \, .
\end{align*}
We have for the statistic of atoms $\atomstat$, that $\meansetof{\atomstat,\ones}=[0,1]^{\seldim}$.
Its faces are enumerated by tuples $(\variableset,\catindexof{\variableset})$, where $\variableset\subset[\seldim]$ and $\catindexof{\variableset}\in\{0,1\}^{\cardof{\variableset}}$.
\begin{align*}
	\genfacesetof{(\variableset,\catindexof{\variableset})}
	= \left\{\meanparamat{\selvariable}\wcols \meanparam\in[0,1]^{\seldim}\ncond\uniquantwrtof{\selindex\in\variableset}{\meanparamat{\indexedselvariable}=\catindexof{\selindex}}\right\} \, .
\end{align*}


\begin{theorem}\label{the:HLNExpressivity}
	The pre-image of faces of $\hlnmeanset$, which subset encodings are in $\hlnsetof{\formulaset}$, are those faces, which are intersections of $\hlnmeanset$ with faces of the cube $[0,1]^\seldim$.
\end{theorem}
\begin{proof}
	Any subset encoding is a boolean tensor and any boolean tensor in $\hlnsetof{\formulaset}$ has a representation with boolean activation core. % $\actcore\in\bigotimes$
	If the activation core is elementary, it has a basis+ decomposition with rank 1, since we have leg dimensions of 2 for boolean statistics.
	Let $\variableset\subset[\seldim]$ be the legs, which vector is a basis vector, and let $\catindexof{\variableset}\in\{0,1\}^{\cardof{\variableset}}$ be the tuple storing to $\selindex\in\variableset$ the number of the basis vector by $\catindexof{\selindex}$.
	Therefore, we have a parametrization of the boolean tensors in $\hlnsetof{\formulaset}$ by
	\begin{align*}
		\hlnsetof{\formulaset} \cup \bigotimes_{\catenumeratorin}\{0,1\}^{2}
		= \bigcup_{\variableset\subset[\seldim]} \bigcup_{\catindexof{\variableset}\in\{0,1\}^{\cardof{\variableset}}}
		\contractionof{\rencodingofat{\formulaset}{\headvariableof{[\seldim]},\shortcatvariables},\onehotmapofat{\catindexof{\variableset}}{\headvariableof{\variableset}}}{\shortcatvariables}
	\end{align*}

	For any tuple $(\variableset,\catindexof{\variableset})$ and any $\shortcatindices$ we then have
	\begin{align*}
		\contractionof{\rencodingofat{\formulaset}{\headvariableof{[\seldim]},\shortcatvariables},\onehotmapofat{\catindexof{\variableset}}{\headvariableof{\variableset}}}{\indexedshortcatvariables}=1
		\quad \Leftrightarrow \quad
		\uniquantwrtof{\selindex\in\variableset}{\sencsstatat{\indexedshortcatvariables,\indexedselvariable}=\catindexof{\selindex}}
	\end{align*}
	And thus
	\begin{align*}
		\left\{\shortcatindices\wcols\contractionof{\rencodingofat{\formulaset}{\headvariableof{[\seldim]},\shortcatvariables},\onehotmapofat{\catindexof{\variableset}}{\headvariableof{\variableset}}}{\indexedshortcatvariables}=1\right\}
		= \hlnmeanset \cup \genfacesetof{(\variableset,\catindexof{\variableset})} \, .
	\end{align*}
	Since to each boolean tensors in $\hlnsetof{\formulaset}$ and to each face in
	Note, that there might be multiple pairs $(\variableset,\catindexof{\variableset})$ to a boolean tensor in $\hlnsetof{\formulaset}$, corresponding to situations where multiple faces of the cube have identical intersections with $\hlnmeanset$.
%	Contractions with the computation network are effectively constraining only a subset of statistic coordinates (use the ones-trivial-property of relational encodings).
%	Any $\meanparam$ being the statistic encoding of a coordinate in the support of this contraction thus satisfies contraint $\meanparamat{\indexedselvariable}=0$ respectively $\meanparamat{\indexedselvariable}=1$, which are known to define faces of the cube.
\end{proof}

The vertices of $\hlnmeanset$ consist of $\{\meanparam\}$ where $\meanparamat{\selvariable}=\sencsstatat{\indexedshortcatvariables,\selvariable}$ for an $\shortcatindices$.
As a corollary of \theref{the:HLNExpressivity}, their pre-image is always in $\hlnsetof{\formulaset}$.

\begin{corollary}
	All encoded pre-images of vertices are in $\hlnsetof{\formulaset}$.
\end{corollary}
\begin{proof}
	Since vertices in $\hlnmeanset$ are also vertices of the cube.
\end{proof}

% Vertices as Basis
Pre-images of vertices have a representation with basis tensors as activation tensors.

\begin{corollary}
	When $\formulaset$ is the set of atomic formulas, all encoded pre-images of faces are in $\hlnsetof{\formulaset}$.
\end{corollary}
\begin{proof}
	Since for atomic formulas, the mean parameter polytope is the cube $[0,1]^\seldim$.
\end{proof}

% Generalization to non-boolean features
For non-boolean features, we can derive limited versions of the above statement.
Any face, which is the intersection with faces of the distorted cube
\begin{align*}
	\bigtimes_{\selindexin} [\min_{\catindex}\sstatcoordinateofat{\selindex}{\indexedcatvariable},\max_{\catindex}\sstatcoordinateofat{\selindex}{\indexedcatvariable}]
\end{align*}
has a representation of its encoded pre-image with an activation tensor of basis+ rank 1.
Since for leg dimensions larger than 2 the boolean tensors with elementary decomposition contain more tensors than the basis+ tensors of rank 1, we can represent further faces.



%
\begin{theorem}
	The encoded pre-image of any face of rank $r$ is represented by a activation core of basis $\cpformat$ rank $r$.
\end{theorem}
\begin{proof}
	We build for each vertex $\{\meanparam\}$ a basis vector $\bigotimes_{\selindex\in\seldim}\onehotmapofat{\meanparamat{\indexedselvariable}}{\headvariableof{\selindex}}$, which contraction with $\rencodingof{\formulaset}$.
	Note, that the vertex base measures have disjoint support and their disjunction is thus a summation.
\end{proof}


\subsect{Case of tree computation networks}

In this case, the mean polytope can be embedded into a markov network and characterized by local consistency of the mean parameters of the markov network.


\subsect{Examples}

We can relate our two standard examples of the atomic and the minterm formula sets to well-studied polytopes, namely the $\catorder$-dimensional hypercube and the standard simplex (see Lecture~0 in \cite{ziegler_lectures_2013} )

\begin{example}[Atomic formulas]
	The assumption of \theref{the:sufficientHLNExpressivity} is satisfied in the case for atomic formulas, where the formulas  $\formulaof{\formulaset,\canparam}$ are the minterms, which are always satisfiable in exactly one situation.
	The mean polytope in this case is the $\catorder$-dimensional hypercube,
		\[ \meansetof{\atomformulaset,\ones} = [0,1]^{\catorder} \]
	which is called a simple polytope, since each vertex is contained in the minimal number of $\catorder$ facets.
\end{example}

\begin{example}[Minterm formulas]
	The mean polytope is in the case of the minterm exponential family is the $2^\catorder-1$-dimensional standard simplex.
	In this case, $\hlnsetof{\mintermformulaset}$ contains any distribution and therefore trivially realizes any mean parameter in $\meansetof{\mintermformulaset,\ones}$.
\end{example}



\sect{Entropic Motivation of unconstrained Parameter Estimation} \label{sec:parameterEstimation} % Check for redundancy with the mln introduction chapter!

% Repetition and result transfering
Markov Logic Networks are exponential families with statistics by a set $\formulaset$ of propositional formulas.
We furthermore allow for propositional formulas as base measures, to also include the discussion of Hybrid Logic Networks.
Based on this, we apply the theory of probabilistic inference, developed in \charef{cha:probReasoning} for the generic exponential families.

\subsect{Maximum Likelihood in Hybrid Logic Networks}

% Special example: MLE
The Maximum Likelihood Problem on a hybrid logic network family is the moment projection
\begin{align*}
	\argmax_{\canparamat{\selvariable}\in\rr^{\seldim}} \quad
	\centropyof{\probtensor}{\expdistof{(\sstat,\canparam,\basemeasure)}}
\end{align*}
in the case $\probtensor=\empdistribution$ for a sample selector map $\datamap$.

% Backward map
The moment projection coincides, after dropping constant terms in case of non-trivial base measure, with the backward map
\begin{align*}
	\argmax_{\canparamat{\selvariable}\in\rr^{\seldim}} \quad
	\sbcontraction{\canparamat{\selvariable},\meanparamwith} - \cumfunctionof{\canparamat{\selvariable}}
\end{align*}
where
\begin{align*}
	\meanparamwith
	= \sbcontractionof{\sencmlnstat,\probtensor}{\selvariable}
	\quad \text{and} \quad
	\cumfunctionof{\canparamat{\selvariable}}
	= \sbcontraction{\expof{ \sbcontractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables} }, \basemeasure} \, .
\end{align*}

% Extension to HLN
We now extend to Hybrid Logic Networks
\begin{align*}
	\argmax_{\secprobtensor\in\hlnsetof{\formulaset}} \quad
	\centropyof{\probtensor}{\secprobtensor}
\end{align*}



\begin{corollary}
	Let $\meanparamwith = \contractionof{\probtensor,\sencfset}{\selvariable}$ and
		\[ \secformulaset = \{\enumformula \, : \, \meanparamat{\indexedselvariable} \in \{0,1\}\} \quad , \quad
		\basemeasureofat{\secformulaset,\meanparam}{\shortcatvariables}
		= \bigwedge_{\enumformula\in\secformulaset} \lnot^{(1-\meanparamat{\indexedselvariable})} \enumformulaat{\shortcatvariables}
		\, . \]
	If $\meanparamwith$ is reproduceable by a positive distribution with respect to $\basemeasureofat{\secformulaset,\meanparam}{\shortcatvariables} $, then the solution of the M-projection of $\probtensor$ onto the set of hybrid logic networks is representable by $\formulaset$ then coincides with the projection of $\probtensor$ onto $\expfamilyof{\formulaset/\secformulaset,\basemeasureof{\secformulaset,\meanparam}}$.
\end{corollary}
%\begin{proof}
%	Work by weight cutoff, in the limit of hard logics?
%\end{proof}

\subsect{Maximum Entropy in Hybrid Logic Networks}


% Special example: MaxEnt
The Maximum Entropy Problem for a statistic $\mlnstat$ is %Markov Logic Networks is
\begin{align}
	\argmax_{\probtensor} \quad \sentropyof{\probtensor}
	\quad \text{subject to} \quad
	\sbcontractionof{\probtensor,\sencmlnstat}{\selvariable}
	 =  \meanparamwith
\end{align}



\begin{corollary}[of \theref{the:maxEntMaxLikeDuality}]
	%\red{Works only for $\meanparamat{\indexedselvariable}\in\interiorof{\hlnmeanset}$}
	Let $\empdistribution$ be a distribution such that there is a positive distribution $\probtensor$ with $\sbcontractionof{\probtensor,\sencmlnstat}{\selvariable} = \sbcontractionof{\empdistribution,\sencmlnstat}{\selvariable}$.
	Among all positive distributions $\probtensor$ of $\atomstates$ satisfying this moment matching condition, the Markov Logic Network with formulas $\formulaset$ and weights $\canparam$ being the solution of the maximum likelihood problem has minimal entropy.
\end{corollary}

% Unique property of MLN
We notice, that the solution of the maximum entropy problem is thus a Markov Logic Network.
This is remarkable, because this motivates our restriction to Markov Logic Networks as those distributions with maximal entropy given satisfaction rates of formulas in $\formulaset$.


% Extension to HLN
When now extend to the situations $\meanparamat{\indexedselvariable}\in\{0,1\}$ can appear.
It that case the formula is entailed or contradicted by the facts, and dropping should be considered in both cases.

The max entropy - max likelihood duality still holds for hybrid logic networks as we show in the next theorem.

\begin{theorem}
	Given a set of formulas $\tilde{\formulaset}$ and $\tilde{\meanparam}$, with coordinates $\tilde{\meanparam}_\selindex\in[0,1]$ in the closed interval $[0,1]$.
	If the corresponding maximum entropy problem is feasible, its solution is a hybrid logic network with
	\begin{itemize}
		\item $\hardformulaset= \{\enumformula : \selindexin, \meanparamat{\indexedselvariable} = 1\} \cup \{\lnot\enumformula : \selindexin, \meanparamat{\indexedselvariable} = 0\} $
		\item $\softformulaset = \{\enumformula : \selindexin, \meanparamat{\indexedselvariable} \in (0,1)\}$
		\item $\canparam$ being the backward map evaluated at the vector $\meanparam$ consisting of the coordinates of $\tilde{\meanparam}$ not in $\{0,1\}$
	\end{itemize}
\end{theorem}
\begin{proof}
	Feasible distributions have a density with base measure by $\hardformulaset$, we therefore reduce the set of distributions in the argmax to those with density to the base measure.
	The max entropy is a max entropy problem with respect to that base measure, where we only keep the constraints to the mean parameters different from $\{0,1\}$ (those are trivially satisfied).
	The statement then follows from the generic property (see Sec3.1 in \cite{wainwright_graphical_2008}).
\end{proof}






\sect{Alternating Algorithms to Approximate the Backward Map}\label{sec:alternatingParEstMLN}

Let us now introduce an implementation of the Alternating Moment Matching Algorithm~\ref{alg:AMM} in case of Markov Logic Networks.
To solve the moment matching condition at a formula $\enumformula$ we refine \lemref{lem:mmContractionEquation} in the following.

\begin{lemma}\label{ref:lemMMinMLN}
	Let there be a base measure $\basemeasure$, a formula selecting map $\formulaset=\{\enumformula \, : \, \selindexin\}$ and weights $\canparam$, and choose $\selindexin$ such that $\enumformula  \notin \{\onesat{\shortcatvariables},\zerosat{\shortcatvariables}\}$.
	The moment matching condition relative to $\canparam$, $\selindexin$ and $\datameanat{\indexedselvariable}\in(0,1)$ is then satisfied, if
	\begin{align} \label{sol:momentMatchingExformula}
	 	\weightat{\indexedselvariable} = \lnof{
		\frac{\datameanat{\indexedselvariable}}{(1-\datameanat{\indexedselvariable})}
		\cdot \frac{\hypercoreat{\catvariableof{\enumformula }=0}}{\hypercoreat{\catvariableof{\enumformula }=1}}
		}
	\end{align}
	where by $\hypercoreat{\catvariableof{\enumformula }}$ we denote the contraction
	\begin{align*}
	 	\hypercoreat{\catvariableof{\enumformula}}
		= \contractionof{\{\rencodingof{\enumformula} \, : \, \selindexin\}
		\cup\{\actcoreof{\tilde{\selindex}} : \tilde{\selindex} \in [\seldim], \tilde{\selindex}\neq\selindex\}
		\cup\{\basemeasure\}}{\catvariableof{\enumformula}} \, .
	\end{align*}
\end{lemma}
\begin{proof}
	Since $\imageof{\enumformula}\subset[2]$ we have
	\begin{align*}
		\idrestrictedto{\imageof{\enumformula}} = \onehotmapofat{1}{\catvariableof{\enumformula}}
	\end{align*}
	and the moment matching condition is by \lemref{lem:mmContractionEquation} satisfied if
	\begin{align*}
		\sbcontraction{\actcoreof{\selindex}, \onehotmapof{1}, \hypercore}
			= \sbcontraction{\actcoreof{\selindex},\hypercore} \cdot \datameanat{\indexedselvariable} \, .
	\end{align*}
	This is equal to
	\begin{align*}
		\expof{\canparamat{\indexedselvariable}} \cdot \hypercoreat{\catvariableof{\enumformula}=1}
		= \left( \expof{\canparamat{\indexedselvariable}} \cdot \hypercoreat{\catvariableof{\enumformula}=1} + \hypercoreat{\catvariableof{\enumformula}=0} \right) \cdot \datameanat{\indexedselvariable} \, .
	\end{align*}
	Rearranging the equations this is equal to
	\begin{align*}
	 	\hypercoreat{\catvariableof{\enumformula}}
		= \contractionof{\{\rencodingof{\enumformula}\}
		\cup\{\actcoreof{\tilde{\selindex}} : \tilde{\selindex} \in [\seldim], \tilde{\selindex}\neq\selindex\}
		\cup\{\basemeasure\}}{\selvariable} \, .
	\end{align*}
	We notice that the right side is well defined, since we have by assumption $\datameanat{\indexedselvariable}, (1- \datameanat{\indexedselvariable}) \neq 0$ and $\hypercoreat{\catvariableof{\enumformula}=0}, \hypercoreat{\catvariableof{\enumformula}=1} \neq 0$ since Markov Logic networks are positive distributions and $\enumformula \notin \{\onesat{\shortcatvariables},\zerosat{\shortcatvariables}\}$.
\end{proof}


%% Hard network reference!
In the case $\datameanat{\indexedselvariable}\in\{0,1\}$ the moment matching conditions are not satisfiable for $\canparamat{\indexedselvariable}\in\rr$.
But, we notice, that in the limit $\canparamat{\indexedselvariable}\rightarrow \infty $ (respectively $-\infty$) we have
	\[ \meanparamat{\indexedselvariable} \rightarrow  1 \quad \text{(respectively $0$)}\, ,  \]
and the moment matching can be satisfied up to arbitrary precision.
In \secref{sec:hardNetworks} we will allow infinite weights and interpret the corresponding factors by logical formulas.
As a consequence, we will able to fit graphical models, which we will call hybrid networks on arbitrary satisfiable mean parameters.

%
The cases $\hypercoreat{\catvariableof{\enumformula}=1}=0$, respectively $\hypercoreat{\catvariableof{\enumformula}=1}=0$ only appear for nontrivial formulas when the distribution is not positive.
This is not the case for Markov Logic Networks, but will happen when formulas are added as cores of a Markov Network.
This situation will has been investigated in \secref{sec:hardNetworks}.


% Concave likelihood 
Since the likelihood is concave (see \cite{koller_probabilistic_2009}), there are not local maxima the coordinate descent could run into and coordinate descent will give a monotonic improvement of the likelihood.

We suggest an alternating optimization by Algorithm~\ref{alg:AWO}, solving the moment matching equation iteratively for all formulas $\exformulain$ and repeat the optimization until a convergence criterion is met.
This is an coordinate ascent algorithm, when interpreted the loss $\lossof{\expdist}$ as an objective depending on the vector $\canparam$.

\begin{algorithm}[hbt!]
\caption{Alternating Weight Optimization (AWO)}\label{alg:AWO}
\begin{algorithmic}
%% INPUT: Numerated formula set, mean parameter $\datameanat{\selvariable}$
%\For{$\exformula\in\formulaset$}
	\Require Empirical distribution $\empdistribution$, boolean features $\mlnstat$ %and base measure $\basemeasure$
	\Ensure Canonical parameter $\canparamwith$, such that $\expdist$ is the (approximative) moment projection of $\empdistribution$ onto $\expfamily$
	\hrule
    \State Compute $\datameanat{\selvariable}= \sbcontractionof{\empdistribution,\sencsstat}{\selvariable}$
	\State $\kb = \ones$, $\secnodes=\varnothing$
\For{$\selindexin$}
	\If{$\meanparamat{\indexedselvariable}=1$}
		\[ \hardformulaset \algdefsymbol \hardformulaset \cup \{\enumformula\}\]
	\ElsIf{$\meanparamat{\indexedselvariable}=0$}
		\[ \hardformulaset \algdefsymbol \hardformulaset \cup \{\lnot\enumformula\}\]
	\Else
		\State $\secnodes\algdefsymbol \secnodes \cup \{\selindex\}$
	\EndIf
\EndFor
\For{$\selindex\in\secnodes$}
		\State Compute
		\[ \hypercoreat{\catvariableof{\enumformula}}
		\algdefsymbol \sbcontractionof{\rencodingof{\enumformula}}{\catvariableof{\enumformula}} \]
		\State Set
		\begin{align*}
	 		\canparamat{\indexedselvariable}
			\algdefsymbol \lnof{
			\frac{\datameanat{\indexedselvariable}}{(1-\datameanat{\indexedselvariable})}
			\cdot \frac{\hypercoreat{\catvariableof{\enumformula}=0}}{\hypercoreat{\catvariableof{\enumformula}=1}}
			}
		\end{align*}
\EndFor
\If {$\sbcontraction{\kb}=0$}
	 \State \textbf{raise} "Inconsistent Knowledge Base"
\EndIf
\While{Convergence criterion is not met}
\For{$\selindex\in\secnodes$}
	\State Compute
	\begin{align*}
	 	\hypercoreat{\catvariableof{\enumformula}}
		= \contractionof{\{\rencodingof{\enumformula} \, : \, \selindexin\}
		\cup\{\actcoreof{\tilde{\selindex}} : \tilde{\selindex} \in [\seldim], \tilde{\selindex}\neq\selindex\}
		\cup\{\basemeasure\}}{\catvariableof{\enumformula}}
	\end{align*}
	\State Set
	\begin{align*}
	 	\canparamat{\indexedselvariable} = \lnof{
		\frac{\datameanat{\indexedselvariable}}{(1-\datameanat{\indexedselvariable})}
		\cdot \frac{\hypercoreat{\catvariableof{\enumformula}=0}}{\hypercoreat{\catvariableof{\enumformula}=1}}
		}
	\end{align*}
\EndFor
\EndWhile
	\State \Return $\canparamwith$
\end{algorithmic}
\end{algorithm}


% Independent formulas
In the initialization phase of Algorithm~\ref{alg:AWO}, each parameters is initialized relative to a uniform distribution.
The algorithm would be finished, if the variables $\catvariableof{\exformula}$ are independent.
This would be the case, if the Markov Logic Network consists of atomic formulas only.
When they fail to be independent, the adjustment of the weights influence the marginal distribution of other formulas and we need an alternating optimization.
% 
This situation corresponds with couplings of the weights by a partition contraction, which does not factorize into terms to each formula.


% Inference
\red{
Solving Equation~\ref{sol:momentMatchingExformula} requires inference of a current model by answering a query.}
This can be a bottleneck and circumvented by approximative inference, see e.g. CAMEL \cite{ganapathi_constrained_2008}.



\begin{remark}[Grouping of coordinates with trivial sum]
	When having a set of coordinates, such that the coordinate functions are binary and sum to the trivial tensor, one can find simultaneous updates to the canonical parameters, such that the partition function is staying invariant.
	Given a parameter $\canparam^t$ we compute
		\[ \meanparam^t = \contractionof{\expdistof{(\sstat,\canparam^t)}, \sstat}{\selvariable} \]
	and build the update
		\[ \canparam^{t+1} = \canparam^t + \lnof{\meanparam^{\datamap}}{\meanparam^t} \, . \]
	Then, $\canparam^{t+1}$ satisfies the moment matching equations for all coordinates in the set.


	The assumptions are met when taking all features to any hyperedge in a Markov Network seen as an exponential family.
	In that case, the update algorithm is refered to as  Iterative Proportional Fitting \cite{wainwright_graphical_2008}.
	Further, when activating both $\exformula$ and $\lnot\exformula$.
\end{remark}


\sect{Forward and backward mappings in closed form}

% Closed form availability
We recall from \charef{cha:probReasoning}, that while forward mappings are always in closed form by contractions, backward mapping in general do not have a closed form representation.
Instead, the backward map is in general implicitly characterized by a maximum entropy problem constrained to matching expected sufficient statistics.
We investigate in this section specific examples, where closed forms are available for both.
In these cases, parameter estimation can thus be solved by application of the inverse on the expected sufficient statistics with respect to the empirical distribution, and iterative algorithms can be avoided.

% Usage
%When the backward map $\backwardmap$ is available in closed form, we directly get optimal parameters by the inversion acting on the satisfaction rate and can avoid iterative algorithms of parameter estimation.

\subsect{Maxterms and Minterms}

Minterms (respectively maxterms) are ways in propositional logics to get a syntactical formula representation based on a formula to each world which is a model (respectively fails to be a model).
We have already studied in \secref{sec:MLNMaxMintermRep} how to represent any distribution as a MLN of maxterms (respectively minterms), see \theref{the:maximalClausesRepresentation}.

We use the tuple enumeration of the maxterms and minterms by $\atomstates$ introduced in \secref{sec:termClauseDecomposition}.
With respect to this enumeration the canonical parameters and mean parameters are tensors in $\bigotimes_{\atomenumeratorin}\rr^2$.
%% Interpretation of the mean parameters
Since the statistic of the minterm family is the identity, the mean parameters for the minterm family are
	\[ \meanparamat{\selvariableof{[\atomorder]}=\catindexof{[\atomorder]}}
	= \probat{\catindexof{[\atomorder]}}
	\]
and therefore after a relabeling of categorical variables to selection variables $\meanparam=\probtensor$.
For maxterms we have analogously
	\[ \meanparamat{\selvariableof{[\atomorder]}=\catindexof{[\atomorder]}}
	= 1-\probat{\catindexof{[\atomorder]}}
	\]
and $\meanparam = \onesat{}-\probtensor$.
We can use these insights to provide a characterization of the forward and backward maps of the minterm and maxterm family.

\begin{theorem}
	Given the Markov Logic Networks to the formula sets
		\[ \mintermformulaset := \{ \mintermof{\atomindices} \, : \, \atomindicesin\} \quad \text{and} \quad
		\maxtermformulaset := \{ \maxtermof{\atomindices} \, : \, \atomindicesin\}  \]
	of all minterms, respectively of all mapterms, the forward mapping are
		%\[ \forwardmapwrt{\mintermformulaset}: \bigotimes_{\atomenumeratorin}\rr^{2} \rightarrow \bigotimes_{\atomenumeratorin}\rr^{2} \]
		\[ \forwardmapwrt{\mlnmintermsymbol}(\canparam) = \normationofwrt{\expof{\canparam}}{\shortcatvariables}{\varnothing}
		\quad \text{and} \quad
		 \forwardmapwrt{\mlnmaxtermsymbol}(\canparam) = \normationofwrt{\expof{-\canparam}}{\shortcatvariables}{\varnothing} \, , \]
	where in a slight abuse of notation we assigned the variables $\shortcatvariables$ to the canonical parameters $\canparam$.

	Possible choices of the backward mappings are
		%\[ \backwardmapwrt{\mlnmintermsymbol}: \bigotimes_{\atomenumeratorin}\rr^{2} \rightarrow \bigotimes_{\atomenumeratorin}\rr^{2} \]
		\[ \backwardmapwrt{\mlnmintermsymbol}(\meanparam) = \lnof{\meanparam}
			\quad \text{and} \quad
			\backwardmapwrt{\maxtermformulaset}(\meanparam) = -\lnof{\meanparam} \, .
		 \]
\end{theorem}
\begin{proof}
	For the minterms we use that
		\[ \mintermformulaset[\shortcatvariables,\catvariableof{\mintermformulaset}]  = \identityat{\shortcatvariables,\catvariableof{\maxtermformulaset}}\]
	and get
		\[ \forwardmapwrt{\mlnmintermsymbol}(\canparam)
		= \normationof{
		\expof{\contractionof{\{\mintermformulaset, \canparam\}}{\shortcatvariables}}
		}{\shortcatvariables}
		=
		\normationof{\expof{\canparam}}{\shortcatvariables} \, .
		\]

	We notice that for any $\meanparam$ in the image of the forward map we have
		\[ \forwardmapwrt{\mlnmintermsymbol}(\backwardmapwrt{\mlnmintermsymbol}(\meanparam)) = \meanparam \]
	Therefore, $\backwardmapwrt{\mintermformulaset}$ is indeed a backward mapping to the exponential family of minterms.

	For the maxterms we use that
		\[ \maxtermformulaset[\shortcatvariables,\catvariableof{\maxtermformulaset}] = \onesat{\shortcatvariables,\catvariableof{\maxtermformulaset}}-\identityat{\shortcatvariables,\catvariableof{\maxtermformulaset}} \]
	and get
	\begin{align*}
		\forwardmapwrt{\mlnmaxtermsymbol}(\canparam)
		& = \normationof{
		\expof{\contractionof{\{\mintermformulaset, \canparam\}}{\shortcatvariables}}
		}{\shortcatvariables} \\
		& = \normationof{\{
		\expof{\contractionof{\{\ones, \canparam\}}{\shortcatvariables}},
		\expof{-\contractionof{\canparam}{\shortcatvariables}} \}
		}{\shortcatvariables} \\
		& = \normationof{
		\expof{-\canparam}
		}{\shortcatvariables}
	\end{align*}
	where we used, that $\expof{\contractionof{\{\ones, \canparam\}}{\shortcatvariables}}$ is a multiple of $\onesat{\shortcatvariables}$ and is thus eliminated in the normation.
	For any $\meanparam\in\imageof{\forwardmapwrt{\mlnmaxtermsymbol}}$ we have
		\[ \forwardmapwrt{\mlnmaxtermsymbol}(\backwardmapwrt{\mlnmaxtermsymbol}(\meanparam) )
		= \meanparam
		%-\lnof{\expof{-\canparam}} + \contractionof{\expof{-\canparam}}{\varnothing} \cdot \onesat{\shortcatvariables}
		%= \canparam + \contractionof{\expof{-\canparam}}{\varnothing} \cdot \onesat{\shortcatvariables}
		\]
	and $\backwardmapwrt{\mlnmintermsymbol}$ is thus a backward map for the exponential family of maxterms.
\end{proof}

% Fitting arbitrary distributions
Any positive probability distribution can thus be fitted by minterms when we choose $\canparam=\lnof{\probtensor}$, respectively by maxterms when we choose $\canparam=\ones-\lnof{\probtensor}$.
Thus, we have identified a subset of $2^{\atomorder}$ formulas, which is rich enough to fit any distribution.





\subsect{Atomic formulas}

% Repeat atomic formulas
Let us now derive a closed form backward mapping for the statistic
	\[ \atomformulaset := \{\atomicformulaof{\atomenumerator}: \atomenumeratorin\} \, . \]

The mean parameters coincide with the queries on the atomic formulas, that is the marginal
	\[ \meanparamat{\selvariable=\atomenumerator} = \probat{\catvariableof{\atomenumerator}=1}  \, . \]

\begin{theorem}
	Given a Markov Logic Network with the statistic $\atomformulaset$ of atomic formulas, the forward mapping from canonical parameters to mean parameters is the coordinatewise sigmoid, that is
		\[ \forwardmapwrtof{\mlnatomsymbol}{\canparamat{\selvariable}} = \frac{\expof{\canparamat{\selvariable}}}{\onesat{\selvariable}+\expof{\canparamat{\selvariable}}}   \]
	where the quotient is performed coordinatewise.

	A backward mapping is the coordinatewise logit, that is
		\[ \backwardmapwrt{\mlnatomsymbol}(\meanparamwith)
		= \lnof{\frac{
			\meanparamwith
			}{
			\onesat{\selvariable}-\meanparamwith
			}}  \, . \]
\end{theorem}
\begin{proof}
	We have for any $\canparamat{\selvariable}\in\rr^{\atomorder}$
		\[ \probofat{(\atomformulaset,\canparam)}{\shortcatvariables}
		= \bigotimes_{\atomenumeratorin} \normationof{\expof{\canparamat{\selvariable=\atomenumerator}\cdot \atomicformulaof{\atomenumerator}}}{\catvariableof{\atomenumerator}}  \, . \]


	For any $\atomenumeratorin$ it therefore holds, that
	\begin{align*}
		\forwardmapwrtof{\mlnatomsymbol}{\canparamat{\selvariable}}[\selvariable=\atomenumerator]
		&=\sbcontraction{\atomicformulaof{\atomenumerator},  \probofat{(\atomformulaset,\canparam)}{\shortcatvariables}} \\
		&=\sbcontraction{\atomicformulaof{\atomenumerator},  \normationof{\expof{\canparamat{\selvariable=\atomenumerator}\cdot \atomicformulaof{\atomenumerator}}}{\catvariableof{\atomenumerator}}} \\
		& = \frac{\expof{\canparamat{\selvariable=\atomenumerator}}}{1+\expof{\canparamat{\selvariable=\atomenumerator}}} \, .
	\end{align*}

	Since the coordinatewise logit is the inverse function of the coordinatewise sigmoid the map
	\begin{align*}
		\backwardmapwrtof{\mlnatomsymbol}{\meanparamwith}[\selvariable=\atomenumerator]
		& = \lnof{\frac{\meanparamat{\selvariable=\atomenumerator}}{1- \meanparamat{\selvariable=\atomenumerator}}}
	\end{align*}
	satisfies for any $\meanparam$ in the image of the forward map
	\begin{align*}
		\forwardmapwrt{\mlnatomsymbol}(\backwardmapwrt{\mlnatomsymbol}(\meanparam)) = \meanparam
	\end{align*}
	and is therefore a backward map.
\end{proof}


% Representation by selection tensor networks
In a selection tensor networks they are represented by a single neuron with identity connective and variable selection to all atoms.
We will investigate such examples in more detail in \charef{cha:sparseCalculus}, where atomic formulas Markov Logic Networks are specific cases of monomial decomposition of order 1.

% Interpretation of the result as independence approximation
The maximum likelihood estimator of a positive probability distribution by the MLN of atomic formulas is therefore the tensor product of the marginal distributions.
The Kullback-Leibler divergence between the distribution and its projection is the mutual information of the atoms, see for example Chapter~8 in \cite{mackay_information_2003}.

\begin{remark}[Decomposition into systems of atomic networks]
	\red{By Independence Decomposition we reduce to a system of atomic MLN.
	The minterms of such MLNs are the literals.
	By redundancy (literals sum up to $\ones$), it suffices to take only the positive or the negative literal.
	}
%	We set the weights of $\weightof{\lnot\atomicformulaof{\atomenumerator}}=0$ (corresponding with a gauge normation of the energy offset symmetry). % Not needed!
\end{remark}






\sect{Constrained parameter estimation in the minterm family}

% Naive exponential family
We approach structure learning as constrained parameter estimation in the naive exponential family (see \secref{sec:mintermExpFamily}), which coincides with the minterm family $\formulasetof{\mlnmintermsymbol}$.
The minterm family is defined by the statistic $\sstat = \identityat{\shortcatvariables, \selvariableof{[\catorder]}}$ and has energy tensors coinciding with the canonical parameters.

% Convex polytope characterization
\red{For the minterm family, we have as mean parameter set the convex hull of one-hot encodings.
Each basis vector is an extreme point is an extreme point.
}


By \theref{the:mintermExpressivityMLN} all positive distributions are member of the minterm markov logic network family.
This expressivity result was generalized to arbitrary distributions, when allowing for formulas as basemeasures by \theref{the:mintermExpressivityHLN}.

Finding the distribution maximizing the likelihood of data would then be the empirical distribution.
In this case we would have $\datameanat{\selvariableof{[\catorder]}=\shortcatindices} = \empdistributionat{\shortcatvariables=\shortcatindices}$ and the maximum likelihood distribution is found by the problem
\begin{align*}
	\argmax_{\canparam\in\facspace}  \sbcontraction{\canparam,\empdistribution} - \cumfunctionof{\canparam} \,
\end{align*}
which is solved at $\canparam=\lnof{\empdistribution}$ with $\probtensorof{(\identity,\lnof{\empdistribution})}= \empdistribution$.
This follows from $\lossof{\probtensorof{(\identity,\canparam)}}=\kldivof{\empdistribution}{\probtensorof{(\identity,\canparam)}}$, which is by Gibbs inequality minimized at $\probtensorof{(\identity,\canparam)}=\empdistribution$, which is the case for $\canparam = \lnof{\empdistribution}$.

We here allow for $\lnof{0}=-\infty$, with the convention of $\expof{-\infty}=0$, to handle datasets where specific worlds are not represented.
\red{Better: Use \theref{the:mintermExpressivityHLN} with basemeasure dropping non appearing data.}


% Regularization
To avoid this overfitting situation, we regularize by restricting the parameter to be a set $\energyhypothesis\subset\facspace$ and state
\begin{align}\tag{$\mathrm{P}_{\energyhypothesis, \empdistribution}$}\label{prob:restrictedNaiveMLE}
	\argmax_{\canparam\in\energyhypothesis}  \sbcontraction{\canparam,\empdistribution} - \cumfunctionof{\canparam} \, .
\end{align}

Problem~\ref{prob:restricedNaiveMLE} has two important types of instantiation, which we discuss in the next sections.

\subsect{Parameter Estimation}

% Parameter Estimation
\red{Projecting onto the markov logic family to the statistic $\formulaset$ is the instance of Problem~\ref{prob:restricedNaiveMLE} with the hypothesis choice}
%When the $\formulaset$ is known we take $\energyhypothesis$ as the linear hull 
	\[ \energyhypothesisof{\formulaset} = \spanof{\{\formula : \formula\in\formulaset \}} \, . \]
Then, the problem is the parameter estimation problem studied in \secref{sec:parameterEstimation}.
To see this, we reparametrize by the coefficient vectors of the elements in the span, which are then understood as the canonical parameter of the respective distribution in the markov logic family to $\formulaset$.


\begin{remark}[Overparametrization]
	Taking $\formulaset$ to consist of all propositional formulas, we get a massive overparametrization:
	The essential statistics maps to a $2^{\left(2^\atomorder \right)}$ dimensional real vector space.
	All possible distributions of the $\atomorder$ atomic variables are mapped to an $2^\atomorder-1$ dimensional submanifold, where also the essential statistics maps to.

	Thus, to identify probabilistic knowledge bases, we need to drastically restrict the shape of formulas allowed.
	It is in principle impossible to decide which formulas to be activated, based only on statistics and not on prior assumptions.

	%The nodes of a Markov Propositional Network are all formulas in a propositional theory and the hyperedges all possible decompositons.
	When having $\atomorder$ atoms, there are $2^{\atomorder}$ states in the factored system.
	Since each state can either be a model of a formula or not, there are
		\[ \cardof{\formulaset} = 2^{\big(2^\atomorder \big)} \]
	formulas.
	Having, for example, $\atomorder=10$, then $\cardof{\formulaset}>10^{308}$.


	% Regularization by sparsity
	One regularization is by allowing only a small number of formulas to be active.
	This corresponds with regularization with $\sparsityof{\canparam}$.
	The problem is then non-convex.


	% Regularization by formula size
	A further regularization strategy is the restriction of the size of the possible formulas to maintain interpretability.
	Thus, we choose small formula selection networks.
\end{remark}




\subsect{Structure Learning}

% Structure Learning
The problem of structure learning arises, when the set of parameters in Problem~\ref{prob:restricedNaiveMLE} is choosen as
	\[ \energyhypothesisof{\formulasuperset}= \bigcup_{\formulaset\in\formulasuperset} \spanof{\formulaset} \, .  \] %\energyhypothesisof{\formulaset}\, .
In this case, the problem in general fails to be convex.

% Subspace instuition
Each formula set $\formulaset$ represents a subspace in the parameters of the minterm family, which is spanned by the propositional formulas $\exformula\in\formulaset$.

%\red{Intuition by subspaces in the minterm parameters, which are selected by a nonlinear objective, to distinguish from compressed sensing.}







\sect{Greedy Structure Learning}


%Motivation 
It can be impracticle to learn all formulas at once, since the set $\formulasuperset$ often grows combinatorically, for example when choosing as a powerset of formulas.
\red{Further, we need to avoid overfitting and carefully choose a hypothesis.}
To avoid intractabilities and overfitting, one can choose a greedy approach and learn in addition formulas $\exformula$ when already having learned a set $\formulaset$ of formulas.
We in this section assume a current model $\currentdistribution$, which is a generic positive distribution not necessarily a Markov Logic Network. % or Hybrid Logic Network.

% 
We will use the effective selection tensor network representation of exponentially many formulas described in \charef{cha:formulaSelection} and select from them a small subset.

%\red{Alternative discussion: Can use current distribution as base measure and apply moment matching as first order condition.}


\subsect{Greedy formula inclusions}

Having a current set of formulas $\formulaset$ we want to choose the best $\formula\in\fselectionmap$ to extend the set of formulas to $\formulaset\cup\{\formula\}$ in a way minimizing the cross entropy.
Given this, add each step we solve the greedy cross entropy minimization
\begin{align}\label{prob:perfectGreedy}\tag{$\mathrm{P}_{\datamap,\formulaset,\fselectionmap}$}
	\argmin_{\formula\in\fselectionmap} \argmin_{\canparam\in\rr^{\cardof{\formulaset}+1}}
	\centropyof{\empdistribution}{\expdistof{(\formulaset\cup\{\formula\},\canparam,\basemeasure)}} \, .
\end{align}


A brute force solution would require parameter estimation for each candidate in $\fselectionmap$.
We provide two more efficient approximative heuristics in the following (see Chapter~20 in \cite{koller_probabilistic_2009}).


\subsect{Gain Heuristic}

In the gain heuristic, only the parameters of the new formula are optimized and the others left unchanged.
This amounts to
\begin{align}\label{prob:greedyGain}\tag{$\mathrm{P}^{\mathrm{gain}}_{\datamap,\formulaset,\fselectionmap}$}
	\argmin_{\formula\in\fselectionmap} \left ( \min_{\canparamat{\cardof{\formulaset}}\in\rr}
	\centropyof{\empdistribution}{\expdistof{(\formulaset\cup\{\formula\},\canparam,\basemeasure)}} \right) \, .
\end{align}
Here we denote by $\canparam$ the first $\cardof{\formulaset}$ coordinates of the M-projection $\currentdistribution$  of $\empdistribution$ onto $\formulaset$ and the variable new coordinate at position $\canparamat{\cardof{\formulaset}}$.

\begin{lemma}
	The gain heuristic objective is an upper bound on the true greedy objective.
\end{lemma}
\begin{proof}
Since
\begin{align*}
	&\argmin_{\formula\in\fselectionmap} \left( \argmin_{\canparam\in\rr^{\cardof{\formulaset}+1}}
	\centropyof{\empdistribution}{\expdistof{(\formulaset\cup\{\formula\},\canparam,\basemeasure)}} \right) \\
	&\quad \leq 	\argmin_{\formula\in\fselectionmap} \left ( \argmin_{\canparamat{\cardof{\formulaset}}\in\rr}
	\centropyof{\empdistribution}{\expdistof{(\formulaset\cup\{\formula\},\canparam,\basemeasure)}} \right) \, .
\end{align*}
\end{proof}


% Minterm family interpretation
Further, this is \probref{prob:restrictedNaiveMLE} in the case
\begin{align*}
	\energyhypothesis = \lnof{\currentdistribution} + \cup_{\formula\in\formulaset} \spanof{\formula} \, .
\end{align*}



% For single formula
Let us choose a formula $\formula\in\formulaset$ and consider Problem~\ref{prob:restrictedNaiveMLE}  in the case
\begin{align*}
	\energyhypothesisof{\formula} = \lnof{\currentdistribution} + \spanof{\formula} \, .
\end{align*}
This is parameter estimation on the exponential family with the single feature $\formula$ and the base measure $\currentdistribution$.
Therefore we can apply the theory of \charef{cha:probReasoning} and characterize the solution by the $\weight$ satisfying the moment matching condition
\begin{align*}
	\contraction{\currentdistribution, \normationof{\expof{\weight}}{\shortcatvariables} } = \contraction{\empdistribution, \formula} \, .
\end{align*}
We state the solution of this condition in the next theorem.

\begin{theorem}
	Problem~\eqref{prob:greedyGain} is solved at any
	\begin{align*}
		\hat{\canparam} = \weightof{\hat{\formula}} \cdot \hat{\formula}
	\end{align*}
	where the formula $\hat{\formula}$ is in
	\begin{align*}
		\hat{\formula} \in \argmax_{\formula\in\formulaset} \kldivof{\sbcontraction{\empdistribution,\formula}}{\sbcontraction{\currentdistribution,\formula}}
	\end{align*}
	and $\weightof{\hat{\formula}}$ is the weight of $\hat{\formula}$ in the solution of Problem~\ref{prob:restrictedNaiveMLE} with $\Gamma = \currentdistribution + \mathrm{span}(\exformula)$.
	Here we denote by $\kldivof{p_1}{p_2}$ the Kullback-Leibler divergence between Bernoulli distributions with parameters $p_1,p_2\in[0,1]$, that is
		\[ \kldivof{p_1}{p_2} = p_1 \cdot \lnof{\frac{p_1}{p_2}} + (1-p_1) \cdot \lnof{\frac{(1-p_1)}{(1-p_2)}}  \]
\end{theorem}
\begin{proof}
	% Solution of the problem restricted to
	For any formula $\formula$, the inner minimum of Problem~\eqref{prob:greedyGain} is by \lemref{ref:lemMMinMLN} taken at
		\[ \weightof{\formula} = \lnof{\frac{\datamean}{(1-\datamean)}\cdot \frac{(1-\currentmean)}{\currentmean}}  \]
	where
		\[ \currentmean = \sbcontraction{\currentdistribution,\formula} \]
	and
		\[ \datamean = \sbcontraction{\empdistribution,\formula} \, . \]

	The difference of the likelihood at the current distribution and the optimum is
	\begin{align*}
		\centropyof{\empdistribution}{\currentdistribution}
		- \centropyof{\empdistribution}{\expdistof{(\extendedformulaset,\extendedcanparam,\basemeasure)}}
		= \datamean \cdot \weightof{\formula} - \cumfunctionwrtof{\extendedformulaset,\basemeasure}{\extendedcanparam} \, .
	\end{align*}

	% Loss gain at optimum
	We use the representation scheme of Theorem~\ref{the:hybridNetworkRepresentation} and get
	\begin{align*}
		\sbcontraction{\currentdistribution, \expof{\weightof{\formula} \cdot \formula}}
		& = \sbcontraction{\currentdistribution, \rencodingofat{\formula}{\catvariableof{\formula}}, \actcoreofat{\formula}{\catvariableof{\formula}}} \\
		& = (1-\currentmean) + \currentmean\cdot \expof{\weightof{\formula}} \\
		& = (1 - \currentmean) + \frac{\datamean \cdot (1-\currentmean)}{(1-\datamean)} \\
		& = (1-\currentmean) \cdot \frac{1}{(1-\datamean)} \, .
	\end{align*}
	% Refining the cumulant term
	It follows, that
	\begin{align*}
		\cumfunctionwrtof{\extendedformulaset,\basemeasure}{\extendedcanparam}
		& = \lnof{\sbcontraction{\currentdistribution, \expof{\weightof{\formula} \cdot \formula}}} \\
		& = \lnof{1-\currentmean} - \lnof{1-\datamean} \, .
	\end{align*}
	% Refining the mean product term
	We further have
	\begin{align*}
		\datamean \cdot \weightof{\formula}
		= \datamean \cdot \left[ \lnof{\frac{\datamean}{(1-\datamean)}\cdot \frac{(1-\currentmean)}{\currentmean}}  \right]
		= \datamean \lnof{\datamean} - \datamean \lnof{1-\datamean} + \datamean \lnof{1-\currentmean} - \datamean \lnof{\currentmean}
	\end{align*}
	and arrive at
	\begin{align*}
		& \centropyof{\empdistribution}{\currentdistribution}
		- \centropyof{\empdistribution}{\expdistof{(\exformula,\weightof{\formula},\currentdistribution)}} \\
		& \quad =  \datamean \lnof{\datamean} - \datamean \lnof{1-\datamean} + \datamean \lnof{1-\currentmean} - \datamean \lnof{\currentmean}
		-  \lnof{1-\currentmean} - \lnof{1-\datamean} \\
		& \quad = \left( -\datamean \lnof{\currentmean} - (1-\datamean) \lnof{1-\currentmean} \right)  - \left( -\datamean \lnof{\datamean} - (1-\datamean) \lnof{1-\datamean} \right) \, .
	\end{align*}
	By definition, this is the Kullback-Leibler divergence between Bernoulli distributions with parameters $\datamean$ and $\currentmean$.
	%
	Since the gain in the likelihood loss when restricting to $\energyhypothesis = \spanof{\formula}$ is thus given by $\kldivof{\sbcontraction{\empdistribution,\formula}}{\sbcontraction{\currentdistribution,\formula}}$, we have that Problem~\ref{prob:restrictedNaiveCE}  in the case $\energyhypothesis = \bigcup_{\formula\in\formulaset}\spanof{\formula}$ is solved at $\estcanparam = \weightof{\hat{\formula}}\cdot \hat{\formula}$ where
		\[ \hat{\formula} = \kldivof{\sbcontraction{\empdistribution,\formula}}{\sbcontraction{\currentdistribution,\formula}} \, . \]
\end{proof}

\red{Thus, we solve the grain heuristic with a coordinatewise transform of the mean parameter tensors to $\empdistribution$ and $\currentdistribution$, using the Bernoulli Kullback-Leibler divergence as transform function.}


% Interpretation
One therefore takes the formula, which marginal distribution in the current model and the targeted distribution are differing at most, measured in the KL divergence.

% Optimization method
One optimization method would thus be the computation of the mean parameters to both distribution, building the coordinatewise KL divergence and choosing the maximum.
Since we need to evaluate each coordinate, this can be intractable for large sets of formulas.


% Further weight optimization
Further improvement of the model can be achieved by iteratively optimizing the other weights as well, since their corresponding moment matching conditions might be violated after the integration of a new formula.
This would require the computation of backward mappings for each candidate formula, for which we only have an alternating approach in general.



\subsect{Gradient heuristic and the proposal distribution}

\red{Advantage: Might avoid formulawise calculus, when sampling from proposal distribution.
Brute force solution of gain heuristic require formulawise approach.}

We now derive a heuristic of choosing features based on the maximal coordinate of the gradient when differentiating the canonical parameter in the minterm family.
To prepare for this, we build the gradient of the loss
%For the naive exponential family 
\begin{align*}
	\lossof{\expdistof{(\naivestat, \naivecanparam)}}
	%= \frac{1}{\datanum} \sum_{\datindexin}\lnof{\expdistofat{(\naivestat, \naivecanparam)}{\shortcatvariables=\datamapat{\datindex}}}
	= \contraction{\empdistribution, \sencodingof{\naivestat}, \naivecanparam} - \lnof{\contraction{\expof{\contractionof{\sencodingof{\naivestat}, \naivecanparam}{\shortcatvariables}}}}
\end{align*}
as
\begin{align*}
	\gradwrt{\naivecanparamat{\selvariable}} \lossof{\expdistof{(\naivestat, \naivecanparam)}}
	&= \contractionof{\sencodingof{\naivestat},\empdistribution}{\selvariable} - \contractionof{\sencodingof{\naivestat},\expdistof{(\naivestat, \naivecanparam)}}{\selvariable} \\
	&= \empdistribution - \expdistof{(\naivestat, \naivecanparam)} \, .
\end{align*}

%% Single feature
%Given a feature $\exfunction[\shortcatvariables]$ we vary the naive parameters by a function on $\canparam\in\rr$ by
%\begin{align*}
%	 \naivestat(\canparam) %=  \mlntensor + \weight_{\selindices} \rencodingof{\exformula_{\selindices}}
%	= \naivestat(0) + \canparam\cdot\exfunction
%\end{align*}
%and get a likelihood gradient of
%\begin{align*}
%	 \frac{\partial \lossof{\expdistof{(\naivestat(\canparam), \naivecanparam)}}}{\partial\canparam} 
%	 &= \sbcontraction{
%	 	\frac{\partial\lossof{\expdistof{(\naivestat, \naivecanparam)}}}{\partial\naivecanparam}|_{\naivecanparam(0)},
%		\frac{\partial\naivecanparam(\canparam)}{\partial\canparam} 
%	 }  \\
%	 &= \contraction{\empdistribution,\exfunction} -   \contraction{\expdistof{(\naivestat, \naivecanparam)},\exfunction} \, .
%\end{align*}


%% Positive and Negative Search
The gradient shows the typical decomposition into a positive and a negative phase.
While the positive phase comes from the data term and prefers directions of large data support, the negative phase originates in the partition function and draws the gradient away from directions already supported by the current model $\expdistof{(\naivestat, \naivecanparam)}$.
%% Regularization functionality
The negative phase is a regularization, by comparing with what has already been learned.
When nothing has been learned so far, we can take the current model to be the uniform distribution, which is the naive exponential family with vanishing canonical parameters.



%% Collection of features by selection
Given a set $\fselectionmap$ of features we vary $\naivecanparam$ by the function
\begin{align*}
	 \exfunction(\canparam) = \naivecanparam + \sbcontractionof{\canparam,\sencodingof{\fselectionmap}}{\shortcatvariables} \, .
\end{align*}
At $\canparam=0$ we have the gradient of the loss of the parametrized formula by
\begin{align*}
	 \gradwrtat{\canparam}{0}
	 \lossof{\expdistof{(\naivestat,\exfunction(\canparam),\basemeasure)}}
	 &= \sbcontraction{
	 	 \gradwrtat{\exfunction(\canparam)}{\naivecanparam}  \lossof{\expdistof{(\naivestat,\exfunction(\canparam),\basemeasure)}},
		 \gradwrtat{\canparam}{0}  \exfunction(\canparam)
	 }  \\
	 &= \sbcontractionof{\empdistribution,\sencodingof{\sstat}}{\selvariable} -   \sbcontractionof{\expdistof{(\naivestat, \naivecanparam, \basemeasure)},\sencodingof{\sstat}}{\selvariable} \, .
\end{align*}


%% Grafting
We want to choose the formula, which is best aligned with the gradient of the log-likelihood, that is using a formula selecting map $\fselectionmap$
\begin{align} \label{prob:greedyGrad} \tag{$\mathrm{P}^{\mathrm{grad}}_{\datamap,\formulaset,\fselectionmap}$}
	\argmax_{\selindex\in[\seldim]} \sbcontractionof{\empdistribution,\fselectionmap}{\indexedselvariable}
	- \sbcontractionof{\expdistof{(\naivestat, \naivecanparam, \basemeasure)},\fselectionmap}{\indexedselvariable} \, .
\end{align}
This method is known as the gradient heuristic or grafting.
% Mean parameter interpretation
The objective of Problem~\eqref{prob:greedyGrad} has another interpretation by the difference of the mean parameter $\datamean$ and $\currentmean$ of the projections of the empirical and current distributions on the family to $\fselectionmap$. % ! NOT the proposal family, those have transposed statistic

%% Formula alignment perspective
Problem~\eqref{prob:greedyGrad} is further equivalent to the formula alignment
\begin{align*}
	\argmax_{\formula\in\fselectionmap} \sbcontraction{\formula,\empdistribution-\currentdistribution} \, .
\end{align*}
The objective can be interpreted as the difference of the satisfaction probability of the formula with respect to the empirical distribution and the current distribution.
%We can choose selection architectures to efficiently parametrize the formulas in the hypothesis $\fselectionmap$ and rewrite the problem as
%\begin{align*}
%	\argmax_{\selindexin} \contractionof{ \gradwrtat{\canparam}{\canparam=0} \lossof{\expdist}}{\indexedselvariable}
%\end{align*}
%This is thus equivalent to the problem \ref{prob:greedyGrad}, when taking all formulas selectable by $\formulaset$ as the hypothesis $\Gamma$.












\subsect{Iterations}

Let us now iterate the search for a best formula at a current model with the optimization of weights after each step.
The result is Algorithm~\ref{alg:greedyStructureLearning}, which is a greedy algorithm adding iteratively the currently best feature.

\begin{algorithm}[hbt!]
\caption{Greedy Structure Learning}\label{alg:greedyStructureLearning}
\begin{algorithmic}
	\Require Empirical distribution $\empdistribution$, hypothesis $\fselectionmap$ of formulas
	\Ensure Distribution $\expdist$ approximating $\empdistribution$
	\hrule
	\State Initialize
		\[ \currentdistribution \algdefsymbol \frac{1}{\prod_{\catenumeratorin}\catdimof{\atomenumerator}} \cdot \onesat{\shortcatvariables} \quad, \quad \formulaset = \varnothing \]
	\While{Stopping criterion is not met}
		% REFINE! Work in data
		\State \textbf{Structure Learning:} Compute a (approximative) solution $\hat{\formula}$ to Problem~\ref{prob:restrictedNaiveMLE} and add the formula to $\formulaset$, i.e.
				\[ \formulaset \algdefsymbol \formulaset \cup\{\hat{\formula}\} \]
			Extend dimension of $\selvariable$ by one, by $\formulaof{\seldim}=\hat{\formula}$ and $\canparamat{\seldim}=0$
		\State \textbf{Weight Estimation:} Estimate the best weights for the added formula and recalibrate the weights of the previous formulas, by calling Algorithm~\ref{alg:AWO}.
				\[ \currentdistribution \algdefsymbol \expdistof{\formulaset, \canparam} \]
\EndWhile
	\State \Return $\formulaset$, $\canparam$ %, $\kb$
\end{algorithmic}
\end{algorithm}



%% Energy Storage -> Useful after learning for energy-based inference
When having used the same learning architecture multiple times, the energy of the corresponding formulas are all representable by a formula selecting architecture.
Their energy term is therefore a contraction of the selecting tensor with a parameter tensor $\canparam$ in a basis CP decomposition with rank by the number of learned formulas.
When mutiple selection architectures have been used, the energy is a sum of such contractions.
% 
Let us note, that this representation is useful after learning, when performing energy-based inference algorithms on the result.
During learning, one needs to instantiate the proposal distribution, which requires instantiation of the probability tensor.
\red{However, one could alternate data energy-based and use this as a particle-based proxy for the probability tensor.}


\begin{remark}[Sparsification by Thresholding]
	To maintain a small set of active formulas, one could combine greedy learning approaches with thresholding on the coordinates of $\canparam$.
	This is a standard procedure in Iterative Hard Thresholding algorithms of Compressed Sensing, but note that here we do not have a linear in $\canparam$ objective.
\end{remark}




\sect{Proposal distribution}


% Proposal distribution
Let us now understand the likelihood gradient as the energy tensor of a probability distribution, which we call the proposal distribution.

\begin{definition}[Proposal Distribution]
	Let there be a base distribution $\currentdistribution$, a targeted distribution $\empdistribution$ and a formula selecting map $\fselectionmap[\shortcatvariables, \selvariable]$.
	The proposal distribution at inverse temperature $\invtemp>0$ is the distribution of $\selvariable$ defined by
	\begin{align*}
		\normationof{\expof{\sbcontractionof{\invtemp\cdot(\empdistribution-\currentdistribution),\fselectionmap}{\selvariable}} }{\selvariable} \, .
	\end{align*}
	The proposal distribution is the member of the exponential family with statistics $\fselectionmap$ and parameter $\invtemp\cdot(\empdistribution-\currentdistribution)$.
\end{definition}


%. Exponential family
The proposal distribution is in the exponential family with sufficient statistic by the formula selecting map $\fselectionmap$, namely the member with the canonical parameters $\canparam=\empdistribution-\currentdistribution$.
Of further interest are tempered proposal distributions, which are in the same exponential family with canonical parameters $\invtemp\cdot(\empdistribution-\currentdistribution)$ where $\invtemp>0$ is the inverse temperature parameter.

% MLN
As Markov Logic Networks, the proposal distributions are in exponential families with the sufficient statistic defined in terms of formula selecting maps.
While Markov Logic Networks contract the maps on the selection variables $\selvariable$, the proposal distributions contract them along the categorical variables $\catvariable$ to define energy tensors.

% Methods to solve mode search
The grafting Problem~\eqref{prob:greedyGrad} is the search for the mode of the proposal distribution.
To solve grafting, we thus need to answer a mode query, for which we can apply the methods introduced in \charef{cha:probReasoning}, such as Gibbs Sampling or Mean Field Approximations in combination with annealing.


\subsect{Mean parameter polytope}

The mean parameter polytope of the proposal distribution with statistic $\proposalstat$ is the convex hull of the formulas in $\formulaset$, that is
\begin{align*}
	\meansetof{\proposalstat}
	= \convhullof{\sencodingof{\proposalstat}{\indexedselvariable,\shortcatvariables} \, : \, \selindexin{}}
	= \convhullof{\formulaat{\shortcatvariables} \, : \, \formula\in\fselectionmap}
\end{align*}


% 0/1
As it was the case for Markov Logic Networks, the mean parameter polytopes are instances of a $0/1$-polytopes \cite{ziegler_lectures_2000,gillmann_01-polytopes_2007}.

% Interpretation as formulas
The extreme points are the formulas selectable by the formula selecting map $\fselectionmap$.


\sect{Discussion}

\begin{remark}[Bayesian approach]
	We only treated the estimation of a single resulting distribution by the data, while in a Bayesian approach one typically considers an uncertainty over possible distributions.
	% MAP
	\red{When treating $\canparam$ as a random tensor, which prior distribution is given and posteriori distribution wanted, we have a more involved Bayesian approach.}
	When having a prior $\probat{\mlnparameters}$ over the Markov Logic Networks we alternatively want to find the parameters $\mlnparameters$ solving the maximum a posteriori problem
	\begin{align}
		\argmax_{\mlnparameters} \mlnprobat{\data}\cdot \probat{\mlnparameters}\, .
	\end{align}
\end{remark}

To summarize some insights on the mean polytopes $\hlnmeanset$:
\begin{itemize}
	\item If and only íf all coordinates are in $\{0,1\}$ then an extreme points, then $\meanparam$ is reproduced by a hard logic network.
	\item If some mean params in $\{0,1\}$, then not in the interior, and not reproduced by a markov logic network.
		Back direction not correct: There are interior points where no coordinate in $\{0,1\}$.
	\item If not in the interior, we can identify with base measure refinement a base measure, such that reproducable by a distribution representable by the base measure.
\end{itemize}


% Polytopes - MLN 
The polytopes of mean parameters to hybrid logic networks and proposal distributions are an interesting connection between the fields of combinatorical optimization and the study of expressivity of tensor networks.
% Minimal Connectivity: Local consitency - Hierarchical Tucker
This is of special interest, when the computation cores of a hybrid logic network are minimally connected, the mean parameters are captured by local consistencies.
Similar investigations have been made in the field of tensor networks, where minimal connected tensor networks are refered to by Hierarchical Tucker formats (HT).
Minimal connection is exploited in the tensor network community to show numerical properties of the format, such as closedness and existence of best approximators.














