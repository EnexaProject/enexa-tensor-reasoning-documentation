\section{Representing Logic Networks}

Markov Logic Networks exploit the efficiency and interpretability of logical calculus as well as the expressivity of graphical models. 

%Markov Logic Networks are probability functions of truth assignments to logical functions.
%They respect propositional logic as hard constraints, but have beyond that freedom to shape probability distributions on possible situations.
%To capture these properties, we define them as graphical models with structure cores representing propositional logics and activation cores representing the specification of probability distributions.
% We in this part employ them to combine the probabilistic and the logical paradigm.



\subsection{Markov Logic Networks as Exponential Families}

We introduce Markov Logic Networks in the formalism of exponential families (see Section~\ref{sec:exponentialFamilies}).

\begin{definition}[Markov Logic Networks]
	Markov Logic Networks are exponential families $\mlnexpfamily$ with sufficient statistics by functions
		\[ \formulaset : \atomstates \rightarrow \bigtimes_{\exformulain}[2] \subset \rr^{\cardof{\formulaset}} \]
	defined coordinatewise by propositional formulas $\exformulain$.
\end{definition}

% Binary Statistics as propositional formula
Since the image of each feature is contained in $[2]$, they are propositional formulas (see Def.~\ref{def:formulas}).

% Characterization of MLNs among exponential families: When choosing binary features
Conversely, any binary feature $\sstatcoordinateof{\statenumerator}$ of an exponential family defines a propositional formula (see Definition~\ref{def:formulas}).
Thus, any exponential family of distributions of $\atomstates$, such that $\imageof{\sstatcoordinateof{\statenumerator}}\subset\{0,1\}$ for all $\statenumeratorin$ is a set of Markov Logic Networks with fixed formulas.

% Formula Selecting Networks
%We will further study the sparse representation of formula sets in Chapter~\ref{cha:architectures}.


The sufficient statistics consistent in a map $\formulaset$ of formulas brings the following advantages:
\begin{itemize}
	\item Numerical Advantage: The sufficient statistics is decomposable into logical connectives. 
	If the formulas are sparse (in the sense of limited number of connectives necessary in their representation), this gives rise to efficient tensor network decompositions of the relational encoding.
	\item Statistical Advantage: Since each formula is Boolean valued, the coordinates of the sufficient statistic are Bernoulli variables. 
	Due to their boundedness, they and their averages (by Hoeffdings inequality) are sub-Gaussian variables with favorable concentration properties (absence of heavy tails).
\end{itemize}


\begin{remark}[Alternative Definitions]
	We here defined MLNs on propositional logic, while originally they are defined in FOL.
	The relation of both frameworks will be discussed further in Chapter~\ref{cha:folModels}.
	Besides that we allow for degeneracy and improper MLNs.
\end{remark}


%\subsubsection{Interpretation as Graphical Models}






\subsection{Tensor Network Representation of MLNs}

Based on the previous discussion on the representation of exponential families by tensor networks in Section~\ref{sec:exponentialFamilies} we now derive a representation for Markov Logic Networks.

\begin{theorem}[Relational Encodings for Markov Logic Networks]
	A Markov Logic Network to a set of formulas $\formulaset = \{\formulaof{\selindex} \, : \, \selindexin\}$ is represented as
	\begin{align*}
		\mlnprobat{\shortcatvariables} = 
		\normationof{
			\{\rencodingofat{\formulaof{\selindex}}{\catvariableof{\formulaof{\selindex}},\shortcatvariables} : \selindexin \} 
			\cup \{ 
			\headcoreofat{\formulaof{\selindex},\canparamat{\selvariable=\selindex}}{\catvariableof{\formulaof{\selindex}}} 
			: \selindexin \}
		}{\shortcatvariables}
	\end{align*}
	where we denote for each $\selindexin$ a head core
	\begin{align*}
		\headcoreofat{\formulaof{\selindex},\canparamat{\indexedselvariableof{}}}{\catvariableof{\formulaof{\selindex}}} 
		= \begin{bmatrix} 1 & \expof{\canparamat{\indexedselvariableof{}}} \end{bmatrix}[\catvariableof{\formulaof{\selindex}}] \, .
	\end{align*}
\end{theorem}
\begin{proof}
	The claim follows from Theorem~\ref{def:expFamilyTensorRep} and the following contraction equations.
	We have with the grouped variable $\catvariableof{\formulaset} = \{\catvariableof{\formulaof{\selindex}}\, : \, \selindexin\}$
	\begin{align*}
		\rencodingofat{\formulaset}{\shortcatvariables,\catvariableof{\formulaset}}
		= \contractionof{\{\rencodingofat{\formulaof{\selindex}}{\catvariableof{\formulaof{\selindex}},\shortcatvariables} : \selindexin \}}{\shortcatvariables,\catvariableof{\formulaset}} \, .
	\end{align*}
	Since we have a Markov Logic Network we have $\imageof{\formulaof{\selindex}}\subset [2]$ and thus
	\begin{align*}
	 	\headcoreofat{\formulaof{\selindex},\canparamat{\indexedselvariableof{}}}{\indexedcatvariableof{\formulaof{\selindex}}} 
		= \begin{cases}
			1 & \text{for} \quad \catvariableof{\formulaof{\selindex}} = 0 \\
			\expof{\canparamat{\indexedselvariableof{}}} & \text{for} \quad \catvariableof{\formulaof{\selindex}} = 1
		\end{cases}  
	\end{align*}
	Using these equations, the claim follows from Theorem~\ref{def:expFamilyTensorRep}.
\end{proof}

\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/mln/factor.tex}
\end{center}
\caption{Factor of a Markov Logic Network to a formula $\formulaof{\selindex}$.}
% Where $\headcoreofat{\formulaof{\selindex}}{\catvariableof{\formulaof{\selindex}}} =\begin{bmatrix} 1 & \expof{\weightof{\exformula}} \end{bmatrix}[\catvariableof{\exformula}] $}
\label{fig:mlnFactor}
\end{figure}

% 
Since any member of an exponential family is a Markov Network with tensors to each coordinate of the statistic, also Markov Logic Networks are Markov Networks.

\begin{corollary}\label{cor:MLNasMN}
	Given a set $\formulaset$ of formulas on atomic variables $\catvariableof{\nodes}$, we construct a $\graph=(\nodes,\edges)$, where $\nodes$ are decorated by the atoms and
		\[ \edges = \{ \nodesof{\formula}: \formula\in\formulaset \} \, , \]
	where by $\nodesof{\formula}$ we denote the minimal set such that there exists a tensor $\hypercoreat{\catvariableof{\nodesof{\formula}}}$ with
		\[ \formulaat{\catvariableof{\nodes}} = \hypercoreat{\catvariableof{\nodesof{\formula}}} \otimes \onesat{\catvariableof{\nodes/\nodesof{\formula}}} \, . \]		
	Any Markov Logic Network $\mlnparameters$ is then a Markov Network given the graph $\graphof{\formulaset}$
	$\{\expof{\canparamat{\selvariable=\selindex}\cdot\formulaof{\selindex}}
\, :\,\selindexin\}$.
\end{corollary}


% MLN as graphical models
Markov Logic Networks are Markov Networks with the factors given in a restricted form from the weighted truth of a formula.
Each formula is seen as a factor of the graphical model.

There are two sparsity mechanisms drastically reducing the number of parameters (and loosing generality):
\begin{itemize}
	\item Factors/Formulas contain only subsets of atoms (already in Corollary~\ref{cor:MLNasMN} exploited):
		The underlying assumptions of conditional independence loss generality.
	\item Structure in the factors: In MLN each factor corresponds with a formula evaluated on possible worlds.
		Again, any possible factor can be represented by a formula, but we concentrate on small formulas (see Theorem \ref{the:FormulaToTensor}).
\end{itemize}


% 
\red{We can extend the set of variables, by including the hidden formulas, and get a Markov Network of the relational encodings of connectives and headcores.
Here hidden variables are additional variables facilitating the decomposition, but not appearing in open variables of contractions when doing reasoning.
One can then exploit redundancies and make sure that every subresult is computed just once, by dropping relational encodings with identical head functions.
}


\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/mln/decomposed_representation.tex}
\end{center}
\caption{Example of a decomposed Markov Network representation of a Markov Logic Network with formulas $\{\formulaof{0} = a\lor b, \formulaof{1} = a \lor b \lor \lnot c\}$.
	Since both formulas share the subformula $a\lor b$, their contracted factors have a representation by a connected tensor network.}
% Where $\headcoreofat{\formulaof{\selindex}}{\catvariableof{\formulaof{\selindex}}} =\begin{bmatrix} 1 & \expof{\weightof{\exformula}} \end{bmatrix}[\catvariableof{\exformula}] $}
\label{fig:mlnDecRep}
\end{figure}


%\begin{theorem}[Selection encodings for Energy representation]
%	\red{More the definition of exponential families.}
%	The energy of Markov Logic Networks is the contraction
%		\[ \mlnenergy = \sbcontractionof{\sencodingof{\formulaset},\canparam}{\shortcatvariables} \, . \]
%\end{theorem}


%When the further factor cores contain only one variable, we can label them by the formula $\exformula$ corresponding with that node $[1,\expof{\weightof{\exformula}}]$.
%\begin{definition}{Markov Logic Networks}
%	Given a set of formulas $\mlnformulaset$ with weights $\weight:\mlnformulaset\rightarrow\rr$ the Markov Logic Network is the distribution
%		\[ \mlnprobat{\indexedcatvariables}= \frac{1}{\partitionfunctionof{\mlnparameters}} \expof{
%			\sum_{\mlnformulain} \exformula(\atomindices) \weightof{\exformula}
%			} \]
%	where the partition function
%		\[ \partitionfunctionof{\mlnparameters} = \sum_{\atomindicesin}  \expof{
%			\sum_{\mlnformulain} \exformula(\atomindices) \weightof{\exformula}
%		} \]
%	ensures the normation.
%\end{definition}

%where the partition function is the contraction 
%\begin{align}
%	\partitionfunctionof{\mlnparameters} = \sbcontraction{\expof{\mlnenergy}} \, . 
%\end{align}



\subsubsection{Energy tensors of Markov Logic Networks}

%% Tensor Representation of MLN
With the energy tensor
\begin{align}
	\mlnenergy 
	= \sum_{\selindexin} \canparamat{\selvariable=\selindex} \cdot \formulaofat{\selindex}{\shortcatvariables} 
	= \sbcontractionof{\sencodingofat{\formulaset}{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables} 
\end{align}
the MLN is the distribution
\begin{align}
	\mlnprobat{\shortcatvariables} = \normationofwrt{\expof{\mlnenergy}}{\shortcatvariables}{\varnothing} \, . 
\end{align}

In case of a common structure of the formulas in a Markov Logic Network, Formula selecting networks can be applied to represent their energies.

% Energy representation
%The weighted sum of formulas is then the energy of the Markov Logic Network.
We represent the superposition of formulas as a contraction with s parameter tensor.
Given a factored parametrization of formulas $\exformula_{\parindices}$ with indices $\selindexof{\parenumerator}$ we have the superposition by the network representation:
\begin{center}
	\input{PartII/tikz_pics/mln/energy_contraction.tex}
\end{center}


% Representation 
If the number of atoms and parameters gets large, it is important to represent the tensor ${\exformula_{\parindices}}$ efficiently in tensor network format and avoid contractions.
To avoid inefficiency issues, we also have to represent the parameter tensor $\canparam$ in a tensor network format to improve the variance of estimations (see Chapter~\ref{cha:mlnConcentration}) and provide efficient numerical algorithms.

% Fail of full probability representation
However, when required to instantiate the probability distribution of a Markov Logic Network as a tensor network, we need to exponentiate and normate the energy tensor, a task for which relational encodings are required.
For such tasks, contractions of formula selecting networks are not sufficient and each formula with a nonvanishing weight needs to be instantiated as a factor tensor of a Markov Network. 






\subsection{Expressivity}\label{sec:MLNMaxMintermRep}

Based on Markov Logic Networks containing only maxterms and minterms (see Definition~\ref{def:clauses}), we here provide an expressivity study.
There are $2^{\atomorder}$ maxterms and $2^{\atomorder}$ minterms which are enough to represent any probability distribution as we show next.

\begin{theorem}\label{the:maximalClausesRepresentation}\label{the:mintermExpressivityMLN}
	Let there be a positive probability tensor %distribution of the worlds to the atoms $\shortcatvariables$ with probability tensor
		 \[ \probof{\shortcatvariables} \in \bigotimes_{\atomenumeratorin}\rr^2 \, . \] %= \frac{\expof{\mlntensor}}{\partitionfunctionof{\mlntensor}} \, . \]
	Then the Markov Logic Network of minterms (see Definition~\ref{def:clauses})
		\[ \mintermformulaset = \{\mintermof{\atomindices} \, : \, \atomindices\in\atomstates \}\]
	with parameters %with nonzero weights at the maxterms indexed by $\atomindicesin$
		\[ \canparamat{\selvariableof{0}=\catindexof{0},\ldots,\selvariableof{\atomorder-1}=\catindexof{\atomorder-1}}% \weightof{\mintermof{\atomindices}} 
		= \ln \probof{\indexedcatvariables} \]
	coincides with $\probof{\shortcatvariables}$.

	Further, the Markov Logic Network of maxterms
		\[ \maxtermformulaset = \{\maxtermof{\atomindices} \, : \, \atomindices\in\atomstates \}\]
	with wparameters
		\[ \canparamat{\selvariableof{0}=\catindexof{0},\ldots,\selvariableof{\atomorder-1}=\catindexof{\atomorder-1}} %\weightof{\maxtermof{\atomindices}} 
		= - \ln\probof{\indexedcatvariables} \]
	coincides with $\probof{\shortcatvariables}$.
\end{theorem}
\begin{proof}
	It suffices to show, that in both cases of choosing $\formulaset$ by minterms or maxterms with the respective parameters
		\[ \mlnenergy =  \ln\probof{\shortcatvariables} \]
	and therefore
		\[ \mlnprobat{\shortcatvariables} 
		= \sbnormationof{\expof{\mlnenergy}}{\shortcatvariables} 
		=  \sbcontractionof{\expof{\mlnenergy}}{\shortcatvariables} 
		= \probof{\shortcatvariables}\, . \]
	
	In the case of minterms, we notice that for any $\atomindicesin$
		\[ \mintermof{\atomindices}[\shortcatvariables] = \onehotmapofat{\atomindices}{\shortcatvariables} \]
	and thus with the weights in the claim
		\[ \sum_{\atomindicesin} 
		\left( \ln \probof{\indexedcatvariables} \right) \cdot \mintermof{\atomindices}[\shortcatvariables] 
		= \ln\probof{\shortcatvariables} \, .
		 \]

	For the maxterms we have analogously
		\[ \maxtermof{\atomindices}[\shortcatvariables] = \onesat{\shortcatvariables} - \onehotmapofat{\catindices}{\shortcatvariables} \]
	and thus that the maximal clauses coincide with the one-hot encodings of respective states.
	We thus have
	\begin{align*}
		& \sum_{\atomindicesin} 
		\left( - \ln \probof{\indexedcatvariables} \right) \cdot \maxtermof{\atomindices}[\shortcatvariables] \\
		& =
		\left(  \sum_{\nodes_0\subset [\atomorder]} 
		\left( - \ln \probof{\indexedcatvariables} \right) \cdot \onesat{\shortcatvariables} \right) \\
		& \quad + 
		\left(  \sum_{\nodes_0\subset [\atomorder]} 
		\left(  \ln \probof{\indexedcatvariables} \right) \cdot 
		\onehotmapofat{\catindices}{\shortcatvariables} 
		\right) 
		 \\
		 & = \ln\probof{\shortcatvariables} + \lambda \cdot  \onesat{\shortcatvariables}\,,
	\end{align*}
	where $\lambda$ is a constant.
\end{proof}

% Redundant parametrization
In general, this representation is redundant, since any offset of the weight by $\lambda\cdot\ones$ results in the same distribution.
However, the only $\bar{\canparam}$ are multiples of $\onesat{\shortcatvariables}$.

% Comparison with previous schemes
Theorem~\ref{the:maximalClausesRepresentation} is the analogue in Markov Logic to Theorem~\ref{the:tensorToMaxMinTerms}, which shows that any binary tensor has a representation by a logical formula, to probability tensors.
Here we require positive distributions for well-defined energy tensors.


\begin{remark}[Representation of Markov Networks]
% Composition of Markov Networks
	If a probability distribution is representable as a Markov Network, we only need to activate clauses and terms, which variables are contained in factors.
	\red{Make a theorem out of that?}
\end{remark}

%\subsection{Markov Logical Networks as Graphical Models}









\subsection{Examples}


\subsubsection{Distribution of independent variables}

We show next, the independent positive distributions are representable by tuning the $\atomorder$ weights of the atomic formulas and keeping all other weights zero.

\begin{theorem}\label{the:independentAtomicMLN}
	Let $\probat{\shortcatvariables}$ be a positive probability distribution, such that disjoint subsets of atoms are independent from each other.
	Then $\probat{\shortcatvariables}$ is the Markov Logic Network of atomic formulas
		\[ \atomformulaset = \{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin \} \]
	and parameters
		\[ \canparamat{\selvariable=\catenumerator} 
		= \lnof{\frac{
		\contractionof{\probtensor}{\catvariableof{\catenumerator}=1}
		}{
		\contractionof{\probtensor}{\catvariableof{\catenumerator}=0}
		}} \]
%	Any distribution such that the atom satisfaction is independent from each other is reproducable by a MLN with nonzero weights only for the atomic formulas.
\end{theorem}
\begin{proof}
%	Using the independent assumptions, the probability tensor factorizes into normed vectors to each atom, with are transformed atomic formulas (leaving out the neutral ones tensors).
%	We then find a weight to each atom such that the vector is reproduced by the contraction with the activation core.
	
	By Theorem~\ref{the:independenceProductCriterion} we get a decomposition 
		\[ \probat{\shortcatvariables} = \bigotimes_{\catenumeratorin} \probofat{\catenumerator}{\catvariableof{\catenumerator}} \,  \]
	where 
		\[ \probofat{\catenumerator}{\catvariableof{\catenumerator}} = \sbcontractionof{\probtensor}{\catvariableof{\catenumerator}} \, . \]
	
	By assumption of positivity, the vector $\probofat{\catenumerator}{\catvariableof{\catenumerator}}$ is positive for each $\catenumeratorin$ and the parameter
		%\[ \canparam^\catenumerator = \lnof{\frac{\probofat{\catenumerator}{\catvariableof{\catenumerator}=1}}{\probofat{\catenumerator}{\catvariableof{\catenumerator}=0}}} \]
		\[ \canparam^\catenumerator 
		= \lnof{\frac{
		\probofat{\catenumerator}{\catvariableof{\catenumerator}=1}
		}{
		\probofat{\catenumerator}{\catvariableof{\catenumerator}=0}
		}} \]
	well-defined.
	
	We then notice, that 
		\[ \expdistofat{(\{\atomicformulaof{\catenumerator}\},\canparam^{\catenumerator})}{\catvariableof{\catenumerator}} 
		= \probofat{\catenumerator}{\catvariableof{\catenumerator}}\]
	and therefore with the parameter vector of dimension $\seldim=\catorder$ defined as
		\[ \canparamat{\selvariable} = \sum_{\catenumeratorin} \canparam^{\catenumerator} \cdot \onehotmapofat{\catenumerator}{\selvariable}  \]
	we have
	\begin{align*}
	 	 \expdistofat{(\{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin\},\canparam)}{\shortcatvariables} 
		& = \bigotimes_{\catenumeratorin} \expdistofat{(\{\atomicformulaof{\catenumerator}\},\canparam^{\catenumerator})}{\catvariableof{\catenumerator}} \\
		& = \bigotimes_{\catenumeratorin} \probofat{\catenumerator}{\catvariableof{\catenumerator}} \\
		& = \probat{\shortcatvariables} \, . 
	\end{align*}
\end{proof}

%In general, the statistic to an atomic formula measures the marginal distribution. -> To Parameter Estimation

% Failing to be positive -> Hybrid networks
In Theorem~\ref{the:independentAtomicMLN} we made the assumption of positive distributions.
If the distribution fails to be positive, we still get a decomposition into distributions of each variable, but there is at least one factor failing to be positive.
Such factors need to be treated by hybrid logic networks, that is they are base measure for an exponential family coinciding with a logical literal (see Chapter~\ref{cha:hardNetworks}.

% Energy representation
All atomic formulas can be selected by a single variable selecting tensor, that is
	\[ \energytensorofat{(\{\atomicformulaof{\catenumerator} \, : \, \catenumeratorin\},\canparam)}{\shortcatvariables}
	= \sbcontractionof{\vselectionmapat{\shortcatvariables,\selvariable},\canparamat{\selvariable}}{\shortcatvariables} \, . 
	\]
	
% Holds also more generally for any formula! -> Place it earlier?
In case of negative $\canparamat{\catenumerator}$ it is convenient to replace $\atomicformulaof{\catenumerator}$ by $\lnot\atomicformulaof{\catenumerator}$, in order to facilitate the interpretation.
The probability distribution is left invariant, when also replacing $\canparamat{\catenumerator}$ by $-\canparamat{\catenumerator}$.



\subsubsection{Boltzmann machines as MLNs}

%\red{Add sufficient statistics?}

A Boltzmann machine is a member of an exponential family with the energy tensor
	\[ \energytensorofat{W,b}{\indexedcatvariables} = 
	\sum_{\atomenumerator,\secatomenumerator \in [\atomorder]} 
		W[\selvariableof{\vselectionsymbol,0}=\atomenumerator, \selvariableof{\vselectionsymbol,1}=\secatomenumerator] \catindexof{\atomenumerator} \catindexof{\secatomenumerator} 
	+ \sum_{\atomenumerator,\secatomenumerator \in [\atomorder]} b[\selvariableof{\vselectionsymbol,0}=\atomenumerator] \, . \]


%sufficient statistic 
%	\[ \sstat : \atomstates \rightarrow (\rr^{\catorder}\otimes \rr^{\catorder}) \times \rr^{\catorder} \]
%by interaction term
%	\[ \sstat(\shortcatindices) = (\catindexof{\atomenumerator} \Leftrightarrow \catindexof{\secatomenumerator})_{\atomenumerator,\secatomenumerator \in[\atomorder]} \]
%and by potential term
%	\[ \sstat(\shortcatindices) =  (\catvariableof{\atomenumerator})_{\atomenumeratorin} \, . \]

We notice, that this coincides with the energy tensor of a Markov Logic Network with formula set 
	\[ \formulaset = \{ \catvariableof{\atomenumerator} \Leftrightarrow \catvariableof{\secatomenumerator} \, : \, \atomenumerator,\secatomenumerator \in[\atomorder] \} 
	\cup \{ \catvariableof{\atomenumerator}\, : \, \atomenumeratorin \} \, \]
with cardinality $\atomorder^2+\atomorder$.

Each formula is in the expressivity of an architecture consisting of a single binary logical neuron selecting any variable of $\shortcatvariables$ in each argument and selecting connectives $\{\eqbincon,\lpasbincon\}$, where by $\lpasbincon$ we refer to a connective passing the first argument, defined for $\catindexofin{0}, \catindexofin{1}$ as 
	\[ \lpasbincon[\indexedcatvariableof{0},\indexedcatvariableof{1}] = \vselectionmapat{\indexedcatvariableof{0},\catvariableof{1},\selvariableof{\vselectionsymbol}=0} \, . \]

The weight is
	\[ \canparam 
	= \onehotmapofat{0}{\selvariableof{\cselectionsymbol}} \otimes W 
	+ \onehotmapofat{1}{\selvariableof{\cselectionsymbol}} \otimes b[\selvariableof{\vselectionsymbol,0}] \otimes  \onehotmapofat{0}{\selvariableof{\vselectionsymbol,0}} 
	\]
	
And we have
	\[ \energytensorofat{W,b}{\shortcatvariables} = 
	\sbcontractionof{\fsnnat{\shortcatvariables,\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}}, \canparamat{\selvariableof{\cselectionsymbol},\selvariableof{\vselectionsymbol,0},\selvariableof{\vselectionsymbol,1}}}{\shortcatvariables} \, . \]


\begin{figure}[h]
\begin{center}
	\input{PartII/tikz_pics/mln/boltzmann_energy.tex}
\end{center}
\caption{Tensor network representation of the energy of a Boltzmann machine}
\label{fig:boltzmannEnergy}
\end{figure}


%where by $(\cdot,\cdot)|_{0}$
%To connect with the formalism of Boltzmann machines, let us identify the visible units of a Boltzmann machines with the atoms in a propositional theory.

%Boltzmann machines are then reproduced by taking $\atomorder^2+\atomorder$ many formulas, namely those measuring the correlations and the marginal distributions.
%To be more precise, the correlation between atom $\atomicformulaof{\atomenumerator}$ and $\atomicformulaof{\secatomenumerator}$ is measured by the satisfaction rate of the formula 
%	\[ \exformula_{\atomenumerator,\secatomenumerator} = \atomicformulaof{\atomenumerator} \leftrightarrow \atomicformulaof{\secatomenumerator}\]

%\begin{theorem}
%	Any Boltzmann machine over $\atomorder$ units with interaction matrix $U\in\rr^{\atomorder\times\atomorder}$ and potential term $b\in\rr^{\atomorder}$ (MacKay Book notation) is a MLN where the only nonzero weights are 
%		\[ \weightof{\atomicformulaof{\atomenumerator} } = b_{\atomenumerator} \quad, \quad \atomenumeratorin \]
%	and 
%		\[ \weightof{ \exformula_{\atomenumerator,\secatomenumerator} } = U_{\atomenumerator, \secatomenumerator} \quad , \quad \atomenumerator,\secatomenumerator \in [\atomorder]\] 
%\end{theorem}

\red{
Often Boltzmann machines are formulated with hidden variables.
To average those out, one needs to instantiate the probability distribution instead of the energy tensor and leave only visible variables open in a contraction.
}


Markov Logic Networks go beyond the Boltzmann machines already for binary formulas, by the flexibility to capture further dependencies beyond the correlation.
We can use any binary logical connective and have an associated formula where we can put a weight on.



%\begin{remark}[Hopfield networks]
%	Also interesting for MLNs is a Hopfield perspective.
%	Having an initialization the update can be interpreted as a Gibbs sampling step at temperature $0$ (since deterministic update).
%\end{remark}


%\begin{remark}[Representation by Formula Selecting Networks]
%	When choosing an architecture with a single neurons selecting on both arguments any atom and having the logical biconditional as fixed connective.
%	The sufficient statistics of the empirical distribution is the correlation matrix between the atoms.
%\end{remark}

%
%\subsubsection{MLNs as higher-order deep Boltzmann Machines}
%
%\red{Unclear whether this makes sense: Hidden nodes depend deterministically on the observed!}
%
%We understand 
%\begin{itemize}
%	\item Atoms as observed nodes (might be redundant, since they are formulas themselves)
%	\item Formulas (including the atomic formulas) as hidden nodes
%\end{itemize}
%
%Non-atomic formulas have a hard coded deterministic dependency on the atomic formulas.
%
%The MLN distribution is given by associating a potential to some hidden nodes.








\subsection{Applications}

Markov Logic Networks as neuro-symbolic architectures:
\begin{itemize}
	\item Neural Paradigm here by decompositions of logical formulas into their connectives.
		In more generality by decompositions of sufficient statistics into composed functions, using Basis Calculus.
		Deeper nodes as carrying correlations of lower nodes.
	\item Symbolic Paradigm by interpretability of propositional logics.
\end{itemize}


Markov Logic Networks as trainable Machine Learning models:
\begin{itemize}
	\item Expressivity: Can represent any distributions, as shown by Theorem~\ref{the:maximalClausesRepresentation}, with $2^d$ formulas.
	\item Efficiency: Can only handle small subsets of possible formulas, since their possible number is huge.
		Tensor networks provide means to efficiently represent formulas depending on many variables and reason based on contractions.
	\item Differentiability: Distributions are differentiable functions of their weights, see Parameter Estimation Chapter. 
		The log-likelihood of data is therefore also differentiable function of their weights and we can exploit first-order methods in their optimization.
	\item Structure Learning: We need to find differentiable parametrizations of logical formulas respecting a chosen architecture.
		In Chapter~\ref{cha:formulaBatches} such representations are described based on Selector Tensor Networks.
\end{itemize}





%\subsection{Symbolic Paradigm by Logics}
%Logical formulas (which are maps from $\atomstates$ to $[2]$ as explained before) correspond with a function to be represented by a neural network.

%\subsubsection{Neural Paradigm by Formula Decompositions}

%% Decompositions
%In logics, formula can be decomposed into logical connectives acting on smaller formulas as has been shown in Chapter~\ref{cha:FormulaTensors}.






