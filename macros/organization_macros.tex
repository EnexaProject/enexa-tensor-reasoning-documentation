% Text Macros
\newcommand{\python}{$\mathrm{python}$ }
\newcommand{\tnreason}{$\mathrm{tnreason}$ }

\newcommand{\spengine}{$\mathrm{engine}$ }
\newcommand{\sprepresentation}{$\mathrm{representation}$ }
\newcommand{\spreasoning}{$\mathrm{reasoning}$ }
\newcommand{\spapplication}{$\mathrm{application}$ }

\newcommand{\layeronespec}{\textbf{Layer 1}: Storage and manipulations}
\newcommand{\layertwospec}{\textbf{Layer 2}: Specification of workload}
\newcommand{\layerthreespec}{\textbf{Layer 3}: Applications in reasoning}

% Current tnreason version
\newcommand{\curvertnreason}{2.0.0}

% Report Chapters
\newcommand{\partonetext}{Foundations} % was Classical Approaches
\newcommand{\chatextprobRepresentation}{Probability Distributions}
\newcommand{\chatextprobReasoning}{Probabilistic Inference}
\newcommand{\chatextlogicalRepresentation}{Propositional Logic}
\newcommand{\chatextlogicalReasoning}{Logical Inference}

\newcommand{\parttwotext}{Hybrid Logic Networks} % was Neuro-Symbolic Applications
\newcommand{\chatextformulaSelection}{Formula Selecting Networks}
\newcommand{\chatextnetworkRepresentation}{Hybrid Logic Representation}
\newcommand{\chatextnetworkReasoning}{Hybrid Logic Inference}
\newcommand{\chatextconcentration}{Probabilistic Guarantees}
\newcommand{\chatextfolModels}{First Order Logic}

\newcommand{\partthreetext}{Contraction Calculus}
\newcommand{\chatextcoordinateCalculus}{Coordinate Calculus}
\newcommand{\chatextbasisCalculus}{Basis Calculus}
\newcommand{\chatextsparseCalculus}{Sparse Representation}
\newcommand{\chatextapproximation}{Tensor Approximation}
\newcommand{\chatextmessagePassing}{Message Passing}

\newcommand{\focusonespec}{Focus~I: Representation}
\newcommand{\focustwospec}{Foucs~II: Reasoning}

% Sections in notation chapter and subsection in implementation.notation chapter, bn for basic notation
\newcommand{\bncategoricals}{Categorical Variables and Representations}
\newcommand{\bntensors}{Tensors}
\newcommand{\bncontractions}{Contractions}
\newcommand{\bnencoding}{Function encoding schemes}

\newcommand{\defref}[1]{Def.~\ref{#1}}
\newcommand{\theref}[1]{The.~\ref{#1}}
\newcommand{\lemref}[1]{Lem.~\ref{#1}}
\newcommand{\corref}[1]{Cor.~\ref{#1}}
\newcommand{\algoref}[1]{Algorithm~\ref{#1}}
\newcommand{\probref}[1]{Problem~\eqref{#1}}
\newcommand{\exaref}[1]{Example~\ref{#1}}
\newcommand{\parref}[1]{Part~\ref{#1}}
\newcommand{\charef}[1]{Chapter~\ref{#1}}
\newcommand{\secref}[1]{Sect.~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\assref}[1]{Assumption~\ref{#1}}
\newcommand{\remref}[1]{Remark~\ref{#1}}


%% Key Concepts of Part I

% Decomposition mechanisms
\newcommand{\independenceMechanism}{independence mechanism}
\newcommand{\IndependenceMechanism}{Independence mechanism}
\newcommand{\computationMechanism}{computation mechanism}
\newcommand{\ComputationMechanism}{Computation mechanism}

%% Key Concepts of Part II

% Networks
\newcommand{\MarkovLogicNetwork}{Markov Logic Network}
\newcommand{\HardLogicNetwork}{Hard Logic Network}
\newcommand{\HybridLogicNetwork}{Hybrid Logic Network}
\newcommand{\MarkovLogicNetworks}{Markov Logic Networks}
\newcommand{\HardLogicNetworks}{Hard Logic Networks}
\newcommand{\HybridLogicNetworks}{Hybrid Logic Networks}

% Sparsity
\newcommand{\decompositionSparsity}{decomposition sparsity}
\newcommand{\DecompositionSparsity}{Decomposition sparsity}
\newcommand{\selectionSparsity}{selection sparsity}
\newcommand{\SelectionSparsity}{Selection sparsity}
\newcommand{\polynomialSparsity}{polynomial sparsity}
\newcommand{\PolynomialSparsity}{Polynomial sparsity}

% Tensor Structure
\newcommand{\substitionStructure}{substitution structure}
\newcommand{\SubstitutionStructure}{Substitution structure}
\newcommand{\worldStructure}{world structure}
\newcommand{\WorldStructure}{World structure}



% Part Intro Texts (unused in scrbook_tnreason)
\newcommand{\partoneintrotext}{
    The computational automation of reasoning is rooted both in the probabilistic and the logical reasoning tradition.
    Both draw on the same ontological commitment that systems have a factored representation, that is their states are described by assignments to a set of variables.
    Based on this commitment both approaches bear a natural tensor representation of their states and a formalism of the respective reasoning algorithms based on multilinear methods.
}

\newcommand{\parttwointrotext}{
    We now employ tensor networks to define architectures and algorithms for neuro-symbolic reasoning based on the logical and probabilistic foundations.
    Markov Logic Networks will be taken as generative models to be learned from data, using formula selecting tensor networks and likelihood optimization algorithms.
}

\newcommand{\partthreeintrotext}{
    Based on the logical interpretation we often handle tensor calculus with specific tensors.
    Often, they are boolean (that is their coordinates are in $\{0,1\}$ corresponding with a Boolean), and sparse (that is having a decomposition with less storage demand).
    We investigate it in this part in more depth the properties of such tensors, which where exploited in the previous parts.
}