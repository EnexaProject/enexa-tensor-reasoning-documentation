% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.0 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{abadi_tensorflow_2016}{misc}{}
      \name{author}{40}{}{%
        {{hash=9a04ae935da573a7aa9c83afdf2fd845}{%
           family={Abadi},
           familyi={A\bibinitperiod},
           given={Martín},
           giveni={M\bibinitperiod}}}%
        {{hash=37e1df772f97a2e492755e2006294387}{%
           family={Agarwal},
           familyi={A\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=08cf514c08137f94d73b590060797f5b}{%
           family={Barham},
           familyi={B\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=a79c0c4b084471b2c8e0e9a38b4cb2b7}{%
           family={Brevdo},
           familyi={B\bibinitperiod},
           given={Eugene},
           giveni={E\bibinitperiod}}}%
        {{hash=c0d10aaf985cebf8d0497e1828f9313f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Zhifeng},
           giveni={Z\bibinitperiod}}}%
        {{hash=6f712ddfd730736ee54fdc87a4abf7a2}{%
           family={Citro},
           familyi={C\bibinitperiod},
           given={Craig},
           giveni={C\bibinitperiod}}}%
        {{hash=84d9f354fa0b45dae996f27dad2c6607}{%
           family={Corrado},
           familyi={C\bibinitperiod},
           given={Greg\bibnamedelima S.},
           giveni={G\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=a4c859221cc41e2db6d63b486f955ba2}{%
           family={Davis},
           familyi={D\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
        {{hash=bd7f88635b51bc5abb3d5a2cab967724}{%
           family={Devin},
           familyi={D\bibinitperiod},
           given={Matthieu},
           giveni={M\bibinitperiod}}}%
        {{hash=193bcec5240237591ad8fb697869f013}{%
           family={Ghemawat},
           familyi={G\bibinitperiod},
           given={Sanjay},
           giveni={S\bibinitperiod}}}%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=a9385c3940c7e3dc9dfad01621d190d3}{%
           family={Harp},
           familyi={H\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=edc6964c6549e1839ca94d3d61e03f76}{%
           family={Irving},
           familyi={I\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=3b62e7eedeec9f7a7f7f3c3ec88637f1}{%
           family={Isard},
           familyi={I\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=9fce03efe6b3331a1b93ed2e7c0da9d5}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Yangqing},
           giveni={Y\bibinitperiod}}}%
        {{hash=b243d8e26a729c55dd5af5f97763dded}{%
           family={Jozefowicz},
           familyi={J\bibinitperiod},
           given={Rafal},
           giveni={R\bibinitperiod}}}%
        {{hash=f2bc899b1160163417da7bf510f15d33}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=73fc2bbbbbe06838174fc0490ee66f67}{%
           family={Kudlur},
           familyi={K\bibinitperiod},
           given={Manjunath},
           giveni={M\bibinitperiod}}}%
        {{hash=a1a07ba9db1e853a9dd9623d81e8f26a}{%
           family={Levenberg},
           familyi={L\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod}}}%
        {{hash=55eafec933a6ef975eb0229394df31c0}{%
           family={Mane},
           familyi={M\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=f34904ef0a6e0864175e4c446f9fcf14}{%
           family={Monga},
           familyi={M\bibinitperiod},
           given={Rajat},
           giveni={R\bibinitperiod}}}%
        {{hash=1f07f887fdbaea3ca8281745735256d1}{%
           family={Moore},
           familyi={M\bibinitperiod},
           given={Sherry},
           giveni={S\bibinitperiod}}}%
        {{hash=d83fd9fc0c2a25f04b7525cfe4ac3ab8}{%
           family={Murray},
           familyi={M\bibinitperiod},
           given={Derek},
           giveni={D\bibinitperiod}}}%
        {{hash=78be47e08348476a254b1217fd9041ca}{%
           family={Olah},
           familyi={O\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=29ea22df63f174ac629e9ef100b40484}{%
           family={Schuster},
           familyi={S\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod}}}%
        {{hash=8f128e70084608a2c29c497ebd794f87}{%
           family={Shlens},
           familyi={S\bibinitperiod},
           given={Jonathon},
           giveni={J\bibinitperiod}}}%
        {{hash=0a0b028c6b85c46f368317d0c5bfe3a0}{%
           family={Steiner},
           familyi={S\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=1754a40f9d600dea756c1dd1047ce170}{%
           family={Talwar},
           familyi={T\bibinitperiod},
           given={Kunal},
           giveni={K\bibinitperiod}}}%
        {{hash=c7360a7ebeba8420b8c7e0eeb0cb81ac}{%
           family={Tucker},
           familyi={T\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=8051922e7bd286f884bfbd1023ef62f5}{%
           family={Vanhoucke},
           familyi={V\bibinitperiod},
           given={Vincent},
           giveni={V\bibinitperiod}}}%
        {{hash=bab2011f2b9e7af47ab45308470b9faf}{%
           family={Vasudevan},
           familyi={V\bibinitperiod},
           given={Vijay},
           giveni={V\bibinitperiod}}}%
        {{hash=d9ad2ba70a84203f22b0dab9080eedfc}{%
           family={Viegas},
           familyi={V\bibinitperiod},
           given={Fernanda},
           giveni={F\bibinitperiod}}}%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=cfc28a19549f0d8ff117cf79be90d213}{%
           family={Warden},
           familyi={W\bibinitperiod},
           given={Pete},
           giveni={P\bibinitperiod}}}%
        {{hash=c66133f96104e3c0dcd2f73e11469ab0}{%
           family={Wattenberg},
           familyi={W\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=8f71c897f89022a26511a6fbbd6108b6}{%
           family={Wicke},
           familyi={W\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=b8e2e9f67019f464cefe5db1c822ffd9}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Yuan},
           giveni={Y\bibinitperiod}}}%
        {{hash=fdfa73146952ceef0abc202cc397781c}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Xiaoqiang},
           giveni={X\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{fullhash}{1cd4d6db8cba66ede8ce2b69466158cd}
      \strng{bibnamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authorbibnamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authornamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authorfullhash}{1cd4d6db8cba66ede8ce2b69466158cd}
      \field{labelalpha}{Aba+16}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.}
      \field{month}{3}
      \field{note}{arXiv:1603.04467 [cs]}
      \field{shorttitle}{{TensorFlow}}
      \field{title}{{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2016}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1603.04467
      \endverb
      \verb{file}
      \verb Preprint PDF:/Users/alexgoessmann/Zotero/storage/6IRK2ZQ5/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/NVW9HV3T/1603.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1603.04467
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1603.04467
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Distributed,Parallel,and Cluster Computing}
    \endentry
    \entry{agerbeck_multi-agent_2008}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=fd231f4f0ce8f802061321a92838762f}{%
           family={Agerbeck},
           familyi={A\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=23895d691ce6553213b9c345c8d2abb2}{%
           family={Hansen},
           familyi={H\bibinitperiod},
           given={Mikael},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{fullhash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{bibnamehash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{authorbibnamehash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{authornamehash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{authorfullhash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \field{labelalpha}{AH08}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This master's project concerns the use of multi-agent design principles in making efficient solvers for NP-complete problems. The design of computer programs as multi-agent systems presents a new and very promising software engineering paradigm, where systems are described as individual problem-solving agents pursuing high-level goals. Recently, researchers have started to apply the multi-agent paradigm to the construction of efficient solvers for NP-complete problems. This has resulted in very effective tools for routing problems, graph partitioning and SAT-solving. The objective of the present project is to make further studies into the application of multi-agent principles to solving NP-complete problems. More specifically, the project has the following two goals. First, it should result in a general discussion of the use of multi-agent approaches to solving NP-complete problems. This should include a discussion of strengths and weaknesses compared to other approaches of solving the same problems. Second, it should result in a concrete software tool for solving n2 £ n2 Sudoku puzzles, which is known to be an NP-complete problem. The tool should be benchmarked against other solvers for Sudoku.}
      \field{title}{A {Multi}-{Agent} {Approach} to {Solving} {NP}-{Complete} {Problems}}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2008}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/8LVX8CKC/Agerbeck und Hansen - 2008 - A Multi-Agent Approach to Solving NP-Complete Problems.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.semanticscholar.org/paper/A-Multi-Agent-Approach-to-Solving-NP-Complete-Agerbeck-Hansen/3762bf7893da14839e06ae000b9e04d63dac8af4
      \endverb
      \verb{url}
      \verb https://www.semanticscholar.org/paper/A-Multi-Agent-Approach-to-Solving-NP-Complete-Agerbeck-Hansen/3762bf7893da14839e06ae000b9e04d63dac8af4
      \endverb
    \endentry
    \entry{antoniou_semantic_2012}{book}{}
      \name{author}{4}{}{%
        {{hash=3bedb033d421d9b43a2ec240debf37a0}{%
           family={Antoniou},
           familyi={A\bibinitperiod},
           given={Grigoris},
           giveni={G\bibinitperiod}}}%
        {{hash=b2d7e1f159093aa2ea8379d9b6bc6911}{%
           family={Groth},
           familyi={G\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=fb842925937c8dd94b68589e01b84f96}{%
           family={Harmelen},
           familyi={H\bibinitperiod},
           given={Frank\bibnamedelima Van},
           giveni={F\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=c322c048f090ff4f07130fd85d1114c2}{%
           family={Hoekstra},
           familyi={H\bibinitperiod},
           given={Rinke},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge (Mass.)}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{4f98eeb533ab51a9dcdb5f47cede0903}
      \strng{fullhash}{d08305c8c63d28e8a6ae55e94f94f0db}
      \strng{bibnamehash}{4f98eeb533ab51a9dcdb5f47cede0903}
      \strng{authorbibnamehash}{4f98eeb533ab51a9dcdb5f47cede0903}
      \strng{authornamehash}{4f98eeb533ab51a9dcdb5f47cede0903}
      \strng{authorfullhash}{d08305c8c63d28e8a6ae55e94f94f0db}
      \field{labelalpha}{Ant+12}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A new edition of the widely used guide to the key ideas, languages, and technologies of the Semantic WebThe development of the Semantic Web, with machine-readable content, has the potential to revolutionize the World Wide Web and its uses. A Semantic Web Primer provides an introduction and guide to this continuously evolving field, describing its key ideas, languages, and technologies. Suitable for use as a textbook or for independent study by professionals, it concentrates on undergraduate-level fundamental concepts and techniques that will enable readers to proceed with building applications on their own and includes exercises, project descriptions, and annotated references to relevant online materials.The third edition of this widely used text has been thoroughly updated, with significant new material that reflects a rapidly developing field. Treatment of the different languages (OWL2, rules) expands the coverage of RDF and OWL, defining the data model independently of XML and including coverage of N3/Turtle and RDFa. A chapter is devoted to OWL2, the new W3C standard. This edition also features additional coverage of the query language SPARQL, the rule language RIF and the possibility of interaction between rules and ontology languages and applications. The chapter on Semantic Web applications reflects the rapid developments of the past few years. A new chapter offers ideas for term projects. Additional material, including updates on the technological trends and research directions, can be found at http://www.semanticwebprimer.org.}
      \field{edition}{third edition}
      \field{isbn}{978-0-262-01828-9}
      \field{month}{8}
      \field{title}{A {Semantic} {Web} {Primer}, third edition}
      \field{year}{2012}
    \endentry
    \entry{avila_garcez_connectionist_1999}{article}{}
      \name{author}{2}{}{%
        {{hash=4d2b98eb7de13916ec42552ec5c259ee}{%
           family={Avila\bibnamedelima Garcez},
           familyi={A\bibinitperiod\bibinitdelim G\bibinitperiod},
           given={Artur\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=9affecbfe51d0fd75ae0999379cdb855}{%
           family={Zaverucha},
           familyi={Z\bibinitperiod},
           given={Gerson},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{fullhash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{bibnamehash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{authorbibnamehash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{authornamehash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{authorfullhash}{8175ba9d84db9fd244eea4049301b35d}
      \field{labelalpha}{AZ99}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents the Connectionist Inductive Learning and Logic Programming System (C-IL2P). C-IL2P is a new massively parallel computational model based on a feedforward Artificial Neural Network that integrates inductive learning from examples and background knowledge, with deductive learning from Logic Programming. Starting with the background knowledge represented by a propositional logic program, a translation algorithm is applied generating a neural network that can be trained with examples. The results obtained with this refined network can be explained by extracting a revised logic program from it. Moreover, the neural network computes the stable model of the logic program inserted in it as background knowledge, or learned with the examples, thus functioning as a parallel system for Logic Programming. We have successfully applied C-IL2P to two real-world problems of computational biology, specifically DNA sequence analyses. Comparisons with the results obtained by some of the main neural, symbolic, and hybrid inductive learning systems, using the same domain knowledge, show the effectiveness of C-IL2P.}
      \field{issn}{1573-7497}
      \field{journaltitle}{Applied Intelligence}
      \field{month}{7}
      \field{number}{1}
      \field{title}{The {Connectionist} {Inductive} {Learning} and {Logic} {Programming} {System}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{11}
      \field{year}{1999}
      \field{urldateera}{ce}
      \field{pages}{59\bibrangedash 77}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1023/A:1008328630915
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1023/A:1008328630915
      \endverb
      \verb{url}
      \verb https://doi.org/10.1023/A:1008328630915
      \endverb
      \keyw{machine learning,Artificial Intelligence,artificial neural networks,computational biology,logic programming,theory refinement}
    \endentry
    \entry{badreddine_logic_2022}{article}{}
      \name{author}{4}{}{%
        {{hash=d73c12aa21b94170ac4aeb2a43b7fdf9}{%
           family={Badreddine},
           familyi={B\bibinitperiod},
           given={Samy},
           giveni={S\bibinitperiod}}}%
        {{hash=70658466246873155f5b9b3a737cdd4e}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur},
           giveni={A\bibinitperiod},
           prefix={d'Avila},
           prefixi={d\bibinitperiod}}}%
        {{hash=cfd7b672fd7926ef1136b9790438416d}{%
           family={Serafini},
           familyi={S\bibinitperiod},
           given={Luciano},
           giveni={L\bibinitperiod}}}%
        {{hash=fecf9d2471f6f2bf1fe02253df553444}{%
           family={Spranger},
           familyi={S\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{bffa1c78d0f905872f09e3bba1099174}
      \strng{fullhash}{c7bbeb9ca1be0100378ace27e57e379b}
      \strng{bibnamehash}{bffa1c78d0f905872f09e3bba1099174}
      \strng{authorbibnamehash}{bffa1c78d0f905872f09e3bba1099174}
      \strng{authornamehash}{bffa1c78d0f905872f09e3bba1099174}
      \strng{authorfullhash}{c7bbeb9ca1be0100378ace27e57e379b}
      \field{labelalpha}{Bad+22}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{2}
      \field{title}{Logic {Tensor} {Networks}}
      \field{urlday}{20}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{303}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{103649}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.artint.2021.103649
      \endverb
      \verb{file}
      \verb Akzeptierte Version:/Users/alexgoessmann/Zotero/storage/XKSPFEM9/Badreddine et al. - 2022 - Logic Tensor Networks.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/MB3PW4KU/S0004370221002009.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370221002009
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370221002009
      \endverb
      \keyw{Deep learning and reasoning,Many-valued logics,Neurosymbolic AI}
    \endentry
    \entry{balazevic_tucker_2019}{article}{}
      \name{author}{3}{}{%
        {{hash=c3a3cd634299d92769a1b0770f3a581d}{%
           family={Balazevic},
           familyi={B\bibinitperiod},
           given={Ivana},
           giveni={I\bibinitperiod}}}%
        {{hash=bfb3fcac1af14d0fb934c4fee4c0ea05}{%
           family={Allen},
           familyi={A\bibinitperiod},
           given={Carl},
           giveni={C\bibinitperiod}}}%
        {{hash=5c63f1e06139e794f16b23bff79f44d0}{%
           family={Hospedales},
           familyi={H\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{fullhash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{bibnamehash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{authorbibnamehash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{authornamehash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{authorfullhash}{54766c326fbb4521dec81a9d42b03b6a}
      \field{labelalpha}{BAH19}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively straightforward but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms previous state-of-the-art models across standard link prediction datasets, acting as a strong baseline for more elaborate models. We show that TuckER is a fully expressive model, derive sufficient bounds on its embedding dimensionalities and demonstrate that several previously introduced linear models can be viewed as special cases of TuckER.}
      \field{journaltitle}{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}
      \field{note}{Conference Name: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) Place: Hong Kong, China Publisher: Association for Computational Linguistics}
      \field{shorttitle}{{TuckER}}
      \field{title}{{TuckER}: {Tensor} {Factorization} for {Knowledge} {Graph} {Completion}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{5184\bibrangedash 5193}
      \range{pages}{10}
      \verb{doi}
      \verb 10.18653/v1/D19-1522
      \endverb
      \verb{file}
      \verb Akzeptierte Version:/Users/alexgoessmann/Zotero/storage/FJRTWBDV/Balazevic et al. - 2019 - TuckER Tensor Factorization for Knowledge Graph Completion.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.aclweb.org/anthology/D19-1522
      \endverb
      \verb{url}
      \verb https://www.aclweb.org/anthology/D19-1522
      \endverb
    \endentry
    \entry{barabasi_network_2016}{book}{}
      \name{author}{1}{}{%
        {{hash=15bacd7ddfc866c5dd96976d4b0244d5}{%
           family={Barabási},
           familyi={B\bibinitperiod},
           given={Albert-László},
           giveni={A\bibinithyphendelim L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{fullhash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{bibnamehash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{authorbibnamehash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{authornamehash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{authorfullhash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \field{labelalpha}{Bar16}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Networks are everywhere, from the internet, to social networks, and the genetic networks that determine our biological existence. Illustrated throughout in full colour, this pioneering textbook, spanning a wide range of topics from physics to computer science, engineering, economics and the social sciences, introduces network science to an interdisciplinary audience. From the origins of the six degrees of separation to explaining why networks are robust to random failures, the author explores how viruses like Ebola and H1N1 spread, and why it is that our friends have more friends than we do. Using numerous real-world examples, this innovatively designed text includes clear delineation between undergraduate and graduate level material. The mathematical formulas and derivations are included within Advanced Topics sections, enabling use at a range of levels. Extensive online resources, including films and software for network analysis, make this a multifaceted companion for anyone with an interest in network science.}
      \field{edition}{Illustrated edition}
      \field{isbn}{978-1-107-07626-6}
      \field{month}{7}
      \field{title}{Network {Science}}
      \field{year}{2016}
    \endentry
    \entry{bellman_adaptive_1961}{book}{}
      \name{author}{1}{}{%
        {{hash=2b0e24f47216c32d9ca0eb6f643646da}{%
           family={Bellman},
           familyi={B\bibinitperiod},
           given={Richard\bibnamedelima E.},
           giveni={R\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {New Jersey}%
      }
      \list{publisher}{1}{%
        {Princeton University Press}%
      }
      \strng{namehash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{fullhash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{bibnamehash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{authorbibnamehash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{authornamehash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{authorfullhash}{2b0e24f47216c32d9ca0eb6f643646da}
      \field{labelalpha}{Bel61}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The aim of this work is to present a unified approach to the modern field of control theory and to provide a technique for making problems involving deterministic, stochastic, and adaptive processes of both linear and nonlinear type amenable to machine solution. Mr. Bellman has used the theory of dynamic programming to formulate, analyze, and prepare these processes for numerical treatment by digital computers. The unique concept of the book is that of a single problem stretching from recognition and formulation to analytic treatment and computational solution. Due to the emphasis upon ideas and concepts, this book is equally suited for the pure and applied mathematician, and for control engineers in all fields. Originally published in 1961. The Princeton Legacy Library uses the latest print-on-demand technology to again make available previously out-of-print books from the distinguished backlist of Princeton University Press. These editions preserve the original texts of these important books while presenting them in durable paperback and hardcover editions. The goal of the Princeton Legacy Library is to vastly increase access to the rich scholarly heritage found in the thousands of books published by Princeton University Press since its founding in 1905.}
      \field{isbn}{978-1-4008-7466-8}
      \field{note}{Publication Title: Adaptive Control Processes}
      \field{title}{Adaptive {Control} {Processes}}
      \field{urlday}{7}
      \field{urlmonth}{2}
      \field{urlyear}{2021}
      \field{year}{1961}
      \field{urldateera}{ce}
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/JY2L2MIB/html.html:text/html
      \endverb
    \endentry
    \entry{beylkin_algorithms_2005}{article}{}
      \name{author}{2}{}{%
        {{hash=4c320df8c1884cdf0cf4e2c5fd74cee9}{%
           family={Beylkin},
           familyi={B\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=90f304b60f077049361bf9ba18c8fa7a}{%
           family={Mohlenkamp},
           familyi={M\bibinitperiod},
           given={Martin\bibnamedelima J.},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{fullhash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{bibnamehash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{authorbibnamehash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{authornamehash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{authorfullhash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \field{labelalpha}{BM05}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Nearly every numerical analysis algorithm has computational complexity that scales exponentially in the underlying physical dimension. The separated representation, introduced previously, allows many operations to be performed with scaling that is formally linear in the dimension. In this paper we further develop this representation by (i) discussing the variety of mechanisms that allow it to be surprisingly eﬃcient; (ii) addressing the issue of conditioning; (iii) presenting algorithms for solving linear systems within this framework; and (iv) demonstrating methods for dealing with antisymmetric functions, as arise in the multiparticle Schr¨odinger equation in quantum mechanics. Numerical examples are given.}
      \field{issn}{1064-8275, 1095-7197}
      \field{journaltitle}{SIAM Journal on Scientific Computing}
      \field{month}{1}
      \field{number}{6}
      \field{title}{Algorithms for {Numerical} {Analysis} in {High} {Dimensions}}
      \field{urlday}{11}
      \field{urlmonth}{12}
      \field{urlyear}{2019}
      \field{volume}{26}
      \field{year}{2005}
      \field{urldateera}{ce}
      \field{pages}{2133\bibrangedash 2159}
      \range{pages}{27}
      \verb{doi}
      \verb 10.1137/040604959
      \endverb
      \verb{file}
      \verb Beylkin und Mohlenkamp - 2005 - Algorithms for Numerical Analysis in High Dimensio.pdf:/Users/alexgoessmann/Zotero/storage/FJLW6WC5/Beylkin und Mohlenkamp - 2005 - Algorithms for Numerical Analysis in High Dimensio.pdf:application/pdf
      \endverb
    \endentry
    \entry{pan_tentris_2020}{incollection}{}
      \name{author}{6}{}{%
        {{hash=c32b41e85d5e8a64725b07a7910fcfab}{%
           family={Bigerl},
           familyi={B\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=7c5b22dd6d774288712bf3e9ab7cd18c}{%
           family={Conrads},
           familyi={C\bibinitperiod},
           given={Felix},
           giveni={F\bibinitperiod}}}%
        {{hash=8ff8bf39b7836be59f71591a851c00dd}{%
           family={Behning},
           familyi={B\bibinitperiod},
           given={Charlotte},
           giveni={C\bibinitperiod}}}%
        {{hash=a753a4fff61dd522108d75601d2d4252}{%
           family={Sherif},
           familyi={S\bibinitperiod},
           given={Mohamed\bibnamedelima Ahmed},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=d2274fbe986995b8ac0dabe2949b6c6f}{%
           family={Saleem},
           familyi={S\bibinitperiod},
           given={Muhammad},
           giveni={M\bibinitperiod}}}%
        {{hash=aaffae6ab501c5d30bd5ba943863c330}{%
           family={Ngonga\bibnamedelima Ngomo},
           familyi={N\bibinitperiod\bibinitdelim N\bibinitperiod},
           given={Axel-Cyrille},
           giveni={A\bibinithyphendelim C\bibinitperiod}}}%
      }
      \name{editor}{8}{}{%
        {{hash=278c599f0a2e44df256b1d60ef313ea9}{%
           family={Pan},
           familyi={P\bibinitperiod},
           given={Jeff\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=4fb77a4bd449d40757fa6f195ad238d4}{%
           family={Tamma},
           familyi={T\bibinitperiod},
           given={Valentina},
           giveni={V\bibinitperiod}}}%
        {{hash=3eb1335a7c6a4acf8ca2e632ae7f6a09}{%
           family={d’Amato},
           familyi={d\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod}}}%
        {{hash=f8032db780b653246bf029dcbffb6859}{%
           family={Janowicz},
           familyi={J\bibinitperiod},
           given={Krzysztof},
           giveni={K\bibinitperiod}}}%
        {{hash=a37941fd1ccfe1e67aede331acaa7a60}{%
           family={Fu},
           familyi={F\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=a2be0ec850b04b1f5e9fe3aca6bf5226}{%
           family={Polleres},
           familyi={P\bibinitperiod},
           given={Axel},
           giveni={A\bibinitperiod}}}%
        {{hash=9c134e61f193ba681c90bf21d73b5bd2}{%
           family={Seneviratne},
           familyi={S\bibinitperiod},
           given={Oshani},
           giveni={O\bibinitperiod}}}%
        {{hash=6cab81d607599e0adc135714724fa896}{%
           family={Kagal},
           familyi={K\bibinitperiod},
           given={Lalana},
           giveni={L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{f7dbbdc6ba16ea0b42fba7e139bea313}
      \strng{fullhash}{15b70d3e0224d899d7380f6ff7c1536b}
      \strng{bibnamehash}{f7dbbdc6ba16ea0b42fba7e139bea313}
      \strng{authorbibnamehash}{f7dbbdc6ba16ea0b42fba7e139bea313}
      \strng{authornamehash}{f7dbbdc6ba16ea0b42fba7e139bea313}
      \strng{authorfullhash}{15b70d3e0224d899d7380f6ff7c1536b}
      \strng{editorbibnamehash}{5ac4286d40d30d03998e458efb80853e}
      \strng{editornamehash}{5ac4286d40d30d03998e458efb80853e}
      \strng{editorfullhash}{181ca458bd90b841509e6207ad220276}
      \field{labelalpha}{Big+20}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{The {Semantic} {Web} – {ISWC} 2020}
      \field{isbn}{978-3-030-62418-7 978-3-030-62419-4}
      \field{note}{Series Title: Lecture Notes in Computer Science}
      \field{title}{Tentris – {A} {Tensor}-{Based} {Triple} {Store}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{12506}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{56\bibrangedash 73}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/978-3-030-62419-4_4
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/10.1007/978-3-030-62419-4_4
      \endverb
      \verb{url}
      \verb https://link.springer.com/10.1007/978-3-030-62419-4_4
      \endverb
    \endentry
    \entry{casazza_introduction_2013}{incollection}{}
      \name{author}{3}{}{%
        {{hash=410d97c82a87cce712d1169d5fe05808}{%
           family={Casazza},
           familyi={C\bibinitperiod},
           given={Peter\bibnamedelima G.},
           giveni={P\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=2e7b05c7e0c405468d37834ab05737b3}{%
           family={Kutyniok},
           familyi={K\bibinitperiod},
           given={Gitta},
           giveni={G\bibinitperiod}}}%
        {{hash=6bd7f9f0bea75326150f6fd2fdad5f6e}{%
           family={Philipp},
           familyi={P\bibinitperiod},
           given={Friedrich},
           giveni={F\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=410d97c82a87cce712d1169d5fe05808}{%
           family={Casazza},
           familyi={C\bibinitperiod},
           given={Peter\bibnamedelima G.},
           giveni={P\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=2e7b05c7e0c405468d37834ab05737b3}{%
           family={Kutyniok},
           familyi={K\bibinitperiod},
           given={Gitta},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Boston}%
      }
      \list{publisher}{1}{%
        {Birkhäuser}%
      }
      \strng{namehash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{fullhash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{bibnamehash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{authorbibnamehash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{authornamehash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{authorfullhash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{editorbibnamehash}{ead7359e306f40ee469ee359c9363e26}
      \strng{editornamehash}{ead7359e306f40ee469ee359c9363e26}
      \strng{editorfullhash}{ead7359e306f40ee469ee359c9363e26}
      \field{labelalpha}{CKP13}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To date, frames have established themselves as a standard notion in applied mathematics, computer science, and engineering as a means to derive redundant, yet stable decompositions of a signal for analysis or transmission, while also promoting sparse expansions. The reconstruction procedure is then based on one of the associated dual frames, which—in the case of a Parseval frame—can be chosen to be the frame itself. In this chapter, we provide a comprehensive review of the basics of finite frame theory upon which the subsequent chapters are based. After recalling some background information on Hilbert space theory and operator theory, we introduce the notion of a frame along with some crucial properties and construction procedures. Then we discuss algorithmic aspects such as basic reconstruction algorithms and present brief introductions to diverse applications and extensions of frames. The subsequent chapters of this book will then extend key topics in many intriguing directions.}
      \field{booktitle}{Finite {Frames}: {Theory} and {Applications}}
      \field{isbn}{978-0-8176-8373-3}
      \field{title}{Introduction to {Finite} {Frame} {Theory}}
      \field{urlday}{4}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{year}{2013}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 53}
      \range{pages}{53}
      \verb{doi}
      \verb 10.1007/978-0-8176-8373-3_1
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-0-8176-8373-3_1
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-0-8176-8373-3_1
      \endverb
    \endentry
    \entry{chandrasekaran_convex_2012}{article}{}
      \name{author}{4}{}{%
        {{hash=6c455027167a2474bcbecfd17ec411fa}{%
           family={Chandrasekaran},
           familyi={C\bibinitperiod},
           given={Venkat},
           giveni={V\bibinitperiod}}}%
        {{hash=3503059e1c0c778913607c87d8c5173a}{%
           family={Recht},
           familyi={R\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=8c175284da0c757c59f059ff13d91dde}{%
           family={Parrilo},
           familyi={P\bibinitperiod},
           given={Pablo\bibnamedelima A.},
           giveni={P\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=4dfdd3890b62d46ff882fa27ef7f2af0}{%
           family={Willsky},
           familyi={W\bibinitperiod},
           given={Alan\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{583c31774c04233f0926feec90ea3afb}
      \strng{fullhash}{ff0ef736c7e2776b160177289bf22907}
      \strng{bibnamehash}{583c31774c04233f0926feec90ea3afb}
      \strng{authorbibnamehash}{583c31774c04233f0926feec90ea3afb}
      \strng{authornamehash}{583c31774c04233f0926feec90ea3afb}
      \strng{authorfullhash}{ff0ef736c7e2776b160177289bf22907}
      \field{labelalpha}{Cha+12}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1615-3375, 1615-3383}
      \field{journaltitle}{Foundations of Computational Mathematics}
      \field{month}{12}
      \field{number}{6}
      \field{title}{The {Convex} {Geometry} of {Linear} {Inverse} {Problems}}
      \field{urlday}{1}
      \field{urlmonth}{8}
      \field{urlyear}{2019}
      \field{volume}{12}
      \field{year}{2012}
      \field{urldateera}{ce}
      \field{pages}{805\bibrangedash 849}
      \range{pages}{45}
      \verb{doi}
      \verb 10.1007/s10208-012-9135-7
      \endverb
      \verb{file}
      \verb Chandrasekaran et al. - 2012 - The Convex Geometry of Linear Inverse Problems.pdf:/Users/alexgoessmann/Zotero/storage/3MVAW86X/Chandrasekaran et al. - 2012 - The Convex Geometry of Linear Inverse Problems.pdf:application/pdf
      \endverb
    \endentry
    \entry{cichocki_era_2014}{article}{}
      \name{author}{1}{}{%
        {{hash=08ffe474d1c0bebee63a5696b8bc17dd}{%
           family={Cichocki},
           familyi={C\bibinitperiod},
           given={Andrzej},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{fullhash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{bibnamehash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{authorbibnamehash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{authornamehash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{authorfullhash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \field{labelalpha}{Cic14}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Many problems in computational neuroscience, neuroinformatics, pattern/image recognition, signal processing and machine learning generate massive amounts of multidimensional data with multiple aspects and high dimensionality. Tensors (i.e., multi-way arrays) provide often a natural and compact representation for such massive multidimensional data via suitable low-rank approximations. Big data analytics require novel technologies to efficiently process huge datasets within tolerable elapsed times. Such a new emerging technology for multidimensional big data is a multiway analysis via tensor networks (TNs) and tensor decompositions (TDs) which represent tensors by sets of factor (component) matrices and lower-order (core) tensors. Dynamic tensor analysis allows us to discover meaningful hidden structures of complex data and to perform generalizations by capturing multi-linear and multi-aspect relationships. We will discuss some fundamental TN models, their mathematical and graphical descriptions and associated learning algorithms for large-scale TDs and TNs, with many potential applications including: Anomaly detection, feature extraction, classification, cluster analysis, data fusion and integration, pattern recognition, predictive modeling, regression, time series analysis and multiway component analysis. Keywords: Large-scale HOSVD, Tensor decompositions, CPD, Tucker models, Hierarchical Tucker (HT) decomposition, low-rank tensor approximations (LRA), Tensorization/Quantization, tensor train (TT/QTT) - Matrix Product States (MPS), Matrix Product Operator (MPO), DMRG, Strong Kronecker Product (SKP).}
      \field{journaltitle}{arXiv:1403.2048 [cs]}
      \field{month}{3}
      \field{note}{arXiv: 1403.2048}
      \field{shorttitle}{Era of {Big} {Data} {Processing}}
      \field{title}{Era of {Big} {Data} {Processing}: {A} {New} {Approach} via {Tensor} {Networks} and {Tensor} {Decompositions}}
      \field{urlday}{16}
      \field{urlmonth}{3}
      \field{urlyear}{2019}
      \field{year}{2014}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv\:1403.2048 PDF:/Users/alexgoessmann/Zotero/storage/2RGL553C/Cichocki - 2014 - Era of Big Data Processing A New Approach via Ten.pdf:application/pdf;arXiv.org Snapshot:/Users/alexgoessmann/Zotero/storage/YDJ7VX4M/1403.html:text/html
      \endverb
      \keyw{Computer Science - Emerging Technologies,Check: Tensor Decomposition}
    \endentry
    \entry{cichocki_tensor_2015}{article}{}
      \name{author}{7}{}{%
        {{hash=08ffe474d1c0bebee63a5696b8bc17dd}{%
           family={Cichocki},
           familyi={C\bibinitperiod},
           given={Andrzej},
           giveni={A\bibinitperiod}}}%
        {{hash=c0e4066e714faa43098fa185074e7fda}{%
           family={Mandic},
           familyi={M\bibinitperiod},
           given={Danilo},
           giveni={D\bibinitperiod}}}%
        {{hash=422686a2e68522fbd3ed47c5f42b0fc7}{%
           family={De\bibnamedelima Lathauwer},
           familyi={D\bibinitperiod\bibinitdelim L\bibinitperiod},
           given={Lieven},
           giveni={L\bibinitperiod}}}%
        {{hash=11506400eabf2f393c9bf5b2f5ea3aa9}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Guoxu},
           giveni={G\bibinitperiod}}}%
        {{hash=8a239bd07f908087a20dfcf87572e035}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Qibin},
           giveni={Q\bibinitperiod}}}%
        {{hash=ec082fda5bf6ea5dc36385b22e35ea06}{%
           family={Caiafa},
           familyi={C\bibinitperiod},
           given={Cesar},
           giveni={C\bibinitperiod}}}%
        {{hash=fab8f9863c52c69f930c9839ad237404}{%
           family={PHAN},
           familyi={P\bibinitperiod},
           given={HUY\bibnamedelima ANH},
           giveni={H\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{fe89acd1fc18bfcd61f5d60877595c4f}
      \strng{fullhash}{f824fb050585cd9c96ec7600a5f3a99c}
      \strng{bibnamehash}{fe89acd1fc18bfcd61f5d60877595c4f}
      \strng{authorbibnamehash}{fe89acd1fc18bfcd61f5d60877595c4f}
      \strng{authornamehash}{fe89acd1fc18bfcd61f5d60877595c4f}
      \strng{authorfullhash}{f824fb050585cd9c96ec7600a5f3a99c}
      \field{labelalpha}{Cic+15}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The widespread use of multisensor technology and the emergence of big data sets have highlighted the limitations of standard flat-view matrix models and the necessity to move toward more versatile data analysis tools. We show that higher-order tensors (i.e., multiway arrays) enable such a fundamental paradigm shift toward models that are essentially polynomial, the uniqueness of which, unlike the matrix methods, is guaranteed under very mild and natural conditions. Benefiting from the power of multilinear algebra as their mathematical backbone, data analysis techniques using tensor decompositions are shown to have great flexibility in the choice of constraints which match data properties and extract more general latent components in the data than matrix-based methods.}
      \field{issn}{1558-0792}
      \field{journaltitle}{IEEE Signal Processing Magazine}
      \field{month}{3}
      \field{note}{Conference Name: IEEE Signal Processing Magazine}
      \field{number}{2}
      \field{shorttitle}{Tensor {Decompositions} for {Signal} {Processing} {Applications}}
      \field{title}{Tensor {Decompositions} for {Signal} {Processing} {Applications}: {From} two-way to multiway component analysis}
      \field{volume}{32}
      \field{year}{2015}
      \field{pages}{145\bibrangedash 163}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1109/MSP.2013.2297439
      \endverb
      \verb{file}
      \verb IEEE Xplore Abstract Record:/Users/alexgoessmann/Zotero/storage/PUSDA9DX/7038247.html:text/html;Volltext:/Users/alexgoessmann/Zotero/storage/TPA95G4X/Cichocki et al. - 2015 - Tensor Decompositions for Signal Processing Applic.pdf:application/pdf
      \endverb
      \keyw{Matrix decomposition,Data models,Tensile stress,Big data,Data analysis,Sensors}
    \endentry
    \entry{clifford_markov_1971}{article}{}
      \name{author}{2}{}{%
        {{hash=b820cb9e1fc07ca0974992bb8a99c6c6}{%
           family={Clifford},
           familyi={C\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod}}}%
        {{hash=08a376882401bc1c10ca00e178fdc184}{%
           family={Hammersley},
           familyi={H\bibinitperiod},
           given={J.\bibnamedelimi M.},
           giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \strng{namehash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{fullhash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{bibnamehash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{authorbibnamehash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{authornamehash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{authorfullhash}{6869f36d335715f8c9eb387cea521c5c}
      \field{labelalpha}{CH71}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Unpublished}
      \field{note}{Publisher: University of Oxford}
      \field{title}{Markov fields on finite graphs and lattices}
      \field{urlday}{11}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{1971}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/9H3CY45B/Clifford und Hammersley - 1971 - Markov fields on finite graphs and lattices.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://ora.ox.ac.uk/objects/uuid:4ea849da-1511-4578-bb88-6a8d02f457a6
      \endverb
      \verb{url}
      \verb https://ora.ox.ac.uk/objects/uuid:4ea849da-1511-4578-bb88-6a8d02f457a6
      \endverb
    \endentry
    \entry{cohen_tensorlog_2020}{article}{}
      \name{author}{3}{}{%
        {{hash=be006ecd16e3b55e58d8a895354f80e8}{%
           family={Cohen},
           familyi={C\bibinitperiod},
           given={William},
           giveni={W\bibinitperiod}}}%
        {{hash=bac6a7a5d8c835c2ebe5288a6fe722b4}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Fan},
           giveni={F\bibinitperiod}}}%
        {{hash=0dccc789340e78e1fa270ba488b2f967}{%
           family={Mazaitis},
           familyi={M\bibinitperiod},
           given={Kathryn\bibnamedelima Rivard},
           giveni={K\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{f0d351bed453e3e9245e03adce264797}
      \strng{fullhash}{f0d351bed453e3e9245e03adce264797}
      \strng{bibnamehash}{f0d351bed453e3e9245e03adce264797}
      \strng{authorbibnamehash}{f0d351bed453e3e9245e03adce264797}
      \strng{authornamehash}{f0d351bed453e3e9245e03adce264797}
      \strng{authorfullhash}{f0d351bed453e3e9245e03adce264797}
      \field{labelalpha}{CYM20}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.}
      \field{issn}{1076-9757}
      \field{journaltitle}{Journal of Artificial Intelligence Research}
      \field{month}{2}
      \field{shorttitle}{{TensorLog}}
      \field{title}{{TensorLog}: {A} {Probabilistic} {Database} {Implemented} {Using} {Deep}-{Learning} {Infrastructure}}
      \field{urlday}{20}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{67}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{285\bibrangedash 325}
      \range{pages}{41}
      \verb{doi}
      \verb 10.1613/jair.1.11944
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/QK9JGLW2/Cohen et al. - 2020 - TensorLog A Probabilistic Database Implemented Using Deep-Learning Infrastructure.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://jair.org/index.php/jair/article/view/11944
      \endverb
      \verb{url}
      \verb https://jair.org/index.php/jair/article/view/11944
      \endverb
    \endentry
    \entry{cover_elements_2006}{book}{}
      \name{author}{2}{}{%
        {{hash=fe5b7bbda12c7502f55ddb9eeded8622}{%
           family={Cover},
           familyi={C\bibinitperiod},
           given={Thomas\bibnamedelima M.},
           giveni={T\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=e6518a9e085a3ee5221c27f5d8f826dc}{%
           family={Thomas},
           familyi={T\bibinitperiod},
           given={Joy\bibnamedelima A.},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Hoboken, N.J}%
      }
      \list{publisher}{1}{%
        {Wiley-Interscience}%
      }
      \strng{namehash}{e427ba6c0eda5292558c30c786236c01}
      \strng{fullhash}{e427ba6c0eda5292558c30c786236c01}
      \strng{bibnamehash}{e427ba6c0eda5292558c30c786236c01}
      \strng{authorbibnamehash}{e427ba6c0eda5292558c30c786236c01}
      \strng{authornamehash}{e427ba6c0eda5292558c30c786236c01}
      \strng{authorfullhash}{e427ba6c0eda5292558c30c786236c01}
      \field{labelalpha}{CT06}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The latest edition of this classic is updated with new problem sets and material The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory. All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points. The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.}
      \field{edition}{2nd edition}
      \field{isbn}{978-0-471-24195-9}
      \field{month}{9}
      \field{title}{Elements of {Information} {Theory}}
      \field{year}{2006}
    \endentry
    \entry{degroot_probability_2016}{book}{}
      \name{author}{1}{}{%
        {{hash=a003c6a3b15a33bf2d075ba0e21f809c}{%
           family={DeGroot},
           familyi={D\bibinitperiod},
           given={Morris\bibnamedelima H.},
           giveni={M\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{publisher}{1}{%
        {PEARSON INDIA}%
      }
      \strng{namehash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{fullhash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{bibnamehash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{authorbibnamehash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{authornamehash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{authorfullhash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \field{labelalpha}{DeG16}
      \field{sortinit}{D}
      \field{sortinithash}{2ef1bd9a78cc71eb74d7231c635177b8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Brand New}
      \field{isbn}{978-93-325-7387-1}
      \field{month}{1}
      \field{title}{Probability and {Statistics}}
      \field{year}{2016}
    \endentry
    \entry{demir_drill-_2021}{article}{}
      \name{author}{2}{}{%
        {{hash=0fdd0add34c43fbcb7685ace9926ce07}{%
           family={Demir},
           familyi={D\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=aaffae6ab501c5d30bd5ba943863c330}{%
           family={Ngonga\bibnamedelima Ngomo},
           familyi={N\bibinitperiod\bibinitdelim N\bibinitperiod},
           given={Axel-Cyrille},
           giveni={A\bibinithyphendelim C\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {eng}%
      }
      \strng{namehash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{fullhash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{bibnamehash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{authorbibnamehash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{authornamehash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{authorfullhash}{2deba4d47a871d960ec9ed6806236b5b}
      \field{labelalpha}{DN21}
      \field{sortinit}{D}
      \field{sortinithash}{2ef1bd9a78cc71eb74d7231c635177b8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{CoRR}
      \field{title}{{DRILL}- {Deep} {Reinforcement} {Learning} for {Refinement} {Operators} in {ALC}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{abs/2106.15373}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/J97XWWW6/25217.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://ris.uni-paderborn.de/record/25217
      \endverb
      \verb{url}
      \verb https://ris.uni-paderborn.de/record/25217
      \endverb
    \endentry
    \entry{espig_variational_2012}{article}{}
      \name{author}{4}{}{%
        {{hash=1e64c76ecca3f4c3da8e872e5547d7e8}{%
           family={Espig},
           familyi={E\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod}}}%
        {{hash=01bcfafc59c11a565163fcf9ef2a0907}{%
           family={Hackbusch},
           familyi={H\bibinitperiod},
           given={Wolfgang},
           giveni={W\bibinitperiod}}}%
        {{hash=9fa4aa09a1f933a0b479542d3d8f02b8}{%
           family={Rohwedder},
           familyi={R\bibinitperiod},
           given={Thorsten},
           giveni={T\bibinitperiod}}}%
        {{hash=cd91fef38e4801ef30c61055e408b8b5}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Reinhold},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{252453e0b9026385cd8b45f3c59c5602}
      \strng{fullhash}{5d38a9e09a4be73336eb565acdb5cfe5}
      \strng{bibnamehash}{252453e0b9026385cd8b45f3c59c5602}
      \strng{authorbibnamehash}{252453e0b9026385cd8b45f3c59c5602}
      \strng{authornamehash}{252453e0b9026385cd8b45f3c59c5602}
      \strng{authorfullhash}{5d38a9e09a4be73336eb565acdb5cfe5}
      \field{labelalpha}{Esp+12}
      \field{sortinit}{E}
      \field{sortinithash}{f615fb9c6fba11c6f962fb3fd599810e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this article we introduce a calculus of variations for sums of elementary tensors and apply it to functionals of practical interest. The survey provides all necessary ingredients for applying minimization methods in a general setting. The important cases of target functionals which are linear and quadratic with respect to the tensor product are discussed, and combinations of these functionals are presented in detail. As an example, we consider the solution of a linear system in structured tensor format. Moreover, we discuss the solution of an eigenvalue problem with sums of elementary tensors. This example can be viewed as a prototype of a constrained minimization problem. For the numerical treatment, we suggest a method which has the same order of complexity as the popular alternating least square algorithm and demonstrate the rate of convergence in numerical tests.}
      \field{issn}{0945-3245}
      \field{journaltitle}{Numerische Mathematik}
      \field{month}{11}
      \field{number}{3}
      \field{title}{Variational calculus with sums of elementary tensors of fixed rank}
      \field{urlday}{13}
      \field{urlmonth}{10}
      \field{urlyear}{2021}
      \field{volume}{122}
      \field{year}{2012}
      \field{urldateera}{ce}
      \field{pages}{469\bibrangedash 488}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1007/s00211-012-0464-x
      \endverb
      \verb{file}
      \verb Springer Full Text PDF:/Users/alexgoessmann/Zotero/storage/5NZ74BCJ/Espig et al. - 2012 - Variational calculus with sums of elementary tenso.pdf:application/pdf
      \endverb
    \endentry
    \entry{falco_minimal_2012}{article}{}
      \name{author}{2}{}{%
        {{hash=d06f4f4eeb734a17f1446d71fa0fe557}{%
           family={Falco},
           familyi={F\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod}}}%
        {{hash=01bcfafc59c11a565163fcf9ef2a0907}{%
           family={Hackbusch},
           familyi={H\bibinitperiod},
           given={Wolfgang},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{fullhash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{bibnamehash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{authorbibnamehash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{authornamehash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{authorfullhash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \field{labelalpha}{FH12}
      \field{sortinit}{F}
      \field{sortinithash}{669c706c6f1fbf3b5a83d26f1d9e9e72}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{n this paper we introduce and develop the notion of minimal subspaces in the framework of algebraic and topological tensor product spaces. This mathematical structure arises in a natural way in the study of tensor representations. We use minimal subspaces to prove the existence of a best approximation, for any element in a Banach tensor space, by means of a tensor given in a typical representation format (Tucker, hierarchical, or tensor train). We show that this result holds in a tensor Banach space with a norm stronger than the injective norm and in an intersection of finitely many Banach tensor spaces satisfying some additional conditions. Examples using topological tensor products of standard Sobolev spaces are given}
      \field{journaltitle}{Foundations of Computational Mathematics}
      \field{month}{12}
      \field{title}{On {Minimal} {Subspaces} in {Tensor} {Representations}}
      \field{volume}{12}
      \field{year}{2012}
      \field{pages}{765\bibrangedash 803}
      \range{pages}{39}
      \verb{doi}
      \verb 10.1007/s10208-012-9136-6
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/ZUJB6QZE/Falco und Hackbusch - 2012 - On Minimal Subspaces in Tensor Representations.pdf:application/pdf
      \endverb
    \endentry
    \entry{foucart_mathematical_2013}{book}{}
      \name{author}{2}{}{%
        {{hash=852f06e143387da6c5a4375756356399}{%
           family={Foucart},
           familyi={F\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
        {{hash=750a4d4191d5837bca7b0ed740eebabc}{%
           family={Rauhut},
           familyi={R\bibinitperiod},
           given={Holger},
           giveni={H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Birkhäuser Basel}%
      }
      \strng{namehash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{fullhash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{bibnamehash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{authorbibnamehash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{authornamehash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{authorfullhash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \field{labelalpha}{FR13}
      \field{sortinit}{F}
      \field{sortinithash}{669c706c6f1fbf3b5a83d26f1d9e9e72}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{At the intersection of mathematics, engineering, and computer science sits the thriving field of compressive sensing. Based on the premise that data acquisition and compression can be performed simultaneously, compressive sensing finds applications in imaging, signal processing, and many other domains. In the areas of applied mathematics, electrical engineering, and theoretical computer science, an explosion of research activity has already followed the theoretical results that highlighted the efficiency of the basic principles. The elegant ideas behind these principles are also of independent interest to pure mathematicians.A Mathematical Introduction to Compressive Sensing gives a detailed account of the core theory upon which the field is build. With only moderate prerequisites, it is an excellent textbook for graduate courses in mathematics, engineering, and computer science. It also serves as a reliable resource for practitioners and researchers in these disciplines who want to acquire a careful understanding of the subject. A Mathematical Introduction to Compressive Sensing uses a mathematical perspective to present the core of the theory underlying compressive sensing.}
      \field{isbn}{978-0-8176-4947-0}
      \field{series}{Applied and {Numerical} {Harmonic} {Analysis}}
      \field{title}{A {Mathematical} {Introduction} to {Compressive} {Sensing}}
      \field{urlday}{11}
      \field{urlmonth}{12}
      \field{urlyear}{2019}
      \field{year}{2013}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-0-8176-4948-7
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/VRC9U6NH/9780817649470.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.springer.com/de/book/9780817649470
      \endverb
      \verb{url}
      \verb https://www.springer.com/de/book/9780817649470
      \endverb
    \endentry
    \entry{python_software_foundation_python_2025}{misc}{}
      \name{author}{1}{}{%
        {{hash=9688a568902e4b07b60557b566bdd820}{%
           family={Foundation},
           familyi={F\bibinitperiod},
           given={Python\bibnamedelima Software},
           giveni={P\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{9688a568902e4b07b60557b566bdd820}
      \strng{fullhash}{9688a568902e4b07b60557b566bdd820}
      \strng{bibnamehash}{9688a568902e4b07b60557b566bdd820}
      \strng{authorbibnamehash}{9688a568902e4b07b60557b566bdd820}
      \strng{authornamehash}{9688a568902e4b07b60557b566bdd820}
      \strng{authorfullhash}{9688a568902e4b07b60557b566bdd820}
      \field{labelalpha}{Fou25}
      \field{sortinit}{F}
      \field{sortinithash}{669c706c6f1fbf3b5a83d26f1d9e9e72}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{4}
      \field{title}{Python {Language} {Reference}, version 3.13.2}
      \field{urlday}{6}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{urldateera}{ce}
      \verb{file}
      \verb 3.13.2 Documentation:/Users/alexgoessmann/Zotero/storage/JJDPYYBU/3.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://docs.python.org/3/
      \endverb
      \verb{url}
      \verb https://docs.python.org/3/
      \endverb
    \endentry
    \entry{galarraga_amie_2013}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=18674af5afb94551a3ce682e24e6706d}{%
           family={Galárraga},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelima Antonio},
           giveni={L\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=04816795c9877feddae45c038e2508f3}{%
           family={Teflioudi},
           familyi={T\bibinitperiod},
           given={Christina},
           giveni={C\bibinitperiod}}}%
        {{hash=1b753e139c8ab7af870d74e326b796ad}{%
           family={Hose},
           familyi={H\bibinitperiod},
           given={Katja},
           giveni={K\bibinitperiod}}}%
        {{hash=55d7f682b85e1f9066365cc900f8331c}{%
           family={Suchanek},
           familyi={S\bibinitperiod},
           given={Fabian},
           giveni={F\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Rio de Janeiro Brazil}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{1d5ca0dcef50a6c0946037e7a697d1ee}
      \strng{fullhash}{5c770da4ca763c325e09a0b506f3ed61}
      \strng{bibnamehash}{1d5ca0dcef50a6c0946037e7a697d1ee}
      \strng{authorbibnamehash}{1d5ca0dcef50a6c0946037e7a697d1ee}
      \strng{authornamehash}{1d5ca0dcef50a6c0946037e7a697d1ee}
      \strng{authorfullhash}{5c770da4ca763c325e09a0b506f3ed61}
      \field{labelalpha}{Gal+13}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive Logic Programming (ILP) can be used to mine logical rules from the KB. These rules can help deduce and add missing knowledge to the KB. While ILP is a mature ﬁeld, mining logical rules from KBs is diﬀerent in two aspects: First, current rule mining systems are easily overwhelmed by the amount of data (state-of-the art systems cannot even run on today’s KBs). Second, ILP usually requires counterexamples. KBs, however, implement the open world assumption (OWA), meaning that absent data cannot be used as counterexamples. In this paper, we develop a rule mining model that is explicitly tailored to support the OWA scenario. It is inspired by association rule mining and introduces a novel measure for conﬁdence. Our extensive experiments show that our approach outperforms state-of-the-art approaches in terms of precision and coverage. Furthermore, our system, AMIE, mines rules orders of magnitude faster than state-of-the-art approaches.}
      \field{booktitle}{Proceedings of the 22nd international conference on {World} {Wide} {Web}}
      \field{isbn}{978-1-4503-2035-1}
      \field{month}{5}
      \field{shorttitle}{{AMIE}}
      \field{title}{{AMIE}: association rule mining under incomplete evidence in ontological knowledge bases}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2013}
      \field{urldateera}{ce}
      \field{pages}{413\bibrangedash 422}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/2488388.2488425
      \endverb
      \verb{file}
      \verb Galárraga et al. - 2013 - AMIE association rule mining under incomplete evidence in ontological knowledge bases.pdf:/Users/alexgoessmann/Zotero/storage/5J7XU7WH/Galárraga et al. - 2013 - AMIE association rule mining under incomplete evidence in ontological knowledge bases.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/2488388.2488425
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/2488388.2488425
      \endverb
    \endentry
    \entry{ganapathi_constrained_2008}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=f6a7c1bcc9e1d51e613d3f4e5115677c}{%
           family={Ganapathi},
           familyi={G\bibinitperiod},
           given={Varun},
           giveni={V\bibinitperiod}}}%
        {{hash=2ee1a3110b48ca8da37c9092275ff42c}{%
           family={Vickrey},
           familyi={V\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=b8fbef1897da5bf46822ced31bc865c6}{%
           family={Duchi},
           familyi={D\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=9c785bcf6b8a0b99e44695299a6aecb2}{%
           family={Koller},
           familyi={K\bibinitperiod},
           given={Daphne},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Arlington, Virginia, USA}%
      }
      \list{publisher}{1}{%
        {AUAI Press}%
      }
      \strng{namehash}{e68528775f03ffb76e496bc61b8253a1}
      \strng{fullhash}{f412d5b2551da307a380b061e20d36b1}
      \strng{bibnamehash}{e68528775f03ffb76e496bc61b8253a1}
      \strng{authorbibnamehash}{e68528775f03ffb76e496bc61b8253a1}
      \strng{authornamehash}{e68528775f03ffb76e496bc61b8253a1}
      \strng{authorfullhash}{f412d5b2551da307a380b061e20d36b1}
      \field{labelalpha}{Gan+08}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Parameter estimation in Markov random fields (MRFs) is a difficult task, in which inference over the network is run in the inner loop of a gradient descent procedure. Replacing exact inference with approximate methods such as loopy belief propagation (LBP) can suffer from poor convergence. In this paper, we provide a different approach for combining MRF learning and Bethe approximation. We consider the dual of maximum likelihood Markov network learning — maximizing entropy with moment matching constraints — and then approximate both the objective and the constraints in the resulting optimization problem. Unlike previous work along these lines (Teh \&amp; Welling, 2003), our formulation allows parameter sharing between features in a general log-linear model, parameter regularization and conditional training. We show that piecewise training (Sutton \&amp; McCallum, 2005) is a very restricted special case of this formulation. We study two optimization strategies: one based on a single convex approximation and one that uses repeated convex approximations. We show results on several real-world networks that demonstrate that these algorithms can significantly outperform learning with loopy and piece-wise. Our results also provide a framework for analyzing the trade-offs of different relaxations of the entropy objective and of the constraints.}
      \field{booktitle}{Proceedings of the {Twenty}-{Fourth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}}
      \field{isbn}{978-0-9749039-4-1}
      \field{month}{7}
      \field{series}{{UAI}'08}
      \field{title}{Constrained approximate maximum entropy learning of {Markov} random fields}
      \field{urlday}{29}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2008}
      \field{urldateera}{ce}
      \field{pages}{196\bibrangedash 203}
      \range{pages}{8}
    \endentry
    \entry{garcez_neural-symbolic_2019}{misc}{}
      \name{author}{6}{}{%
        {{hash=70658466246873155f5b9b3a737cdd4e}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur\bibnamedelima d'Avila},
           giveni={A\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=d25d4f83ae540b2b728a5423e9fbe85b}{%
           family={Gori},
           familyi={G\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=8a6bd56db2a68b7d2f595538052b85e9}{%
           family={Lamb},
           familyi={L\bibinitperiod},
           given={Luis\bibnamedelima C.},
           giveni={L\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=cfd7b672fd7926ef1136b9790438416d}{%
           family={Serafini},
           familyi={S\bibinitperiod},
           given={Luciano},
           giveni={L\bibinitperiod}}}%
        {{hash=fecf9d2471f6f2bf1fe02253df553444}{%
           family={Spranger},
           familyi={S\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=47cf7a42e3145f38a9f22b292476284a}{%
           family={Tran},
           familyi={T\bibinitperiod},
           given={Son\bibnamedelima N.},
           giveni={S\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{2a101fc4b616118ed7bec25944d03c40}
      \strng{fullhash}{d0eeb1e4c9ede67d86f1db0bb58c4f99}
      \strng{bibnamehash}{2a101fc4b616118ed7bec25944d03c40}
      \strng{authorbibnamehash}{2a101fc4b616118ed7bec25944d03c40}
      \strng{authornamehash}{2a101fc4b616118ed7bec25944d03c40}
      \strng{authorfullhash}{d0eeb1e4c9ede67d86f1db0bb58c4f99}
      \field{labelalpha}{Gar+19}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.}
      \field{month}{5}
      \field{note}{arXiv:1905.06088 [cs]}
      \field{shorttitle}{Neural-{Symbolic} {Computing}}
      \field{title}{Neural-{Symbolic} {Computing}: {An} {Effective} {Methodology} for {Principled} {Integration} of {Machine} {Learning} and {Reasoning}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1905.06088
      \endverb
      \verb{file}
      \verb Preprint PDF:/Users/alexgoessmann/Zotero/storage/6NGQJ8Z6/Garcez et al. - 2019 - Neural-Symbolic Computing An Effective Methodology for Principled Integration of Machine Learning a.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/RQ2KIXXN/1905.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1905.06088
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1905.06088
      \endverb
      \keyw{Computer Science - Artificial Intelligence}
    \endentry
    \entry{gels_pgelssscikit_tt_2025}{misc}{}
      \name{author}{1}{}{%
        {{hash=aae598c66790875fb3f97e9e696f287f}{%
           family={Gelß},
           familyi={G\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{aae598c66790875fb3f97e9e696f287f}
      \strng{fullhash}{aae598c66790875fb3f97e9e696f287f}
      \strng{bibnamehash}{aae598c66790875fb3f97e9e696f287f}
      \strng{authorbibnamehash}{aae598c66790875fb3f97e9e696f287f}
      \strng{authornamehash}{aae598c66790875fb3f97e9e696f287f}
      \strng{authorfullhash}{aae598c66790875fb3f97e9e696f287f}
      \field{labelalpha}{Gel25}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Tensor Train Toolbox}
      \field{month}{6}
      \field{note}{original-date: 2018-11-23T13:10:49Z}
      \field{title}{{PGelss}/scikit\_tt}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://github.com/PGelss/scikit_tt
      \endverb
      \verb{url}
      \verb https://github.com/PGelss/scikit_tt
      \endverb
      \keyw{als,data-driven,dmrg,dynamic-mode-decomposition,dynamical-systems,extended-dynamic-mode-decomposition,mals,mandy,quantum-simulation,scikit,slim-decomposition,tensor,tensor-decomposition,tensor-network,tensor-train}
    \endentry
    \entry{gels_multidimensional_2019}{article}{}
      \name{author}{4}{}{%
        {{hash=aae598c66790875fb3f97e9e696f287f}{%
           family={Gelß},
           familyi={G\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
        {{hash=0c9864cb960ba6285d9bf3e76a0060a9}{%
           family={Klus},
           familyi={K\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=878172bfd75f93a96d3646d7b66c3e48}{%
           family={Eisert},
           familyi={E\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
        {{hash=c40cc853c219070fbe80745919805677}{%
           family={Schütte},
           familyi={S\bibinitperiod},
           given={Christof},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{41185c195913dab8e19a507c1b227dd1}
      \strng{fullhash}{9daf53e832a5e5f1372a37a1ba7c0096}
      \strng{bibnamehash}{41185c195913dab8e19a507c1b227dd1}
      \strng{authorbibnamehash}{41185c195913dab8e19a507c1b227dd1}
      \strng{authornamehash}{41185c195913dab8e19a507c1b227dd1}
      \strng{authorfullhash}{9daf53e832a5e5f1372a37a1ba7c0096}
      \field{labelalpha}{Gel+19}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A key task in the field of modeling and analyzing nonlinear dynamical systems is the recovery of unknown governing equations from measurement data only. There is a wide range of application areas for this important instance of system identification, ranging from industrial engineering and acoustic signal processing to stock market models. In order to find appropriate representations of underlying dynamical systems, various data-driven methods have been proposed by different communities. However, if the given data sets are high-dimensional, then these methods typically suffer from the curse of dimensionality. To significantly reduce the computational costs and storage consumption, we propose the method multidimensional approximation of nonlinear dynamical systems (MANDy) which combines data-driven methods with tensor network decompositions. The efficiency of the introduced approach will be illustrated with the aid of several high-dimensional nonlinear dynamical systems.}
      \field{issn}{1555-1415}
      \field{journaltitle}{Journal of Computational and Nonlinear Dynamics}
      \field{month}{4}
      \field{number}{6}
      \field{title}{Multidimensional {Approximation} of {Nonlinear} {Dynamical} {Systems}}
      \field{urlday}{1}
      \field{urlmonth}{5}
      \field{urlyear}{2019}
      \field{volume}{14}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{061006--061006\bibrangedash 12}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1115/1.4043148
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/9DAMBNLE/Gelß et al. - 2019 - Multidimensional Approximation of Nonlinear Dynami.pdf:application/pdf
      \endverb
    \endentry
    \entry{getoor_introduction_2019}{book}{}
      \name{author}{2}{}{%
        {{hash=88e86bf57e4d0841ba9010d52469237c}{%
           family={Getoor},
           familyi={G\bibinitperiod},
           given={Lise},
           giveni={L\bibinitperiod}}}%
        {{hash=58d1c2675fb9ba29754fd8d952001eed}{%
           family={Taskar},
           familyi={T\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{fullhash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{bibnamehash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{authorbibnamehash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{authornamehash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{authorfullhash}{ef1236351c153eeb9d7a29940c6dade9}
      \field{labelalpha}{GT19}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Advanced statistical modeling and knowledge representation techniques for a newly emerging area of machine learning and probabilistic reasoning; includes introductory material, tutorials for different proposed approaches, and applications.Handling inherent uncertainty and exploiting compositional structure are fundamental to understanding and designing large-scale systems. Statistical relational learning builds on ideas from probability theory and statistics to address uncertainty while incorporating tools from logic, databases and programming languages to represent structure. In Introduction to Statistical Relational Learning, leading researchers in this emerging area of machine learning describe current formalisms, models, and algorithms that enable effective and robust reasoning about richly structured systems and data. The early chapters provide tutorials for material used in later chapters, offering introductions to representation, inference and learning in graphical models, and logic. The book then describes object-oriented approaches, including probabilistic relational models, relational Markov networks, and probabilistic entity-relationship models as well as logic-based formalisms including Bayesian logic programs, Markov logic, and stochastic logic programs. Later chapters discuss such topics as probabilistic models with unknown objects, relational dependency networks, reinforcement learning in relational domains, and information extraction. By presenting a variety of approaches, the book highlights commonalities and clarifies important differences among proposed approaches and, along the way, identifies important representational and algorithmic issues. Numerous applications are provided throughout.}
      \field{isbn}{978-0-262-53868-8}
      \field{month}{9}
      \field{title}{Introduction to {Statistical} {Relational} {Learning}}
      \field{year}{2019}
    \endentry
    \entry{gillmann_01-polytopes_2007}{thesis}{}
      \name{author}{1}{}{%
        {{hash=7ed2386e25c966f9ad2297317d840df0}{%
           family={Gillmann},
           familyi={G\bibinitperiod},
           given={Rafael},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \strng{namehash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{fullhash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{bibnamehash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{authorbibnamehash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{authornamehash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{authorfullhash}{7ed2386e25c966f9ad2297317d840df0}
      \field{labelalpha}{Gil07}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{0/1-Polytopen kann man in vielen Gebieten der diskreten Mathematik begegnen. Das bekannteste ist als "polyedrische Kombinatorik" bekannt. Auch wenn man die polyedrische Kombinatorik meist mit kombinatorischer Optimierung in Verbindung bringt, gibt es wichtige Zusammenhänge zu anderen Gebieten, zum Beispiel zum Zählen von kombinatorischen Strukturen. Trotz ihres breiten Anwendungsspektrums existiert keine umfassende Theorie über 0/1-Polytope. Es scheint so als seien 0/1-Polytope besonders komplizierte Objekte. Die Ergebnisse, welche in dieser Dissertation dargestellt werden, gruppieren sich um drei Hauptthemen: (1) obere Schranken für die minimale Anzahl von Seiten (Kanten und Facetten), (2) eine Vermutung von Mihail und Vazirani über die Kantenexpansion und (3) zufällige 0/1-Polytope. Zufälligen 0/1-Polytope können aus zwei verschiedenen Blickwinkeln betrachtet werden. Zum einen kann man Zufall benutzen, um die Komplexität allgemeiner 0/1-Polytope zu durchbrechen. Zum anderen kann man untersuchen, welche Eigenschaften zufällige 0/1-Polytope mit hoher Wahrscheinlichkeit besitzen. Dies sind dann typische Eigenschaften von 0/1-Polytopen und können nützlich sein, um Eigenschaften spezieller 0/1-Polytope einzuordnen. Nach einer kurzen Einführung in die Grundlagen der Polytoptheorie und einen Überblick über den aktuellen Stand der Forschung auf dem Gebiet der 0/1-Polytope, werden Grundlagen aus der Wahrscheinlichkeitstheorie kurz eingeführt. Die Ergebnisse zu zufälligen 0/1-Polytopen (3) umfassen: (a) Schwellenwert Resultate mit exponentieller Konvergenz zu der Wahrscheinlichkeit, dass \$k\$ zufällige Ecken eines zufälligen 0/1-Polytopes eine \$(k-1)\$-dimensionale Seite bilden; (b) Es gibt 0/1-Polytope mit exponentiell kleiner Eckenexpansion; (c) für jedes \$k\$ gibt es eine Konstante \$c\_k\$, so dass ein zufälliges 0/1-Polytop in \$[0,1]{\textasciicircum}d\$ mit höchstens \$2{\textasciicircum}\{cd\}\$ Ecken sehr wahrscheinlich \$k\$-nachbarschaftlich ist. Ergebnis (b) zeigt, dass es für die Vermutung von Mihail und Vazirani (2) nicht genügt die Eckenexpansion zu untersuchen. Allerdings scheint es schwierig oder unmöglich zu sein, dieses Negativresultat auf die Kantenexpansion auszuweiten. Ergebnisse (a) und (c) zeigen typische Eigenschaften von zufälligen 0/1-Polytopen. Hieraus lässt sich schließen, dass ein vollständiger Graph für 0/1-Polytope aus der kombinatorischen Optimierung nichts besonderes sind, da diese Polytope meistens nur sub-exponentiell viele Ecken haben. Bekannte Beispiele für 0/1-Polytope aus der kombinatorischen Optimierung sind Traveling Salesman Polytope'' und Cut-Polytope''. Im letzten Kapitel stellen wir revlex-initiale 0/1-Polytope vor. Diese speziellen Knap{\textbackslash}-sack-Poly{\textbackslash}-tope haben sehr wenige Facetten und Kanten. Wir benutzen diese Polytope, um zu zeigen, dass es in jeder Dimension \$d\$ und sinnvoller Eckenzahl \$n\$ ein \$d\$-dimensionales 0/1-Polytope mit \$n\$ Ecken gibt, welches höchstens \$3d\$ Facetten und Eckengrad höchstens \$d+8\$ hat. Dies sind die ersten Resultate zu oberen Schranken an die minimale Anzahl der Seiten von 0/1-Polytopen (1). Trotz des dünnen Graphen erfüllen diese 0/1-Polytope die Vermutung von Mihail und Vazirani (2). Abschließend betrachten wir noch kurz "erweiterte revlex-initiale 0/1-Polytope", welche eine weiterführende Richtung in der Untersuchung von Knapsack-Polytopen im Hinblick auf die Vermutung von Mihail und Vazirani sein können.}
      \field{month}{2}
      \field{shorttitle}{0/1-{Polytopes}}
      \field{title}{0/1-{Polytopes}: {Typical} and {Extremal} {Properties}}
      \field{type}{phdthesis}
      \field{urlday}{4}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2007}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/XVZHS784/Gillmann - 2007 - 01-Polytopes Typical and Extremal Properties.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://depositonce.tu-berlin.de/items/urn:nbn:de:kobv:83-opus-14695
      \endverb
      \verb{url}
      \verb https://depositonce.tu-berlin.de/items/urn:nbn:de:kobv:83-opus-14695
      \endverb
    \endentry
    \entry{giovanni_russo_vito_latora_complex_2017}{book}{}
      \name{author}{1}{}{%
        {{hash=401604d888b5c37ccd5e617322bf6077}{%
           family={Giovanni\bibnamedelimb Russo\bibnamedelimb Vito\bibnamedelima Latora},
           familyi={G\bibinitperiod\bibinitdelim R\bibinitperiod\bibinitdelim V\bibinitperiod\bibinitdelim L\bibinitperiod},
           given={Vincenzo\bibnamedelima Nicosia},
           giveni={V\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge, United Kingdom ; New York, NY}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{401604d888b5c37ccd5e617322bf6077}
      \strng{fullhash}{401604d888b5c37ccd5e617322bf6077}
      \strng{bibnamehash}{401604d888b5c37ccd5e617322bf6077}
      \strng{authorbibnamehash}{401604d888b5c37ccd5e617322bf6077}
      \strng{authornamehash}{401604d888b5c37ccd5e617322bf6077}
      \strng{authorfullhash}{401604d888b5c37ccd5e617322bf6077}
      \field{labelalpha}{Gio17}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Networks constitute the backbone of complex systems, from the human brain to computer communications, transport infrastructures to online social systems and metabolic reactions to financial markets. Characterising their structure improves our understanding of the physical, biological, economic and social phenomena that shape our world. Rigorous and thorough, this textbook presents a detailed overview of the new theory and methods of network science. Covering algorithms for graph exploration, node ranking and network generation, among others, the book allows students to experiment with network models and real-world data sets, providing them with a deep understanding of the basics of network theory and its practical applications. Systems of growing complexity are examined in detail, challenging students to increase their level of skill. An engaging presentation of the important principles of network science makes this the perfect reference for researchers and undergraduate and graduate students in physics, mathematics, engineering, biology, neuroscience and the social sciences.}
      \field{isbn}{978-1-107-10318-4}
      \field{month}{9}
      \field{shorttitle}{Complex {Networks}}
      \field{title}{Complex {Networks}: {Principles}, {Methods} and {Applications}. {With} 58 exercises}
      \field{year}{2017}
    \endentry
    \entry{glasser_expressive_2019}{article}{}
      \name{author}{5}{}{%
        {{hash=a7ed7dd14e38aee77c04c621e9db9479}{%
           family={Glasser},
           familyi={G\bibinitperiod},
           given={Ivan},
           giveni={I\bibinitperiod}}}%
        {{hash=5ad899c94bf216d9a3334f13a79e79b0}{%
           family={Sweke},
           familyi={S\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=b7f7b29051f4774bb6f5496c8bde3137}{%
           family={Pancotti},
           familyi={P\bibinitperiod},
           given={Nicola},
           giveni={N\bibinitperiod}}}%
        {{hash=878172bfd75f93a96d3646d7b66c3e48}{%
           family={Eisert},
           familyi={E\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
        {{hash=fe13755c257fb2ccfe9490c05415070a}{%
           family={Cirac},
           familyi={C\bibinitperiod},
           given={Ignacio},
           giveni={I\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{66af410c151996011c9006cfa3995e37}
      \strng{fullhash}{c6452342297aeb8ac430772a8ba73410}
      \strng{bibnamehash}{66af410c151996011c9006cfa3995e37}
      \strng{authorbibnamehash}{66af410c151996011c9006cfa3995e37}
      \strng{authornamehash}{66af410c151996011c9006cfa3995e37}
      \strng{authorfullhash}{c6452342297aeb8ac430772a8ba73410}
      \field{labelalpha}{Gla+19}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Advances in Neural Information Processing Systems}
      \field{title}{Expressive power of tensor-network factorizations for probabilistic modeling}
      \field{urlday}{6}
      \field{urlmonth}{7}
      \field{urlyear}{2021}
      \field{volume}{32}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/HE7UYQQ8/Glasser et al. - 2019 - Expressive power of tensor-network factorizations .pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/F9Z9J49G/b86e8d03fe992d1b0e19656875ee557c-Abstract.html:text/html
      \endverb
    \endentry
    \entry{goesmann_uniform_2021}{thesis}{}
      \name{author}{1}{}{%
        {{hash=159c9dead125485200f20029d40453fb}{%
           family={Goeßmann},
           familyi={G\bibinitperiod},
           given={Alex\bibnamedelima Christoph},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Technische Universität Berlin}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Berlin}%
      }
      \strng{namehash}{159c9dead125485200f20029d40453fb}
      \strng{fullhash}{159c9dead125485200f20029d40453fb}
      \strng{bibnamehash}{159c9dead125485200f20029d40453fb}
      \strng{authorbibnamehash}{159c9dead125485200f20029d40453fb}
      \strng{authornamehash}{159c9dead125485200f20029d40453fb}
      \strng{authorfullhash}{159c9dead125485200f20029d40453fb}
      \field{labelalpha}{Goe21}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This thesis contributes to the uniform concentration approach towards guaranteeing the generalization of learned models. We show probabilistic bounds on various uniform concentration events and demonstrate their utility in recovery guarantees. The thesis is organized in three parts. In the first part, we develop a unified theoretical framework for the concentration of random variables and the uniform concentration of stochastic processes. We introduce functionals of stochastic processes and apply them in bounds on the supremum. Then we develop methods to transfer uniform concentration events into success guarantees for empirical risk minimization problems. The second part of this thesis investigates classes of structured random distributions. More precisely, we derive bounds on the uniform concentration of contracted random tensors, which are decomposable into tensor network formats. In particular, we show exact moment bounds on contracted Gaussian tensor networks, which are tensor networks consistent of independent standard Gaussian random cores. By applying comparison theorems for Gaussian variables, the upper moment bounds are extended to more generic Orlicz tensor networks, which are characterized by weaker assumptions made on the random cores. Furthermore, we derive bounds on the concentration of Haar tensor networks, which random cores follow the Haar distribution of Stiefel manifolds. For all examples we continue to provide probabilistic bounds on uniform concentration events, which imply recovery guarantees for tensor regression problems. We further apply our findings in deriving success guarantees for efficient algorithms solving tensor regression problems. In the third part, we transfer our findings to bounds on the uniform concentration of neural networks following two approaches. First, we derive concentration bounds for shallow ReLU networks with respect to standard Gaussian distributions, where we introduce parameter embeddings that capture the concentration structure. Second, we bound the Rademacher complexity of deep neural networks, which are activated by a contraction, by Rademacher complexities of linear functions. This enables the proof of recovery guarantees for neural networks, which are trained on structured data.}
      \field{note}{Accepted: 2021-12-30T15:00:58Z}
      \field{title}{Uniform {Concentration} of {Tensor} and {Neural} {Networks}: {An} {Approach} towards {Recovery} {Guarantees}}
      \field{type}{{PhD} {Thesis}}
      \field{urlday}{13}
      \field{urlmonth}{1}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/8QLGC9FD/Goeßmann - 2021 - Uniform concentration of tensor and neural network.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/ZKMYB6H4/15990.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://depositonce.tu-berlin.de/handle/11303/15990
      \endverb
      \verb{url}
      \verb https://depositonce.tu-berlin.de/handle/11303/15990
      \endverb
    \endentry
    \entry{goesmann_tensor_2020}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=5efe76d0bd128291a2b5981da86e7f18}{%
           family={Goeßmann},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=14286c23fc543acfeff36e8ee4505caf}{%
           family={Roth},
           familyi={R\bibinitperiod},
           given={Ingo},
           giveni={I\bibinitperiod}}}%
        {{hash=2e7b05c7e0c405468d37834ab05737b3}{%
           family={Kutyniok},
           familyi={K\bibinitperiod},
           given={Gitta},
           giveni={G\bibinitperiod}}}%
        {{hash=67ec7578dee8df82799cd8f2b05d8461}{%
           family={Götte},
           familyi={G\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=5ad899c94bf216d9a3334f13a79e79b0}{%
           family={Sweke},
           familyi={S\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=878172bfd75f93a96d3646d7b66c3e48}{%
           family={Eisert},
           familyi={E\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{5375874d19924cacc4d1804281515215}
      \strng{fullhash}{f6202aef2a2b81eab4ac07788df1e65c}
      \strng{bibnamehash}{5375874d19924cacc4d1804281515215}
      \strng{authorbibnamehash}{5375874d19924cacc4d1804281515215}
      \strng{authornamehash}{5375874d19924cacc4d1804281515215}
      \strng{authorfullhash}{f6202aef2a2b81eab4ac07788df1e65c}
      \field{labelalpha}{Goe+20}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To date, scalable methods for data-driven identiﬁcation of non-linear governing equations do not exploit or offer insight into the fundamental underlying physical structure. In this work, we show that various physical constraints can be captured via tensor network based parameterizations for the governing equation, which naturally ensures scalability. In addition to providing analytic results motivating the use of such models for realistic physical systems, we demonstrate that efﬁcient rank-adaptive optimization algorithms can be used to learn optimal tensor network models without requiring a priori knowledge of the exact tensor ranks.}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems} - {First} {Workshop} on {Quantum} {Tensor} {Networks} in {Machine} {Learning}}
      \field{title}{Tensor network approaches for data-driven identiﬁcation of non-linear dynamical laws}
      \field{year}{2020}
      \field{pages}{21}
      \range{pages}{1}
      \verb{file}
      \verb Goeßmann et al. - Tensor network approaches for data-driven identiﬁc.pdf:/Users/alexgoessmann/Zotero/storage/GVJXJ77H/Goeßmann et al. - Tensor network approaches for data-driven identiﬁc.pdf:application/pdf
      \endverb
    \endentry
    \entry{grasedyck_hierarchical_2010}{article}{}
      \name{author}{1}{}{%
        {{hash=f94961b8e70ac7cf8e129bd9cfdb015f}{%
           family={Grasedyck},
           familyi={G\bibinitperiod},
           given={Lars},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{fullhash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{bibnamehash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{authorbibnamehash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{authornamehash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{authorfullhash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \field{labelalpha}{Gra10}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We define the hierarchical singular value decomposition (SVD) for tensors of order d ≥ 2. This hierarchical SVD has properties like the matrix SVD (and collapses to the SVD in d = 2), and we prove these. In particular, one can find low rank (almost) best approximations in a hierarchical format (H-Tucker) which requires only O((d - 1)k3 + dnk) parameters, where d is the order of the tensor, n the size of the modes, and k the (hierarchical) rank. The H-Tucker format is a specialization of the Tucker format and it contains as a special case all (canonical) rank k tensors. Based on this new concept of a hierarchical SVD we present algorithms for hierarchical tensor calculations allowing for a rigorous error analysis. The complexity of the truncation (finding lower rank approximations to hierarchical rank k tensors) is in O((d-1)k4+dnk2) and the attainable accuracy is just 2-3 digits less than machine precision.}
      \field{journaltitle}{SIAM J. Matrix Analysis Applications}
      \field{month}{1}
      \field{title}{Hierarchical {Singular} {Value} {Decomposition} of {Tensors}}
      \field{volume}{31}
      \field{year}{2010}
      \field{pages}{2029\bibrangedash 2054}
      \range{pages}{26}
      \verb{doi}
      \verb 10.1137/090764189
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/J9ZJ3RB3/Grasedyck - 2010 - Hierarchical Singular Value Decomposition of Tenso.pdf:application/pdf
      \endverb
    \endentry
    \entry{hackbusch_new_2009}{article}{}
      \name{author}{2}{}{%
        {{hash=d16d68319ab32deb0bd277d06fe083ef}{%
           family={Hackbusch},
           familyi={H\bibinitperiod},
           given={W.},
           giveni={W\bibinitperiod}}}%
        {{hash=4fdf09f9beb2649fb4b25b311b841e4c}{%
           family={Kühn},
           familyi={K\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{fullhash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{bibnamehash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{authorbibnamehash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{authornamehash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{authorfullhash}{25d066aa917d3f5efc86f10e0fee65ba}
      \field{labelalpha}{HK09}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The paper presents a new scheme for the representation of tensors which is well-suited for high-order tensors. The construction is based on a hierarchy of tensor product subspaces spanned by orthonormal bases. The underlying binary tree structure makes it possible to apply standard Linear Algebra tools for performing arithmetical operations and for the computation of data-sparse approximations. In particular, a truncation algorithm can be implemented which is based on the standard matrix singular value decomposition (SVD) method.}
      \field{issn}{1531-5851}
      \field{journaltitle}{Journal of Fourier Analysis and Applications}
      \field{month}{10}
      \field{number}{5}
      \field{title}{A {New} {Scheme} for the {Tensor} {Representation}}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2021}
      \field{volume}{15}
      \field{year}{2009}
      \field{urldateera}{ce}
      \field{pages}{706\bibrangedash 722}
      \range{pages}{17}
      \verb{file}
      \verb Springer Full Text PDF:/Users/alexgoessmann/Zotero/storage/SJUUNNLH/Hackbusch und Kühn - 2009 - A New Scheme for the Tensor Representation.pdf:application/pdf
      \endverb
    \endentry
    \entry{hackbusch_tensor_2012}{book}{}
      \name{author}{1}{}{%
        {{hash=01bcfafc59c11a565163fcf9ef2a0907}{%
           family={Hackbusch},
           familyi={H\bibinitperiod},
           given={Wolfgang},
           giveni={W\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Berlin Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer-Verlag}%
      }
      \strng{namehash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{fullhash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{bibnamehash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{authorbibnamehash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{authornamehash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{authorfullhash}{01bcfafc59c11a565163fcf9ef2a0907}
      \field{labelalpha}{Hac12}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Special numerical techniques are already needed to deal with nxn matrices for large n.Tensor data are of size nxnx...xn=n{\textasciicircum}d, where n{\textasciicircum}d exceeds the computer memory by far. They appear for problems of high spatial dimensions. Since standard methods fail, a particular tensor calculus is needed to treat such problems. The monograph describes the methods how tensors can be practically treated and how numerical operations can be performed. Applications are problems from quantum chemistry, approximation of multivariate functions, solution of pde, e.g., with stochastic coefficients, etc. ​}
      \field{isbn}{978-3-642-28026-9}
      \field{series}{Springer {Series} in {Computational} {Mathematics}}
      \field{title}{Tensor {Spaces} and {Numerical} {Tensor} {Calculus}}
      \field{urlday}{30}
      \field{urlmonth}{1}
      \field{urlyear}{2020}
      \field{year}{2012}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-3-642-28027-6
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/PC9F8YHM/9783642280269.html:text/html
      \endverb
    \endentry
    \entry{hiriart-urruty_convex_1993}{book}{}
      \name{author}{2}{}{%
        {{hash=f8a4abcac6bfbded8d5bc07918abfa8c}{%
           family={Hiriart-Urruty},
           familyi={H\bibinithyphendelim U\bibinitperiod},
           given={Jean-Baptiste},
           giveni={J\bibinithyphendelim B\bibinitperiod}}}%
        {{hash=7b2c40c2d473e37d7475deabb7519a7a}{%
           family={Lemarechal},
           familyi={L\bibinitperiod},
           given={Claude},
           giveni={C\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{fullhash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{bibnamehash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{authorbibnamehash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{authornamehash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{authorfullhash}{15add3e39676068ac7c32a9a39e5d45d}
      \field{labelalpha}{HL93}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{From the reviews: "The account is quite detailed and is written in a manner that will appeal to analysts and numerical practitioners alike...they contain everything from rigorous proofs to tables of numerical calculations.... one of the strong features of these books...that they are designed not for the expert, but for those who whish to learn the subject matter starting from little or no background...there are numerous examples, and counter-examples, to back up the theory...To my knowledge, no other authors have given such a clear geometric account of convex analysis." "This innovative text is well written, copiously illustrated, and accessible to a wide audience"}
      \field{edition}{1993rd edition}
      \field{isbn}{978-3-540-56852-0}
      \field{month}{10}
      \field{shorttitle}{Convex {Analysis} and {Minimization} {Algorithms} {II}}
      \field{title}{Convex {Analysis} and {Minimization} {Algorithms} {II}: {Advanced} {Theory} and {Bundle} {Methods}}
      \field{year}{1993}
    \endentry
    \entry{hitchcock_expression_1927}{article}{}
      \name{author}{1}{}{%
        {{hash=ff8777b641a9baeb6609898cf9b139b3}{%
           family={Hitchcock},
           familyi={H\bibinitperiod},
           given={Frank\bibnamedelima L.},
           giveni={F\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{fullhash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{bibnamehash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{authorbibnamehash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{authornamehash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{authorfullhash}{ff8777b641a9baeb6609898cf9b139b3}
      \field{labelalpha}{Hit27}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1467-9590}
      \field{journaltitle}{Journal of Mathematics and Physics}
      \field{number}{1-4}
      \field{title}{The {Expression} of a {Tensor} or a {Polyadic} as a {Sum} of {Products}}
      \field{urlday}{9}
      \field{urlmonth}{2}
      \field{urlyear}{2021}
      \field{volume}{6}
      \field{year}{1927}
      \field{urldateera}{ce}
      \field{pages}{164\bibrangedash 189}
      \range{pages}{26}
      \verb{doi}
      \verb https://doi.org/10.1002/sapm192761164
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/UABK5I79/sapm192761164.html:text/html
      \endverb
    \endentry
    \entry{hochreiter_toward_2022}{article}{}
      \name{author}{1}{}{%
        {{hash=41b31e29fb2bdbf9f5c9c1b0d5b3e815}{%
           family={Hochreiter},
           familyi={H\bibinitperiod},
           given={Sepp},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{fullhash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{bibnamehash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{authorbibnamehash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{authornamehash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{authorfullhash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \field{labelalpha}{Hoc22}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0001-0782, 1557-7317}
      \field{journaltitle}{Communications of the ACM}
      \field{month}{1}
      \field{number}{4}
      \field{title}{Toward a broad {AI}}
      \field{urlday}{22}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{65}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{56\bibrangedash 57}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1145/3512715
      \endverb
      \verb{file}
      \verb Hochreiter - 2022 - Toward a broad AI.pdf:/Users/alexgoessmann/Zotero/storage/8ZDZBDY8/Hochreiter - 2022 - Toward a broad AI.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/3512715
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/3512715
      \endverb
    \endentry
    \entry{hogan_knowledge_2021}{book}{}
      \name{author}{10}{}{%
        {{hash=56f2d88e83f41f6e1f0d9a0197c4c7eb}{%
           family={Hogan},
           familyi={H\bibinitperiod},
           given={Aidan},
           giveni={A\bibinitperiod}}}%
        {{hash=019293949e6289df4e3b751f3f9ecf4e}{%
           family={Blomqvist},
           familyi={B\bibinitperiod},
           given={Eva},
           giveni={E\bibinitperiod}}}%
        {{hash=ac9f2c71e19d14efaa82ad1908d35843}{%
           family={Cochez},
           familyi={C\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=3eb1335a7c6a4acf8ca2e632ae7f6a09}{%
           family={d’Amato},
           familyi={d\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod}}}%
        {{hash=96fa7de0e15705cafb281ade1eaecfd0}{%
           family={Melo},
           familyi={M\bibinitperiod},
           given={Gerard\bibnamedelima de},
           giveni={G\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=5feae03b05f14f0a5fd1f3e07badd8e6}{%
           family={Gutierrez},
           familyi={G\bibinitperiod},
           given={Claudio},
           giveni={C\bibinitperiod}}}%
        {{hash=5ddbf3af24e3802f8842794c8c86b60c}{%
           family={Kirrane},
           familyi={K\bibinitperiod},
           given={Sabrina},
           giveni={S\bibinitperiod}}}%
        {{hash=7fe6d31d4006448f174550c3599ec2d2}{%
           family={Gayo},
           familyi={G\bibinitperiod},
           given={Jose\bibnamedelimb Emilio\bibnamedelima Labra},
           giveni={J\bibinitperiod\bibinitdelim E\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=6fe24dce481a67e247581d4b20435709}{%
           family={Navigli},
           familyi={N\bibinitperiod},
           given={Roberto},
           giveni={R\bibinitperiod}}}%
        {{hash=9aabe7235b5cda4ebcca85f21cf40b60}{%
           family={Neumaier},
           familyi={N\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{fullhash}{1aa6c5d2b67b513060a2c6a632b79d9a}
      \strng{bibnamehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{authorbibnamehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{authornamehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{authorfullhash}{1aa6c5d2b67b513060a2c6a632b79d9a}
      \field{labelalpha}{Hog+21}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book provides a comprehensive and accessible introduction to knowledge graphs, which have recently garnered notable attention from both industry and academia. Knowledge graphs are founded on the principle of applying a graph-based abstraction to data, and are now broadly deployed in scenarios that require integrating and extracting value from multiple, diverse sources of data at large scale. The book defines knowledge graphs and provides a high-level overview of how they are used. It presents and contrasts popular graph models that are commonly used to represent data as graphs, and the languages by which they can be queried before describing how the resulting data graph can be enhanced with notions of schema, identity, and context. The book discusses how ontologies and rules can be used to encode knowledge as well as how inductive techniques—based on statistics, graph analytics, machine learning, etc.—can be used to encode and extract knowledge. It covers techniques for the creation, enrichment, assessment, and refinement of knowledge graphs and surveys recent open and enterprise knowledge graphs and the industries or applications within which they have been most widely adopted. The book closes by discussing the current limitations and future directions along which knowledge graphs are likely to evolve. This book is aimed at students, researchers, and practitioners who wish to learn more about knowledge graphs and how they facilitate extracting value from diverse data at large scale. To make the book accessible for newcomers, running examples and graphical notation are used throughout. Formal definitions and extensive references are also provided for those who opt to delve more deeply into specific topics.}
      \field{edition}{1st edition}
      \field{isbn}{978-3-031-00790-3}
      \field{month}{11}
      \field{title}{Knowledge {Graphs}}
      \field{year}{2021}
    \endentry
    \entry{holtz_manifolds_2012}{article}{}
      \name{author}{3}{}{%
        {{hash=609411433449cf1b01b9596d5ea8e17a}{%
           family={Holtz},
           familyi={H\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=9fa4aa09a1f933a0b479542d3d8f02b8}{%
           family={Rohwedder},
           familyi={R\bibinitperiod},
           given={Thorsten},
           giveni={T\bibinitperiod}}}%
        {{hash=cd91fef38e4801ef30c61055e408b8b5}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Reinhold},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{fullhash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{bibnamehash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{authorbibnamehash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{authornamehash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{authorfullhash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \field{labelalpha}{HRS12}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, the format of TT tensors (Hackbusch and Kühn in J Fourier Anal Appl 15:706–722, 2009; Oseledets in SIAM J Sci Comput 2009, submitted; Oseledets and Tyrtyshnikov in SIAM J Sci Comput 31:5, 2009; Oseledets and Tyrtyshnikov in Linear Algebra Appl 2009, submitted) has turned out to be a promising new format for the approximation of solutions of high dimensional problems. In this paper, we prove some new results for the TT representation of a tensor U ∈ Rn1×···×nd and for the manifold of tensors of TT-rank r . As a ﬁrst result, we prove that the TT (or compression) ranks ri of a tensor U are unique and equal to the respective separation ranks of U if the components of the TT decomposition are required to fulﬁl a certain maximal rank condition. We then show that the set T of TT tensors of ﬁxed rank r locally forms an embedded manifold in Rn1×···×nd , therefore preserving the essential theoretical properties of the Tucker format, but often showing an improved scaling behaviour. Extending a similar approach for matrices (Conte and Lubich in M2AN 44:759, 2010), we introduce certain gauge conditions to obtain a unique representation of the tangent space TU T of T and deduce a local parametrization of the TT manifold. The parametrisation of TU T is often crucial for an algorithmic treatment of high-dimensional time-dependent PDEs and minimisation problems (Lubich in From quantum to classical molecular dynamics: reduced methods and numerical analysis, 2008). We conclude with remarks on those applications and present some numerical examples.}
      \field{issn}{0029-599X, 0945-3245}
      \field{journaltitle}{Numerische Mathematik}
      \field{month}{4}
      \field{number}{4}
      \field{title}{On manifolds of tensors of fixed {TT}-rank}
      \field{urlday}{18}
      \field{urlmonth}{12}
      \field{urlyear}{2019}
      \field{volume}{120}
      \field{year}{2012}
      \field{urldateera}{ce}
      \field{pages}{701\bibrangedash 731}
      \range{pages}{31}
      \verb{doi}
      \verb 10.1007/s00211-011-0419-7
      \endverb
      \verb{file}
      \verb Holtz et al. - 2012 - On manifolds of tensors of fixed TT-rank.pdf:/Users/alexgoessmann/Zotero/storage/YQFMECP7/Holtz et al. - 2012 - On manifolds of tensors of fixed TT-rank.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/s00211-011-0419-7
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/s00211-011-0419-7
      \endverb
    \endentry
    \entry{jouppi_tpu_2023}{inproceedings}{}
      \name{author}{14}{}{%
        {{hash=d73f1690b4ba58964edeb173f8284822}{%
           family={Jouppi},
           familyi={J\bibinitperiod},
           given={Norm},
           giveni={N\bibinitperiod}}}%
        {{hash=d115e2de1127519c117a7252205efcdb}{%
           family={Kurian},
           familyi={K\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod}}}%
        {{hash=d8125976d0f2f0c4fddc5c4adeda313d}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Sheng},
           giveni={S\bibinitperiod}}}%
        {{hash=a539c46db76440bd088ebd30ad1927a0}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=79fadc0c81fb0a5cb1f16bb3b3180113}{%
           family={Nagarajan},
           familyi={N\bibinitperiod},
           given={Rahul},
           giveni={R\bibinitperiod}}}%
        {{hash=6d9f86b0b8af0276594dbfd4261598ca}{%
           family={Nai},
           familyi={N\bibinitperiod},
           given={Lifeng},
           giveni={L\bibinitperiod}}}%
        {{hash=98e48fd2ef83c15cb4e696e6243eef05}{%
           family={Patil},
           familyi={P\bibinitperiod},
           given={Nishant},
           giveni={N\bibinitperiod}}}%
        {{hash=1e17c7dbc5693c565cab32541b456a87}{%
           family={Subramanian},
           familyi={S\bibinitperiod},
           given={Suvinay},
           giveni={S\bibinitperiod}}}%
        {{hash=6305d4902371102e8d1f9f933d9c2845}{%
           family={Swing},
           familyi={S\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod}}}%
        {{hash=12cebccf7d831a1b00e38b044c768df4}{%
           family={Towles},
           familyi={T\bibinitperiod},
           given={Brian},
           giveni={B\bibinitperiod}}}%
        {{hash=732617f9246a38995e8cf24fe9ebaaa1}{%
           family={Young},
           familyi={Y\bibinitperiod},
           given={Clifford},
           giveni={C\bibinitperiod}}}%
        {{hash=d1654f226dfdf60d9b41a59d7e0d44d1}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Xiang},
           giveni={X\bibinitperiod}}}%
        {{hash=335463264b30d83d055bbbda6e0ac19b}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Zongwei},
           giveni={Z\bibinitperiod}}}%
        {{hash=95486d328725c1c6a933929c9442fe2a}{%
           family={Patterson},
           familyi={P\bibinitperiod},
           given={David\bibnamedelima A},
           giveni={D\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{11df723acd237366fe67e6bddc0831f8}
      \strng{fullhash}{4b1a2cf5cadf39661555df21f6736642}
      \strng{bibnamehash}{11df723acd237366fe67e6bddc0831f8}
      \strng{authorbibnamehash}{11df723acd237366fe67e6bddc0831f8}
      \strng{authornamehash}{11df723acd237366fe67e6bddc0831f8}
      \strng{authorfullhash}{4b1a2cf5cadf39661555df21f6736642}
      \field{labelalpha}{Jou+23}
      \field{sortinit}{J}
      \field{sortinithash}{fce5f8d0bd05e8d93f3dbe21c78897ca}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are \&lt;5\% of system cost and \&lt;3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of {\textasciitilde}60\% of peak FLOPS/second. For similar sized systems, it is {\textasciitilde}4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use {\textasciitilde}2--6x less energy and produce {\textasciitilde}20x less CO2e than contemporary DSAs in typical on-premise data centers.}
      \field{booktitle}{Proceedings of the 50th {Annual} {International} {Symposium} on {Computer} {Architecture}}
      \field{isbn}{979-8-4007-0095-8}
      \field{month}{6}
      \field{series}{{ISCA} '23}
      \field{shorttitle}{{TPU} v4}
      \field{title}{{TPU} v4: {An} {Optically} {Reconfigurable} {Supercomputer} for {Machine} {Learning} with {Hardware} {Support} for {Embeddings}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 14}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1145/3579371.3589350
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/G3GKWRAE/Jouppi et al. - 2023 - TPU v4 An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Emb.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/3579371.3589350
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/3579371.3589350
      \endverb
    \endentry
    \entry{kolda_tensor_2009}{article}{}
      \name{author}{2}{}{%
        {{hash=a17e10e04fa2d01fdb9bfc9a669b4182}{%
           family={Kolda},
           familyi={K\bibinitperiod},
           given={Tamara\bibnamedelima G.},
           giveni={T\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=e452c0602af86438cb78062b7bac80bb}{%
           family={Bader},
           familyi={B\bibinitperiod},
           given={Brett\bibnamedelima W.},
           giveni={B\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{fullhash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{bibnamehash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{authorbibnamehash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{authornamehash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{authorfullhash}{782cc4ad2fea7afb684072cddcaf4d10}
      \field{labelalpha}{KB09}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.}
      \field{issn}{0036-1445, 1095-7200}
      \field{journaltitle}{SIAM Review}
      \field{month}{8}
      \field{number}{3}
      \field{title}{Tensor {Decompositions} and {Applications}}
      \field{urlday}{28}
      \field{urlmonth}{1}
      \field{urlyear}{2021}
      \field{volume}{51}
      \field{year}{2009}
      \field{urldateera}{ce}
      \field{pages}{455\bibrangedash 500}
      \range{pages}{46}
      \verb{doi}
      \verb 10.1137/07070111X
      \endverb
      \verb{file}
      \verb Kolda und Bader - 2009 - Tensor Decompositions and Applications.pdf:/Users/alexgoessmann/Zotero/storage/IJHSGP4F/Kolda und Bader - 2009 - Tensor Decompositions and Applications.pdf:application/pdf
      \endverb
    \endentry
    \entry{koller_probabilistic_2009}{book}{}
      \name{author}{2}{}{%
        {{hash=9c785bcf6b8a0b99e44695299a6aecb2}{%
           family={Koller},
           familyi={K\bibinitperiod},
           given={Daphne},
           giveni={D\bibinitperiod}}}%
        {{hash=987ee418ae32e261a8a5385e14cda479}{%
           family={Friedman},
           familyi={F\bibinitperiod},
           given={Nir},
           giveni={N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge, Mass.}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{fullhash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{bibnamehash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{authorbibnamehash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{authornamehash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{authorfullhash}{644f618617c37fd03b6bc0248cd308d5}
      \field{labelalpha}{KF09}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions.Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.}
      \field{edition}{1. edition}
      \field{isbn}{978-0-262-01319-2}
      \field{month}{7}
      \field{shorttitle}{Probabilistic {Graphical} {Models}}
      \field{title}{Probabilistic {Graphical} {Models}: {Principles} and {Techniques}}
      \field{year}{2009}
    \endentry
    \entry{kouagou_neural_2022}{misc}{}
      \name{author}{4}{}{%
        {{hash=c94d14254735bb0aa1815bba2883a633}{%
           family={Kouagou},
           familyi={K\bibinitperiod},
           given={N'Dah\bibnamedelima Jean},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=776296590b83e52cd7b5c0a34ed2467b}{%
           family={Heindorf},
           familyi={H\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=0fdd0add34c43fbcb7685ace9926ce07}{%
           family={Demir},
           familyi={D\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=a53ea1e67bff64c9ebe4ffbf19a852d2}{%
           family={Ngomo},
           familyi={N\bibinitperiod},
           given={Axel-Cyrille\bibnamedelima Ngonga},
           giveni={A\bibinithyphendelim C\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{cb853b4fb1a5d516d37b1eb4d9c99b2d}
      \strng{fullhash}{d172fd5714e9bce105e79b8f781f356a}
      \strng{bibnamehash}{cb853b4fb1a5d516d37b1eb4d9c99b2d}
      \strng{authorbibnamehash}{cb853b4fb1a5d516d37b1eb4d9c99b2d}
      \strng{authornamehash}{cb853b4fb1a5d516d37b1eb4d9c99b2d}
      \strng{authorfullhash}{d172fd5714e9bce105e79b8f781f356a}
      \field{extraname}{1}
      \field{labelalpha}{Kou+22}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most existing approaches for class expression learning in description logics are search algorithms. As the search space of these approaches is infinite, they often fail to scale to large learning problems. Our main intuition is that class expression learning can be regarded as a translation problem. Based thereupon, we propose a new family of class expression learning approaches which we dub neural class expression synthesis. Instances of this new family circumvent the high search costs entailed by current algorithms by translating training examples into class expressions in a fashion akin to machine translation solutions. Consequently, they are not subject to the runtime limitations of search-based approaches post training. We study three instances of this novel family of approaches to synthesize class expressions from sets of positive and negative examples. An evaluation of our approach on four benchmark datasets suggests that it can effectively synthesize high-quality class expressions with respect to the input examples in approximately one second on average. Moreover, a comparison to other state-of-the-art approaches suggests that we achieve better F-measures on large datasets. For reproducibility purposes, we provide our implementation as well as pretrained models in our public GitHub repository at https://github.com/fosterreproducibleresearch/NCES.}
      \field{month}{12}
      \field{note}{arXiv:2111.08486 [cs]}
      \field{title}{Neural {Class} {Expression} {Synthesis}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{file}
      \verb Kouagou et al. - 2022 - Neural Class Expression Synthesis.pdf:/Users/alexgoessmann/Zotero/storage/67GHBI5I/Kouagou et al. - 2022 - Neural Class Expression Synthesis.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2111.08486
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2111.08486
      \endverb
      \keyw{Computer Science - Artificial Intelligence}
    \endentry
    \entry{pesquita_neural_2023}{incollection}{}
      \name{author}{4}{}{%
        {{hash=a54332a00b61dd46638bd0d7050c30aa}{%
           family={Kouagou},
           familyi={K\bibinitperiod},
           given={N’Dah\bibnamedelima Jean},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=776296590b83e52cd7b5c0a34ed2467b}{%
           family={Heindorf},
           familyi={H\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=0fdd0add34c43fbcb7685ace9926ce07}{%
           family={Demir},
           familyi={D\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=aaffae6ab501c5d30bd5ba943863c330}{%
           family={Ngonga\bibnamedelima Ngomo},
           familyi={N\bibinitperiod\bibinitdelim N\bibinitperiod},
           given={Axel-Cyrille},
           giveni={A\bibinithyphendelim C\bibinitperiod}}}%
      }
      \name{editor}{8}{}{%
        {{hash=01de40b70b6987089f9a248cabe85202}{%
           family={Pesquita},
           familyi={P\bibinitperiod},
           given={Catia},
           giveni={C\bibinitperiod}}}%
        {{hash=acf0f4df6af5ba7da3c0594e4282b4aa}{%
           family={Jimenez-Ruiz},
           familyi={J\bibinithyphendelim R\bibinitperiod},
           given={Ernesto},
           giveni={E\bibinitperiod}}}%
        {{hash=8befb538a26e9b5b5c5cdc987c0796df}{%
           family={McCusker},
           familyi={M\bibinitperiod},
           given={Jamie},
           giveni={J\bibinitperiod}}}%
        {{hash=fbf83994f4c3aa3f53bcc9a549a96b25}{%
           family={Faria},
           familyi={F\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=2c8f7c698df5ca2bc9477059127b1f21}{%
           family={Dragoni},
           familyi={D\bibinitperiod},
           given={Mauro},
           giveni={M\bibinitperiod}}}%
        {{hash=3b9c611a002db08364a65ed5f2d88056}{%
           family={Dimou},
           familyi={D\bibinitperiod},
           given={Anastasia},
           giveni={A\bibinitperiod}}}%
        {{hash=80e25efccf4a0a840a57cf28aaf4b72b}{%
           family={Troncy},
           familyi={T\bibinitperiod},
           given={Raphael},
           giveni={R\bibinitperiod}}}%
        {{hash=4af9fa5cec1be62b092168e1285d0bb5}{%
           family={Hertling},
           familyi={H\bibinitperiod},
           given={Sven},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer Nature Switzerland}%
      }
      \strng{namehash}{0e8d59226d25c98e0969bac255d595e7}
      \strng{fullhash}{36d34769856e0d044036394d86d97a4c}
      \strng{bibnamehash}{0e8d59226d25c98e0969bac255d595e7}
      \strng{authorbibnamehash}{0e8d59226d25c98e0969bac255d595e7}
      \strng{authornamehash}{0e8d59226d25c98e0969bac255d595e7}
      \strng{authorfullhash}{36d34769856e0d044036394d86d97a4c}
      \strng{editorbibnamehash}{4166c9bb32774750f224250d77e875ab}
      \strng{editornamehash}{4166c9bb32774750f224250d77e875ab}
      \strng{editorfullhash}{1de6e60b957c9b1e8817fd68fcf43244}
      \field{extraname}{2}
      \field{labelalpha}{Kou+23}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{The {Semantic} {Web}}
      \field{isbn}{978-3-031-33454-2 978-3-031-33455-9}
      \field{note}{Series Title: Lecture Notes in Computer Science}
      \field{title}{Neural {Class} {Expression} {Synthesis}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{13870}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{209\bibrangedash 226}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/978-3-031-33455-9_13
      \endverb
      \verb{file}
      \verb Kouagou et al. - 2023 - Neural Class Expression Synthesis.pdf:/Users/alexgoessmann/Zotero/storage/2JW9DENY/Kouagou et al. - 2023 - Neural Class Expression Synthesis.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/10.1007/978-3-031-33455-9_13
      \endverb
      \verb{url}
      \verb https://link.springer.com/10.1007/978-3-031-33455-9_13
      \endverb
    \endentry
    \entry{landsberg_tensors_2011}{book}{}
      \name{author}{1}{}{%
        {{hash=fd6874950b4ff2280f2bcff26f2c7dd5}{%
           family={Landsberg},
           familyi={L\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {American Mathematical Society}%
      }
      \strng{namehash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{fullhash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{bibnamehash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{authorbibnamehash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{authornamehash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{authorfullhash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \field{labelalpha}{Lan11}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Advancing research. Creating connections.}
      \field{isbn}{978-0-8218-6907-9 978-0-8218-8481-2 978-0-8218-8483-6 978-1-4704-0923-4}
      \field{month}{12}
      \field{series}{Graduate {Studies} in {Mathematics}}
      \field{shorttitle}{Tensors}
      \field{title}{Tensors: {Geometry} and {Applications}}
      \field{urlday}{10}
      \field{urlmonth}{2}
      \field{urlyear}{2021}
      \field{volume}{128}
      \field{year}{2011}
      \field{urldateera}{ce}
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/DUUS295M/128.html:text/html
      \endverb
    \endentry
    \entry{lehmann_class_2011}{article}{}
      \name{author}{4}{}{%
        {{hash=045d4294ee6b6936c99d278b857478b4}{%
           family={Lehmann},
           familyi={L\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
        {{hash=74de32830575f67e1479f58f47d2111d}{%
           family={Auer},
           familyi={A\bibinitperiod},
           given={Sören},
           giveni={S\bibinitperiod}}}%
        {{hash=cd0cd9cd5488d7e1a1d8553008b2419e}{%
           family={Bühmann},
           familyi={B\bibinitperiod},
           given={Lorenz},
           giveni={L\bibinitperiod}}}%
        {{hash=bbf3fe6d44f55d043fa4a59e4aa2c555}{%
           family={Tramp},
           familyi={T\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{f3943df71780a02de8efff3344001915}
      \strng{fullhash}{afc665691c357cb9797f3af3e5bdbee3}
      \strng{bibnamehash}{f3943df71780a02de8efff3344001915}
      \strng{authorbibnamehash}{f3943df71780a02de8efff3344001915}
      \strng{authornamehash}{f3943df71780a02de8efff3344001915}
      \strng{authorfullhash}{afc665691c357cb9797f3af3e5bdbee3}
      \field{labelalpha}{Leh+11}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{While the number of knowledge bases in the Semantic Web increases, the maintenance and creation of ontology schemata still remain a challenge. In particular creating class expressions constitutes one of the more demanding aspects of ontology engineering. In this article we describe how to adapt a semi-automatic method for learning OWL class expressions to the ontology engineering use case. Specifically, we describe how to extend an existing learning algorithm for the class learning problem. We perform rigorous performance optimization of the underlying algorithms for providing instant suggestions to the user. We also present two plugins, which use the algorithm, for the popular Protégé and OntoWiki ontology editors and provide a preliminary evaluation on real ontologies.}
      \field{issn}{1570-8268}
      \field{journaltitle}{Journal of Web Semantics}
      \field{month}{3}
      \field{number}{1}
      \field{title}{Class expression learning for ontology engineering}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{9}
      \field{year}{2011}
      \field{urldateera}{ce}
      \field{pages}{71\bibrangedash 81}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1016/j.websem.2011.01.001
      \endverb
      \verb{file}
      \verb ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/FJ2IMSV5/S1570826811000023.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1570826811000023
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1570826811000023
      \endverb
      \keyw{Heuristics,OWL,Concept learning,Ontology editor plugins,Ontology engineering,Supervised machine learning}
    \endentry
    \entry{mackay_information_2003}{book}{}
      \name{author}{1}{}{%
        {{hash=99e8c42965ec01aec3921af3e8ecd658}{%
           family={MacKay},
           familyi={M\bibinitperiod},
           given={David\bibnamedelimb J.\bibnamedelimi C.},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{fullhash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{bibnamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authorbibnamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authornamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authorfullhash}{99e8c42965ec01aec3921af3e8ecd658}
      \field{labelalpha}{Mac03}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Information theory and inference, taught together in this exciting textbook, lie at the heart of many important areas of modern technology - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics and cryptography. The book introduces theory in tandem with applications. Information theory is taught alongside practical communication systems such as arithmetic coding for data compression and sparse-graph codes for error-correction. Inference techniques, including message-passing algorithms, Monte Carlo methods and variational approximations, are developed alongside applications to clustering, convolutional codes, independent component analysis, and neural networks. Uniquely, the book covers state-of-the-art error-correcting codes, including low-density-parity-check codes, turbo codes, and digital fountain codes - the twenty-first-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning, and for undergraduate or graduate courses. It also provides an unparalleled entry point for professionals in areas as diverse as computational biology, financial engineering and machine learning.}
      \field{edition}{Illustrated Edition}
      \field{isbn}{978-0-521-64298-9}
      \field{month}{9}
      \field{title}{Information {Theory}, {Inference} and {Learning} {Algorithms}}
      \field{year}{2003}
    \endentry
    \entry{mackworth_consistency_1977}{article}{}
      \name{author}{1}{}{%
        {{hash=350f48b82a8f1edb46d9d200435480f9}{%
           family={Mackworth},
           familyi={M\bibinitperiod},
           given={Alan\bibnamedelima K.},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \strng{namehash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{fullhash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{bibnamehash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{authorbibnamehash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{authornamehash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{authorfullhash}{350f48b82a8f1edb46d9d200435480f9}
      \field{labelalpha}{Mac77}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Artificial intelligence tasks which can be formulated as constraint satisfaction problems, with which this paper is for the most part concerned, are usually by solved backtracking the examining the thrashing behavior that nearly always accompanies backtracking, identifying three of its causes and proposing remedies for them we are led to a class of algorithms whoch can profitably be used to eliminate local (node, arc and path) inconsistencies before any attempt is made to construct a complete solution. A more general paradigm for attacking these tasks is the altenation of constraint manipulation and case analysis producing an OR problem graph which may be searched in any of the usual ways. Many authors, particularly Montanari and Waltz, have contributed to the development of these ideas; a secondary aim of this paper is to trace that history. The primary aim is to provide an accessible, unified framework, within which to present the algorithms including a new path consistency algorithm, to discuss their relationships and the may applications, both realized and potential of network consistency algorithms.}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{2}
      \field{number}{1}
      \field{title}{Consistency in networks of relations}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{volume}{8}
      \field{year}{1977}
      \field{urldateera}{ce}
      \field{pages}{99\bibrangedash 118}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1016/0004-3702(77)90007-8
      \endverb
      \verb{file}
      \verb ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/YK9L3CVK/0004370277900078.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/0004370277900078
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/0004370277900078
      \endverb
    \endentry
    \entry{marra_statistical_2024}{article}{}
      \name{author}{4}{}{%
        {{hash=31c22669ece71e8badb9d793d5ec710a}{%
           family={Marra},
           familyi={M\bibinitperiod},
           given={Giuseppe},
           giveni={G\bibinitperiod}}}%
        {{hash=12caa4fda26a3cda1f2855a260f35bd4}{%
           family={Dumančić},
           familyi={D\bibinitperiod},
           given={Sebastijan},
           giveni={S\bibinitperiod}}}%
        {{hash=2f35c8160c2583a2c38ae8050688b9bd}{%
           family={Manhaeve},
           familyi={M\bibinitperiod},
           given={Robin},
           giveni={R\bibinitperiod}}}%
        {{hash=d71ef24842e2fd1df4b1989ff0d24ff9}{%
           family={De\bibnamedelima Raedt},
           familyi={D\bibinitperiod\bibinitdelim R\bibinitperiod},
           given={Luc},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{9b10129f97a71170ad23ac4c769a1703}
      \strng{fullhash}{8f1a4091160994d81aefb515ef272e07}
      \strng{bibnamehash}{9b10129f97a71170ad23ac4c769a1703}
      \strng{authorbibnamehash}{9b10129f97a71170ad23ac4c769a1703}
      \strng{authornamehash}{9b10129f97a71170ad23ac4c769a1703}
      \strng{authorfullhash}{8f1a4091160994d81aefb515ef272e07}
      \field{labelalpha}{Mar+24}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{3}
      \field{shorttitle}{From statistical relational to neurosymbolic artificial intelligence}
      \field{title}{From statistical relational to neurosymbolic artificial intelligence: {A} survey}
      \field{urlday}{20}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{328}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{104062}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.artint.2023.104062
      \endverb
      \verb{file}
      \verb ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/NRNUENXF/S0004370223002084.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370223002084
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370223002084
      \endverb
      \keyw{Neurosymbolic AI,Learning and reasoning,Probabilistic logics,Statistical relational AI}
    \endentry
    \entry{mccarthy_programs_1959}{incollection}{}
      \name{author}{1}{}{%
        {{hash=627c3779677d0eaf3312e5b67e8cbe0c}{%
           family={McCarthy},
           familyi={M\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {London}%
      }
      \list{publisher}{1}{%
        {Her Majesty's Stationary Office}%
      }
      \strng{namehash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{fullhash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{bibnamehash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{authorbibnamehash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{authornamehash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{authorfullhash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \field{labelalpha}{McC59}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the {Teddington} {Conference} on the {Mechanization} of {Thought} {Processes}}
      \field{title}{Programs with {Common} {Sense}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{1959}
      \field{urldateera}{ce}
      \field{pages}{75\bibrangedash 91}
      \range{pages}{17}
      \verb{file}
      \verb Programs with Common Sense | BibSonomy:/Users/alexgoessmann/Zotero/storage/XQHW6V6P/73597914e6cc614c92d8eedf4a1dab52.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://www-formal.stanford.edu/jmc/mcc59.html
      \endverb
      \verb{url}
      \verb http://www-formal.stanford.edu/jmc/mcc59.html
      \endverb
    \endentry
    \entry{motzkin_beitrage_1936}{thesis}{}
      \name{author}{1}{}{%
        {{hash=320a51b5f8cb5732a5484001e0b4202a}{%
           family={Motzkin},
           familyi={M\bibinitperiod},
           given={Theodore\bibnamedelima S.},
           giveni={T\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Buchdruckerei Azriel}%
      }
      \list{language}{1}{%
        {ger}%
      }
      \list{location}{1}{%
        {Jerusalem}%
      }
      \strng{namehash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{fullhash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{bibnamehash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{authorbibnamehash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{authornamehash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{authorfullhash}{320a51b5f8cb5732a5484001e0b4202a}
      \field{labelalpha}{Mot36}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{note}{Book Title: Beiträge zur Theorie der linearen Ungleichungen}
      \field{title}{Beiträge zur {Theorie} der linearen {Ungleichungen}}
      \field{type}{phdthesis}
      \field{year}{1936}
    \endentry
    \entry{muggleton_inductive_1994}{article}{}
      \name{author}{2}{}{%
        {{hash=2e8a001498de1734264220eb4a0c92b6}{%
           family={Muggleton},
           familyi={M\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=d71ef24842e2fd1df4b1989ff0d24ff9}{%
           family={De\bibnamedelima Raedt},
           familyi={D\bibinitperiod\bibinitdelim R\bibinitperiod},
           given={Luc},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{fullhash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{bibnamehash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{authorbibnamehash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{authornamehash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{authorfullhash}{a50f5bc8d92eef0c604dae9e424e6241}
      \field{labelalpha}{MD94}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{journaltitle}{The Journal of Logic Programming}
      \field{note}{Publisher: Elsevier}
      \field{shorttitle}{Inductive logic programming}
      \field{title}{Inductive logic programming: {Theory} and methods}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{19}
      \field{year}{1994}
      \field{urldateera}{ce}
      \field{pages}{629\bibrangedash 679}
      \range{pages}{51}
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/0743106694900353
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/0743106694900353
      \endverb
    \endentry
    \entry{murphy_probabilistic_2022}{book}{}
      \name{author}{1}{}{%
        {{hash=99413be56c82adf72b6474dcdf8d3023}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Kevin\bibnamedelima P.},
           giveni={K\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge, Massachusetts London, England}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{fullhash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{bibnamehash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{authorbibnamehash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{authornamehash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{authorfullhash}{99413be56c82adf72b6474dcdf8d3023}
      \field{labelalpha}{Mur22}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A detailed and up-to-date introduction to machine learning, presented through the unifying lens of probabilistic modeling and Bayesian decision theory.This book offers a detailed and up-to-date introduction to machine learning (including deep learning) through the unifying lens of probabilistic modeling and Bayesian decision theory. The book covers mathematical background (including linear algebra and optimization), basic supervised learning (including linear and logistic regression and deep neural networks), as well as more advanced topics (including transfer learning and unsupervised learning). End-of-chapter exercises allow students to apply what they have learned, and an appendix covers notation. Probabilistic Machine Learning grew out of the author’s 2012 book, Machine Learning: A Probabilistic Perspective. More than just a simple update, this is a completely new book that reflects the dramatic developments in the field since 2012, most notably deep learning. In addition, the new book is accompanied by online Python code, using libraries such as scikit-learn, JAX, PyTorch, and Tensorflow, which can be used to reproduce nearly all the figures; this code can be run inside a web browser using cloud-based notebooks, and provides a practical complement to the theoretical topics discussed in the book. This introductory text will be followed by a sequel that covers more advanced topics, taking the same probabilistic approach.}
      \field{isbn}{978-0-262-04682-4}
      \field{month}{3}
      \field{shorttitle}{Probabilistic {Machine} {Learning}}
      \field{title}{Probabilistic {Machine} {Learning}: {An} {Introduction}}
      \field{year}{2022}
    \endentry
    \entry{nickel_three-way_2011}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=9754396c9b0fc2f09bd396146fbf2ef3}{%
           family={Nickel},
           familyi={N\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=7b4bb47afe2e1fc9925f6178ea46b207}{%
           family={Tresp},
           familyi={T\bibinitperiod},
           given={Volker},
           giveni={V\bibinitperiod}}}%
        {{hash=9559fe65ed2c0877cf14a66fe1f8e9b3}{%
           family={Kriegel},
           familyi={K\bibinitperiod},
           given={Hans-Peter},
           giveni={H\bibinithyphendelim P\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Madison, WI, USA}%
      }
      \list{publisher}{1}{%
        {Omnipress}%
      }
      \strng{namehash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{fullhash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{bibnamehash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{authorbibnamehash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{authornamehash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{authorfullhash}{cbc1a834438f98968deed5b0ed65db6c}
      \field{labelalpha}{NTK11}
      \field{sortinit}{N}
      \field{sortinithash}{f7242c3ed3dc50029fca1be76c497c7c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.}
      \field{booktitle}{Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}}
      \field{isbn}{978-1-4503-0619-5}
      \field{month}{6}
      \field{series}{{ICML}'11}
      \field{title}{A three-way model for collective learning on multi-relational data}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2011}
      \field{urldateera}{ce}
      \field{pages}{809\bibrangedash 816}
      \range{pages}{8}
    \endentry
    \entry{nickel_review_2016}{article}{}
      \name{author}{4}{}{%
        {{hash=9754396c9b0fc2f09bd396146fbf2ef3}{%
           family={Nickel},
           familyi={N\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=d01ba1edf5711066e4d420d9df8d8f43}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=7b4bb47afe2e1fc9925f6178ea46b207}{%
           family={Tresp},
           familyi={T\bibinitperiod},
           given={Volker},
           giveni={V\bibinitperiod}}}%
        {{hash=835fa77790b1d554c00910dc1147619d}{%
           family={Gabrilovich},
           familyi={G\bibinitperiod},
           given={Evgeniy},
           giveni={E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{37798d3144a8e42a41d5eb16f42b9e1e}
      \strng{fullhash}{78e825a3c051c43892f860e21527cfa3}
      \strng{bibnamehash}{37798d3144a8e42a41d5eb16f42b9e1e}
      \strng{authorbibnamehash}{37798d3144a8e42a41d5eb16f42b9e1e}
      \strng{authornamehash}{37798d3144a8e42a41d5eb16f42b9e1e}
      \strng{authorfullhash}{78e825a3c051c43892f860e21527cfa3}
      \field{labelalpha}{Nic+16}
      \field{sortinit}{N}
      \field{sortinithash}{f7242c3ed3dc50029fca1be76c497c7c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The ﬁrst is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google’s Knowledge Vault project as an example of such combination.}
      \field{issn}{0018-9219, 1558-2256}
      \field{journaltitle}{Proceedings of the IEEE}
      \field{month}{1}
      \field{number}{1}
      \field{title}{A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{104}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{11\bibrangedash 33}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1109/JPROC.2015.2483592
      \endverb
      \verb{file}
      \verb Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowledge Graphs.pdf:/Users/alexgoessmann/Zotero/storage/SYKDZ9EC/Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowledge Graphs.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/7358050/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/7358050/
      \endverb
    \endentry
    \entry{nikolic_survey_2022}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=792f1ec4ae9f91a2da9265fa20afae88}{%
           family={Nikolić},
           familyi={N\bibinitperiod},
           given={Goran\bibnamedelima S.},
           giveni={G\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=dd6aed843e8af934ad41ce82a5dbee4e}{%
           family={Dimitrijević},
           familyi={D\bibinitperiod},
           given={Bojan\bibnamedelima R.},
           giveni={B\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=9a7228f0144f45d43d9518124f3c8674}{%
           family={Nikolić},
           familyi={N\bibinitperiod},
           given={Tatjana\bibnamedelima R.},
           giveni={T\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=8391265dedbfb6653d4a3a4927f5e82a}{%
           family={Stojcev},
           familyi={S\bibinitperiod},
           given={Mile\bibnamedelima K.},
           giveni={M\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \strng{namehash}{78219851dbcbe8ce0837da8e8089c1c6}
      \strng{fullhash}{405a6ee83447c47de66d8fc7a7b6afc0}
      \strng{bibnamehash}{78219851dbcbe8ce0837da8e8089c1c6}
      \strng{authorbibnamehash}{78219851dbcbe8ce0837da8e8089c1c6}
      \strng{authornamehash}{78219851dbcbe8ce0837da8e8089c1c6}
      \strng{authorfullhash}{405a6ee83447c47de66d8fc7a7b6afc0}
      \field{labelalpha}{Nik+22}
      \field{sortinit}{N}
      \field{sortinithash}{f7242c3ed3dc50029fca1be76c497c7c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The CPU, GPU, and TPU are three different types of processing units. For the overall performance of the computer, the CPU is responsible. For delivering high-end graphics and video quality, the GPU is responsible. Along with the CPU, the GPU is a piece of additional hardware. TPU is used in the field of Artificial Intelligence, Machine Learning, and Deep Learning. Each of the three processing units has its own set of functions. This article may be of help to a reader with aim to understand the distinctions between the CPU, GPU, and TPU processing units.}
      \field{booktitle}{2022 57th {International} {Scientific} {Conference} on {Information}, {Communication} and {Energy} {Systems} and {Technologies} ({ICEST})}
      \field{month}{6}
      \field{shorttitle}{A {Survey} of {Three} {Types} of {Processing} {Units}}
      \field{title}{A {Survey} of {Three} {Types} of {Processing} {Units}: {CPU}, {GPU} and {TPU}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 6}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1109/ICEST55168.2022.9828625
      \endverb
      \verb{file}
      \verb IEEE Xplore Abstract Record:/Users/alexgoessmann/Zotero/storage/346ZFMIA/9828625.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9828625
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9828625
      \endverb
      \keyw{Central Processing Unit,Computer performance,CPU,Deep learning,GPU,Graphics processing units,Hardware,hardware accelerator,Quality assessment,TPU,Video recording}
    \endentry
    \entry{orus_tensor_2019}{article}{}
      \name{author}{1}{}{%
        {{hash=220e5a12680385b58fbfac3e4405944d}{%
           family={Orús},
           familyi={O\bibinitperiod},
           given={Román},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{220e5a12680385b58fbfac3e4405944d}
      \strng{fullhash}{220e5a12680385b58fbfac3e4405944d}
      \strng{bibnamehash}{220e5a12680385b58fbfac3e4405944d}
      \strng{authorbibnamehash}{220e5a12680385b58fbfac3e4405944d}
      \strng{authornamehash}{220e5a12680385b58fbfac3e4405944d}
      \strng{authorfullhash}{220e5a12680385b58fbfac3e4405944d}
      \field{labelalpha}{Orú19}
      \field{sortinit}{O}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Originally developed in the context of condensed-matter physics and based on renormalization group ideas, tensor networks have been revived thanks to quantum information theory and the progress in understanding the role of entanglement in quantum many-body systems. Moreover, tensor network states have turned out to play a key role in other scientific disciplines. In this context, here I provide an overview of the basic concepts and key developments in the field. I briefly discuss the most important tensor network structures and algorithms, together with an outline of advances related to global and gauge symmetries, fermions, topological order, classification of phases, entanglement Hamiltonians, holografic duality, artificial intelligence, the 2D Hubbard model, 2D quantum antiferromagnets, conformal field theory, quantum chemistry, disordered systems and many-body localization.}
      \field{issn}{2522-5820}
      \field{journaltitle}{Nature Reviews Physics}
      \field{month}{9}
      \field{number}{9}
      \field{title}{Tensor networks for complex quantum systems}
      \field{urlday}{5}
      \field{urlmonth}{7}
      \field{urlyear}{2021}
      \field{volume}{1}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{538\bibrangedash 550}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1038/s42254-019-0086-7
      \endverb
      \verb{file}
      \verb Eingereichte Version:/Users/alexgoessmann/Zotero/storage/9FQ9Y6UB/Orús - 2019 - Tensor networks for complex quantum systems.pdf:application/pdf;Full Text PDF:/Users/alexgoessmann/Zotero/storage/SUTVREJ7/Orús - 2019 - Tensor networks for complex quantum systems.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/H88TAZGS/s42254-019-0086-7.html:text/html
      \endverb
      \keyw{Condensed-matter physics,Quantum information,Theoretical physics}
    \endentry
    \entry{oseledets_breaking_2009}{article}{}
      \name{author}{2}{}{%
        {{hash=51eaea8336c314e73559b6b8edb6a970}{%
           family={Oseledets},
           familyi={O\bibinitperiod},
           given={I.\bibnamedelimi V.},
           giveni={I\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=11f730529183e1d7182389f6608ab2f5}{%
           family={Tyrtyshnikov},
           familyi={T\bibinitperiod},
           given={E.\bibnamedelimi E.},
           giveni={E\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{fullhash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{bibnamehash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{authorbibnamehash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{authornamehash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{authorfullhash}{2d13c393e6278022f0d13835d8ad1fe5}
      \field{labelalpha}{OT09}
      \field{sortinit}{O}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{For d-dimensional tensors with possibly large \$d{>}3\$, an hierarchical data structure, called the Tree-Tucker format, is presented as an alternative to the canonical decomposition. It has asymptotically the same (and often even smaller) number of representation parameters and viable stability properties. The approach involves a recursive construction described by a tree with the leafs corresponding to the Tucker decompositions of three-dimensional tensors, and is based on a sequence of SVDs for the recursively obtained unfolding matrices and on the auxiliary dimensions added to the initial “spatial” dimensions. It is shown how this format can be applied to the problem of multidimensional convolution. Convincing numerical examples are given.}
      \field{issn}{1064-8275}
      \field{journaltitle}{SIAM Journal on Scientific Computing}
      \field{month}{1}
      \field{note}{Publisher: Society for Industrial and Applied Mathematics}
      \field{number}{5}
      \field{title}{Breaking the {Curse} of {Dimensionality}, {Or} {How} to {Use} {SVD} in {Many} {Dimensions}}
      \field{urlday}{12}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{volume}{31}
      \field{year}{2009}
      \field{urldateera}{ce}
      \field{pages}{3744\bibrangedash 3759}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1137/090748330
      \endverb
      \verb{file}
      \verb Eingereichte Version:/Users/alexgoessmann/Zotero/storage/PAFGIT3R/Oseledets und Tyrtyshnikov - 2009 - Breaking the Curse of Dimensionality, Or How to Us.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/IADYGZMV/090748330.html:text/html
      \endverb
    \endentry
    \entry{paszke_pytorch_2019}{misc}{}
      \name{author}{21}{}{%
        {{hash=56bf0b340039cf8594436a624ff548a9}{%
           family={Paszke},
           familyi={P\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=4ba5062e5919c814aceec188d54c01f2}{%
           family={Gross},
           familyi={G\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
        {{hash=e5dfae4582081d649e3a0d5342050016}{%
           family={Massa},
           familyi={M\bibinitperiod},
           given={Francisco},
           giveni={F\bibinitperiod}}}%
        {{hash=b5815e1692fa2d0c1f44eecf509bd7c4}{%
           family={Lerer},
           familyi={L\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=b75383e6b48c8360c7a60031424c85cf}{%
           family={Bradbury},
           familyi={B\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=f897ed422c34d95af2e22778dfc2607e}{%
           family={Chanan},
           familyi={C\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=046269e070246feb6f394141db80ed87}{%
           family={Killeen},
           familyi={K\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=c40352c194e60a3ef458ee7e8685afb5}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Zeming},
           giveni={Z\bibinitperiod}}}%
        {{hash=6e45f49ec618e619efad90c8e8a61f0c}{%
           family={Gimelshein},
           familyi={G\bibinitperiod},
           given={Natalia},
           giveni={N\bibinitperiod}}}%
        {{hash=f65a80959d520337ae99a0798515036c}{%
           family={Antiga},
           familyi={A\bibinitperiod},
           given={Luca},
           giveni={L\bibinitperiod}}}%
        {{hash=954cf7680b6ce14813973eccdca3c4bc}{%
           family={Desmaison},
           familyi={D\bibinitperiod},
           given={Alban},
           giveni={A\bibinitperiod}}}%
        {{hash=048232cf7c525fbc0bc93052fe8cee03}{%
           family={Köpf},
           familyi={K\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
        {{hash=b9e701339e56fd0b171145b08288a1b7}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=42ac264897098b400e1367e5922c9b0d}{%
           family={DeVito},
           familyi={D\bibinitperiod},
           given={Zach},
           giveni={Z\bibinitperiod}}}%
        {{hash=d814afaa50b9e22ab92cc9f8f9a9e43a}{%
           family={Raison},
           familyi={R\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=3feeeebee8583ecc208f7fb3e0a55068}{%
           family={Tejani},
           familyi={T\bibinitperiod},
           given={Alykhan},
           giveni={A\bibinitperiod}}}%
        {{hash=e18536d5cb7543731fbf2ca1a4908732}{%
           family={Chilamkurthy},
           familyi={C\bibinitperiod},
           given={Sasank},
           giveni={S\bibinitperiod}}}%
        {{hash=0a0b028c6b85c46f368317d0c5bfe3a0}{%
           family={Steiner},
           familyi={S\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod}}}%
        {{hash=998a001f16bb57c079c1d5afb1cb02c8}{%
           family={Fang},
           familyi={F\bibinitperiod},
           given={Lu},
           giveni={L\bibinitperiod}}}%
        {{hash=3f19c633bbfb847db6a0e71d3659eacd}{%
           family={Bai},
           familyi={B\bibinitperiod},
           given={Junjie},
           giveni={J\bibinitperiod}}}%
        {{hash=8ef51a0906e47d2b4472c4e714ed598f}{%
           family={Chintala},
           familyi={C\bibinitperiod},
           given={Soumith},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{fullhash}{4842db6c92a33147f588935fdde44a69}
      \strng{bibnamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorbibnamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authornamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorfullhash}{4842db6c92a33147f588935fdde44a69}
      \field{labelalpha}{Pas+19}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.}
      \field{month}{12}
      \field{note}{arXiv:1912.01703 [cs]}
      \field{shorttitle}{{PyTorch}}
      \field{title}{{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1912.01703
      \endverb
      \verb{file}
      \verb Preprint PDF:/Users/alexgoessmann/Zotero/storage/Q7NZBUK6/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/NEDNHQYI/1912.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1912.01703
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1912.01703
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Mathematical Software}
    \endentry
    \entry{pearl_probabilistic_1988}{book}{}
      \name{author}{1}{}{%
        {{hash=809f695b398afbb54b544c49e8d1bbbb}{%
           family={Pearl},
           familyi={P\bibinitperiod},
           given={Judea},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {s.l.}%
      }
      \list{publisher}{1}{%
        {Morgan Kaufmann}%
      }
      \strng{namehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{fullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{bibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorbibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authornamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorfullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \field{extraname}{1}
      \field{labelalpha}{Pea88}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Probabilistic Reasoning in Intelligent Systems is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty-and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition-in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.}
      \field{isbn}{978-1-55860-479-7}
      \field{month}{9}
      \field{shorttitle}{Probabilistic {Reasoning} in {Intelligent} {Systems}}
      \field{title}{Probabilistic {Reasoning} in {Intelligent} {Systems}: {Networks} of {Plausible} {Inference}}
      \field{year}{1988}
    \endentry
    \entry{pearl_causality_2009}{book}{}
      \name{author}{1}{}{%
        {{hash=809f695b398afbb54b544c49e8d1bbbb}{%
           family={Pearl},
           familyi={P\bibinitperiod},
           given={Judea},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Cambridge New York, NY Port Melbourne New Delhi Singapore}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{fullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{bibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorbibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authornamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorfullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \field{extraname}{2}
      \field{labelalpha}{Pea09}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.}
      \field{edition}{2}
      \field{isbn}{978-0-521-89560-6}
      \field{month}{11}
      \field{shorttitle}{Causality}
      \field{title}{Causality: {Models}, {Reasoning} and {Inference}. {Ausgezeichnet}: {ACM} {Turing} {Award} for {Transforming} {Artificial} {Intelligence} 2011}
      \field{year}{2009}
    \endentry
    \entry{penrose_spinors_1987}{book}{}
      \name{author}{1}{}{%
        {{hash=f51dfd339a12125d0f82a2d93e66d9b7}{%
           family={Penrose},
           familyi={P\bibinitperiod},
           given={Roger},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{fullhash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{bibnamehash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{authorbibnamehash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{authornamehash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{authorfullhash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \field{labelalpha}{Pen87}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This volume introduces and systematically develops the calculus of 2-spinors. This is the first detailed exposition of this technique which leads not only to a deeper understanding of the structure of space-time, but also provides shortcuts to some very tedious calculations. Many results are given here for the first time.}
      \field{isbn}{978-0-521-33707-6}
      \field{month}{2}
      \field{shorttitle}{Spinors and {Space}-{Time}}
      \field{title}{Spinors and {Space}-{Time}: {Volume} 1, {Two}-{Spinor} {Calculus} and {Relativistic} {Fields}}
      \field{year}{1987}
    \endentry
    \entry{perez-garcia_matrix_2007}{article}{}
      \name{author}{4}{}{%
        {{hash=99ab157e40dc6cc9e7c8ca6aef64277a}{%
           family={Perez-Garcia},
           familyi={P\bibinithyphendelim G\bibinitperiod},
           given={D.},
           giveni={D\bibinitperiod}}}%
        {{hash=085ab1d28e21b822e7bebcfb7d68f99f}{%
           family={Verstraete},
           familyi={V\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=4bf6e0e0b644336c6c5e578a60aa21aa}{%
           family={Wolf},
           familyi={W\bibinitperiod},
           given={M.\bibnamedelimi M.},
           giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=e95cc0745e2fbc83da963e5d6ab9c104}{%
           family={Cirac},
           familyi={C\bibinitperiod},
           given={J.\bibnamedelimi I.},
           giveni={J\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \strng{namehash}{82a32fa9d2c9290465d1f60ba6e159a0}
      \strng{fullhash}{e51d433ca5c53cb17c9242ffea109fc3}
      \strng{bibnamehash}{82a32fa9d2c9290465d1f60ba6e159a0}
      \strng{authorbibnamehash}{82a32fa9d2c9290465d1f60ba6e159a0}
      \strng{authornamehash}{82a32fa9d2c9290465d1f60ba6e159a0}
      \strng{authorfullhash}{e51d433ca5c53cb17c9242ffea109fc3}
      \field{labelalpha}{Per+07}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This work gives a detailed investigation of matrix product state (MPS) representations for pure multipartite quantum states. We determine the freedom in representations with and without translation symmetry, derive respective canonical forms and provide efficient methods for obtaining them. Results on frustration free Hamiltonians and the generation of MPS are extended, and the use of the MPS-representation for classical simulations of quantum systems is discussed.}
      \field{issn}{1533-7146}
      \field{journaltitle}{Quantum Information \& Computation}
      \field{month}{7}
      \field{number}{5}
      \field{title}{Matrix product state representations}
      \field{volume}{7}
      \field{year}{2007}
      \field{pages}{401\bibrangedash 430}
      \range{pages}{30}
    \endentry
    \entry{puljak_tn4ml_2025}{misc}{}
      \name{author}{6}{}{%
        {{hash=5b0d2c5fe3ec56c4f02c712c175c0a1c}{%
           family={Puljak},
           familyi={P\bibinitperiod},
           given={Ema},
           giveni={E\bibinitperiod}}}%
        {{hash=3bf65995782a01f1090447d046b9c277}{%
           family={Sanchez-Ramirez},
           familyi={S\bibinithyphendelim R\bibinitperiod},
           given={Sergio},
           giveni={S\bibinitperiod}}}%
        {{hash=1fccb051da8e9ecc280dfe648edef35e}{%
           family={Masot-Llima},
           familyi={M\bibinithyphendelim L\bibinitperiod},
           given={Sergi},
           giveni={S\bibinitperiod}}}%
        {{hash=560618423ca0bbd53e34893a4fc5f464}{%
           family={Vallès-Muns},
           familyi={V\bibinithyphendelim M\bibinitperiod},
           given={Jofre},
           giveni={J\bibinitperiod}}}%
        {{hash=e27c89b3acba42f3f93dbcd9eeb3cf18}{%
           family={Garcia-Saez},
           familyi={G\bibinithyphendelim S\bibinitperiod},
           given={Artur},
           giveni={A\bibinitperiod}}}%
        {{hash=1c22a097d7d81eb61830498bc7f766dc}{%
           family={Pierini},
           familyi={P\bibinitperiod},
           given={Maurizio},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{3f9f538bee76744434e035cd84a448c6}
      \strng{fullhash}{89520fcc07c2ed28387c51d25409e780}
      \strng{bibnamehash}{3f9f538bee76744434e035cd84a448c6}
      \strng{authorbibnamehash}{3f9f538bee76744434e035cd84a448c6}
      \strng{authornamehash}{3f9f538bee76744434e035cd84a448c6}
      \strng{authorfullhash}{89520fcc07c2ed28387c51d25409e780}
      \field{labelalpha}{Pul+25}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Tensor Networks have emerged as a prominent alternative to neural networks for addressing Machine Learning challenges in foundational sciences, paving the way for their applications to real-life problems. This paper introduces tn4ml, a novel library designed to seamlessly integrate Tensor Networks into optimization pipelines for Machine Learning tasks. Inspired by existing Machine Learning frameworks, the library offers a user-friendly structure with modules for data embedding, objective function definition, and model training using diverse optimization strategies. We demonstrate its versatility through two examples: supervised learning on tabular data and unsupervised learning on an image dataset. Additionally, we analyze how customizing the parts of the Machine Learning pipeline for Tensor Networks influences performance metrics.}
      \field{month}{2}
      \field{note}{arXiv:2502.13090 [cs]}
      \field{shorttitle}{tn4ml}
      \field{title}{tn4ml: {Tensor} {Network} {Training} and {Customization} for {Machine} {Learning}}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2502.13090
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2502.13090
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2502.13090
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Mathematical Software,Quantum Physics}
    \endentry
    \entry{richardson_markov_2006}{article}{}
      \name{author}{2}{}{%
        {{hash=f96c5999895363061e2fe84b4cd057df}{%
           family={Richardson},
           familyi={R\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=926269a097c38456f5b32d7efb1d1a80}{%
           family={Domingos},
           familyi={D\bibinitperiod},
           given={Pedro},
           giveni={P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{fullhash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{bibnamehash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{authorbibnamehash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{authornamehash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{authorfullhash}{1021fc8259bc734b2f6116bc44be825e}
      \field{labelalpha}{RD06}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a simple approach to combining ﬁrst-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a ﬁrst-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it speciﬁes a ground Markov network containing one feature for each possible grounding of a ﬁrst-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efﬁciently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.}
      \field{issn}{0885-6125, 1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{month}{2}
      \field{number}{1-2}
      \field{title}{Markov logic networks}
      \field{urlday}{14}
      \field{urlmonth}{1}
      \field{urlyear}{2023}
      \field{volume}{62}
      \field{year}{2006}
      \field{urldateera}{ce}
      \field{pages}{107\bibrangedash 136}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1007/s10994-006-5833-1
      \endverb
      \verb{file}
      \verb Richardson und Domingos - 2006 - Markov logic networks.pdf:/Users/alexgoessmann/Zotero/storage/AJNEGGHC/Richardson und Domingos - 2006 - Markov logic networks.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/s10994-006-5833-1
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/s10994-006-5833-1
      \endverb
    \endentry
    \entry{robeva_duality_2019}{article}{}
      \name{author}{2}{}{%
        {{hash=fb455355b6019b9c510857c2cfba2bf8}{%
           family={Robeva},
           familyi={R\bibinitperiod},
           given={Elina},
           giveni={E\bibinitperiod}}}%
        {{hash=7f5aa105b6bbae27a02d4b92bd405efc}{%
           family={Seigal},
           familyi={S\bibinitperiod},
           given={Anna},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{5ac373fae89445f66b40cfee543db403}
      \strng{fullhash}{5ac373fae89445f66b40cfee543db403}
      \strng{bibnamehash}{5ac373fae89445f66b40cfee543db403}
      \strng{authorbibnamehash}{5ac373fae89445f66b40cfee543db403}
      \strng{authornamehash}{5ac373fae89445f66b40cfee543db403}
      \strng{authorfullhash}{5ac373fae89445f66b40cfee543db403}
      \field{labelalpha}{RS19}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this article we show the duality between tensor networks and undirected graphical models with discrete variables. We study tensor networks on hypergraphs, which we call tensor hypernetworks. We show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph. We translate various notions under duality. For example, marginalization in a graphical model is dual to contraction in the tensor network. Algorithms also translate under duality. We show that belief propagation corresponds to a known algorithm for tensor network contraction. This article is a reminder that the research areas of graphical models and tensor networks can benefit from interaction.}
      \field{issn}{2049-8772}
      \field{journaltitle}{Information and Inference: A Journal of the IMA}
      \field{month}{6}
      \field{number}{2}
      \field{title}{Duality of graphical models and tensor networks}
      \field{urlday}{6}
      \field{urlmonth}{7}
      \field{urlyear}{2021}
      \field{volume}{8}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{273\bibrangedash 288}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1093/imaiai/iay009
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/EFRFIZTN/Robeva und Seigal - 2019 - Duality of graphical models and tensor networks.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/5X57ZXMD/5041985.html:text/html
      \endverb
    \endentry
    \entry{rockafellar_convex_1997}{book}{}
      \name{author}{1}{}{%
        {{hash=b5c9101ddde9e15784b1c0d7ca7b7221}{%
           family={Rockafellar},
           familyi={R\bibinitperiod},
           given={Ralph\bibnamedelima Tyrell},
           giveni={R\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Princeton}%
      }
      \list{publisher}{1}{%
        {Princeton University Press}%
      }
      \strng{namehash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{fullhash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{bibnamehash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{authorbibnamehash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{authornamehash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{authorfullhash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \field{labelalpha}{Roc97}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Available for the first time in paperback, R. Tyrrell Rockafellar's classic study presents readers with a coherent branch of nonlinear mathematical analysis that is especially suited to the study of optimization problems. Rockafellar's theory differs from classical analysis in that differentiability assumptions are replaced by convexity assumptions. The topics treated in this volume include: systems of inequalities, the minimum or maximum of a convex function over a convex set, Lagrange multipliers, minimax theorems and duality, as well as basic results about the structure of convex sets and the continuity and differentiability of convex functions and saddle- functions. This book has firmly established a new and vital area not only for pure mathematics but also for applications to economics and engineering. A sound knowledge of linear algebra and introductory real analysis should provide readers with sufficient background for this book. There is also a guide for the reader who may be using the book as an introduction, indicating which parts are essential and which may be skipped on a first reading.}
      \field{edition}{reprint edition}
      \field{isbn}{978-0-691-01586-6}
      \field{month}{1}
      \field{title}{Convex {Analysis}}
      \field{year}{1997}
    \endentry
    \entry{rudolph_foundations_2011}{incollection}{}
      \name{author}{1}{}{%
        {{hash=4408b0bc9147789330e0059378241c8f}{%
           family={Rudolph},
           familyi={R\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \name{editor}{7}{}{%
        {{hash=a2be0ec850b04b1f5e9fe3aca6bf5226}{%
           family={Polleres},
           familyi={P\bibinitperiod},
           given={Axel},
           giveni={A\bibinitperiod}}}%
        {{hash=3eb1335a7c6a4acf8ca2e632ae7f6a09}{%
           family={d’Amato},
           familyi={d\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod}}}%
        {{hash=9282d63c7c5bda72aad9b39bc68fddeb}{%
           family={Arenas},
           familyi={A\bibinitperiod},
           given={Marcelo},
           giveni={M\bibinitperiod}}}%
        {{hash=1c08b5520041a41764ad0bc82b33746f}{%
           family={Handschuh},
           familyi={H\bibinitperiod},
           given={Siegfried},
           giveni={S\bibinitperiod}}}%
        {{hash=a3e7bd065215c1374025027954e59068}{%
           family={Kroner},
           familyi={K\bibinitperiod},
           given={Paula},
           giveni={P\bibinitperiod}}}%
        {{hash=100253a17b68cb3ad272498cd158b79f}{%
           family={Ossowski},
           familyi={O\bibinitperiod},
           given={Sascha},
           giveni={S\bibinitperiod}}}%
        {{hash=126c61ec2fb5ce6ac5e3bf5d81a405c9}{%
           family={Patel-Schneider},
           familyi={P\bibinithyphendelim S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{4408b0bc9147789330e0059378241c8f}
      \strng{fullhash}{4408b0bc9147789330e0059378241c8f}
      \strng{bibnamehash}{4408b0bc9147789330e0059378241c8f}
      \strng{authorbibnamehash}{4408b0bc9147789330e0059378241c8f}
      \strng{authornamehash}{4408b0bc9147789330e0059378241c8f}
      \strng{authorfullhash}{4408b0bc9147789330e0059378241c8f}
      \strng{editorbibnamehash}{1b6e04195267c58a21ad7803e856576f}
      \strng{editornamehash}{1b6e04195267c58a21ad7803e856576f}
      \strng{editorfullhash}{6e3175baa3af8695e784af0948525630}
      \field{labelalpha}{Rud11}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This chapter accompanies the foundational lecture on Description Logics (DLs) at the 7th Reasoning Web Summer School in Galway, Ireland, 2011. It introduces basic notions and facts about this family of logics which has significantly gained in importance over the recent years as these logics constitute the formal basis for today’s most expressive ontology languages, the OWL (Web Ontology Language) family.}
      \field{booktitle}{Reasoning {Web}. {Semantic} {Technologies} for the {Web} of {Data}: 7th {International} {Summer} {School} 2011, {Galway}, {Ireland}, {August} 23-27, 2011, {Tutorial} {Lectures}}
      \field{isbn}{978-3-642-23032-5}
      \field{title}{Foundations of {Description} {Logics}}
      \field{urlday}{14}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2011}
      \field{urldateera}{ce}
      \field{pages}{76\bibrangedash 136}
      \range{pages}{61}
      \verb{doi}
      \verb 10.1007/978-3-642-23032-5_2
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-642-23032-5_2
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-642-23032-5_2
      \endverb
    \endentry
    \entry{russell_artificial_2021}{book}{}
      \name{author}{2}{}{%
        {{hash=143fa183327d9fcd9de18eec99d6ca97}{%
           family={Russell},
           familyi={R\bibinitperiod},
           given={Stuart},
           giveni={S\bibinitperiod}}}%
        {{hash=5de798d5fa3c0236c0478134cd23f52a}{%
           family={Norvig},
           familyi={N\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Boston}%
      }
      \list{publisher}{1}{%
        {Pearson}%
      }
      \strng{namehash}{b280605b721b4ebcba5395298499f924}
      \strng{fullhash}{b280605b721b4ebcba5395298499f924}
      \strng{bibnamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authorbibnamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authornamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authorfullhash}{b280605b721b4ebcba5395298499f924}
      \field{labelalpha}{RN21}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Thelong-anticipated revision of ArtificialIntelligence: A Modern Approach explores the full breadth and depth of the field of artificialintelligence (AI). The 4th Edition brings readers up to date on the latest technologies,presents concepts in a more unified manner, and offers new or expanded coverageof machine learning, deep learning, transfer learning, multi agent systems,robotics, natural language processing, causality, probabilistic programming,privacy, fairness, and safe AI.}
      \field{edition}{4}
      \field{isbn}{978-1-292-40113-3}
      \field{month}{5}
      \field{shorttitle}{Artificial {Intelligence}}
      \field{title}{Artificial {Intelligence}: {A} {Modern} {Approach}, {Global} {Edition}: {A} {Modern} {Approach}, {Global} {Edition}}
      \field{year}{2021}
    \endentry
    \entry{li_linear_2017}{incollection}{}
      \name{author}{3}{}{%
        {{hash=4585c2764c84ad855c78ef492089cd89}{%
           family={Sakama},
           familyi={S\bibinitperiod},
           given={Chiaki},
           giveni={C\bibinitperiod}}}%
        {{hash=1cc5d773bcbcfb4748990f19a2aaf2fe}{%
           family={Inoue},
           familyi={I\bibinitperiod},
           given={Katsumi},
           giveni={K\bibinitperiod}}}%
        {{hash=1a99e9f5aaf14b0a1e707546a6a0763b}{%
           family={Sato},
           familyi={S\bibinitperiod},
           given={Taisuke},
           giveni={T\bibinitperiod}}}%
      }
      \name{editor}{5}{}{%
        {{hash=56ba559f18e74cf681fe113aba3fa9af}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Gang},
           giveni={G\bibinitperiod}}}%
        {{hash=85670d97bd4b38184f6db06c023a19e6}{%
           family={Ge},
           familyi={G\bibinitperiod},
           given={Yong},
           giveni={Y\bibinitperiod}}}%
        {{hash=4b5e80777fd7e5d6b45a520f4dabe35e}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Zili},
           giveni={Z\bibinitperiod}}}%
        {{hash=59ead1ce694d996ebe61da34178792ee}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Zhi},
           giveni={Z\bibinitperiod}}}%
        {{hash=4f9200560583acf9c9918dc90828a228}{%
           family={Blumenstein},
           familyi={B\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{2fdcc809da537861747cf27964e41ec3}
      \strng{fullhash}{2fdcc809da537861747cf27964e41ec3}
      \strng{bibnamehash}{2fdcc809da537861747cf27964e41ec3}
      \strng{authorbibnamehash}{2fdcc809da537861747cf27964e41ec3}
      \strng{authornamehash}{2fdcc809da537861747cf27964e41ec3}
      \strng{authorfullhash}{2fdcc809da537861747cf27964e41ec3}
      \strng{editorbibnamehash}{4bc5fe7e313cd37e7052af92d5b78ecd}
      \strng{editornamehash}{4bc5fe7e313cd37e7052af92d5b78ecd}
      \strng{editorfullhash}{e71855149b8692c242bc88d7e7a460c5}
      \field{labelalpha}{SIS17}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper introduces a novel approach for computing logic programming semantics based on multilinear algebra. First, a propositional Herbrand base is represented in a vector space and if-then rules in a program are encoded in a matrix. Then we provide methods of computing the least model of a Horn logic program, minimal models of a disjunctive logic program, and stable models of a normal logic program by algebraic manipulation of higher-order tensors. The result of this paper exploits a new connection between linear algebraic computation and symbolic computation, which has potential to realize logical inference in huge scale of knowledge bases.}
      \field{booktitle}{Knowledge {Science}, {Engineering} and {Management}}
      \field{isbn}{978-3-319-63557-6 978-3-319-63558-3}
      \field{note}{Series Title: Lecture Notes in Computer Science}
      \field{title}{Linear {Algebraic} {Characterization} of {Logic} {Programs}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{10412}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{520\bibrangedash 533}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/978-3-319-63558-3_44
      \endverb
      \verb{file}
      \verb Sakama et al. - 2017 - Linear Algebraic Characterization of Logic Programs.pdf:/Users/alexgoessmann/Zotero/storage/5FT67GGU/Sakama et al. - 2017 - Linear Algebraic Characterization of Logic Programs.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/978-3-319-63558-3_44
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/978-3-319-63558-3_44
      \endverb
    \endentry
    \entry{sander_large-scale_2025}{misc}{}
      \name{author}{9}{}{%
        {{hash=934a98c4d9d2154d39d3caac04c17ebf}{%
           family={Sander},
           familyi={S\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
        {{hash=8c66ced8a43a8802c4d608df4e037412}{%
           family={Fröhlich},
           familyi={F\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=4d2fe2daf40a829cbad79f3cf60017df}{%
           family={Eigel},
           familyi={E\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=878172bfd75f93a96d3646d7b66c3e48}{%
           family={Eisert},
           familyi={E\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
        {{hash=aae598c66790875fb3f97e9e696f287f}{%
           family={Gelß},
           familyi={G\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
        {{hash=dd1462c8449f0d93707a2a574ae0fa01}{%
           family={Hintermüller},
           familyi={H\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=dd7853bf2542178e88a9b8779c5553e7}{%
           family={Milbradt},
           familyi={M\bibinitperiod},
           given={Richard\bibnamedelima M.},
           giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=942bfb89eecd82018713153f8561459b}{%
           family={Wille},
           familyi={W\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=a0392e761cae0a163d14dc6b48494423}{%
           family={Mendl},
           familyi={M\bibinitperiod},
           given={Christian\bibnamedelima B.},
           giveni={C\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{f53c1f6c2284fc307d3218c1bec37d8d}
      \strng{fullhash}{f7ed6a16ffddf61cd3e32c78f0f9613c}
      \strng{bibnamehash}{f53c1f6c2284fc307d3218c1bec37d8d}
      \strng{authorbibnamehash}{f53c1f6c2284fc307d3218c1bec37d8d}
      \strng{authornamehash}{f53c1f6c2284fc307d3218c1bec37d8d}
      \strng{authorfullhash}{f7ed6a16ffddf61cd3e32c78f0f9613c}
      \field{labelalpha}{San+25}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Understanding the precise interaction mechanisms between quantum systems and their environment is crucial for advancing stable quantum technologies, designing reliable experimental frameworks, and building accurate models of real-world phenomena. However, simulating open quantum systems, which feature complex non-unitary dynamics, poses significant computational challenges that require innovative methods to overcome. In this work, we introduce the tensor jump method (TJM), a scalable, embarrassingly parallel algorithm for stochastically simulating large-scale open quantum systems, specifically Markovian dynamics captured by Lindbladians. This method is built on three core principles where, in particular, we extend the Monte Carlo wave function (MCWF) method to matrix product states, use a dynamic time-dependent variational principle (TDVP) to significantly reduce errors during time evolution, and introduce what we call a sampling MPS to drastically reduce the dependence on the simulation's time step size. We demonstrate that this method scales more effectively than previous methods and ensures convergence to the Lindbladian solution independent of system size, which we show both rigorously and numerically. Finally, we provide evidence of its utility by simulating Lindbladian dynamics of XXX Heisenberg models up to a thousand spins using a consumer-grade CPU. This work represents a significant step forward in the simulation of large-scale open quantum systems, with the potential to enable discoveries across various domains of quantum physics, particularly those where the environment plays a fundamental role, and to both dequantize and facilitate the development of more stable quantum hardware.}
      \field{month}{1}
      \field{note}{arXiv:2501.17913 [quant-ph]}
      \field{title}{Large-scale stochastic simulation of open quantum systems}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2501.17913
      \endverb
      \verb{file}
      \verb Preprint PDF:/Users/alexgoessmann/Zotero/storage/FZI4UPFY/Sander et al. - 2025 - Large-scale stochastic simulation of open quantum systems.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/JGQC3S4N/2501.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2501.17913
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2501.17913
      \endverb
      \keyw{Quantum Physics,Condensed Matter - Other Condensed Matter}
    \endentry
    \entry{sarker_neuro-symbolic_2022}{article}{}
      \name{author}{4}{}{%
        {{hash=8058d6e1f797c6dfb26ccc11667983bf}{%
           family={Sarker},
           familyi={S\bibinitperiod},
           given={Md\bibnamedelima Kamruzzaman},
           giveni={M\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=b72846c7bce9dc4527c22588488d735a}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Lu},
           giveni={L\bibinitperiod}}}%
        {{hash=233e223f7d899ce0eecb0211d820a0dc}{%
           family={Eberhart},
           familyi={E\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
        {{hash=6c91a433a7cc7ab507b9bd279fa64cac}{%
           family={Hitzler},
           familyi={H\bibinitperiod},
           given={Pascal},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{b777d932502942548aa3fbaacb50e4f9}
      \strng{fullhash}{5365057c633ea9eb768b225849fcd208}
      \strng{bibnamehash}{b777d932502942548aa3fbaacb50e4f9}
      \strng{authorbibnamehash}{b777d932502942548aa3fbaacb50e4f9}
      \strng{authornamehash}{b777d932502942548aa3fbaacb50e4f9}
      \strng{authorfullhash}{5365057c633ea9eb768b225849fcd208}
      \field{labelalpha}{Sar+22}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Neuro-Symbolic Artificial Intelligence – the combination of symbolic methods with methods that are based on artificial neural networks – has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.}
      \field{issn}{18758452, 09217126}
      \field{journaltitle}{AI Communications}
      \field{month}{3}
      \field{number}{3}
      \field{shorttitle}{Neuro-symbolic artificial intelligence}
      \field{title}{Neuro-symbolic artificial intelligence: {Current} trends}
      \field{urlday}{17}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{34}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{197\bibrangedash 209}
      \range{pages}{13}
      \verb{doi}
      \verb 10.3233/AIC-210084
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/DT6NNMMP/Sarker et al. - 2022 - Neuro-symbolic artificial intelligence Current trends.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AIC-210084
      \endverb
      \verb{url}
      \verb https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AIC-210084
      \endverb
    \endentry
    \entry{sato_linear_2017}{article}{}
      \name{author}{1}{}{%
        {{hash=1a99e9f5aaf14b0a1e707546a6a0763b}{%
           family={Sato},
           familyi={S\bibinitperiod},
           given={Taisuke},
           giveni={T\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{fullhash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{bibnamehash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{authorbibnamehash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{authornamehash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{authorfullhash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \field{labelalpha}{Sat17}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a fundamentally new approach to Datalog evaluation. Given a linear Datalog program DB written using N constants and binary predicates, we first translate if-and-only-if completions of clauses in DB into a set E q (DB) of matrix equations with a non-linear operation, where relations in M DB, the least Herbrand model of DB, are encoded as adjacency matrices. We then translate E q (DB) into another, but purely linear matrix equations Ẽ q (DB). It is proved that the least solution of Ẽ q (DB) in the sense of matrix ordering is converted to the least solution of E q (DB) and the latter gives M DB as a set of adjacency matrices. Hence, computing the least solution of Ẽ q (DB) is equivalent to computing M DB specified by DB. For a class of tail recursive programs and for some other types of programs, our approach achieves O(N 3) time complexity irrespective of the number of variables in a clause since only matrix operations costing O(N 3) or less are used. We conducted two experiments that compute the least Herbrand models of linear Datalog programs. The first experiment computes transitive closure of artificial data and real network data taken from the Koblenz Network Collection. The second one compared the proposed approach with the state-of-the-art symbolic systems including two Prolog systems and two ASP systems, in terms of computation time for a transitive closure program and the same generation program. In the experiment, it is observed that our linear algebraic approach runs 101 {\textasciitilde} 104 times faster than the symbolic systems when data is not sparse. Our approach is inspired by the emergence of big knowledge graphs and expected to contribute to the realization of rich and scalable logical inference for knowledge graphs.}
      \field{issn}{1471-0684, 1475-3081}
      \field{journaltitle}{Theory and Practice of Logic Programming}
      \field{month}{5}
      \field{note}{Publisher: Cambridge University Press}
      \field{number}{3}
      \field{title}{A linear algebraic approach to datalog evaluation}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{17}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{244\bibrangedash 265}
      \range{pages}{22}
      \verb{doi}
      \verb 10.1017/S1471068417000023
      \endverb
      \verb{file}
      \verb Eingereichte Version:/Users/alexgoessmann/Zotero/storage/C2AIFVTN/Sato - 2017 - A linear algebraic approach to datalog evaluation.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/linear-algebraic-approach-to-datalog-evaluation/CED3EEB903D9D8A16843CFCA5AC4D577
      \endverb
      \verb{url}
      \verb https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/linear-algebraic-approach-to-datalog-evaluation/CED3EEB903D9D8A16843CFCA5AC4D577
      \endverb
      \keyw{Datalog,least model,matrix,vector space}
    \endentry
    \entry{serafini_learning_2016}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=cfd7b672fd7926ef1136b9790438416d}{%
           family={Serafini},
           familyi={S\bibinitperiod},
           given={Luciano},
           giveni={L\bibinitperiod}}}%
        {{hash=ad4b7cd82e81faa28e2e876ecd78447d}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod},
           prefix={d’Avila},
           prefixi={d\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer-Verlag}%
      }
      \strng{namehash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{fullhash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{bibnamehash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{authorbibnamehash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{authornamehash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{authorfullhash}{55b3fa6b8621a130ff936fc4ddd15383}
      \field{labelalpha}{SG16}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The paper introduces real logic: a framework that seamlessly integrates logical deductive reasoning with efficient, data-driven relational learning. Real logic is based on full first order language. Terms are interpreted in n-dimensional feature vectors, while predicates are interpreted in fuzzy sets. In real logic it is possible to formally define the following two tasks: (i) learning from data in presence of logical constraints, and (ii) reasoning on formulas exploiting concrete data. We implement real logic in an deep learning architecture, called logic tensor networks, based on Google’s primitives. The paper concludes with experiments on a simple but representative example of knowledge completion.}
      \field{booktitle}{{AI}*{IA} 2016 {Advances} in {Artificial} {Intelligence}: {XVth} {International} {Conference} of the {Italian} {Association} for {Artificial} {Intelligence}, {Genova}, {Italy}, {November} 29 – {December} 1, 2016, {Proceedings}}
      \field{isbn}{978-3-319-49129-5}
      \field{month}{11}
      \field{title}{Learning and {Reasoning} with {Logic} {Tensor} {Networks}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{334\bibrangedash 348}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1007/978-3-319-49130-1_25
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-319-49130-1_25
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-319-49130-1_25
      \endverb
    \endentry
    \entry{shalev-schwartz_shai_understanding_2014}{book}{}
      \name{author}{2}{}{%
        {{hash=750e22690c4fd3d72534c6f366e3c709}{%
           family={{Shalev-Schwartz, Shai}},
           familyi={S\bibinitperiod}}}%
        {{hash=481307faa6325b344d16f9db06d8db17}{%
           family={{Ben-David, Shai}},
           familyi={B\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{fullhash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{bibnamehash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{authorbibnamehash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{authornamehash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{authorfullhash}{c9722f58073756b2c8b41d61e5cff116}
      \field{labelalpha}{SB14}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.}
      \field{isbn}{978-1-107-05713-5}
      \field{month}{7}
      \field{shorttitle}{Understanding {Machine} {Learning}}
      \field{title}{Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}}
      \field{year}{2014}
    \endentry
    \entry{shannon_mathematical_1948}{article}{}
      \name{author}{1}{}{%
        {{hash=22e2130b8d96f93daba648ff62af869b}{%
           family={Shannon},
           familyi={S\bibinitperiod},
           given={C.\bibnamedelimi E.},
           giveni={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{22e2130b8d96f93daba648ff62af869b}
      \strng{fullhash}{22e2130b8d96f93daba648ff62af869b}
      \strng{bibnamehash}{22e2130b8d96f93daba648ff62af869b}
      \strng{authorbibnamehash}{22e2130b8d96f93daba648ff62af869b}
      \strng{authornamehash}{22e2130b8d96f93daba648ff62af869b}
      \strng{authorfullhash}{22e2130b8d96f93daba648ff62af869b}
      \field{labelalpha}{Sha48}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1538-7305}
      \field{journaltitle}{Bell System Technical Journal}
      \field{note}{\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1948.tb01338.x}
      \field{number}{3}
      \field{title}{A {Mathematical} {Theory} of {Communication}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{27}
      \field{year}{1948}
      \field{urldateera}{ce}
      \field{pages}{379\bibrangedash 423}
      \range{pages}{45}
      \verb{doi}
      \verb 10.1002/j.1538-7305.1948.tb01338.x
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/6M2N6KGS/j.1538-7305.1948.tb01338.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x
      \endverb
      \verb{url}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x
      \endverb
    \endentry
    \entry{de_silva_tensor_2008}{article}{}
      \name{author}{2}{}{%
        {{hash=8914ed3ba6a5221539f9df16b4db08ac}{%
           family={Silva},
           familyi={S\bibinitperiod},
           given={Vin},
           giveni={V\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=12750b991b9d711549fd3537dfdff320}{%
           family={Lim},
           familyi={L\bibinitperiod},
           given={Lek-Heng},
           giveni={L\bibinithyphendelim H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{b045c4cf654638530c16fbd93dc73779}
      \strng{fullhash}{b045c4cf654638530c16fbd93dc73779}
      \strng{bibnamehash}{b045c4cf654638530c16fbd93dc73779}
      \strng{authorbibnamehash}{b045c4cf654638530c16fbd93dc73779}
      \strng{authornamehash}{b045c4cf654638530c16fbd93dc73779}
      \strng{authorfullhash}{b045c4cf654638530c16fbd93dc73779}
      \field{labelalpha}{SL08}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{There has been continued interest in seeking a theorem describing optimal low-rank approximations to tensors of order 3 or higher, that parallels the Eckart–Young theorem for matrices. In this paper, we argue that the naive approach to this problem is doomed to failure because, unlike matrices, tensors of order 3 or higher can fail to have best rank-r approximations. The phenomenon is much more widespread than one might suspect: examples of this failure can be constructed over a wide range of dimensions, orders and ranks, regardless of the choice of norm (or even Br`egman divergence). Moreover, we show that in many instances these counterexamples have positive volume: they cannot be regarded as isolated phenomena. In one extreme case, we exhibit a tensor space in which no rank-3 tensor has an optimal rank-2 approximation. The notable exceptions to this misbehavior are rank-1 tensors and order-2 tensors (i.e. matrices).}
      \field{issn}{0895-4798, 1095-7162}
      \field{journaltitle}{SIAM Journal on Matrix Analysis and Applications}
      \field{month}{1}
      \field{number}{3}
      \field{title}{Tensor {Rank} and the {Ill}-{Posedness} of the {Best} {Low}-{Rank} {Approximation} {Problem}}
      \field{urlday}{17}
      \field{urlmonth}{9}
      \field{urlyear}{2019}
      \field{volume}{30}
      \field{year}{2008}
      \field{urldateera}{ce}
      \field{pages}{1084\bibrangedash 1127}
      \range{pages}{44}
      \verb{doi}
      \verb 10.1137/06066518X
      \endverb
      \verb{file}
      \verb de Silva and Lim - 2008 - Tensor Rank and the Ill-Posedness of the Best Low-.pdf:/Users/alexgoessmann/Zotero/storage/STN8KFRC/de Silva and Lim - 2008 - Tensor Rank and the Ill-Posedness of the Best Low-.pdf:application/pdf
      \endverb
    \endentry
    \entry{simonis_sudoku_2005}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=a0f56137e8c48bf456998afccf528847}{%
           family={Simonis},
           familyi={S\bibinitperiod},
           given={Helmut},
           giveni={H\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Citeseer Sitges, Spain}%
      }
      \strng{namehash}{a0f56137e8c48bf456998afccf528847}
      \strng{fullhash}{a0f56137e8c48bf456998afccf528847}
      \strng{bibnamehash}{a0f56137e8c48bf456998afccf528847}
      \strng{authorbibnamehash}{a0f56137e8c48bf456998afccf528847}
      \strng{authornamehash}{a0f56137e8c48bf456998afccf528847}
      \strng{authorfullhash}{a0f56137e8c48bf456998afccf528847}
      \field{labelalpha}{Sim05}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{{CP} {Workshop} on modeling and reformulating {Constraint} {Satisfaction} {Problems}}
      \field{title}{Sudoku as a constraint problem}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{volume}{12}
      \field{year}{2005}
      \field{urldateera}{ce}
      \field{pages}{13\bibrangedash 27}
      \range{pages}{15}
      \verb{file}
      \verb Available Version (via Google Scholar):/Users/alexgoessmann/Zotero/storage/ALDJFT8I/Simonis - 2005 - Sudoku as a constraint problem.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://ai.dmi.unibas.ch/_files/teaching/fs21/ai/material/ai26-simonis-cp2005ws.pdf
      \endverb
      \verb{url}
      \verb https://ai.dmi.unibas.ch/_files/teaching/fs21/ai/material/ai26-simonis-cp2005ws.pdf
      \endverb
    \endentry
    \entry{stoudenmire_supervised_2016}{incollection}{}
      \name{author}{2}{}{%
        {{hash=80e2277c9051dd68981bb4b55e98d0a8}{%
           family={Stoudenmire},
           familyi={S\bibinitperiod},
           given={Edwin},
           giveni={E\bibinitperiod}}}%
        {{hash=f17b3976b2fddd6a3cb0e96d85a6115b}{%
           family={Schwab},
           familyi={S\bibinitperiod},
           given={David\bibnamedelima J},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \name{editor}{5}{}{%
        {{hash=dc777b608117794a5ff6308cfbba8945}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={D.\bibnamedelimi D.},
           giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=0ef91066dee98e654f9aebd57da00647}{%
           family={Sugiyama},
           familyi={S\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=56e73897155d476124481b099f125669}{%
           family={Luxburg},
           familyi={L\bibinitperiod},
           given={U.\bibnamedelimi V.},
           giveni={U\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=e7e74de725116358b68a6e890c026145}{%
           family={Guyon},
           familyi={G\bibinitperiod},
           given={I.},
           giveni={I\bibinitperiod}}}%
        {{hash=36b98b7ab533936cf1b5716148de704f}{%
           family={Garnett},
           familyi={G\bibinitperiod},
           given={R.},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{314b92ac1c208ed2be766a17d5917007}
      \strng{fullhash}{314b92ac1c208ed2be766a17d5917007}
      \strng{bibnamehash}{314b92ac1c208ed2be766a17d5917007}
      \strng{authorbibnamehash}{314b92ac1c208ed2be766a17d5917007}
      \strng{authornamehash}{314b92ac1c208ed2be766a17d5917007}
      \strng{authorfullhash}{314b92ac1c208ed2be766a17d5917007}
      \strng{editorbibnamehash}{21d9daab5c7d1d638944b0bfa6ad4aef}
      \strng{editornamehash}{21d9daab5c7d1d638944b0bfa6ad4aef}
      \strng{editorfullhash}{070d5597ded4754ff563759b82b5aa4c}
      \field{labelalpha}{SS16}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems} 29}
      \field{title}{Supervised {Learning} with {Tensor} {Networks}}
      \field{urlday}{10}
      \field{urlmonth}{1}
      \field{urlyear}{2020}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{4799\bibrangedash 4807}
      \range{pages}{9}
      \verb{file}
      \verb NIPS Full Text PDF:/Users/alexgoessmann/Zotero/storage/4K5UAMAB/Stoudenmire und Schwab - 2016 - Supervised Learning with Tensor Networks.pdf:application/pdf;NIPS Full Text PDF:/Users/alexgoessmann/Zotero/storage/AET3HRDF/Stoudenmire und Schwab - 2016 - Supervised Learning with Tensor Networks.pdf:application/pdf;NIPS Snapshot:/Users/alexgoessmann/Zotero/storage/5CAEQ3Q8/6211-supervised-learning-with-tensor-networks.html:text/html;NIPS Snapshot:/Users/alexgoessmann/Zotero/storage/DM7LW8Q8/6211-supervised-learning-with-tensor-networks.html:text/html
      \endverb
    \endentry
    \entry{suess_mpnum_2017}{article}{}
      \name{author}{2}{}{%
        {{hash=dd5a1a6450809c1ede59925d5098e64a}{%
           family={Suess},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=dffbdf4b843c5e9f9670ce5e734a6451}{%
           family={Holzäpfel},
           familyi={H\bibinitperiod},
           given={Milan},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{ea98a481cf153ce91c4da384c12be020}
      \strng{fullhash}{ea98a481cf153ce91c4da384c12be020}
      \strng{bibnamehash}{ea98a481cf153ce91c4da384c12be020}
      \strng{authorbibnamehash}{ea98a481cf153ce91c4da384c12be020}
      \strng{authornamehash}{ea98a481cf153ce91c4da384c12be020}
      \strng{authorfullhash}{ea98a481cf153ce91c4da384c12be020}
      \field{labelalpha}{SH17}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Suess et al., (2017). mpnum: A matrix product representation library for Python. Journal of Open Source Software, 2(20), 465, https://doi.org/10.21105/joss.00465}
      \field{issn}{2475-9066}
      \field{journaltitle}{Journal of Open Source Software}
      \field{month}{12}
      \field{number}{20}
      \field{shorttitle}{mpnum}
      \field{title}{mpnum: {A} matrix product representation library for {Python}}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{volume}{2}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{465}
      \range{pages}{1}
      \verb{doi}
      \verb 10.21105/joss.00465
      \endverb
      \verb{urlraw}
      \verb https://joss.theoj.org/papers/10.21105/joss.00465
      \endverb
      \verb{url}
      \verb https://joss.theoj.org/papers/10.21105/joss.00465
      \endverb
    \endentry
    \entry{talagrand_upper_2014}{book}{}
      \name{author}{1}{}{%
        {{hash=c0aa8b0f36f087702a87b6002ab35850}{%
           family={Talagrand},
           familyi={T\bibinitperiod},
           given={Michel},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{fullhash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{bibnamehash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{authorbibnamehash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{authornamehash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{authorfullhash}{c0aa8b0f36f087702a87b6002ab35850}
      \field{labelalpha}{Tal14}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The book develops modern methods and in particular the "generic chaining" to bound stochastic processes. This methods allows in particular to get optimal bounds for Gaussian and Bernoulli processes. Applications are given to stable processes, infinitely divisible processes, matching theorems, the convergence of random Fourier series, of orthogonal series, and to functional analysis. The complete solution of a number of classical problems is given in complete detail, and an ambitious program for future research is laid out.}
      \field{isbn}{978-3-642-54074-5}
      \field{shorttitle}{Upper and {Lower} {Bounds} for {Stochastic} {Processes}}
      \field{title}{Upper and {Lower} {Bounds} for {Stochastic} {Processes}: {Modern} {Methods} and {Classical} {Problems}}
      \field{urlday}{5}
      \field{urlmonth}{5}
      \field{urlyear}{2020}
      \field{year}{2014}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-3-642-54075-2
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/YHG4TIAE/9783642540745.html:text/html
      \endverb
    \endentry
    \entry{towell_knowledge-based_1994}{article}{}
      \name{author}{2}{}{%
        {{hash=ba0f27f99c14c01e07ffa93313bc7e3c}{%
           family={Towell},
           familyi={T\bibinitperiod},
           given={Geoffrey\bibnamedelima G.},
           giveni={G\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=4df87652cd39798cafcfca148693289f}{%
           family={Shavlik},
           familyi={S\bibinitperiod},
           given={Jude\bibnamedelima W.},
           giveni={J\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \strng{namehash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{fullhash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{bibnamehash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{authorbibnamehash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{authornamehash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{authorfullhash}{be79b3a13ca69e2435d1a677ef475a69}
      \field{labelalpha}{TS94}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for accurately classifying examples not seen during training. The challenge of hybrid learning systems is to use the information provided by one source of information to offset information missing from the other source. By so doing, a hybrid learning system should learn more effectively than systems that use only one of the information sources. KBANN (Knowledge-Based Artificial Neural Networks) is a hybrid learning system built on top of connectionist learning techniques. It maps problem-specific “domain theories”, represented in propositional logic, into neural networks and then refines this reformulated knowledge using backpropagation. KBANN is evaluated by extensive empirical tests on two problems from molecular biology. Among other results, these tests show that the networks created by KBANN generalize better than a wide variety of learning systems, as well as several techniques proposed by biologists.}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{10}
      \field{number}{1}
      \field{title}{Knowledge-based artificial neural networks}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{70}
      \field{year}{1994}
      \field{urldateera}{ce}
      \field{pages}{119\bibrangedash 165}
      \range{pages}{47}
      \verb{doi}
      \verb 10.1016/0004-3702(94)90105-8
      \endverb
      \verb{file}
      \verb ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/LLDD5VDQ/0004370294901058.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/0004370294901058
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/0004370294901058
      \endverb
      \keyw{Machine learning,Computational biology,Connectionism,Explanation-based learning,Hybrid algorithms,Theory refinement}
    \endentry
    \entry{trouillon_complex_2017}{misc}{}
      \name{author}{2}{}{%
        {{hash=42de07a57ebce8948963bfa46f960d3e}{%
           family={Trouillon},
           familyi={T\bibinitperiod},
           given={Théo},
           giveni={T\bibinitperiod}}}%
        {{hash=9754396c9b0fc2f09bd396146fbf2ef3}{%
           family={Nickel},
           familyi={N\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{fullhash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{bibnamehash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{authorbibnamehash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{authornamehash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{authorfullhash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \field{labelalpha}{TN17}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Embeddings of knowledge graphs have received significant attention due to their excellent performance for tasks like link prediction and entity resolution. In this short paper, we are providing a comparison of two state-of-the-art knowledge graph embeddings for which their equivalence has recently been established, i.e., ComplEx and HolE [Nickel, Rosasco, and Poggio, 2016; Trouillon et al., 2016; Hayashi and Shimbo, 2017]. First, we briefly review both models and discuss how their scoring functions are equivalent. We then analyze the discrepancy of results reported in the original articles, and show experimentally that they are likely due to the use of different loss functions. In further experiments, we evaluate the ability of both models to embed symmetric and antisymmetric patterns. Finally, we discuss advantages and disadvantages of both models and under which conditions one would be preferable to the other.}
      \field{month}{7}
      \field{note}{arXiv:1707.01475 [cs, stat]}
      \field{shorttitle}{Complex and {Holographic} {Embeddings} of {Knowledge} {Graphs}}
      \field{title}{Complex and {Holographic} {Embeddings} of {Knowledge} {Graphs}: {A} {Comparison}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb Trouillon und Nickel - 2017 - Complex and Holographic Embeddings of Knowledge Graphs A Comparison.pdf:/Users/alexgoessmann/Zotero/storage/RR52API2/Trouillon und Nickel - 2017 - Complex and Holographic Embeddings of Knowledge Graphs A Comparison.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1707.01475
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1707.01475
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{tsilionis_tensor-based_2024}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=1f04abcb36ae84b02e7b05a4ba571aad}{%
           family={Tsilionis},
           familyi={T\bibinitperiod},
           given={Efthimis},
           giveni={E\bibinitperiod}}}%
        {{hash=7ead92ae0e88543d514f2cc4a7600590}{%
           family={Artikis},
           familyi={A\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=b4097f9e90befdf404349dd04af200e9}{%
           family={Paliouras},
           familyi={P\bibinitperiod},
           given={Georgios},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Jeju, South Korea}%
      }
      \list{publisher}{1}{%
        {International Joint Conferences on Artificial Intelligence Organization}%
      }
      \strng{namehash}{84978962e71cc42f91d64c35d1531605}
      \strng{fullhash}{84978962e71cc42f91d64c35d1531605}
      \strng{bibnamehash}{84978962e71cc42f91d64c35d1531605}
      \strng{authorbibnamehash}{84978962e71cc42f91d64c35d1531605}
      \strng{authornamehash}{84978962e71cc42f91d64c35d1531605}
      \strng{authorfullhash}{84978962e71cc42f91d64c35d1531605}
      \field{labelalpha}{TAP24}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a formalization of the Event Calculus (EC) in tensor spaces. The motivation for a tensorbased predicate calculus comes from the area of composite event recognition (CER). As a CER engine, we adopt a logic programming implementation of EC with optimizations for continuous narrative assimilation on data streams. We show how to evaluate EC rules algebraically and solve a linear equation to compute the corresponding models. We demonstrate the scalability of our approach with the use of large datasets from a real-world application domain, and show it outperforms significantly symbolic EC, in terms of processing time.}
      \field{booktitle}{Proceedings of the {Thirty}-{ThirdInternational} {Joint} {Conference} on {Artificial} {Intelligence}}
      \field{isbn}{978-1-956792-04-1}
      \field{month}{8}
      \field{title}{A {Tensor}-{Based} {Formalization} of the {Event} {Calculus}}
      \field{urlday}{24}
      \field{urlmonth}{9}
      \field{urlyear}{2024}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{3584\bibrangedash 3592}
      \range{pages}{9}
      \verb{doi}
      \verb 10.24963/ijcai.2024/397
      \endverb
      \verb{file}
      \verb PDF:/Users/alexgoessmann/Zotero/storage/3J9ZWKGW/Tsilionis et al. - 2024 - A Tensor-Based Formalization of the Event Calculus.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.ijcai.org/proceedings/2024/397
      \endverb
      \verb{url}
      \verb https://www.ijcai.org/proceedings/2024/397
      \endverb
    \endentry
    \entry{vershynin_high-dimensional_2018}{book}{}
      \name{author}{1}{}{%
        {{hash=711c5aa2fe76a4ff674a2c6e2c7a2d91}{%
           family={Vershynin},
           familyi={V\bibinitperiod},
           given={Roman},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {New York, NY}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{fullhash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{bibnamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authorbibnamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authornamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authorfullhash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \field{labelalpha}{Ver18}
      \field{sortinit}{V}
      \field{sortinithash}{75dd7385c90b2252c3ae853a80ca853b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.}
      \field{edition}{1st edition}
      \field{isbn}{978-1-108-41519-4}
      \field{month}{9}
      \field{shorttitle}{High-{Dimensional} {Probability}}
      \field{title}{High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}}
      \field{year}{2018}
    \endentry
    \entry{wainwright_high-dimensional_2019}{book}{}
      \name{author}{1}{}{%
        {{hash=7eb03239453694f59946ad6008e9f0ab}{%
           family={Wainwright},
           familyi={W\bibinitperiod},
           given={Martin\bibnamedelima J.},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{fullhash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{bibnamehash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{authorbibnamehash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{authornamehash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{authorfullhash}{7eb03239453694f59946ad6008e9f0ab}
      \field{labelalpha}{Wai19}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.}
      \field{isbn}{978-1-108-49802-9}
      \field{series}{Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}}
      \field{shorttitle}{High-{Dimensional} {Statistics}}
      \field{title}{High-{Dimensional} {Statistics}: {A} {Non}-{Asymptotic} {Viewpoint}}
      \field{urlday}{23}
      \field{urlmonth}{11}
      \field{urlyear}{2020}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1017/9781108627771
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/65TBDVYU/8A91ECEEC38F46DAB53E9FF8757C7A4E.html:text/html
      \endverb
    \endentry
    \entry{wainwright_graphical_2008}{book}{}
      \name{author}{2}{}{%
        {{hash=7eb03239453694f59946ad6008e9f0ab}{%
           family={Wainwright},
           familyi={W\bibinitperiod},
           given={Martin\bibnamedelima J.},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=ce9a11a5b3beb20bfe74f41a05d92245}{%
           family={Jordan},
           familyi={J\bibinitperiod},
           given={Michael\bibnamedelima Irwin},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Now Publishers Inc}%
      }
      \strng{namehash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{fullhash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{bibnamehash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{authorbibnamehash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{authornamehash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{authorfullhash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \field{labelalpha}{WJ08}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances-including the key problems of computing marginals and modes of probability distributions-are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, Graphical Models, Exponential Families and Variational Inference develops general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. It describes how a wide variety of algorithms- among them sum-product, cluster variational methods, expectation-propagation, mean field methods, and max-product-can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.}
      \field{isbn}{978-1-60198-184-4}
      \field{title}{Graphical {Models}, {Exponential} {Families}, and {Variational} {Inference}}
      \field{year}{2008}
    \endentry
    \entry{wolf_libxerusxerus_2024}{misc}{}
      \name{author}{1}{}{%
        {{hash=9c29d0496ec2ac633524176392651a27}{%
           family={Wolf},
           familyi={W\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {libxerus}%
      }
      \strng{namehash}{9c29d0496ec2ac633524176392651a27}
      \strng{fullhash}{9c29d0496ec2ac633524176392651a27}
      \strng{bibnamehash}{9c29d0496ec2ac633524176392651a27}
      \strng{authorbibnamehash}{9c29d0496ec2ac633524176392651a27}
      \strng{authornamehash}{9c29d0496ec2ac633524176392651a27}
      \strng{authorfullhash}{9c29d0496ec2ac633524176392651a27}
      \field{labelalpha}{Wol24}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A general purpose library for numerical calculations with higher order tensors, Tensor-Train Decompositions / Matrix Product States and other Tensor Networks}
      \field{month}{2}
      \field{note}{original-date: 2016-03-10T15:27:47Z}
      \field{title}{libxerus/xerus}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://github.com/libxerus/xerus
      \endverb
      \verb{url}
      \verb https://github.com/libxerus/xerus
      \endverb
    \endentry
    \entry{wolfram_statistical_1983}{article}{}
      \name{author}{1}{}{%
        {{hash=fc7f119c141fbcc7810a4b3a86f599db}{%
           family={Wolfram},
           familyi={W\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{fullhash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{bibnamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authorbibnamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authornamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authorfullhash}{fc7f119c141fbcc7810a4b3a86f599db}
      \field{extraname}{1}
      \field{labelalpha}{Wol83}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Cellular automata are used as simple mathematical models to investigate self-organization in statistical mechanics. A detailed analysis is given of "elementary" cellular automata consisting of a sequence of sites with values 0 or 1 on a line, with each site evolving deterministically in discrete time steps according to definite rules involving the values of its nearest neighbors. With simple initial configurations, the cellular automata either tend to homogeneous states, or generate self-similar patterns with fractal dimensions ≃ 1.59 or ≃ 1.69. With "random" initial configurations, the irreversible character of the cellular automaton evolution leads to several self-organization phenomena. Statistical properties of the structures generated are found to lie in two universality classes, independent of the details of the initial state or the cellular automaton rules. More complicated cellular automata are briefly considered, and connections with dynamical systems theory and the formal theory of computation are discussed.}
      \field{journaltitle}{Reviews of Modern Physics}
      \field{month}{7}
      \field{note}{Publisher: American Physical Society}
      \field{number}{3}
      \field{title}{Statistical mechanics of cellular automata}
      \field{urlday}{11}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{volume}{55}
      \field{year}{1983}
      \field{urldateera}{ce}
      \field{pages}{601\bibrangedash 644}
      \range{pages}{44}
      \verb{doi}
      \verb 10.1103/RevModPhys.55.601
      \endverb
      \verb{file}
      \verb APS Snapshot:/Users/alexgoessmann/Zotero/storage/9IELTEPH/RevModPhys.55.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://link.aps.org/doi/10.1103/RevModPhys.55.601
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/RevModPhys.55.601
      \endverb
    \endentry
    \entry{wolfram_new_2002}{book}{}
      \name{author}{1}{}{%
        {{hash=fc7f119c141fbcc7810a4b3a86f599db}{%
           family={Wolfram},
           familyi={W\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Champaign (Ill.)}%
      }
      \list{publisher}{1}{%
        {Wolfram Media}%
      }
      \strng{namehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{fullhash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{bibnamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authorbibnamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authornamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authorfullhash}{fc7f119c141fbcc7810a4b3a86f599db}
      \field{extraname}{2}
      \field{labelalpha}{Wol02}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book promises to revolutionize science as we know it' - Daily Telegraph 'Stephen's magnum opus may be the book of the decade if not the century' - Arthur C Clarke Long-awaited work from one of the world's most respected scientists presents a series of dramatic discoveries never before made public. Starting with a collection of computer experiments, Wolfram shows how their unexpected results force a whole new way of looking at the universe. A seminal work of enormous importance. Includes over 950 illustrations. BBC documentary in development.'}
      \field{edition}{Illustrated Edition}
      \field{isbn}{978-1-57955-008-0}
      \field{month}{5}
      \field{title}{A {New} {Kind} of {Science}}
      \field{year}{2002}
    \endentry
    \entry{yang_embedding_2015}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=7b089ac99dfbed4f2eb0e3b33d72ddad}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Bishan},
           giveni={B\bibinitperiod}}}%
        {{hash=d4d56d3d370b3afd4ad2ebf985fbd3f8}{%
           family={Yih},
           familyi={Y\bibinitperiod},
           given={Wen-tau},
           giveni={W\bibinithyphendelim t\bibinitperiod}}}%
        {{hash=a2d92e419c3b04b620f2b0302581e68c}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Xiaodong},
           giveni={X\bibinitperiod}}}%
        {{hash=159bca5607e3797666654a2e3022978a}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Jianfeng},
           giveni={J\bibinitperiod}}}%
        {{hash=2f5fbdc5c3cf91f62a64663cd72397b3}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{8d6fbb7a7abaf78537651ae42485af2f}
      \strng{fullhash}{0804da74c4e7ba8755552124a76406cf}
      \strng{bibnamehash}{8d6fbb7a7abaf78537651ae42485af2f}
      \strng{authorbibnamehash}{8d6fbb7a7abaf78537651ae42485af2f}
      \strng{authornamehash}{8d6fbb7a7abaf78537651ae42485af2f}
      \strng{authorfullhash}{0804da74c4e7ba8755552124a76406cf}
      \field{labelalpha}{Yan+15}
      \field{sortinit}{Y}
      \field{sortinithash}{b8d711a035f7be9840c721c82920477e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a uniﬁed learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq {\textasciicircum} CityInCountrypb, cq ùñ N ationalitypa, cq. We ﬁnd that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-ofthe-art conﬁdence-based rule mining approach in mining Horn rules that involve compositional reasoning.}
      \field{month}{8}
      \field{note}{arXiv:1412.6575 [cs]}
      \field{title}{Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2015}
      \field{urldateera}{ce}
      \verb{file}
      \verb Yang et al. - 2015 - Embedding Entities and Relations for Learning and Inference in Knowledge Bases.pdf:/Users/alexgoessmann/Zotero/storage/WWTTIZ78/Yang et al. - 2015 - Embedding Entities and Relations for Learning and Inference in Knowledge Bases.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1412.6575
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1412.6575
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{ziegler_lectures_2000}{incollection}{}
      \name{author}{1}{}{%
        {{hash=f78d48d2643d689692036d03a7958e8c}{%
           family={Ziegler},
           familyi={Z\bibinitperiod},
           given={Günter\bibnamedelima M.},
           giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=128f517dac6e5636f212c1bf83a7cd9a}{%
           family={Kalai},
           familyi={K\bibinitperiod},
           given={Gil},
           giveni={G\bibinitperiod}}}%
        {{hash=f78d48d2643d689692036d03a7958e8c}{%
           family={Ziegler},
           familyi={Z\bibinitperiod},
           given={Günter\bibnamedelima M.},
           giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Basel}%
      }
      \list{publisher}{1}{%
        {Birkhäuser}%
      }
      \strng{namehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{fullhash}{f78d48d2643d689692036d03a7958e8c}
      \strng{bibnamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authorbibnamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authornamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authorfullhash}{f78d48d2643d689692036d03a7958e8c}
      \strng{editorbibnamehash}{49c5058dd8625f53ab651acd3dd4c2a7}
      \strng{editornamehash}{49c5058dd8625f53ab651acd3dd4c2a7}
      \strng{editorfullhash}{49c5058dd8625f53ab651acd3dd4c2a7}
      \field{extraname}{1}
      \field{labelalpha}{Zie00}
      \field{sortinit}{Z}
      \field{sortinithash}{156173bd08b075d7295bc3e0f4735a04}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{These lectures on the combinatorics and geometry of 0/1-polytopes are meant as anintroductionandinvitation.Rather than heading for an extensive survey on 0/1-polytopes I present some interesting aspects of these objects; all of them are related to some quite recent work and progress.}
      \field{booktitle}{Polytopes — {Combinatorics} and {Computation}}
      \field{isbn}{978-3-0348-8438-9}
      \field{title}{Lectures on 0/1-{Polytopes}}
      \field{urlday}{4}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2000}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 41}
      \range{pages}{41}
      \verb{doi}
      \verb 10.1007/978-3-0348-8438-9_1
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-0348-8438-9_1
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-0348-8438-9_1
      \endverb
    \endentry
    \entry{ziegler_lectures_2013}{book}{}
      \name{author}{1}{}{%
        {{hash=f78d48d2643d689692036d03a7958e8c}{%
           family={Ziegler},
           familyi={Z\bibinitperiod},
           given={Günter\bibnamedelima M.},
           giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {New York}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{fullhash}{f78d48d2643d689692036d03a7958e8c}
      \strng{bibnamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authorbibnamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authornamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authorfullhash}{f78d48d2643d689692036d03a7958e8c}
      \field{extraname}{2}
      \field{labelalpha}{Zie13}
      \field{sortinit}{Z}
      \field{sortinithash}{156173bd08b075d7295bc3e0f4735a04}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Based on a graduate course given at the Technische Universität, Berlin, these lectures present a wealth of material on the modern theory of convex polytopes. The clear and straightforward exposition features many illustrations, and provides complete proofs for most theorems. The material requires only linear algebra as a prerequisite, but takes the reader quickly from the basics to topics of recent research, including a number of unanswered questions. The lectures introduce the basic facts about polytopes, with an emphasis on the methods that yield the results, discuss important examples and elegant constructions, and show the excitement of current work in the field. They will provide interesting and enjoyable reading for researchers as well as students.}
      \field{edition}{1995th edition}
      \field{isbn}{978-0-387-94365-7}
      \field{month}{10}
      \field{title}{Lectures on {Polytopes}}
      \field{year}{2013}
    \endentry
  \enddatalist
  \datalist[entry]{anyt/global//global/global}
    \entry{abadi_tensorflow_2016}{misc}{}
      \name{author}{40}{}{%
        {{hash=9a04ae935da573a7aa9c83afdf2fd845}{%
           family={Abadi},
           familyi={A\bibinitperiod},
           given={Martín},
           giveni={M\bibinitperiod}}}%
        {{hash=37e1df772f97a2e492755e2006294387}{%
           family={Agarwal},
           familyi={A\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=08cf514c08137f94d73b590060797f5b}{%
           family={Barham},
           familyi={B\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=a79c0c4b084471b2c8e0e9a38b4cb2b7}{%
           family={Brevdo},
           familyi={B\bibinitperiod},
           given={Eugene},
           giveni={E\bibinitperiod}}}%
        {{hash=c0d10aaf985cebf8d0497e1828f9313f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Zhifeng},
           giveni={Z\bibinitperiod}}}%
        {{hash=6f712ddfd730736ee54fdc87a4abf7a2}{%
           family={Citro},
           familyi={C\bibinitperiod},
           given={Craig},
           giveni={C\bibinitperiod}}}%
        {{hash=84d9f354fa0b45dae996f27dad2c6607}{%
           family={Corrado},
           familyi={C\bibinitperiod},
           given={Greg\bibnamedelima S.},
           giveni={G\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=a4c859221cc41e2db6d63b486f955ba2}{%
           family={Davis},
           familyi={D\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
        {{hash=bd7f88635b51bc5abb3d5a2cab967724}{%
           family={Devin},
           familyi={D\bibinitperiod},
           given={Matthieu},
           giveni={M\bibinitperiod}}}%
        {{hash=193bcec5240237591ad8fb697869f013}{%
           family={Ghemawat},
           familyi={G\bibinitperiod},
           given={Sanjay},
           giveni={S\bibinitperiod}}}%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=a9385c3940c7e3dc9dfad01621d190d3}{%
           family={Harp},
           familyi={H\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=edc6964c6549e1839ca94d3d61e03f76}{%
           family={Irving},
           familyi={I\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=3b62e7eedeec9f7a7f7f3c3ec88637f1}{%
           family={Isard},
           familyi={I\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=9fce03efe6b3331a1b93ed2e7c0da9d5}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Yangqing},
           giveni={Y\bibinitperiod}}}%
        {{hash=b243d8e26a729c55dd5af5f97763dded}{%
           family={Jozefowicz},
           familyi={J\bibinitperiod},
           given={Rafal},
           giveni={R\bibinitperiod}}}%
        {{hash=f2bc899b1160163417da7bf510f15d33}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=73fc2bbbbbe06838174fc0490ee66f67}{%
           family={Kudlur},
           familyi={K\bibinitperiod},
           given={Manjunath},
           giveni={M\bibinitperiod}}}%
        {{hash=a1a07ba9db1e853a9dd9623d81e8f26a}{%
           family={Levenberg},
           familyi={L\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod}}}%
        {{hash=55eafec933a6ef975eb0229394df31c0}{%
           family={Mane},
           familyi={M\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=f34904ef0a6e0864175e4c446f9fcf14}{%
           family={Monga},
           familyi={M\bibinitperiod},
           given={Rajat},
           giveni={R\bibinitperiod}}}%
        {{hash=1f07f887fdbaea3ca8281745735256d1}{%
           family={Moore},
           familyi={M\bibinitperiod},
           given={Sherry},
           giveni={S\bibinitperiod}}}%
        {{hash=d83fd9fc0c2a25f04b7525cfe4ac3ab8}{%
           family={Murray},
           familyi={M\bibinitperiod},
           given={Derek},
           giveni={D\bibinitperiod}}}%
        {{hash=78be47e08348476a254b1217fd9041ca}{%
           family={Olah},
           familyi={O\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=29ea22df63f174ac629e9ef100b40484}{%
           family={Schuster},
           familyi={S\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod}}}%
        {{hash=8f128e70084608a2c29c497ebd794f87}{%
           family={Shlens},
           familyi={S\bibinitperiod},
           given={Jonathon},
           giveni={J\bibinitperiod}}}%
        {{hash=0a0b028c6b85c46f368317d0c5bfe3a0}{%
           family={Steiner},
           familyi={S\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=1754a40f9d600dea756c1dd1047ce170}{%
           family={Talwar},
           familyi={T\bibinitperiod},
           given={Kunal},
           giveni={K\bibinitperiod}}}%
        {{hash=c7360a7ebeba8420b8c7e0eeb0cb81ac}{%
           family={Tucker},
           familyi={T\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=8051922e7bd286f884bfbd1023ef62f5}{%
           family={Vanhoucke},
           familyi={V\bibinitperiod},
           given={Vincent},
           giveni={V\bibinitperiod}}}%
        {{hash=bab2011f2b9e7af47ab45308470b9faf}{%
           family={Vasudevan},
           familyi={V\bibinitperiod},
           given={Vijay},
           giveni={V\bibinitperiod}}}%
        {{hash=d9ad2ba70a84203f22b0dab9080eedfc}{%
           family={Viegas},
           familyi={V\bibinitperiod},
           given={Fernanda},
           giveni={F\bibinitperiod}}}%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=cfc28a19549f0d8ff117cf79be90d213}{%
           family={Warden},
           familyi={W\bibinitperiod},
           given={Pete},
           giveni={P\bibinitperiod}}}%
        {{hash=c66133f96104e3c0dcd2f73e11469ab0}{%
           family={Wattenberg},
           familyi={W\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=8f71c897f89022a26511a6fbbd6108b6}{%
           family={Wicke},
           familyi={W\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=b8e2e9f67019f464cefe5db1c822ffd9}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Yuan},
           giveni={Y\bibinitperiod}}}%
        {{hash=fdfa73146952ceef0abc202cc397781c}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Xiaoqiang},
           giveni={X\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{fullhash}{1cd4d6db8cba66ede8ce2b69466158cd}
      \strng{bibnamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authorbibnamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authornamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authorfullhash}{1cd4d6db8cba66ede8ce2b69466158cd}
      \field{labelalpha}{Aba+16}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.}
      \field{month}{3}
      \field{note}{arXiv:1603.04467 [cs]}
      \field{shorttitle}{{TensorFlow}}
      \field{title}{{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2016}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1603.04467
      \endverb
      \verb{file}
      \verb Preprint PDF:/Users/alexgoessmann/Zotero/storage/6IRK2ZQ5/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/NVW9HV3T/1603.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1603.04467
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1603.04467
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Distributed,Parallel,and Cluster Computing}
    \endentry
    \entry{agerbeck_multi-agent_2008}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=fd231f4f0ce8f802061321a92838762f}{%
           family={Agerbeck},
           familyi={A\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=23895d691ce6553213b9c345c8d2abb2}{%
           family={Hansen},
           familyi={H\bibinitperiod},
           given={Mikael},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{fullhash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{bibnamehash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{authorbibnamehash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{authornamehash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \strng{authorfullhash}{6ad2dc0c4d11345c7611ab69f8931b5d}
      \field{labelalpha}{AH08}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This master's project concerns the use of multi-agent design principles in making efficient solvers for NP-complete problems. The design of computer programs as multi-agent systems presents a new and very promising software engineering paradigm, where systems are described as individual problem-solving agents pursuing high-level goals. Recently, researchers have started to apply the multi-agent paradigm to the construction of efficient solvers for NP-complete problems. This has resulted in very effective tools for routing problems, graph partitioning and SAT-solving. The objective of the present project is to make further studies into the application of multi-agent principles to solving NP-complete problems. More specifically, the project has the following two goals. First, it should result in a general discussion of the use of multi-agent approaches to solving NP-complete problems. This should include a discussion of strengths and weaknesses compared to other approaches of solving the same problems. Second, it should result in a concrete software tool for solving n2 £ n2 Sudoku puzzles, which is known to be an NP-complete problem. The tool should be benchmarked against other solvers for Sudoku.}
      \field{title}{A {Multi}-{Agent} {Approach} to {Solving} {NP}-{Complete} {Problems}}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2008}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/8LVX8CKC/Agerbeck und Hansen - 2008 - A Multi-Agent Approach to Solving NP-Complete Problems.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.semanticscholar.org/paper/A-Multi-Agent-Approach-to-Solving-NP-Complete-Agerbeck-Hansen/3762bf7893da14839e06ae000b9e04d63dac8af4
      \endverb
      \verb{url}
      \verb https://www.semanticscholar.org/paper/A-Multi-Agent-Approach-to-Solving-NP-Complete-Agerbeck-Hansen/3762bf7893da14839e06ae000b9e04d63dac8af4
      \endverb
    \endentry
    \entry{antoniou_semantic_2012}{book}{}
      \name{author}{4}{}{%
        {{hash=3bedb033d421d9b43a2ec240debf37a0}{%
           family={Antoniou},
           familyi={A\bibinitperiod},
           given={Grigoris},
           giveni={G\bibinitperiod}}}%
        {{hash=b2d7e1f159093aa2ea8379d9b6bc6911}{%
           family={Groth},
           familyi={G\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=fb842925937c8dd94b68589e01b84f96}{%
           family={Harmelen},
           familyi={H\bibinitperiod},
           given={Frank\bibnamedelima Van},
           giveni={F\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=c322c048f090ff4f07130fd85d1114c2}{%
           family={Hoekstra},
           familyi={H\bibinitperiod},
           given={Rinke},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge (Mass.)}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{4f98eeb533ab51a9dcdb5f47cede0903}
      \strng{fullhash}{d08305c8c63d28e8a6ae55e94f94f0db}
      \strng{bibnamehash}{4f98eeb533ab51a9dcdb5f47cede0903}
      \strng{authorbibnamehash}{4f98eeb533ab51a9dcdb5f47cede0903}
      \strng{authornamehash}{4f98eeb533ab51a9dcdb5f47cede0903}
      \strng{authorfullhash}{d08305c8c63d28e8a6ae55e94f94f0db}
      \field{labelalpha}{Ant+12}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A new edition of the widely used guide to the key ideas, languages, and technologies of the Semantic WebThe development of the Semantic Web, with machine-readable content, has the potential to revolutionize the World Wide Web and its uses. A Semantic Web Primer provides an introduction and guide to this continuously evolving field, describing its key ideas, languages, and technologies. Suitable for use as a textbook or for independent study by professionals, it concentrates on undergraduate-level fundamental concepts and techniques that will enable readers to proceed with building applications on their own and includes exercises, project descriptions, and annotated references to relevant online materials.The third edition of this widely used text has been thoroughly updated, with significant new material that reflects a rapidly developing field. Treatment of the different languages (OWL2, rules) expands the coverage of RDF and OWL, defining the data model independently of XML and including coverage of N3/Turtle and RDFa. A chapter is devoted to OWL2, the new W3C standard. This edition also features additional coverage of the query language SPARQL, the rule language RIF and the possibility of interaction between rules and ontology languages and applications. The chapter on Semantic Web applications reflects the rapid developments of the past few years. A new chapter offers ideas for term projects. Additional material, including updates on the technological trends and research directions, can be found at http://www.semanticwebprimer.org.}
      \field{edition}{third edition}
      \field{isbn}{978-0-262-01828-9}
      \field{month}{8}
      \field{title}{A {Semantic} {Web} {Primer}, third edition}
      \field{year}{2012}
    \endentry
    \entry{avila_garcez_connectionist_1999}{article}{}
      \name{author}{2}{}{%
        {{hash=4d2b98eb7de13916ec42552ec5c259ee}{%
           family={Avila\bibnamedelima Garcez},
           familyi={A\bibinitperiod\bibinitdelim G\bibinitperiod},
           given={Artur\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=9affecbfe51d0fd75ae0999379cdb855}{%
           family={Zaverucha},
           familyi={Z\bibinitperiod},
           given={Gerson},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{fullhash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{bibnamehash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{authorbibnamehash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{authornamehash}{8175ba9d84db9fd244eea4049301b35d}
      \strng{authorfullhash}{8175ba9d84db9fd244eea4049301b35d}
      \field{labelalpha}{AZ99}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents the Connectionist Inductive Learning and Logic Programming System (C-IL2P). C-IL2P is a new massively parallel computational model based on a feedforward Artificial Neural Network that integrates inductive learning from examples and background knowledge, with deductive learning from Logic Programming. Starting with the background knowledge represented by a propositional logic program, a translation algorithm is applied generating a neural network that can be trained with examples. The results obtained with this refined network can be explained by extracting a revised logic program from it. Moreover, the neural network computes the stable model of the logic program inserted in it as background knowledge, or learned with the examples, thus functioning as a parallel system for Logic Programming. We have successfully applied C-IL2P to two real-world problems of computational biology, specifically DNA sequence analyses. Comparisons with the results obtained by some of the main neural, symbolic, and hybrid inductive learning systems, using the same domain knowledge, show the effectiveness of C-IL2P.}
      \field{issn}{1573-7497}
      \field{journaltitle}{Applied Intelligence}
      \field{month}{7}
      \field{number}{1}
      \field{title}{The {Connectionist} {Inductive} {Learning} and {Logic} {Programming} {System}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{11}
      \field{year}{1999}
      \field{urldateera}{ce}
      \field{pages}{59\bibrangedash 77}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1023/A:1008328630915
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1023/A:1008328630915
      \endverb
      \verb{url}
      \verb https://doi.org/10.1023/A:1008328630915
      \endverb
      \keyw{machine learning,Artificial Intelligence,artificial neural networks,computational biology,logic programming,theory refinement}
    \endentry
    \entry{badreddine_logic_2022}{article}{}
      \name{author}{4}{}{%
        {{hash=d73c12aa21b94170ac4aeb2a43b7fdf9}{%
           family={Badreddine},
           familyi={B\bibinitperiod},
           given={Samy},
           giveni={S\bibinitperiod}}}%
        {{hash=70658466246873155f5b9b3a737cdd4e}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur},
           giveni={A\bibinitperiod},
           prefix={d'Avila},
           prefixi={d\bibinitperiod}}}%
        {{hash=cfd7b672fd7926ef1136b9790438416d}{%
           family={Serafini},
           familyi={S\bibinitperiod},
           given={Luciano},
           giveni={L\bibinitperiod}}}%
        {{hash=fecf9d2471f6f2bf1fe02253df553444}{%
           family={Spranger},
           familyi={S\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{bffa1c78d0f905872f09e3bba1099174}
      \strng{fullhash}{c7bbeb9ca1be0100378ace27e57e379b}
      \strng{bibnamehash}{bffa1c78d0f905872f09e3bba1099174}
      \strng{authorbibnamehash}{bffa1c78d0f905872f09e3bba1099174}
      \strng{authornamehash}{bffa1c78d0f905872f09e3bba1099174}
      \strng{authorfullhash}{c7bbeb9ca1be0100378ace27e57e379b}
      \field{labelalpha}{Bad+22}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{2}
      \field{title}{Logic {Tensor} {Networks}}
      \field{urlday}{20}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{303}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{103649}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.artint.2021.103649
      \endverb
      \verb{file}
      \verb Akzeptierte Version:/Users/alexgoessmann/Zotero/storage/XKSPFEM9/Badreddine et al. - 2022 - Logic Tensor Networks.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/MB3PW4KU/S0004370221002009.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370221002009
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370221002009
      \endverb
      \keyw{Deep learning and reasoning,Many-valued logics,Neurosymbolic AI}
    \endentry
    \entry{balazevic_tucker_2019}{article}{}
      \name{author}{3}{}{%
        {{hash=c3a3cd634299d92769a1b0770f3a581d}{%
           family={Balazevic},
           familyi={B\bibinitperiod},
           given={Ivana},
           giveni={I\bibinitperiod}}}%
        {{hash=bfb3fcac1af14d0fb934c4fee4c0ea05}{%
           family={Allen},
           familyi={A\bibinitperiod},
           given={Carl},
           giveni={C\bibinitperiod}}}%
        {{hash=5c63f1e06139e794f16b23bff79f44d0}{%
           family={Hospedales},
           familyi={H\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{fullhash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{bibnamehash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{authorbibnamehash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{authornamehash}{54766c326fbb4521dec81a9d42b03b6a}
      \strng{authorfullhash}{54766c326fbb4521dec81a9d42b03b6a}
      \field{labelalpha}{BAH19}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively straightforward but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms previous state-of-the-art models across standard link prediction datasets, acting as a strong baseline for more elaborate models. We show that TuckER is a fully expressive model, derive sufficient bounds on its embedding dimensionalities and demonstrate that several previously introduced linear models can be viewed as special cases of TuckER.}
      \field{journaltitle}{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}
      \field{note}{Conference Name: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) Place: Hong Kong, China Publisher: Association for Computational Linguistics}
      \field{shorttitle}{{TuckER}}
      \field{title}{{TuckER}: {Tensor} {Factorization} for {Knowledge} {Graph} {Completion}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{5184\bibrangedash 5193}
      \range{pages}{10}
      \verb{doi}
      \verb 10.18653/v1/D19-1522
      \endverb
      \verb{file}
      \verb Akzeptierte Version:/Users/alexgoessmann/Zotero/storage/FJRTWBDV/Balazevic et al. - 2019 - TuckER Tensor Factorization for Knowledge Graph Completion.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.aclweb.org/anthology/D19-1522
      \endverb
      \verb{url}
      \verb https://www.aclweb.org/anthology/D19-1522
      \endverb
    \endentry
    \entry{barabasi_network_2016}{book}{}
      \name{author}{1}{}{%
        {{hash=15bacd7ddfc866c5dd96976d4b0244d5}{%
           family={Barabási},
           familyi={B\bibinitperiod},
           given={Albert-László},
           giveni={A\bibinithyphendelim L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{fullhash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{bibnamehash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{authorbibnamehash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{authornamehash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \strng{authorfullhash}{15bacd7ddfc866c5dd96976d4b0244d5}
      \field{labelalpha}{Bar16}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Networks are everywhere, from the internet, to social networks, and the genetic networks that determine our biological existence. Illustrated throughout in full colour, this pioneering textbook, spanning a wide range of topics from physics to computer science, engineering, economics and the social sciences, introduces network science to an interdisciplinary audience. From the origins of the six degrees of separation to explaining why networks are robust to random failures, the author explores how viruses like Ebola and H1N1 spread, and why it is that our friends have more friends than we do. Using numerous real-world examples, this innovatively designed text includes clear delineation between undergraduate and graduate level material. The mathematical formulas and derivations are included within Advanced Topics sections, enabling use at a range of levels. Extensive online resources, including films and software for network analysis, make this a multifaceted companion for anyone with an interest in network science.}
      \field{edition}{Illustrated edition}
      \field{isbn}{978-1-107-07626-6}
      \field{month}{7}
      \field{title}{Network {Science}}
      \field{year}{2016}
    \endentry
    \entry{bellman_adaptive_1961}{book}{}
      \name{author}{1}{}{%
        {{hash=2b0e24f47216c32d9ca0eb6f643646da}{%
           family={Bellman},
           familyi={B\bibinitperiod},
           given={Richard\bibnamedelima E.},
           giveni={R\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {New Jersey}%
      }
      \list{publisher}{1}{%
        {Princeton University Press}%
      }
      \strng{namehash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{fullhash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{bibnamehash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{authorbibnamehash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{authornamehash}{2b0e24f47216c32d9ca0eb6f643646da}
      \strng{authorfullhash}{2b0e24f47216c32d9ca0eb6f643646da}
      \field{labelalpha}{Bel61}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The aim of this work is to present a unified approach to the modern field of control theory and to provide a technique for making problems involving deterministic, stochastic, and adaptive processes of both linear and nonlinear type amenable to machine solution. Mr. Bellman has used the theory of dynamic programming to formulate, analyze, and prepare these processes for numerical treatment by digital computers. The unique concept of the book is that of a single problem stretching from recognition and formulation to analytic treatment and computational solution. Due to the emphasis upon ideas and concepts, this book is equally suited for the pure and applied mathematician, and for control engineers in all fields. Originally published in 1961. The Princeton Legacy Library uses the latest print-on-demand technology to again make available previously out-of-print books from the distinguished backlist of Princeton University Press. These editions preserve the original texts of these important books while presenting them in durable paperback and hardcover editions. The goal of the Princeton Legacy Library is to vastly increase access to the rich scholarly heritage found in the thousands of books published by Princeton University Press since its founding in 1905.}
      \field{isbn}{978-1-4008-7466-8}
      \field{note}{Publication Title: Adaptive Control Processes}
      \field{title}{Adaptive {Control} {Processes}}
      \field{urlday}{7}
      \field{urlmonth}{2}
      \field{urlyear}{2021}
      \field{year}{1961}
      \field{urldateera}{ce}
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/JY2L2MIB/html.html:text/html
      \endverb
    \endentry
    \entry{pan_tentris_2020}{incollection}{}
      \name{author}{6}{}{%
        {{hash=c32b41e85d5e8a64725b07a7910fcfab}{%
           family={Bigerl},
           familyi={B\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=7c5b22dd6d774288712bf3e9ab7cd18c}{%
           family={Conrads},
           familyi={C\bibinitperiod},
           given={Felix},
           giveni={F\bibinitperiod}}}%
        {{hash=8ff8bf39b7836be59f71591a851c00dd}{%
           family={Behning},
           familyi={B\bibinitperiod},
           given={Charlotte},
           giveni={C\bibinitperiod}}}%
        {{hash=a753a4fff61dd522108d75601d2d4252}{%
           family={Sherif},
           familyi={S\bibinitperiod},
           given={Mohamed\bibnamedelima Ahmed},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=d2274fbe986995b8ac0dabe2949b6c6f}{%
           family={Saleem},
           familyi={S\bibinitperiod},
           given={Muhammad},
           giveni={M\bibinitperiod}}}%
        {{hash=aaffae6ab501c5d30bd5ba943863c330}{%
           family={Ngonga\bibnamedelima Ngomo},
           familyi={N\bibinitperiod\bibinitdelim N\bibinitperiod},
           given={Axel-Cyrille},
           giveni={A\bibinithyphendelim C\bibinitperiod}}}%
      }
      \name{editor}{8}{}{%
        {{hash=278c599f0a2e44df256b1d60ef313ea9}{%
           family={Pan},
           familyi={P\bibinitperiod},
           given={Jeff\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=4fb77a4bd449d40757fa6f195ad238d4}{%
           family={Tamma},
           familyi={T\bibinitperiod},
           given={Valentina},
           giveni={V\bibinitperiod}}}%
        {{hash=3eb1335a7c6a4acf8ca2e632ae7f6a09}{%
           family={d’Amato},
           familyi={d\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod}}}%
        {{hash=f8032db780b653246bf029dcbffb6859}{%
           family={Janowicz},
           familyi={J\bibinitperiod},
           given={Krzysztof},
           giveni={K\bibinitperiod}}}%
        {{hash=a37941fd1ccfe1e67aede331acaa7a60}{%
           family={Fu},
           familyi={F\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=a2be0ec850b04b1f5e9fe3aca6bf5226}{%
           family={Polleres},
           familyi={P\bibinitperiod},
           given={Axel},
           giveni={A\bibinitperiod}}}%
        {{hash=9c134e61f193ba681c90bf21d73b5bd2}{%
           family={Seneviratne},
           familyi={S\bibinitperiod},
           given={Oshani},
           giveni={O\bibinitperiod}}}%
        {{hash=6cab81d607599e0adc135714724fa896}{%
           family={Kagal},
           familyi={K\bibinitperiod},
           given={Lalana},
           giveni={L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{f7dbbdc6ba16ea0b42fba7e139bea313}
      \strng{fullhash}{15b70d3e0224d899d7380f6ff7c1536b}
      \strng{bibnamehash}{f7dbbdc6ba16ea0b42fba7e139bea313}
      \strng{authorbibnamehash}{f7dbbdc6ba16ea0b42fba7e139bea313}
      \strng{authornamehash}{f7dbbdc6ba16ea0b42fba7e139bea313}
      \strng{authorfullhash}{15b70d3e0224d899d7380f6ff7c1536b}
      \strng{editorbibnamehash}{5ac4286d40d30d03998e458efb80853e}
      \strng{editornamehash}{5ac4286d40d30d03998e458efb80853e}
      \strng{editorfullhash}{181ca458bd90b841509e6207ad220276}
      \field{labelalpha}{Big+20}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{The {Semantic} {Web} – {ISWC} 2020}
      \field{isbn}{978-3-030-62418-7 978-3-030-62419-4}
      \field{note}{Series Title: Lecture Notes in Computer Science}
      \field{title}{Tentris – {A} {Tensor}-{Based} {Triple} {Store}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{12506}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{56\bibrangedash 73}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/978-3-030-62419-4_4
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/10.1007/978-3-030-62419-4_4
      \endverb
      \verb{url}
      \verb https://link.springer.com/10.1007/978-3-030-62419-4_4
      \endverb
    \endentry
    \entry{beylkin_algorithms_2005}{article}{}
      \name{author}{2}{}{%
        {{hash=4c320df8c1884cdf0cf4e2c5fd74cee9}{%
           family={Beylkin},
           familyi={B\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=90f304b60f077049361bf9ba18c8fa7a}{%
           family={Mohlenkamp},
           familyi={M\bibinitperiod},
           given={Martin\bibnamedelima J.},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{fullhash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{bibnamehash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{authorbibnamehash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{authornamehash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \strng{authorfullhash}{04fe62cb203d5ddc55f09ae3194c8de7}
      \field{labelalpha}{BM05}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Nearly every numerical analysis algorithm has computational complexity that scales exponentially in the underlying physical dimension. The separated representation, introduced previously, allows many operations to be performed with scaling that is formally linear in the dimension. In this paper we further develop this representation by (i) discussing the variety of mechanisms that allow it to be surprisingly eﬃcient; (ii) addressing the issue of conditioning; (iii) presenting algorithms for solving linear systems within this framework; and (iv) demonstrating methods for dealing with antisymmetric functions, as arise in the multiparticle Schr¨odinger equation in quantum mechanics. Numerical examples are given.}
      \field{issn}{1064-8275, 1095-7197}
      \field{journaltitle}{SIAM Journal on Scientific Computing}
      \field{month}{1}
      \field{number}{6}
      \field{title}{Algorithms for {Numerical} {Analysis} in {High} {Dimensions}}
      \field{urlday}{11}
      \field{urlmonth}{12}
      \field{urlyear}{2019}
      \field{volume}{26}
      \field{year}{2005}
      \field{urldateera}{ce}
      \field{pages}{2133\bibrangedash 2159}
      \range{pages}{27}
      \verb{doi}
      \verb 10.1137/040604959
      \endverb
      \verb{file}
      \verb Beylkin und Mohlenkamp - 2005 - Algorithms for Numerical Analysis in High Dimensio.pdf:/Users/alexgoessmann/Zotero/storage/FJLW6WC5/Beylkin und Mohlenkamp - 2005 - Algorithms for Numerical Analysis in High Dimensio.pdf:application/pdf
      \endverb
    \endentry
    \entry{clifford_markov_1971}{article}{}
      \name{author}{2}{}{%
        {{hash=b820cb9e1fc07ca0974992bb8a99c6c6}{%
           family={Clifford},
           familyi={C\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod}}}%
        {{hash=08a376882401bc1c10ca00e178fdc184}{%
           family={Hammersley},
           familyi={H\bibinitperiod},
           given={J.\bibnamedelimi M.},
           giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \strng{namehash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{fullhash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{bibnamehash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{authorbibnamehash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{authornamehash}{6869f36d335715f8c9eb387cea521c5c}
      \strng{authorfullhash}{6869f36d335715f8c9eb387cea521c5c}
      \field{labelalpha}{CH71}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Unpublished}
      \field{note}{Publisher: University of Oxford}
      \field{title}{Markov fields on finite graphs and lattices}
      \field{urlday}{11}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{1971}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/9H3CY45B/Clifford und Hammersley - 1971 - Markov fields on finite graphs and lattices.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://ora.ox.ac.uk/objects/uuid:4ea849da-1511-4578-bb88-6a8d02f457a6
      \endverb
      \verb{url}
      \verb https://ora.ox.ac.uk/objects/uuid:4ea849da-1511-4578-bb88-6a8d02f457a6
      \endverb
    \endentry
    \entry{chandrasekaran_convex_2012}{article}{}
      \name{author}{4}{}{%
        {{hash=6c455027167a2474bcbecfd17ec411fa}{%
           family={Chandrasekaran},
           familyi={C\bibinitperiod},
           given={Venkat},
           giveni={V\bibinitperiod}}}%
        {{hash=3503059e1c0c778913607c87d8c5173a}{%
           family={Recht},
           familyi={R\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=8c175284da0c757c59f059ff13d91dde}{%
           family={Parrilo},
           familyi={P\bibinitperiod},
           given={Pablo\bibnamedelima A.},
           giveni={P\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=4dfdd3890b62d46ff882fa27ef7f2af0}{%
           family={Willsky},
           familyi={W\bibinitperiod},
           given={Alan\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{583c31774c04233f0926feec90ea3afb}
      \strng{fullhash}{ff0ef736c7e2776b160177289bf22907}
      \strng{bibnamehash}{583c31774c04233f0926feec90ea3afb}
      \strng{authorbibnamehash}{583c31774c04233f0926feec90ea3afb}
      \strng{authornamehash}{583c31774c04233f0926feec90ea3afb}
      \strng{authorfullhash}{ff0ef736c7e2776b160177289bf22907}
      \field{labelalpha}{Cha+12}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1615-3375, 1615-3383}
      \field{journaltitle}{Foundations of Computational Mathematics}
      \field{month}{12}
      \field{number}{6}
      \field{title}{The {Convex} {Geometry} of {Linear} {Inverse} {Problems}}
      \field{urlday}{1}
      \field{urlmonth}{8}
      \field{urlyear}{2019}
      \field{volume}{12}
      \field{year}{2012}
      \field{urldateera}{ce}
      \field{pages}{805\bibrangedash 849}
      \range{pages}{45}
      \verb{doi}
      \verb 10.1007/s10208-012-9135-7
      \endverb
      \verb{file}
      \verb Chandrasekaran et al. - 2012 - The Convex Geometry of Linear Inverse Problems.pdf:/Users/alexgoessmann/Zotero/storage/3MVAW86X/Chandrasekaran et al. - 2012 - The Convex Geometry of Linear Inverse Problems.pdf:application/pdf
      \endverb
    \endentry
    \entry{cichocki_tensor_2015}{article}{}
      \name{author}{7}{}{%
        {{hash=08ffe474d1c0bebee63a5696b8bc17dd}{%
           family={Cichocki},
           familyi={C\bibinitperiod},
           given={Andrzej},
           giveni={A\bibinitperiod}}}%
        {{hash=c0e4066e714faa43098fa185074e7fda}{%
           family={Mandic},
           familyi={M\bibinitperiod},
           given={Danilo},
           giveni={D\bibinitperiod}}}%
        {{hash=422686a2e68522fbd3ed47c5f42b0fc7}{%
           family={De\bibnamedelima Lathauwer},
           familyi={D\bibinitperiod\bibinitdelim L\bibinitperiod},
           given={Lieven},
           giveni={L\bibinitperiod}}}%
        {{hash=11506400eabf2f393c9bf5b2f5ea3aa9}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Guoxu},
           giveni={G\bibinitperiod}}}%
        {{hash=8a239bd07f908087a20dfcf87572e035}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Qibin},
           giveni={Q\bibinitperiod}}}%
        {{hash=ec082fda5bf6ea5dc36385b22e35ea06}{%
           family={Caiafa},
           familyi={C\bibinitperiod},
           given={Cesar},
           giveni={C\bibinitperiod}}}%
        {{hash=fab8f9863c52c69f930c9839ad237404}{%
           family={PHAN},
           familyi={P\bibinitperiod},
           given={HUY\bibnamedelima ANH},
           giveni={H\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{fe89acd1fc18bfcd61f5d60877595c4f}
      \strng{fullhash}{f824fb050585cd9c96ec7600a5f3a99c}
      \strng{bibnamehash}{fe89acd1fc18bfcd61f5d60877595c4f}
      \strng{authorbibnamehash}{fe89acd1fc18bfcd61f5d60877595c4f}
      \strng{authornamehash}{fe89acd1fc18bfcd61f5d60877595c4f}
      \strng{authorfullhash}{f824fb050585cd9c96ec7600a5f3a99c}
      \field{labelalpha}{Cic+15}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The widespread use of multisensor technology and the emergence of big data sets have highlighted the limitations of standard flat-view matrix models and the necessity to move toward more versatile data analysis tools. We show that higher-order tensors (i.e., multiway arrays) enable such a fundamental paradigm shift toward models that are essentially polynomial, the uniqueness of which, unlike the matrix methods, is guaranteed under very mild and natural conditions. Benefiting from the power of multilinear algebra as their mathematical backbone, data analysis techniques using tensor decompositions are shown to have great flexibility in the choice of constraints which match data properties and extract more general latent components in the data than matrix-based methods.}
      \field{issn}{1558-0792}
      \field{journaltitle}{IEEE Signal Processing Magazine}
      \field{month}{3}
      \field{note}{Conference Name: IEEE Signal Processing Magazine}
      \field{number}{2}
      \field{shorttitle}{Tensor {Decompositions} for {Signal} {Processing} {Applications}}
      \field{title}{Tensor {Decompositions} for {Signal} {Processing} {Applications}: {From} two-way to multiway component analysis}
      \field{volume}{32}
      \field{year}{2015}
      \field{pages}{145\bibrangedash 163}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1109/MSP.2013.2297439
      \endverb
      \verb{file}
      \verb IEEE Xplore Abstract Record:/Users/alexgoessmann/Zotero/storage/PUSDA9DX/7038247.html:text/html;Volltext:/Users/alexgoessmann/Zotero/storage/TPA95G4X/Cichocki et al. - 2015 - Tensor Decompositions for Signal Processing Applic.pdf:application/pdf
      \endverb
      \keyw{Matrix decomposition,Data models,Tensile stress,Big data,Data analysis,Sensors}
    \endentry
    \entry{cichocki_era_2014}{article}{}
      \name{author}{1}{}{%
        {{hash=08ffe474d1c0bebee63a5696b8bc17dd}{%
           family={Cichocki},
           familyi={C\bibinitperiod},
           given={Andrzej},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{fullhash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{bibnamehash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{authorbibnamehash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{authornamehash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \strng{authorfullhash}{08ffe474d1c0bebee63a5696b8bc17dd}
      \field{labelalpha}{Cic14}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Many problems in computational neuroscience, neuroinformatics, pattern/image recognition, signal processing and machine learning generate massive amounts of multidimensional data with multiple aspects and high dimensionality. Tensors (i.e., multi-way arrays) provide often a natural and compact representation for such massive multidimensional data via suitable low-rank approximations. Big data analytics require novel technologies to efficiently process huge datasets within tolerable elapsed times. Such a new emerging technology for multidimensional big data is a multiway analysis via tensor networks (TNs) and tensor decompositions (TDs) which represent tensors by sets of factor (component) matrices and lower-order (core) tensors. Dynamic tensor analysis allows us to discover meaningful hidden structures of complex data and to perform generalizations by capturing multi-linear and multi-aspect relationships. We will discuss some fundamental TN models, their mathematical and graphical descriptions and associated learning algorithms for large-scale TDs and TNs, with many potential applications including: Anomaly detection, feature extraction, classification, cluster analysis, data fusion and integration, pattern recognition, predictive modeling, regression, time series analysis and multiway component analysis. Keywords: Large-scale HOSVD, Tensor decompositions, CPD, Tucker models, Hierarchical Tucker (HT) decomposition, low-rank tensor approximations (LRA), Tensorization/Quantization, tensor train (TT/QTT) - Matrix Product States (MPS), Matrix Product Operator (MPO), DMRG, Strong Kronecker Product (SKP).}
      \field{journaltitle}{arXiv:1403.2048 [cs]}
      \field{month}{3}
      \field{note}{arXiv: 1403.2048}
      \field{shorttitle}{Era of {Big} {Data} {Processing}}
      \field{title}{Era of {Big} {Data} {Processing}: {A} {New} {Approach} via {Tensor} {Networks} and {Tensor} {Decompositions}}
      \field{urlday}{16}
      \field{urlmonth}{3}
      \field{urlyear}{2019}
      \field{year}{2014}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv\:1403.2048 PDF:/Users/alexgoessmann/Zotero/storage/2RGL553C/Cichocki - 2014 - Era of Big Data Processing A New Approach via Ten.pdf:application/pdf;arXiv.org Snapshot:/Users/alexgoessmann/Zotero/storage/YDJ7VX4M/1403.html:text/html
      \endverb
      \keyw{Computer Science - Emerging Technologies,Check: Tensor Decomposition}
    \endentry
    \entry{casazza_introduction_2013}{incollection}{}
      \name{author}{3}{}{%
        {{hash=410d97c82a87cce712d1169d5fe05808}{%
           family={Casazza},
           familyi={C\bibinitperiod},
           given={Peter\bibnamedelima G.},
           giveni={P\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=2e7b05c7e0c405468d37834ab05737b3}{%
           family={Kutyniok},
           familyi={K\bibinitperiod},
           given={Gitta},
           giveni={G\bibinitperiod}}}%
        {{hash=6bd7f9f0bea75326150f6fd2fdad5f6e}{%
           family={Philipp},
           familyi={P\bibinitperiod},
           given={Friedrich},
           giveni={F\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=410d97c82a87cce712d1169d5fe05808}{%
           family={Casazza},
           familyi={C\bibinitperiod},
           given={Peter\bibnamedelima G.},
           giveni={P\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=2e7b05c7e0c405468d37834ab05737b3}{%
           family={Kutyniok},
           familyi={K\bibinitperiod},
           given={Gitta},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Boston}%
      }
      \list{publisher}{1}{%
        {Birkhäuser}%
      }
      \strng{namehash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{fullhash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{bibnamehash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{authorbibnamehash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{authornamehash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{authorfullhash}{c9c7e83b3e40a774dfbf7d60aff9084b}
      \strng{editorbibnamehash}{ead7359e306f40ee469ee359c9363e26}
      \strng{editornamehash}{ead7359e306f40ee469ee359c9363e26}
      \strng{editorfullhash}{ead7359e306f40ee469ee359c9363e26}
      \field{labelalpha}{CKP13}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To date, frames have established themselves as a standard notion in applied mathematics, computer science, and engineering as a means to derive redundant, yet stable decompositions of a signal for analysis or transmission, while also promoting sparse expansions. The reconstruction procedure is then based on one of the associated dual frames, which—in the case of a Parseval frame—can be chosen to be the frame itself. In this chapter, we provide a comprehensive review of the basics of finite frame theory upon which the subsequent chapters are based. After recalling some background information on Hilbert space theory and operator theory, we introduce the notion of a frame along with some crucial properties and construction procedures. Then we discuss algorithmic aspects such as basic reconstruction algorithms and present brief introductions to diverse applications and extensions of frames. The subsequent chapters of this book will then extend key topics in many intriguing directions.}
      \field{booktitle}{Finite {Frames}: {Theory} and {Applications}}
      \field{isbn}{978-0-8176-8373-3}
      \field{title}{Introduction to {Finite} {Frame} {Theory}}
      \field{urlday}{4}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{year}{2013}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 53}
      \range{pages}{53}
      \verb{doi}
      \verb 10.1007/978-0-8176-8373-3_1
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-0-8176-8373-3_1
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-0-8176-8373-3_1
      \endverb
    \endentry
    \entry{cover_elements_2006}{book}{}
      \name{author}{2}{}{%
        {{hash=fe5b7bbda12c7502f55ddb9eeded8622}{%
           family={Cover},
           familyi={C\bibinitperiod},
           given={Thomas\bibnamedelima M.},
           giveni={T\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=e6518a9e085a3ee5221c27f5d8f826dc}{%
           family={Thomas},
           familyi={T\bibinitperiod},
           given={Joy\bibnamedelima A.},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Hoboken, N.J}%
      }
      \list{publisher}{1}{%
        {Wiley-Interscience}%
      }
      \strng{namehash}{e427ba6c0eda5292558c30c786236c01}
      \strng{fullhash}{e427ba6c0eda5292558c30c786236c01}
      \strng{bibnamehash}{e427ba6c0eda5292558c30c786236c01}
      \strng{authorbibnamehash}{e427ba6c0eda5292558c30c786236c01}
      \strng{authornamehash}{e427ba6c0eda5292558c30c786236c01}
      \strng{authorfullhash}{e427ba6c0eda5292558c30c786236c01}
      \field{labelalpha}{CT06}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The latest edition of this classic is updated with new problem sets and material The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory. All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points. The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.}
      \field{edition}{2nd edition}
      \field{isbn}{978-0-471-24195-9}
      \field{month}{9}
      \field{title}{Elements of {Information} {Theory}}
      \field{year}{2006}
    \endentry
    \entry{cohen_tensorlog_2020}{article}{}
      \name{author}{3}{}{%
        {{hash=be006ecd16e3b55e58d8a895354f80e8}{%
           family={Cohen},
           familyi={C\bibinitperiod},
           given={William},
           giveni={W\bibinitperiod}}}%
        {{hash=bac6a7a5d8c835c2ebe5288a6fe722b4}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Fan},
           giveni={F\bibinitperiod}}}%
        {{hash=0dccc789340e78e1fa270ba488b2f967}{%
           family={Mazaitis},
           familyi={M\bibinitperiod},
           given={Kathryn\bibnamedelima Rivard},
           giveni={K\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{f0d351bed453e3e9245e03adce264797}
      \strng{fullhash}{f0d351bed453e3e9245e03adce264797}
      \strng{bibnamehash}{f0d351bed453e3e9245e03adce264797}
      \strng{authorbibnamehash}{f0d351bed453e3e9245e03adce264797}
      \strng{authornamehash}{f0d351bed453e3e9245e03adce264797}
      \strng{authorfullhash}{f0d351bed453e3e9245e03adce264797}
      \field{labelalpha}{CYM20}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.}
      \field{issn}{1076-9757}
      \field{journaltitle}{Journal of Artificial Intelligence Research}
      \field{month}{2}
      \field{shorttitle}{{TensorLog}}
      \field{title}{{TensorLog}: {A} {Probabilistic} {Database} {Implemented} {Using} {Deep}-{Learning} {Infrastructure}}
      \field{urlday}{20}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{67}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{285\bibrangedash 325}
      \range{pages}{41}
      \verb{doi}
      \verb 10.1613/jair.1.11944
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/QK9JGLW2/Cohen et al. - 2020 - TensorLog A Probabilistic Database Implemented Using Deep-Learning Infrastructure.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://jair.org/index.php/jair/article/view/11944
      \endverb
      \verb{url}
      \verb https://jair.org/index.php/jair/article/view/11944
      \endverb
    \endentry
    \entry{degroot_probability_2016}{book}{}
      \name{author}{1}{}{%
        {{hash=a003c6a3b15a33bf2d075ba0e21f809c}{%
           family={DeGroot},
           familyi={D\bibinitperiod},
           given={Morris\bibnamedelima H.},
           giveni={M\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{publisher}{1}{%
        {PEARSON INDIA}%
      }
      \strng{namehash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{fullhash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{bibnamehash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{authorbibnamehash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{authornamehash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \strng{authorfullhash}{a003c6a3b15a33bf2d075ba0e21f809c}
      \field{labelalpha}{DeG16}
      \field{sortinit}{D}
      \field{sortinithash}{2ef1bd9a78cc71eb74d7231c635177b8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Brand New}
      \field{isbn}{978-93-325-7387-1}
      \field{month}{1}
      \field{title}{Probability and {Statistics}}
      \field{year}{2016}
    \endentry
    \entry{demir_drill-_2021}{article}{}
      \name{author}{2}{}{%
        {{hash=0fdd0add34c43fbcb7685ace9926ce07}{%
           family={Demir},
           familyi={D\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=aaffae6ab501c5d30bd5ba943863c330}{%
           family={Ngonga\bibnamedelima Ngomo},
           familyi={N\bibinitperiod\bibinitdelim N\bibinitperiod},
           given={Axel-Cyrille},
           giveni={A\bibinithyphendelim C\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {eng}%
      }
      \strng{namehash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{fullhash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{bibnamehash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{authorbibnamehash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{authornamehash}{2deba4d47a871d960ec9ed6806236b5b}
      \strng{authorfullhash}{2deba4d47a871d960ec9ed6806236b5b}
      \field{labelalpha}{DN21}
      \field{sortinit}{D}
      \field{sortinithash}{2ef1bd9a78cc71eb74d7231c635177b8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{CoRR}
      \field{title}{{DRILL}- {Deep} {Reinforcement} {Learning} for {Refinement} {Operators} in {ALC}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{abs/2106.15373}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/J97XWWW6/25217.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://ris.uni-paderborn.de/record/25217
      \endverb
      \verb{url}
      \verb https://ris.uni-paderborn.de/record/25217
      \endverb
    \endentry
    \entry{espig_variational_2012}{article}{}
      \name{author}{4}{}{%
        {{hash=1e64c76ecca3f4c3da8e872e5547d7e8}{%
           family={Espig},
           familyi={E\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod}}}%
        {{hash=01bcfafc59c11a565163fcf9ef2a0907}{%
           family={Hackbusch},
           familyi={H\bibinitperiod},
           given={Wolfgang},
           giveni={W\bibinitperiod}}}%
        {{hash=9fa4aa09a1f933a0b479542d3d8f02b8}{%
           family={Rohwedder},
           familyi={R\bibinitperiod},
           given={Thorsten},
           giveni={T\bibinitperiod}}}%
        {{hash=cd91fef38e4801ef30c61055e408b8b5}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Reinhold},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{252453e0b9026385cd8b45f3c59c5602}
      \strng{fullhash}{5d38a9e09a4be73336eb565acdb5cfe5}
      \strng{bibnamehash}{252453e0b9026385cd8b45f3c59c5602}
      \strng{authorbibnamehash}{252453e0b9026385cd8b45f3c59c5602}
      \strng{authornamehash}{252453e0b9026385cd8b45f3c59c5602}
      \strng{authorfullhash}{5d38a9e09a4be73336eb565acdb5cfe5}
      \field{labelalpha}{Esp+12}
      \field{sortinit}{E}
      \field{sortinithash}{f615fb9c6fba11c6f962fb3fd599810e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this article we introduce a calculus of variations for sums of elementary tensors and apply it to functionals of practical interest. The survey provides all necessary ingredients for applying minimization methods in a general setting. The important cases of target functionals which are linear and quadratic with respect to the tensor product are discussed, and combinations of these functionals are presented in detail. As an example, we consider the solution of a linear system in structured tensor format. Moreover, we discuss the solution of an eigenvalue problem with sums of elementary tensors. This example can be viewed as a prototype of a constrained minimization problem. For the numerical treatment, we suggest a method which has the same order of complexity as the popular alternating least square algorithm and demonstrate the rate of convergence in numerical tests.}
      \field{issn}{0945-3245}
      \field{journaltitle}{Numerische Mathematik}
      \field{month}{11}
      \field{number}{3}
      \field{title}{Variational calculus with sums of elementary tensors of fixed rank}
      \field{urlday}{13}
      \field{urlmonth}{10}
      \field{urlyear}{2021}
      \field{volume}{122}
      \field{year}{2012}
      \field{urldateera}{ce}
      \field{pages}{469\bibrangedash 488}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1007/s00211-012-0464-x
      \endverb
      \verb{file}
      \verb Springer Full Text PDF:/Users/alexgoessmann/Zotero/storage/5NZ74BCJ/Espig et al. - 2012 - Variational calculus with sums of elementary tenso.pdf:application/pdf
      \endverb
    \endentry
    \entry{falco_minimal_2012}{article}{}
      \name{author}{2}{}{%
        {{hash=d06f4f4eeb734a17f1446d71fa0fe557}{%
           family={Falco},
           familyi={F\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod}}}%
        {{hash=01bcfafc59c11a565163fcf9ef2a0907}{%
           family={Hackbusch},
           familyi={H\bibinitperiod},
           given={Wolfgang},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{fullhash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{bibnamehash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{authorbibnamehash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{authornamehash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \strng{authorfullhash}{b9e671f3097f728e8aedf5c8ef02bc4e}
      \field{labelalpha}{FH12}
      \field{sortinit}{F}
      \field{sortinithash}{669c706c6f1fbf3b5a83d26f1d9e9e72}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{n this paper we introduce and develop the notion of minimal subspaces in the framework of algebraic and topological tensor product spaces. This mathematical structure arises in a natural way in the study of tensor representations. We use minimal subspaces to prove the existence of a best approximation, for any element in a Banach tensor space, by means of a tensor given in a typical representation format (Tucker, hierarchical, or tensor train). We show that this result holds in a tensor Banach space with a norm stronger than the injective norm and in an intersection of finitely many Banach tensor spaces satisfying some additional conditions. Examples using topological tensor products of standard Sobolev spaces are given}
      \field{journaltitle}{Foundations of Computational Mathematics}
      \field{month}{12}
      \field{title}{On {Minimal} {Subspaces} in {Tensor} {Representations}}
      \field{volume}{12}
      \field{year}{2012}
      \field{pages}{765\bibrangedash 803}
      \range{pages}{39}
      \verb{doi}
      \verb 10.1007/s10208-012-9136-6
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/ZUJB6QZE/Falco und Hackbusch - 2012 - On Minimal Subspaces in Tensor Representations.pdf:application/pdf
      \endverb
    \endentry
    \entry{python_software_foundation_python_2025}{misc}{}
      \name{author}{1}{}{%
        {{hash=9688a568902e4b07b60557b566bdd820}{%
           family={Foundation},
           familyi={F\bibinitperiod},
           given={Python\bibnamedelima Software},
           giveni={P\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{9688a568902e4b07b60557b566bdd820}
      \strng{fullhash}{9688a568902e4b07b60557b566bdd820}
      \strng{bibnamehash}{9688a568902e4b07b60557b566bdd820}
      \strng{authorbibnamehash}{9688a568902e4b07b60557b566bdd820}
      \strng{authornamehash}{9688a568902e4b07b60557b566bdd820}
      \strng{authorfullhash}{9688a568902e4b07b60557b566bdd820}
      \field{labelalpha}{Fou25}
      \field{sortinit}{F}
      \field{sortinithash}{669c706c6f1fbf3b5a83d26f1d9e9e72}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{4}
      \field{title}{Python {Language} {Reference}, version 3.13.2}
      \field{urlday}{6}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{urldateera}{ce}
      \verb{file}
      \verb 3.13.2 Documentation:/Users/alexgoessmann/Zotero/storage/JJDPYYBU/3.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://docs.python.org/3/
      \endverb
      \verb{url}
      \verb https://docs.python.org/3/
      \endverb
    \endentry
    \entry{foucart_mathematical_2013}{book}{}
      \name{author}{2}{}{%
        {{hash=852f06e143387da6c5a4375756356399}{%
           family={Foucart},
           familyi={F\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
        {{hash=750a4d4191d5837bca7b0ed740eebabc}{%
           family={Rauhut},
           familyi={R\bibinitperiod},
           given={Holger},
           giveni={H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Birkhäuser Basel}%
      }
      \strng{namehash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{fullhash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{bibnamehash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{authorbibnamehash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{authornamehash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \strng{authorfullhash}{5c96a8d30fe2de5fbe75618cffa91d04}
      \field{labelalpha}{FR13}
      \field{sortinit}{F}
      \field{sortinithash}{669c706c6f1fbf3b5a83d26f1d9e9e72}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{At the intersection of mathematics, engineering, and computer science sits the thriving field of compressive sensing. Based on the premise that data acquisition and compression can be performed simultaneously, compressive sensing finds applications in imaging, signal processing, and many other domains. In the areas of applied mathematics, electrical engineering, and theoretical computer science, an explosion of research activity has already followed the theoretical results that highlighted the efficiency of the basic principles. The elegant ideas behind these principles are also of independent interest to pure mathematicians.A Mathematical Introduction to Compressive Sensing gives a detailed account of the core theory upon which the field is build. With only moderate prerequisites, it is an excellent textbook for graduate courses in mathematics, engineering, and computer science. It also serves as a reliable resource for practitioners and researchers in these disciplines who want to acquire a careful understanding of the subject. A Mathematical Introduction to Compressive Sensing uses a mathematical perspective to present the core of the theory underlying compressive sensing.}
      \field{isbn}{978-0-8176-4947-0}
      \field{series}{Applied and {Numerical} {Harmonic} {Analysis}}
      \field{title}{A {Mathematical} {Introduction} to {Compressive} {Sensing}}
      \field{urlday}{11}
      \field{urlmonth}{12}
      \field{urlyear}{2019}
      \field{year}{2013}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-0-8176-4948-7
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/VRC9U6NH/9780817649470.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.springer.com/de/book/9780817649470
      \endverb
      \verb{url}
      \verb https://www.springer.com/de/book/9780817649470
      \endverb
    \endentry
    \entry{galarraga_amie_2013}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=18674af5afb94551a3ce682e24e6706d}{%
           family={Galárraga},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelima Antonio},
           giveni={L\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=04816795c9877feddae45c038e2508f3}{%
           family={Teflioudi},
           familyi={T\bibinitperiod},
           given={Christina},
           giveni={C\bibinitperiod}}}%
        {{hash=1b753e139c8ab7af870d74e326b796ad}{%
           family={Hose},
           familyi={H\bibinitperiod},
           given={Katja},
           giveni={K\bibinitperiod}}}%
        {{hash=55d7f682b85e1f9066365cc900f8331c}{%
           family={Suchanek},
           familyi={S\bibinitperiod},
           given={Fabian},
           giveni={F\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Rio de Janeiro Brazil}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{1d5ca0dcef50a6c0946037e7a697d1ee}
      \strng{fullhash}{5c770da4ca763c325e09a0b506f3ed61}
      \strng{bibnamehash}{1d5ca0dcef50a6c0946037e7a697d1ee}
      \strng{authorbibnamehash}{1d5ca0dcef50a6c0946037e7a697d1ee}
      \strng{authornamehash}{1d5ca0dcef50a6c0946037e7a697d1ee}
      \strng{authorfullhash}{5c770da4ca763c325e09a0b506f3ed61}
      \field{labelalpha}{Gal+13}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive Logic Programming (ILP) can be used to mine logical rules from the KB. These rules can help deduce and add missing knowledge to the KB. While ILP is a mature ﬁeld, mining logical rules from KBs is diﬀerent in two aspects: First, current rule mining systems are easily overwhelmed by the amount of data (state-of-the art systems cannot even run on today’s KBs). Second, ILP usually requires counterexamples. KBs, however, implement the open world assumption (OWA), meaning that absent data cannot be used as counterexamples. In this paper, we develop a rule mining model that is explicitly tailored to support the OWA scenario. It is inspired by association rule mining and introduces a novel measure for conﬁdence. Our extensive experiments show that our approach outperforms state-of-the-art approaches in terms of precision and coverage. Furthermore, our system, AMIE, mines rules orders of magnitude faster than state-of-the-art approaches.}
      \field{booktitle}{Proceedings of the 22nd international conference on {World} {Wide} {Web}}
      \field{isbn}{978-1-4503-2035-1}
      \field{month}{5}
      \field{shorttitle}{{AMIE}}
      \field{title}{{AMIE}: association rule mining under incomplete evidence in ontological knowledge bases}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2013}
      \field{urldateera}{ce}
      \field{pages}{413\bibrangedash 422}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/2488388.2488425
      \endverb
      \verb{file}
      \verb Galárraga et al. - 2013 - AMIE association rule mining under incomplete evidence in ontological knowledge bases.pdf:/Users/alexgoessmann/Zotero/storage/5J7XU7WH/Galárraga et al. - 2013 - AMIE association rule mining under incomplete evidence in ontological knowledge bases.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/2488388.2488425
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/2488388.2488425
      \endverb
    \endentry
    \entry{ganapathi_constrained_2008}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=f6a7c1bcc9e1d51e613d3f4e5115677c}{%
           family={Ganapathi},
           familyi={G\bibinitperiod},
           given={Varun},
           giveni={V\bibinitperiod}}}%
        {{hash=2ee1a3110b48ca8da37c9092275ff42c}{%
           family={Vickrey},
           familyi={V\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=b8fbef1897da5bf46822ced31bc865c6}{%
           family={Duchi},
           familyi={D\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=9c785bcf6b8a0b99e44695299a6aecb2}{%
           family={Koller},
           familyi={K\bibinitperiod},
           given={Daphne},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Arlington, Virginia, USA}%
      }
      \list{publisher}{1}{%
        {AUAI Press}%
      }
      \strng{namehash}{e68528775f03ffb76e496bc61b8253a1}
      \strng{fullhash}{f412d5b2551da307a380b061e20d36b1}
      \strng{bibnamehash}{e68528775f03ffb76e496bc61b8253a1}
      \strng{authorbibnamehash}{e68528775f03ffb76e496bc61b8253a1}
      \strng{authornamehash}{e68528775f03ffb76e496bc61b8253a1}
      \strng{authorfullhash}{f412d5b2551da307a380b061e20d36b1}
      \field{labelalpha}{Gan+08}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Parameter estimation in Markov random fields (MRFs) is a difficult task, in which inference over the network is run in the inner loop of a gradient descent procedure. Replacing exact inference with approximate methods such as loopy belief propagation (LBP) can suffer from poor convergence. In this paper, we provide a different approach for combining MRF learning and Bethe approximation. We consider the dual of maximum likelihood Markov network learning — maximizing entropy with moment matching constraints — and then approximate both the objective and the constraints in the resulting optimization problem. Unlike previous work along these lines (Teh \&amp; Welling, 2003), our formulation allows parameter sharing between features in a general log-linear model, parameter regularization and conditional training. We show that piecewise training (Sutton \&amp; McCallum, 2005) is a very restricted special case of this formulation. We study two optimization strategies: one based on a single convex approximation and one that uses repeated convex approximations. We show results on several real-world networks that demonstrate that these algorithms can significantly outperform learning with loopy and piece-wise. Our results also provide a framework for analyzing the trade-offs of different relaxations of the entropy objective and of the constraints.}
      \field{booktitle}{Proceedings of the {Twenty}-{Fourth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}}
      \field{isbn}{978-0-9749039-4-1}
      \field{month}{7}
      \field{series}{{UAI}'08}
      \field{title}{Constrained approximate maximum entropy learning of {Markov} random fields}
      \field{urlday}{29}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2008}
      \field{urldateera}{ce}
      \field{pages}{196\bibrangedash 203}
      \range{pages}{8}
    \endentry
    \entry{garcez_neural-symbolic_2019}{misc}{}
      \name{author}{6}{}{%
        {{hash=70658466246873155f5b9b3a737cdd4e}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur\bibnamedelima d'Avila},
           giveni={A\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=d25d4f83ae540b2b728a5423e9fbe85b}{%
           family={Gori},
           familyi={G\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=8a6bd56db2a68b7d2f595538052b85e9}{%
           family={Lamb},
           familyi={L\bibinitperiod},
           given={Luis\bibnamedelima C.},
           giveni={L\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=cfd7b672fd7926ef1136b9790438416d}{%
           family={Serafini},
           familyi={S\bibinitperiod},
           given={Luciano},
           giveni={L\bibinitperiod}}}%
        {{hash=fecf9d2471f6f2bf1fe02253df553444}{%
           family={Spranger},
           familyi={S\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=47cf7a42e3145f38a9f22b292476284a}{%
           family={Tran},
           familyi={T\bibinitperiod},
           given={Son\bibnamedelima N.},
           giveni={S\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{2a101fc4b616118ed7bec25944d03c40}
      \strng{fullhash}{d0eeb1e4c9ede67d86f1db0bb58c4f99}
      \strng{bibnamehash}{2a101fc4b616118ed7bec25944d03c40}
      \strng{authorbibnamehash}{2a101fc4b616118ed7bec25944d03c40}
      \strng{authornamehash}{2a101fc4b616118ed7bec25944d03c40}
      \strng{authorfullhash}{d0eeb1e4c9ede67d86f1db0bb58c4f99}
      \field{labelalpha}{Gar+19}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.}
      \field{month}{5}
      \field{note}{arXiv:1905.06088 [cs]}
      \field{shorttitle}{Neural-{Symbolic} {Computing}}
      \field{title}{Neural-{Symbolic} {Computing}: {An} {Effective} {Methodology} for {Principled} {Integration} of {Machine} {Learning} and {Reasoning}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1905.06088
      \endverb
      \verb{file}
      \verb Preprint PDF:/Users/alexgoessmann/Zotero/storage/6NGQJ8Z6/Garcez et al. - 2019 - Neural-Symbolic Computing An Effective Methodology for Principled Integration of Machine Learning a.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/RQ2KIXXN/1905.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1905.06088
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1905.06088
      \endverb
      \keyw{Computer Science - Artificial Intelligence}
    \endentry
    \entry{gels_multidimensional_2019}{article}{}
      \name{author}{4}{}{%
        {{hash=aae598c66790875fb3f97e9e696f287f}{%
           family={Gelß},
           familyi={G\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
        {{hash=0c9864cb960ba6285d9bf3e76a0060a9}{%
           family={Klus},
           familyi={K\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=878172bfd75f93a96d3646d7b66c3e48}{%
           family={Eisert},
           familyi={E\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
        {{hash=c40cc853c219070fbe80745919805677}{%
           family={Schütte},
           familyi={S\bibinitperiod},
           given={Christof},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{41185c195913dab8e19a507c1b227dd1}
      \strng{fullhash}{9daf53e832a5e5f1372a37a1ba7c0096}
      \strng{bibnamehash}{41185c195913dab8e19a507c1b227dd1}
      \strng{authorbibnamehash}{41185c195913dab8e19a507c1b227dd1}
      \strng{authornamehash}{41185c195913dab8e19a507c1b227dd1}
      \strng{authorfullhash}{9daf53e832a5e5f1372a37a1ba7c0096}
      \field{labelalpha}{Gel+19}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A key task in the field of modeling and analyzing nonlinear dynamical systems is the recovery of unknown governing equations from measurement data only. There is a wide range of application areas for this important instance of system identification, ranging from industrial engineering and acoustic signal processing to stock market models. In order to find appropriate representations of underlying dynamical systems, various data-driven methods have been proposed by different communities. However, if the given data sets are high-dimensional, then these methods typically suffer from the curse of dimensionality. To significantly reduce the computational costs and storage consumption, we propose the method multidimensional approximation of nonlinear dynamical systems (MANDy) which combines data-driven methods with tensor network decompositions. The efficiency of the introduced approach will be illustrated with the aid of several high-dimensional nonlinear dynamical systems.}
      \field{issn}{1555-1415}
      \field{journaltitle}{Journal of Computational and Nonlinear Dynamics}
      \field{month}{4}
      \field{number}{6}
      \field{title}{Multidimensional {Approximation} of {Nonlinear} {Dynamical} {Systems}}
      \field{urlday}{1}
      \field{urlmonth}{5}
      \field{urlyear}{2019}
      \field{volume}{14}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{061006--061006\bibrangedash 12}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1115/1.4043148
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/9DAMBNLE/Gelß et al. - 2019 - Multidimensional Approximation of Nonlinear Dynami.pdf:application/pdf
      \endverb
    \endentry
    \entry{gels_pgelssscikit_tt_2025}{misc}{}
      \name{author}{1}{}{%
        {{hash=aae598c66790875fb3f97e9e696f287f}{%
           family={Gelß},
           familyi={G\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{aae598c66790875fb3f97e9e696f287f}
      \strng{fullhash}{aae598c66790875fb3f97e9e696f287f}
      \strng{bibnamehash}{aae598c66790875fb3f97e9e696f287f}
      \strng{authorbibnamehash}{aae598c66790875fb3f97e9e696f287f}
      \strng{authornamehash}{aae598c66790875fb3f97e9e696f287f}
      \strng{authorfullhash}{aae598c66790875fb3f97e9e696f287f}
      \field{labelalpha}{Gel25}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Tensor Train Toolbox}
      \field{month}{6}
      \field{note}{original-date: 2018-11-23T13:10:49Z}
      \field{title}{{PGelss}/scikit\_tt}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://github.com/PGelss/scikit_tt
      \endverb
      \verb{url}
      \verb https://github.com/PGelss/scikit_tt
      \endverb
      \keyw{als,data-driven,dmrg,dynamic-mode-decomposition,dynamical-systems,extended-dynamic-mode-decomposition,mals,mandy,quantum-simulation,scikit,slim-decomposition,tensor,tensor-decomposition,tensor-network,tensor-train}
    \endentry
    \entry{gillmann_01-polytopes_2007}{thesis}{}
      \name{author}{1}{}{%
        {{hash=7ed2386e25c966f9ad2297317d840df0}{%
           family={Gillmann},
           familyi={G\bibinitperiod},
           given={Rafael},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \strng{namehash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{fullhash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{bibnamehash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{authorbibnamehash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{authornamehash}{7ed2386e25c966f9ad2297317d840df0}
      \strng{authorfullhash}{7ed2386e25c966f9ad2297317d840df0}
      \field{labelalpha}{Gil07}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{0/1-Polytopen kann man in vielen Gebieten der diskreten Mathematik begegnen. Das bekannteste ist als "polyedrische Kombinatorik" bekannt. Auch wenn man die polyedrische Kombinatorik meist mit kombinatorischer Optimierung in Verbindung bringt, gibt es wichtige Zusammenhänge zu anderen Gebieten, zum Beispiel zum Zählen von kombinatorischen Strukturen. Trotz ihres breiten Anwendungsspektrums existiert keine umfassende Theorie über 0/1-Polytope. Es scheint so als seien 0/1-Polytope besonders komplizierte Objekte. Die Ergebnisse, welche in dieser Dissertation dargestellt werden, gruppieren sich um drei Hauptthemen: (1) obere Schranken für die minimale Anzahl von Seiten (Kanten und Facetten), (2) eine Vermutung von Mihail und Vazirani über die Kantenexpansion und (3) zufällige 0/1-Polytope. Zufälligen 0/1-Polytope können aus zwei verschiedenen Blickwinkeln betrachtet werden. Zum einen kann man Zufall benutzen, um die Komplexität allgemeiner 0/1-Polytope zu durchbrechen. Zum anderen kann man untersuchen, welche Eigenschaften zufällige 0/1-Polytope mit hoher Wahrscheinlichkeit besitzen. Dies sind dann typische Eigenschaften von 0/1-Polytopen und können nützlich sein, um Eigenschaften spezieller 0/1-Polytope einzuordnen. Nach einer kurzen Einführung in die Grundlagen der Polytoptheorie und einen Überblick über den aktuellen Stand der Forschung auf dem Gebiet der 0/1-Polytope, werden Grundlagen aus der Wahrscheinlichkeitstheorie kurz eingeführt. Die Ergebnisse zu zufälligen 0/1-Polytopen (3) umfassen: (a) Schwellenwert Resultate mit exponentieller Konvergenz zu der Wahrscheinlichkeit, dass \$k\$ zufällige Ecken eines zufälligen 0/1-Polytopes eine \$(k-1)\$-dimensionale Seite bilden; (b) Es gibt 0/1-Polytope mit exponentiell kleiner Eckenexpansion; (c) für jedes \$k\$ gibt es eine Konstante \$c\_k\$, so dass ein zufälliges 0/1-Polytop in \$[0,1]{\textasciicircum}d\$ mit höchstens \$2{\textasciicircum}\{cd\}\$ Ecken sehr wahrscheinlich \$k\$-nachbarschaftlich ist. Ergebnis (b) zeigt, dass es für die Vermutung von Mihail und Vazirani (2) nicht genügt die Eckenexpansion zu untersuchen. Allerdings scheint es schwierig oder unmöglich zu sein, dieses Negativresultat auf die Kantenexpansion auszuweiten. Ergebnisse (a) und (c) zeigen typische Eigenschaften von zufälligen 0/1-Polytopen. Hieraus lässt sich schließen, dass ein vollständiger Graph für 0/1-Polytope aus der kombinatorischen Optimierung nichts besonderes sind, da diese Polytope meistens nur sub-exponentiell viele Ecken haben. Bekannte Beispiele für 0/1-Polytope aus der kombinatorischen Optimierung sind Traveling Salesman Polytope'' und Cut-Polytope''. Im letzten Kapitel stellen wir revlex-initiale 0/1-Polytope vor. Diese speziellen Knap{\textbackslash}-sack-Poly{\textbackslash}-tope haben sehr wenige Facetten und Kanten. Wir benutzen diese Polytope, um zu zeigen, dass es in jeder Dimension \$d\$ und sinnvoller Eckenzahl \$n\$ ein \$d\$-dimensionales 0/1-Polytope mit \$n\$ Ecken gibt, welches höchstens \$3d\$ Facetten und Eckengrad höchstens \$d+8\$ hat. Dies sind die ersten Resultate zu oberen Schranken an die minimale Anzahl der Seiten von 0/1-Polytopen (1). Trotz des dünnen Graphen erfüllen diese 0/1-Polytope die Vermutung von Mihail und Vazirani (2). Abschließend betrachten wir noch kurz "erweiterte revlex-initiale 0/1-Polytope", welche eine weiterführende Richtung in der Untersuchung von Knapsack-Polytopen im Hinblick auf die Vermutung von Mihail und Vazirani sein können.}
      \field{month}{2}
      \field{shorttitle}{0/1-{Polytopes}}
      \field{title}{0/1-{Polytopes}: {Typical} and {Extremal} {Properties}}
      \field{type}{phdthesis}
      \field{urlday}{4}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2007}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/XVZHS784/Gillmann - 2007 - 01-Polytopes Typical and Extremal Properties.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://depositonce.tu-berlin.de/items/urn:nbn:de:kobv:83-opus-14695
      \endverb
      \verb{url}
      \verb https://depositonce.tu-berlin.de/items/urn:nbn:de:kobv:83-opus-14695
      \endverb
    \endentry
    \entry{giovanni_russo_vito_latora_complex_2017}{book}{}
      \name{author}{1}{}{%
        {{hash=401604d888b5c37ccd5e617322bf6077}{%
           family={Giovanni\bibnamedelimb Russo\bibnamedelimb Vito\bibnamedelima Latora},
           familyi={G\bibinitperiod\bibinitdelim R\bibinitperiod\bibinitdelim V\bibinitperiod\bibinitdelim L\bibinitperiod},
           given={Vincenzo\bibnamedelima Nicosia},
           giveni={V\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge, United Kingdom ; New York, NY}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{401604d888b5c37ccd5e617322bf6077}
      \strng{fullhash}{401604d888b5c37ccd5e617322bf6077}
      \strng{bibnamehash}{401604d888b5c37ccd5e617322bf6077}
      \strng{authorbibnamehash}{401604d888b5c37ccd5e617322bf6077}
      \strng{authornamehash}{401604d888b5c37ccd5e617322bf6077}
      \strng{authorfullhash}{401604d888b5c37ccd5e617322bf6077}
      \field{labelalpha}{Gio17}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Networks constitute the backbone of complex systems, from the human brain to computer communications, transport infrastructures to online social systems and metabolic reactions to financial markets. Characterising their structure improves our understanding of the physical, biological, economic and social phenomena that shape our world. Rigorous and thorough, this textbook presents a detailed overview of the new theory and methods of network science. Covering algorithms for graph exploration, node ranking and network generation, among others, the book allows students to experiment with network models and real-world data sets, providing them with a deep understanding of the basics of network theory and its practical applications. Systems of growing complexity are examined in detail, challenging students to increase their level of skill. An engaging presentation of the important principles of network science makes this the perfect reference for researchers and undergraduate and graduate students in physics, mathematics, engineering, biology, neuroscience and the social sciences.}
      \field{isbn}{978-1-107-10318-4}
      \field{month}{9}
      \field{shorttitle}{Complex {Networks}}
      \field{title}{Complex {Networks}: {Principles}, {Methods} and {Applications}. {With} 58 exercises}
      \field{year}{2017}
    \endentry
    \entry{glasser_expressive_2019}{article}{}
      \name{author}{5}{}{%
        {{hash=a7ed7dd14e38aee77c04c621e9db9479}{%
           family={Glasser},
           familyi={G\bibinitperiod},
           given={Ivan},
           giveni={I\bibinitperiod}}}%
        {{hash=5ad899c94bf216d9a3334f13a79e79b0}{%
           family={Sweke},
           familyi={S\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=b7f7b29051f4774bb6f5496c8bde3137}{%
           family={Pancotti},
           familyi={P\bibinitperiod},
           given={Nicola},
           giveni={N\bibinitperiod}}}%
        {{hash=878172bfd75f93a96d3646d7b66c3e48}{%
           family={Eisert},
           familyi={E\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
        {{hash=fe13755c257fb2ccfe9490c05415070a}{%
           family={Cirac},
           familyi={C\bibinitperiod},
           given={Ignacio},
           giveni={I\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{66af410c151996011c9006cfa3995e37}
      \strng{fullhash}{c6452342297aeb8ac430772a8ba73410}
      \strng{bibnamehash}{66af410c151996011c9006cfa3995e37}
      \strng{authorbibnamehash}{66af410c151996011c9006cfa3995e37}
      \strng{authornamehash}{66af410c151996011c9006cfa3995e37}
      \strng{authorfullhash}{c6452342297aeb8ac430772a8ba73410}
      \field{labelalpha}{Gla+19}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Advances in Neural Information Processing Systems}
      \field{title}{Expressive power of tensor-network factorizations for probabilistic modeling}
      \field{urlday}{6}
      \field{urlmonth}{7}
      \field{urlyear}{2021}
      \field{volume}{32}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/HE7UYQQ8/Glasser et al. - 2019 - Expressive power of tensor-network factorizations .pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/F9Z9J49G/b86e8d03fe992d1b0e19656875ee557c-Abstract.html:text/html
      \endverb
    \endentry
    \entry{goesmann_tensor_2020}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=5efe76d0bd128291a2b5981da86e7f18}{%
           family={Goeßmann},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=14286c23fc543acfeff36e8ee4505caf}{%
           family={Roth},
           familyi={R\bibinitperiod},
           given={Ingo},
           giveni={I\bibinitperiod}}}%
        {{hash=2e7b05c7e0c405468d37834ab05737b3}{%
           family={Kutyniok},
           familyi={K\bibinitperiod},
           given={Gitta},
           giveni={G\bibinitperiod}}}%
        {{hash=67ec7578dee8df82799cd8f2b05d8461}{%
           family={Götte},
           familyi={G\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=5ad899c94bf216d9a3334f13a79e79b0}{%
           family={Sweke},
           familyi={S\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=878172bfd75f93a96d3646d7b66c3e48}{%
           family={Eisert},
           familyi={E\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{5375874d19924cacc4d1804281515215}
      \strng{fullhash}{f6202aef2a2b81eab4ac07788df1e65c}
      \strng{bibnamehash}{5375874d19924cacc4d1804281515215}
      \strng{authorbibnamehash}{5375874d19924cacc4d1804281515215}
      \strng{authornamehash}{5375874d19924cacc4d1804281515215}
      \strng{authorfullhash}{f6202aef2a2b81eab4ac07788df1e65c}
      \field{labelalpha}{Goe+20}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To date, scalable methods for data-driven identiﬁcation of non-linear governing equations do not exploit or offer insight into the fundamental underlying physical structure. In this work, we show that various physical constraints can be captured via tensor network based parameterizations for the governing equation, which naturally ensures scalability. In addition to providing analytic results motivating the use of such models for realistic physical systems, we demonstrate that efﬁcient rank-adaptive optimization algorithms can be used to learn optimal tensor network models without requiring a priori knowledge of the exact tensor ranks.}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems} - {First} {Workshop} on {Quantum} {Tensor} {Networks} in {Machine} {Learning}}
      \field{title}{Tensor network approaches for data-driven identiﬁcation of non-linear dynamical laws}
      \field{year}{2020}
      \field{pages}{21}
      \range{pages}{1}
      \verb{file}
      \verb Goeßmann et al. - Tensor network approaches for data-driven identiﬁc.pdf:/Users/alexgoessmann/Zotero/storage/GVJXJ77H/Goeßmann et al. - Tensor network approaches for data-driven identiﬁc.pdf:application/pdf
      \endverb
    \endentry
    \entry{goesmann_uniform_2021}{thesis}{}
      \name{author}{1}{}{%
        {{hash=159c9dead125485200f20029d40453fb}{%
           family={Goeßmann},
           familyi={G\bibinitperiod},
           given={Alex\bibnamedelima Christoph},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Technische Universität Berlin}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Berlin}%
      }
      \strng{namehash}{159c9dead125485200f20029d40453fb}
      \strng{fullhash}{159c9dead125485200f20029d40453fb}
      \strng{bibnamehash}{159c9dead125485200f20029d40453fb}
      \strng{authorbibnamehash}{159c9dead125485200f20029d40453fb}
      \strng{authornamehash}{159c9dead125485200f20029d40453fb}
      \strng{authorfullhash}{159c9dead125485200f20029d40453fb}
      \field{labelalpha}{Goe21}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This thesis contributes to the uniform concentration approach towards guaranteeing the generalization of learned models. We show probabilistic bounds on various uniform concentration events and demonstrate their utility in recovery guarantees. The thesis is organized in three parts. In the first part, we develop a unified theoretical framework for the concentration of random variables and the uniform concentration of stochastic processes. We introduce functionals of stochastic processes and apply them in bounds on the supremum. Then we develop methods to transfer uniform concentration events into success guarantees for empirical risk minimization problems. The second part of this thesis investigates classes of structured random distributions. More precisely, we derive bounds on the uniform concentration of contracted random tensors, which are decomposable into tensor network formats. In particular, we show exact moment bounds on contracted Gaussian tensor networks, which are tensor networks consistent of independent standard Gaussian random cores. By applying comparison theorems for Gaussian variables, the upper moment bounds are extended to more generic Orlicz tensor networks, which are characterized by weaker assumptions made on the random cores. Furthermore, we derive bounds on the concentration of Haar tensor networks, which random cores follow the Haar distribution of Stiefel manifolds. For all examples we continue to provide probabilistic bounds on uniform concentration events, which imply recovery guarantees for tensor regression problems. We further apply our findings in deriving success guarantees for efficient algorithms solving tensor regression problems. In the third part, we transfer our findings to bounds on the uniform concentration of neural networks following two approaches. First, we derive concentration bounds for shallow ReLU networks with respect to standard Gaussian distributions, where we introduce parameter embeddings that capture the concentration structure. Second, we bound the Rademacher complexity of deep neural networks, which are activated by a contraction, by Rademacher complexities of linear functions. This enables the proof of recovery guarantees for neural networks, which are trained on structured data.}
      \field{note}{Accepted: 2021-12-30T15:00:58Z}
      \field{title}{Uniform {Concentration} of {Tensor} and {Neural} {Networks}: {An} {Approach} towards {Recovery} {Guarantees}}
      \field{type}{{PhD} {Thesis}}
      \field{urlday}{13}
      \field{urlmonth}{1}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/8QLGC9FD/Goeßmann - 2021 - Uniform concentration of tensor and neural network.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/ZKMYB6H4/15990.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://depositonce.tu-berlin.de/handle/11303/15990
      \endverb
      \verb{url}
      \verb https://depositonce.tu-berlin.de/handle/11303/15990
      \endverb
    \endentry
    \entry{grasedyck_hierarchical_2010}{article}{}
      \name{author}{1}{}{%
        {{hash=f94961b8e70ac7cf8e129bd9cfdb015f}{%
           family={Grasedyck},
           familyi={G\bibinitperiod},
           given={Lars},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{fullhash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{bibnamehash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{authorbibnamehash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{authornamehash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \strng{authorfullhash}{f94961b8e70ac7cf8e129bd9cfdb015f}
      \field{labelalpha}{Gra10}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We define the hierarchical singular value decomposition (SVD) for tensors of order d ≥ 2. This hierarchical SVD has properties like the matrix SVD (and collapses to the SVD in d = 2), and we prove these. In particular, one can find low rank (almost) best approximations in a hierarchical format (H-Tucker) which requires only O((d - 1)k3 + dnk) parameters, where d is the order of the tensor, n the size of the modes, and k the (hierarchical) rank. The H-Tucker format is a specialization of the Tucker format and it contains as a special case all (canonical) rank k tensors. Based on this new concept of a hierarchical SVD we present algorithms for hierarchical tensor calculations allowing for a rigorous error analysis. The complexity of the truncation (finding lower rank approximations to hierarchical rank k tensors) is in O((d-1)k4+dnk2) and the attainable accuracy is just 2-3 digits less than machine precision.}
      \field{journaltitle}{SIAM J. Matrix Analysis Applications}
      \field{month}{1}
      \field{title}{Hierarchical {Singular} {Value} {Decomposition} of {Tensors}}
      \field{volume}{31}
      \field{year}{2010}
      \field{pages}{2029\bibrangedash 2054}
      \range{pages}{26}
      \verb{doi}
      \verb 10.1137/090764189
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/J9ZJ3RB3/Grasedyck - 2010 - Hierarchical Singular Value Decomposition of Tenso.pdf:application/pdf
      \endverb
    \endentry
    \entry{getoor_introduction_2019}{book}{}
      \name{author}{2}{}{%
        {{hash=88e86bf57e4d0841ba9010d52469237c}{%
           family={Getoor},
           familyi={G\bibinitperiod},
           given={Lise},
           giveni={L\bibinitperiod}}}%
        {{hash=58d1c2675fb9ba29754fd8d952001eed}{%
           family={Taskar},
           familyi={T\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{fullhash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{bibnamehash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{authorbibnamehash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{authornamehash}{ef1236351c153eeb9d7a29940c6dade9}
      \strng{authorfullhash}{ef1236351c153eeb9d7a29940c6dade9}
      \field{labelalpha}{GT19}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Advanced statistical modeling and knowledge representation techniques for a newly emerging area of machine learning and probabilistic reasoning; includes introductory material, tutorials for different proposed approaches, and applications.Handling inherent uncertainty and exploiting compositional structure are fundamental to understanding and designing large-scale systems. Statistical relational learning builds on ideas from probability theory and statistics to address uncertainty while incorporating tools from logic, databases and programming languages to represent structure. In Introduction to Statistical Relational Learning, leading researchers in this emerging area of machine learning describe current formalisms, models, and algorithms that enable effective and robust reasoning about richly structured systems and data. The early chapters provide tutorials for material used in later chapters, offering introductions to representation, inference and learning in graphical models, and logic. The book then describes object-oriented approaches, including probabilistic relational models, relational Markov networks, and probabilistic entity-relationship models as well as logic-based formalisms including Bayesian logic programs, Markov logic, and stochastic logic programs. Later chapters discuss such topics as probabilistic models with unknown objects, relational dependency networks, reinforcement learning in relational domains, and information extraction. By presenting a variety of approaches, the book highlights commonalities and clarifies important differences among proposed approaches and, along the way, identifies important representational and algorithmic issues. Numerous applications are provided throughout.}
      \field{isbn}{978-0-262-53868-8}
      \field{month}{9}
      \field{title}{Introduction to {Statistical} {Relational} {Learning}}
      \field{year}{2019}
    \endentry
    \entry{hackbusch_tensor_2012}{book}{}
      \name{author}{1}{}{%
        {{hash=01bcfafc59c11a565163fcf9ef2a0907}{%
           family={Hackbusch},
           familyi={H\bibinitperiod},
           given={Wolfgang},
           giveni={W\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Berlin Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer-Verlag}%
      }
      \strng{namehash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{fullhash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{bibnamehash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{authorbibnamehash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{authornamehash}{01bcfafc59c11a565163fcf9ef2a0907}
      \strng{authorfullhash}{01bcfafc59c11a565163fcf9ef2a0907}
      \field{labelalpha}{Hac12}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Special numerical techniques are already needed to deal with nxn matrices for large n.Tensor data are of size nxnx...xn=n{\textasciicircum}d, where n{\textasciicircum}d exceeds the computer memory by far. They appear for problems of high spatial dimensions. Since standard methods fail, a particular tensor calculus is needed to treat such problems. The monograph describes the methods how tensors can be practically treated and how numerical operations can be performed. Applications are problems from quantum chemistry, approximation of multivariate functions, solution of pde, e.g., with stochastic coefficients, etc. ​}
      \field{isbn}{978-3-642-28026-9}
      \field{series}{Springer {Series} in {Computational} {Mathematics}}
      \field{title}{Tensor {Spaces} and {Numerical} {Tensor} {Calculus}}
      \field{urlday}{30}
      \field{urlmonth}{1}
      \field{urlyear}{2020}
      \field{year}{2012}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-3-642-28027-6
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/PC9F8YHM/9783642280269.html:text/html
      \endverb
    \endentry
    \entry{hitchcock_expression_1927}{article}{}
      \name{author}{1}{}{%
        {{hash=ff8777b641a9baeb6609898cf9b139b3}{%
           family={Hitchcock},
           familyi={H\bibinitperiod},
           given={Frank\bibnamedelima L.},
           giveni={F\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{fullhash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{bibnamehash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{authorbibnamehash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{authornamehash}{ff8777b641a9baeb6609898cf9b139b3}
      \strng{authorfullhash}{ff8777b641a9baeb6609898cf9b139b3}
      \field{labelalpha}{Hit27}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1467-9590}
      \field{journaltitle}{Journal of Mathematics and Physics}
      \field{number}{1-4}
      \field{title}{The {Expression} of a {Tensor} or a {Polyadic} as a {Sum} of {Products}}
      \field{urlday}{9}
      \field{urlmonth}{2}
      \field{urlyear}{2021}
      \field{volume}{6}
      \field{year}{1927}
      \field{urldateera}{ce}
      \field{pages}{164\bibrangedash 189}
      \range{pages}{26}
      \verb{doi}
      \verb https://doi.org/10.1002/sapm192761164
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/UABK5I79/sapm192761164.html:text/html
      \endverb
    \endentry
    \entry{hackbusch_new_2009}{article}{}
      \name{author}{2}{}{%
        {{hash=d16d68319ab32deb0bd277d06fe083ef}{%
           family={Hackbusch},
           familyi={H\bibinitperiod},
           given={W.},
           giveni={W\bibinitperiod}}}%
        {{hash=4fdf09f9beb2649fb4b25b311b841e4c}{%
           family={Kühn},
           familyi={K\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{fullhash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{bibnamehash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{authorbibnamehash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{authornamehash}{25d066aa917d3f5efc86f10e0fee65ba}
      \strng{authorfullhash}{25d066aa917d3f5efc86f10e0fee65ba}
      \field{labelalpha}{HK09}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The paper presents a new scheme for the representation of tensors which is well-suited for high-order tensors. The construction is based on a hierarchy of tensor product subspaces spanned by orthonormal bases. The underlying binary tree structure makes it possible to apply standard Linear Algebra tools for performing arithmetical operations and for the computation of data-sparse approximations. In particular, a truncation algorithm can be implemented which is based on the standard matrix singular value decomposition (SVD) method.}
      \field{issn}{1531-5851}
      \field{journaltitle}{Journal of Fourier Analysis and Applications}
      \field{month}{10}
      \field{number}{5}
      \field{title}{A {New} {Scheme} for the {Tensor} {Representation}}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2021}
      \field{volume}{15}
      \field{year}{2009}
      \field{urldateera}{ce}
      \field{pages}{706\bibrangedash 722}
      \range{pages}{17}
      \verb{file}
      \verb Springer Full Text PDF:/Users/alexgoessmann/Zotero/storage/SJUUNNLH/Hackbusch und Kühn - 2009 - A New Scheme for the Tensor Representation.pdf:application/pdf
      \endverb
    \endentry
    \entry{hiriart-urruty_convex_1993}{book}{}
      \name{author}{2}{}{%
        {{hash=f8a4abcac6bfbded8d5bc07918abfa8c}{%
           family={Hiriart-Urruty},
           familyi={H\bibinithyphendelim U\bibinitperiod},
           given={Jean-Baptiste},
           giveni={J\bibinithyphendelim B\bibinitperiod}}}%
        {{hash=7b2c40c2d473e37d7475deabb7519a7a}{%
           family={Lemarechal},
           familyi={L\bibinitperiod},
           given={Claude},
           giveni={C\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{fullhash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{bibnamehash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{authorbibnamehash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{authornamehash}{15add3e39676068ac7c32a9a39e5d45d}
      \strng{authorfullhash}{15add3e39676068ac7c32a9a39e5d45d}
      \field{labelalpha}{HL93}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{From the reviews: "The account is quite detailed and is written in a manner that will appeal to analysts and numerical practitioners alike...they contain everything from rigorous proofs to tables of numerical calculations.... one of the strong features of these books...that they are designed not for the expert, but for those who whish to learn the subject matter starting from little or no background...there are numerous examples, and counter-examples, to back up the theory...To my knowledge, no other authors have given such a clear geometric account of convex analysis." "This innovative text is well written, copiously illustrated, and accessible to a wide audience"}
      \field{edition}{1993rd edition}
      \field{isbn}{978-3-540-56852-0}
      \field{month}{10}
      \field{shorttitle}{Convex {Analysis} and {Minimization} {Algorithms} {II}}
      \field{title}{Convex {Analysis} and {Minimization} {Algorithms} {II}: {Advanced} {Theory} and {Bundle} {Methods}}
      \field{year}{1993}
    \endentry
    \entry{hochreiter_toward_2022}{article}{}
      \name{author}{1}{}{%
        {{hash=41b31e29fb2bdbf9f5c9c1b0d5b3e815}{%
           family={Hochreiter},
           familyi={H\bibinitperiod},
           given={Sepp},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{fullhash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{bibnamehash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{authorbibnamehash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{authornamehash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \strng{authorfullhash}{41b31e29fb2bdbf9f5c9c1b0d5b3e815}
      \field{labelalpha}{Hoc22}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0001-0782, 1557-7317}
      \field{journaltitle}{Communications of the ACM}
      \field{month}{1}
      \field{number}{4}
      \field{title}{Toward a broad {AI}}
      \field{urlday}{22}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{65}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{56\bibrangedash 57}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1145/3512715
      \endverb
      \verb{file}
      \verb Hochreiter - 2022 - Toward a broad AI.pdf:/Users/alexgoessmann/Zotero/storage/8ZDZBDY8/Hochreiter - 2022 - Toward a broad AI.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/3512715
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/3512715
      \endverb
    \endentry
    \entry{hogan_knowledge_2021}{book}{}
      \name{author}{10}{}{%
        {{hash=56f2d88e83f41f6e1f0d9a0197c4c7eb}{%
           family={Hogan},
           familyi={H\bibinitperiod},
           given={Aidan},
           giveni={A\bibinitperiod}}}%
        {{hash=019293949e6289df4e3b751f3f9ecf4e}{%
           family={Blomqvist},
           familyi={B\bibinitperiod},
           given={Eva},
           giveni={E\bibinitperiod}}}%
        {{hash=ac9f2c71e19d14efaa82ad1908d35843}{%
           family={Cochez},
           familyi={C\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=3eb1335a7c6a4acf8ca2e632ae7f6a09}{%
           family={d’Amato},
           familyi={d\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod}}}%
        {{hash=96fa7de0e15705cafb281ade1eaecfd0}{%
           family={Melo},
           familyi={M\bibinitperiod},
           given={Gerard\bibnamedelima de},
           giveni={G\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=5feae03b05f14f0a5fd1f3e07badd8e6}{%
           family={Gutierrez},
           familyi={G\bibinitperiod},
           given={Claudio},
           giveni={C\bibinitperiod}}}%
        {{hash=5ddbf3af24e3802f8842794c8c86b60c}{%
           family={Kirrane},
           familyi={K\bibinitperiod},
           given={Sabrina},
           giveni={S\bibinitperiod}}}%
        {{hash=7fe6d31d4006448f174550c3599ec2d2}{%
           family={Gayo},
           familyi={G\bibinitperiod},
           given={Jose\bibnamedelimb Emilio\bibnamedelima Labra},
           giveni={J\bibinitperiod\bibinitdelim E\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=6fe24dce481a67e247581d4b20435709}{%
           family={Navigli},
           familyi={N\bibinitperiod},
           given={Roberto},
           giveni={R\bibinitperiod}}}%
        {{hash=9aabe7235b5cda4ebcca85f21cf40b60}{%
           family={Neumaier},
           familyi={N\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{fullhash}{1aa6c5d2b67b513060a2c6a632b79d9a}
      \strng{bibnamehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{authorbibnamehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{authornamehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{authorfullhash}{1aa6c5d2b67b513060a2c6a632b79d9a}
      \field{labelalpha}{Hog+21}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book provides a comprehensive and accessible introduction to knowledge graphs, which have recently garnered notable attention from both industry and academia. Knowledge graphs are founded on the principle of applying a graph-based abstraction to data, and are now broadly deployed in scenarios that require integrating and extracting value from multiple, diverse sources of data at large scale. The book defines knowledge graphs and provides a high-level overview of how they are used. It presents and contrasts popular graph models that are commonly used to represent data as graphs, and the languages by which they can be queried before describing how the resulting data graph can be enhanced with notions of schema, identity, and context. The book discusses how ontologies and rules can be used to encode knowledge as well as how inductive techniques—based on statistics, graph analytics, machine learning, etc.—can be used to encode and extract knowledge. It covers techniques for the creation, enrichment, assessment, and refinement of knowledge graphs and surveys recent open and enterprise knowledge graphs and the industries or applications within which they have been most widely adopted. The book closes by discussing the current limitations and future directions along which knowledge graphs are likely to evolve. This book is aimed at students, researchers, and practitioners who wish to learn more about knowledge graphs and how they facilitate extracting value from diverse data at large scale. To make the book accessible for newcomers, running examples and graphical notation are used throughout. Formal definitions and extensive references are also provided for those who opt to delve more deeply into specific topics.}
      \field{edition}{1st edition}
      \field{isbn}{978-3-031-00790-3}
      \field{month}{11}
      \field{title}{Knowledge {Graphs}}
      \field{year}{2021}
    \endentry
    \entry{holtz_manifolds_2012}{article}{}
      \name{author}{3}{}{%
        {{hash=609411433449cf1b01b9596d5ea8e17a}{%
           family={Holtz},
           familyi={H\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=9fa4aa09a1f933a0b479542d3d8f02b8}{%
           family={Rohwedder},
           familyi={R\bibinitperiod},
           given={Thorsten},
           giveni={T\bibinitperiod}}}%
        {{hash=cd91fef38e4801ef30c61055e408b8b5}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Reinhold},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{fullhash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{bibnamehash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{authorbibnamehash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{authornamehash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \strng{authorfullhash}{fb04d2dd2b48dd58aa3dd0931dddcf46}
      \field{labelalpha}{HRS12}
      \field{sortinit}{H}
      \field{sortinithash}{5f15a7bc777ad49ff15aa4d2831b1681}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, the format of TT tensors (Hackbusch and Kühn in J Fourier Anal Appl 15:706–722, 2009; Oseledets in SIAM J Sci Comput 2009, submitted; Oseledets and Tyrtyshnikov in SIAM J Sci Comput 31:5, 2009; Oseledets and Tyrtyshnikov in Linear Algebra Appl 2009, submitted) has turned out to be a promising new format for the approximation of solutions of high dimensional problems. In this paper, we prove some new results for the TT representation of a tensor U ∈ Rn1×···×nd and for the manifold of tensors of TT-rank r . As a ﬁrst result, we prove that the TT (or compression) ranks ri of a tensor U are unique and equal to the respective separation ranks of U if the components of the TT decomposition are required to fulﬁl a certain maximal rank condition. We then show that the set T of TT tensors of ﬁxed rank r locally forms an embedded manifold in Rn1×···×nd , therefore preserving the essential theoretical properties of the Tucker format, but often showing an improved scaling behaviour. Extending a similar approach for matrices (Conte and Lubich in M2AN 44:759, 2010), we introduce certain gauge conditions to obtain a unique representation of the tangent space TU T of T and deduce a local parametrization of the TT manifold. The parametrisation of TU T is often crucial for an algorithmic treatment of high-dimensional time-dependent PDEs and minimisation problems (Lubich in From quantum to classical molecular dynamics: reduced methods and numerical analysis, 2008). We conclude with remarks on those applications and present some numerical examples.}
      \field{issn}{0029-599X, 0945-3245}
      \field{journaltitle}{Numerische Mathematik}
      \field{month}{4}
      \field{number}{4}
      \field{title}{On manifolds of tensors of fixed {TT}-rank}
      \field{urlday}{18}
      \field{urlmonth}{12}
      \field{urlyear}{2019}
      \field{volume}{120}
      \field{year}{2012}
      \field{urldateera}{ce}
      \field{pages}{701\bibrangedash 731}
      \range{pages}{31}
      \verb{doi}
      \verb 10.1007/s00211-011-0419-7
      \endverb
      \verb{file}
      \verb Holtz et al. - 2012 - On manifolds of tensors of fixed TT-rank.pdf:/Users/alexgoessmann/Zotero/storage/YQFMECP7/Holtz et al. - 2012 - On manifolds of tensors of fixed TT-rank.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/s00211-011-0419-7
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/s00211-011-0419-7
      \endverb
    \endentry
    \entry{jouppi_tpu_2023}{inproceedings}{}
      \name{author}{14}{}{%
        {{hash=d73f1690b4ba58964edeb173f8284822}{%
           family={Jouppi},
           familyi={J\bibinitperiod},
           given={Norm},
           giveni={N\bibinitperiod}}}%
        {{hash=d115e2de1127519c117a7252205efcdb}{%
           family={Kurian},
           familyi={K\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod}}}%
        {{hash=d8125976d0f2f0c4fddc5c4adeda313d}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Sheng},
           giveni={S\bibinitperiod}}}%
        {{hash=a539c46db76440bd088ebd30ad1927a0}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=79fadc0c81fb0a5cb1f16bb3b3180113}{%
           family={Nagarajan},
           familyi={N\bibinitperiod},
           given={Rahul},
           giveni={R\bibinitperiod}}}%
        {{hash=6d9f86b0b8af0276594dbfd4261598ca}{%
           family={Nai},
           familyi={N\bibinitperiod},
           given={Lifeng},
           giveni={L\bibinitperiod}}}%
        {{hash=98e48fd2ef83c15cb4e696e6243eef05}{%
           family={Patil},
           familyi={P\bibinitperiod},
           given={Nishant},
           giveni={N\bibinitperiod}}}%
        {{hash=1e17c7dbc5693c565cab32541b456a87}{%
           family={Subramanian},
           familyi={S\bibinitperiod},
           given={Suvinay},
           giveni={S\bibinitperiod}}}%
        {{hash=6305d4902371102e8d1f9f933d9c2845}{%
           family={Swing},
           familyi={S\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod}}}%
        {{hash=12cebccf7d831a1b00e38b044c768df4}{%
           family={Towles},
           familyi={T\bibinitperiod},
           given={Brian},
           giveni={B\bibinitperiod}}}%
        {{hash=732617f9246a38995e8cf24fe9ebaaa1}{%
           family={Young},
           familyi={Y\bibinitperiod},
           given={Clifford},
           giveni={C\bibinitperiod}}}%
        {{hash=d1654f226dfdf60d9b41a59d7e0d44d1}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Xiang},
           giveni={X\bibinitperiod}}}%
        {{hash=335463264b30d83d055bbbda6e0ac19b}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Zongwei},
           giveni={Z\bibinitperiod}}}%
        {{hash=95486d328725c1c6a933929c9442fe2a}{%
           family={Patterson},
           familyi={P\bibinitperiod},
           given={David\bibnamedelima A},
           giveni={D\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{11df723acd237366fe67e6bddc0831f8}
      \strng{fullhash}{4b1a2cf5cadf39661555df21f6736642}
      \strng{bibnamehash}{11df723acd237366fe67e6bddc0831f8}
      \strng{authorbibnamehash}{11df723acd237366fe67e6bddc0831f8}
      \strng{authornamehash}{11df723acd237366fe67e6bddc0831f8}
      \strng{authorfullhash}{4b1a2cf5cadf39661555df21f6736642}
      \field{labelalpha}{Jou+23}
      \field{sortinit}{J}
      \field{sortinithash}{fce5f8d0bd05e8d93f3dbe21c78897ca}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are \&lt;5\% of system cost and \&lt;3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of {\textasciitilde}60\% of peak FLOPS/second. For similar sized systems, it is {\textasciitilde}4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use {\textasciitilde}2--6x less energy and produce {\textasciitilde}20x less CO2e than contemporary DSAs in typical on-premise data centers.}
      \field{booktitle}{Proceedings of the 50th {Annual} {International} {Symposium} on {Computer} {Architecture}}
      \field{isbn}{979-8-4007-0095-8}
      \field{month}{6}
      \field{series}{{ISCA} '23}
      \field{shorttitle}{{TPU} v4}
      \field{title}{{TPU} v4: {An} {Optically} {Reconfigurable} {Supercomputer} for {Machine} {Learning} with {Hardware} {Support} for {Embeddings}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 14}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1145/3579371.3589350
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/G3GKWRAE/Jouppi et al. - 2023 - TPU v4 An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Emb.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/3579371.3589350
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/3579371.3589350
      \endverb
    \endentry
    \entry{kolda_tensor_2009}{article}{}
      \name{author}{2}{}{%
        {{hash=a17e10e04fa2d01fdb9bfc9a669b4182}{%
           family={Kolda},
           familyi={K\bibinitperiod},
           given={Tamara\bibnamedelima G.},
           giveni={T\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=e452c0602af86438cb78062b7bac80bb}{%
           family={Bader},
           familyi={B\bibinitperiod},
           given={Brett\bibnamedelima W.},
           giveni={B\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{fullhash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{bibnamehash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{authorbibnamehash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{authornamehash}{782cc4ad2fea7afb684072cddcaf4d10}
      \strng{authorfullhash}{782cc4ad2fea7afb684072cddcaf4d10}
      \field{labelalpha}{KB09}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.}
      \field{issn}{0036-1445, 1095-7200}
      \field{journaltitle}{SIAM Review}
      \field{month}{8}
      \field{number}{3}
      \field{title}{Tensor {Decompositions} and {Applications}}
      \field{urlday}{28}
      \field{urlmonth}{1}
      \field{urlyear}{2021}
      \field{volume}{51}
      \field{year}{2009}
      \field{urldateera}{ce}
      \field{pages}{455\bibrangedash 500}
      \range{pages}{46}
      \verb{doi}
      \verb 10.1137/07070111X
      \endverb
      \verb{file}
      \verb Kolda und Bader - 2009 - Tensor Decompositions and Applications.pdf:/Users/alexgoessmann/Zotero/storage/IJHSGP4F/Kolda und Bader - 2009 - Tensor Decompositions and Applications.pdf:application/pdf
      \endverb
    \endentry
    \entry{koller_probabilistic_2009}{book}{}
      \name{author}{2}{}{%
        {{hash=9c785bcf6b8a0b99e44695299a6aecb2}{%
           family={Koller},
           familyi={K\bibinitperiod},
           given={Daphne},
           giveni={D\bibinitperiod}}}%
        {{hash=987ee418ae32e261a8a5385e14cda479}{%
           family={Friedman},
           familyi={F\bibinitperiod},
           given={Nir},
           giveni={N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge, Mass.}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{fullhash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{bibnamehash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{authorbibnamehash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{authornamehash}{644f618617c37fd03b6bc0248cd308d5}
      \strng{authorfullhash}{644f618617c37fd03b6bc0248cd308d5}
      \field{labelalpha}{KF09}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions.Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.}
      \field{edition}{1. edition}
      \field{isbn}{978-0-262-01319-2}
      \field{month}{7}
      \field{shorttitle}{Probabilistic {Graphical} {Models}}
      \field{title}{Probabilistic {Graphical} {Models}: {Principles} and {Techniques}}
      \field{year}{2009}
    \endentry
    \entry{kouagou_neural_2022}{misc}{}
      \name{author}{4}{}{%
        {{hash=c94d14254735bb0aa1815bba2883a633}{%
           family={Kouagou},
           familyi={K\bibinitperiod},
           given={N'Dah\bibnamedelima Jean},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=776296590b83e52cd7b5c0a34ed2467b}{%
           family={Heindorf},
           familyi={H\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=0fdd0add34c43fbcb7685ace9926ce07}{%
           family={Demir},
           familyi={D\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=a53ea1e67bff64c9ebe4ffbf19a852d2}{%
           family={Ngomo},
           familyi={N\bibinitperiod},
           given={Axel-Cyrille\bibnamedelima Ngonga},
           giveni={A\bibinithyphendelim C\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{cb853b4fb1a5d516d37b1eb4d9c99b2d}
      \strng{fullhash}{d172fd5714e9bce105e79b8f781f356a}
      \strng{bibnamehash}{cb853b4fb1a5d516d37b1eb4d9c99b2d}
      \strng{authorbibnamehash}{cb853b4fb1a5d516d37b1eb4d9c99b2d}
      \strng{authornamehash}{cb853b4fb1a5d516d37b1eb4d9c99b2d}
      \strng{authorfullhash}{d172fd5714e9bce105e79b8f781f356a}
      \field{extraname}{1}
      \field{labelalpha}{Kou+22}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most existing approaches for class expression learning in description logics are search algorithms. As the search space of these approaches is infinite, they often fail to scale to large learning problems. Our main intuition is that class expression learning can be regarded as a translation problem. Based thereupon, we propose a new family of class expression learning approaches which we dub neural class expression synthesis. Instances of this new family circumvent the high search costs entailed by current algorithms by translating training examples into class expressions in a fashion akin to machine translation solutions. Consequently, they are not subject to the runtime limitations of search-based approaches post training. We study three instances of this novel family of approaches to synthesize class expressions from sets of positive and negative examples. An evaluation of our approach on four benchmark datasets suggests that it can effectively synthesize high-quality class expressions with respect to the input examples in approximately one second on average. Moreover, a comparison to other state-of-the-art approaches suggests that we achieve better F-measures on large datasets. For reproducibility purposes, we provide our implementation as well as pretrained models in our public GitHub repository at https://github.com/fosterreproducibleresearch/NCES.}
      \field{month}{12}
      \field{note}{arXiv:2111.08486 [cs]}
      \field{title}{Neural {Class} {Expression} {Synthesis}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{file}
      \verb Kouagou et al. - 2022 - Neural Class Expression Synthesis.pdf:/Users/alexgoessmann/Zotero/storage/67GHBI5I/Kouagou et al. - 2022 - Neural Class Expression Synthesis.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2111.08486
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2111.08486
      \endverb
      \keyw{Computer Science - Artificial Intelligence}
    \endentry
    \entry{pesquita_neural_2023}{incollection}{}
      \name{author}{4}{}{%
        {{hash=a54332a00b61dd46638bd0d7050c30aa}{%
           family={Kouagou},
           familyi={K\bibinitperiod},
           given={N’Dah\bibnamedelima Jean},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=776296590b83e52cd7b5c0a34ed2467b}{%
           family={Heindorf},
           familyi={H\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=0fdd0add34c43fbcb7685ace9926ce07}{%
           family={Demir},
           familyi={D\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=aaffae6ab501c5d30bd5ba943863c330}{%
           family={Ngonga\bibnamedelima Ngomo},
           familyi={N\bibinitperiod\bibinitdelim N\bibinitperiod},
           given={Axel-Cyrille},
           giveni={A\bibinithyphendelim C\bibinitperiod}}}%
      }
      \name{editor}{8}{}{%
        {{hash=01de40b70b6987089f9a248cabe85202}{%
           family={Pesquita},
           familyi={P\bibinitperiod},
           given={Catia},
           giveni={C\bibinitperiod}}}%
        {{hash=acf0f4df6af5ba7da3c0594e4282b4aa}{%
           family={Jimenez-Ruiz},
           familyi={J\bibinithyphendelim R\bibinitperiod},
           given={Ernesto},
           giveni={E\bibinitperiod}}}%
        {{hash=8befb538a26e9b5b5c5cdc987c0796df}{%
           family={McCusker},
           familyi={M\bibinitperiod},
           given={Jamie},
           giveni={J\bibinitperiod}}}%
        {{hash=fbf83994f4c3aa3f53bcc9a549a96b25}{%
           family={Faria},
           familyi={F\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=2c8f7c698df5ca2bc9477059127b1f21}{%
           family={Dragoni},
           familyi={D\bibinitperiod},
           given={Mauro},
           giveni={M\bibinitperiod}}}%
        {{hash=3b9c611a002db08364a65ed5f2d88056}{%
           family={Dimou},
           familyi={D\bibinitperiod},
           given={Anastasia},
           giveni={A\bibinitperiod}}}%
        {{hash=80e25efccf4a0a840a57cf28aaf4b72b}{%
           family={Troncy},
           familyi={T\bibinitperiod},
           given={Raphael},
           giveni={R\bibinitperiod}}}%
        {{hash=4af9fa5cec1be62b092168e1285d0bb5}{%
           family={Hertling},
           familyi={H\bibinitperiod},
           given={Sven},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer Nature Switzerland}%
      }
      \strng{namehash}{0e8d59226d25c98e0969bac255d595e7}
      \strng{fullhash}{36d34769856e0d044036394d86d97a4c}
      \strng{bibnamehash}{0e8d59226d25c98e0969bac255d595e7}
      \strng{authorbibnamehash}{0e8d59226d25c98e0969bac255d595e7}
      \strng{authornamehash}{0e8d59226d25c98e0969bac255d595e7}
      \strng{authorfullhash}{36d34769856e0d044036394d86d97a4c}
      \strng{editorbibnamehash}{4166c9bb32774750f224250d77e875ab}
      \strng{editornamehash}{4166c9bb32774750f224250d77e875ab}
      \strng{editorfullhash}{1de6e60b957c9b1e8817fd68fcf43244}
      \field{extraname}{2}
      \field{labelalpha}{Kou+23}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{The {Semantic} {Web}}
      \field{isbn}{978-3-031-33454-2 978-3-031-33455-9}
      \field{note}{Series Title: Lecture Notes in Computer Science}
      \field{title}{Neural {Class} {Expression} {Synthesis}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{13870}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{209\bibrangedash 226}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/978-3-031-33455-9_13
      \endverb
      \verb{file}
      \verb Kouagou et al. - 2023 - Neural Class Expression Synthesis.pdf:/Users/alexgoessmann/Zotero/storage/2JW9DENY/Kouagou et al. - 2023 - Neural Class Expression Synthesis.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/10.1007/978-3-031-33455-9_13
      \endverb
      \verb{url}
      \verb https://link.springer.com/10.1007/978-3-031-33455-9_13
      \endverb
    \endentry
    \entry{landsberg_tensors_2011}{book}{}
      \name{author}{1}{}{%
        {{hash=fd6874950b4ff2280f2bcff26f2c7dd5}{%
           family={Landsberg},
           familyi={L\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {American Mathematical Society}%
      }
      \strng{namehash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{fullhash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{bibnamehash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{authorbibnamehash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{authornamehash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \strng{authorfullhash}{fd6874950b4ff2280f2bcff26f2c7dd5}
      \field{labelalpha}{Lan11}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Advancing research. Creating connections.}
      \field{isbn}{978-0-8218-6907-9 978-0-8218-8481-2 978-0-8218-8483-6 978-1-4704-0923-4}
      \field{month}{12}
      \field{series}{Graduate {Studies} in {Mathematics}}
      \field{shorttitle}{Tensors}
      \field{title}{Tensors: {Geometry} and {Applications}}
      \field{urlday}{10}
      \field{urlmonth}{2}
      \field{urlyear}{2021}
      \field{volume}{128}
      \field{year}{2011}
      \field{urldateera}{ce}
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/DUUS295M/128.html:text/html
      \endverb
    \endentry
    \entry{lehmann_class_2011}{article}{}
      \name{author}{4}{}{%
        {{hash=045d4294ee6b6936c99d278b857478b4}{%
           family={Lehmann},
           familyi={L\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
        {{hash=74de32830575f67e1479f58f47d2111d}{%
           family={Auer},
           familyi={A\bibinitperiod},
           given={Sören},
           giveni={S\bibinitperiod}}}%
        {{hash=cd0cd9cd5488d7e1a1d8553008b2419e}{%
           family={Bühmann},
           familyi={B\bibinitperiod},
           given={Lorenz},
           giveni={L\bibinitperiod}}}%
        {{hash=bbf3fe6d44f55d043fa4a59e4aa2c555}{%
           family={Tramp},
           familyi={T\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{f3943df71780a02de8efff3344001915}
      \strng{fullhash}{afc665691c357cb9797f3af3e5bdbee3}
      \strng{bibnamehash}{f3943df71780a02de8efff3344001915}
      \strng{authorbibnamehash}{f3943df71780a02de8efff3344001915}
      \strng{authornamehash}{f3943df71780a02de8efff3344001915}
      \strng{authorfullhash}{afc665691c357cb9797f3af3e5bdbee3}
      \field{labelalpha}{Leh+11}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{While the number of knowledge bases in the Semantic Web increases, the maintenance and creation of ontology schemata still remain a challenge. In particular creating class expressions constitutes one of the more demanding aspects of ontology engineering. In this article we describe how to adapt a semi-automatic method for learning OWL class expressions to the ontology engineering use case. Specifically, we describe how to extend an existing learning algorithm for the class learning problem. We perform rigorous performance optimization of the underlying algorithms for providing instant suggestions to the user. We also present two plugins, which use the algorithm, for the popular Protégé and OntoWiki ontology editors and provide a preliminary evaluation on real ontologies.}
      \field{issn}{1570-8268}
      \field{journaltitle}{Journal of Web Semantics}
      \field{month}{3}
      \field{number}{1}
      \field{title}{Class expression learning for ontology engineering}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{9}
      \field{year}{2011}
      \field{urldateera}{ce}
      \field{pages}{71\bibrangedash 81}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1016/j.websem.2011.01.001
      \endverb
      \verb{file}
      \verb ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/FJ2IMSV5/S1570826811000023.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1570826811000023
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1570826811000023
      \endverb
      \keyw{Heuristics,OWL,Concept learning,Ontology editor plugins,Ontology engineering,Supervised machine learning}
    \endentry
    \entry{mackay_information_2003}{book}{}
      \name{author}{1}{}{%
        {{hash=99e8c42965ec01aec3921af3e8ecd658}{%
           family={MacKay},
           familyi={M\bibinitperiod},
           given={David\bibnamedelimb J.\bibnamedelimi C.},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{fullhash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{bibnamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authorbibnamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authornamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authorfullhash}{99e8c42965ec01aec3921af3e8ecd658}
      \field{labelalpha}{Mac03}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Information theory and inference, taught together in this exciting textbook, lie at the heart of many important areas of modern technology - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics and cryptography. The book introduces theory in tandem with applications. Information theory is taught alongside practical communication systems such as arithmetic coding for data compression and sparse-graph codes for error-correction. Inference techniques, including message-passing algorithms, Monte Carlo methods and variational approximations, are developed alongside applications to clustering, convolutional codes, independent component analysis, and neural networks. Uniquely, the book covers state-of-the-art error-correcting codes, including low-density-parity-check codes, turbo codes, and digital fountain codes - the twenty-first-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning, and for undergraduate or graduate courses. It also provides an unparalleled entry point for professionals in areas as diverse as computational biology, financial engineering and machine learning.}
      \field{edition}{Illustrated Edition}
      \field{isbn}{978-0-521-64298-9}
      \field{month}{9}
      \field{title}{Information {Theory}, {Inference} and {Learning} {Algorithms}}
      \field{year}{2003}
    \endentry
    \entry{mackworth_consistency_1977}{article}{}
      \name{author}{1}{}{%
        {{hash=350f48b82a8f1edb46d9d200435480f9}{%
           family={Mackworth},
           familyi={M\bibinitperiod},
           given={Alan\bibnamedelima K.},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \strng{namehash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{fullhash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{bibnamehash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{authorbibnamehash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{authornamehash}{350f48b82a8f1edb46d9d200435480f9}
      \strng{authorfullhash}{350f48b82a8f1edb46d9d200435480f9}
      \field{labelalpha}{Mac77}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Artificial intelligence tasks which can be formulated as constraint satisfaction problems, with which this paper is for the most part concerned, are usually by solved backtracking the examining the thrashing behavior that nearly always accompanies backtracking, identifying three of its causes and proposing remedies for them we are led to a class of algorithms whoch can profitably be used to eliminate local (node, arc and path) inconsistencies before any attempt is made to construct a complete solution. A more general paradigm for attacking these tasks is the altenation of constraint manipulation and case analysis producing an OR problem graph which may be searched in any of the usual ways. Many authors, particularly Montanari and Waltz, have contributed to the development of these ideas; a secondary aim of this paper is to trace that history. The primary aim is to provide an accessible, unified framework, within which to present the algorithms including a new path consistency algorithm, to discuss their relationships and the may applications, both realized and potential of network consistency algorithms.}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{2}
      \field{number}{1}
      \field{title}{Consistency in networks of relations}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{volume}{8}
      \field{year}{1977}
      \field{urldateera}{ce}
      \field{pages}{99\bibrangedash 118}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1016/0004-3702(77)90007-8
      \endverb
      \verb{file}
      \verb ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/YK9L3CVK/0004370277900078.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/0004370277900078
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/0004370277900078
      \endverb
    \endentry
    \entry{marra_statistical_2024}{article}{}
      \name{author}{4}{}{%
        {{hash=31c22669ece71e8badb9d793d5ec710a}{%
           family={Marra},
           familyi={M\bibinitperiod},
           given={Giuseppe},
           giveni={G\bibinitperiod}}}%
        {{hash=12caa4fda26a3cda1f2855a260f35bd4}{%
           family={Dumančić},
           familyi={D\bibinitperiod},
           given={Sebastijan},
           giveni={S\bibinitperiod}}}%
        {{hash=2f35c8160c2583a2c38ae8050688b9bd}{%
           family={Manhaeve},
           familyi={M\bibinitperiod},
           given={Robin},
           giveni={R\bibinitperiod}}}%
        {{hash=d71ef24842e2fd1df4b1989ff0d24ff9}{%
           family={De\bibnamedelima Raedt},
           familyi={D\bibinitperiod\bibinitdelim R\bibinitperiod},
           given={Luc},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{9b10129f97a71170ad23ac4c769a1703}
      \strng{fullhash}{8f1a4091160994d81aefb515ef272e07}
      \strng{bibnamehash}{9b10129f97a71170ad23ac4c769a1703}
      \strng{authorbibnamehash}{9b10129f97a71170ad23ac4c769a1703}
      \strng{authornamehash}{9b10129f97a71170ad23ac4c769a1703}
      \strng{authorfullhash}{8f1a4091160994d81aefb515ef272e07}
      \field{labelalpha}{Mar+24}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{3}
      \field{shorttitle}{From statistical relational to neurosymbolic artificial intelligence}
      \field{title}{From statistical relational to neurosymbolic artificial intelligence: {A} survey}
      \field{urlday}{20}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{328}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{104062}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.artint.2023.104062
      \endverb
      \verb{file}
      \verb ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/NRNUENXF/S0004370223002084.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370223002084
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370223002084
      \endverb
      \keyw{Neurosymbolic AI,Learning and reasoning,Probabilistic logics,Statistical relational AI}
    \endentry
    \entry{mccarthy_programs_1959}{incollection}{}
      \name{author}{1}{}{%
        {{hash=627c3779677d0eaf3312e5b67e8cbe0c}{%
           family={McCarthy},
           familyi={M\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {London}%
      }
      \list{publisher}{1}{%
        {Her Majesty's Stationary Office}%
      }
      \strng{namehash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{fullhash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{bibnamehash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{authorbibnamehash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{authornamehash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \strng{authorfullhash}{627c3779677d0eaf3312e5b67e8cbe0c}
      \field{labelalpha}{McC59}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the {Teddington} {Conference} on the {Mechanization} of {Thought} {Processes}}
      \field{title}{Programs with {Common} {Sense}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{1959}
      \field{urldateera}{ce}
      \field{pages}{75\bibrangedash 91}
      \range{pages}{17}
      \verb{file}
      \verb Programs with Common Sense | BibSonomy:/Users/alexgoessmann/Zotero/storage/XQHW6V6P/73597914e6cc614c92d8eedf4a1dab52.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://www-formal.stanford.edu/jmc/mcc59.html
      \endverb
      \verb{url}
      \verb http://www-formal.stanford.edu/jmc/mcc59.html
      \endverb
    \endentry
    \entry{muggleton_inductive_1994}{article}{}
      \name{author}{2}{}{%
        {{hash=2e8a001498de1734264220eb4a0c92b6}{%
           family={Muggleton},
           familyi={M\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=d71ef24842e2fd1df4b1989ff0d24ff9}{%
           family={De\bibnamedelima Raedt},
           familyi={D\bibinitperiod\bibinitdelim R\bibinitperiod},
           given={Luc},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{fullhash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{bibnamehash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{authorbibnamehash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{authornamehash}{a50f5bc8d92eef0c604dae9e424e6241}
      \strng{authorfullhash}{a50f5bc8d92eef0c604dae9e424e6241}
      \field{labelalpha}{MD94}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{journaltitle}{The Journal of Logic Programming}
      \field{note}{Publisher: Elsevier}
      \field{shorttitle}{Inductive logic programming}
      \field{title}{Inductive logic programming: {Theory} and methods}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{19}
      \field{year}{1994}
      \field{urldateera}{ce}
      \field{pages}{629\bibrangedash 679}
      \range{pages}{51}
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/0743106694900353
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/0743106694900353
      \endverb
    \endentry
    \entry{motzkin_beitrage_1936}{thesis}{}
      \name{author}{1}{}{%
        {{hash=320a51b5f8cb5732a5484001e0b4202a}{%
           family={Motzkin},
           familyi={M\bibinitperiod},
           given={Theodore\bibnamedelima S.},
           giveni={T\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Buchdruckerei Azriel}%
      }
      \list{language}{1}{%
        {ger}%
      }
      \list{location}{1}{%
        {Jerusalem}%
      }
      \strng{namehash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{fullhash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{bibnamehash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{authorbibnamehash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{authornamehash}{320a51b5f8cb5732a5484001e0b4202a}
      \strng{authorfullhash}{320a51b5f8cb5732a5484001e0b4202a}
      \field{labelalpha}{Mot36}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{note}{Book Title: Beiträge zur Theorie der linearen Ungleichungen}
      \field{title}{Beiträge zur {Theorie} der linearen {Ungleichungen}}
      \field{type}{phdthesis}
      \field{year}{1936}
    \endentry
    \entry{murphy_probabilistic_2022}{book}{}
      \name{author}{1}{}{%
        {{hash=99413be56c82adf72b6474dcdf8d3023}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Kevin\bibnamedelima P.},
           giveni={K\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Cambridge, Massachusetts London, England}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{fullhash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{bibnamehash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{authorbibnamehash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{authornamehash}{99413be56c82adf72b6474dcdf8d3023}
      \strng{authorfullhash}{99413be56c82adf72b6474dcdf8d3023}
      \field{labelalpha}{Mur22}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A detailed and up-to-date introduction to machine learning, presented through the unifying lens of probabilistic modeling and Bayesian decision theory.This book offers a detailed and up-to-date introduction to machine learning (including deep learning) through the unifying lens of probabilistic modeling and Bayesian decision theory. The book covers mathematical background (including linear algebra and optimization), basic supervised learning (including linear and logistic regression and deep neural networks), as well as more advanced topics (including transfer learning and unsupervised learning). End-of-chapter exercises allow students to apply what they have learned, and an appendix covers notation. Probabilistic Machine Learning grew out of the author’s 2012 book, Machine Learning: A Probabilistic Perspective. More than just a simple update, this is a completely new book that reflects the dramatic developments in the field since 2012, most notably deep learning. In addition, the new book is accompanied by online Python code, using libraries such as scikit-learn, JAX, PyTorch, and Tensorflow, which can be used to reproduce nearly all the figures; this code can be run inside a web browser using cloud-based notebooks, and provides a practical complement to the theoretical topics discussed in the book. This introductory text will be followed by a sequel that covers more advanced topics, taking the same probabilistic approach.}
      \field{isbn}{978-0-262-04682-4}
      \field{month}{3}
      \field{shorttitle}{Probabilistic {Machine} {Learning}}
      \field{title}{Probabilistic {Machine} {Learning}: {An} {Introduction}}
      \field{year}{2022}
    \endentry
    \entry{nickel_review_2016}{article}{}
      \name{author}{4}{}{%
        {{hash=9754396c9b0fc2f09bd396146fbf2ef3}{%
           family={Nickel},
           familyi={N\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=d01ba1edf5711066e4d420d9df8d8f43}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=7b4bb47afe2e1fc9925f6178ea46b207}{%
           family={Tresp},
           familyi={T\bibinitperiod},
           given={Volker},
           giveni={V\bibinitperiod}}}%
        {{hash=835fa77790b1d554c00910dc1147619d}{%
           family={Gabrilovich},
           familyi={G\bibinitperiod},
           given={Evgeniy},
           giveni={E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{37798d3144a8e42a41d5eb16f42b9e1e}
      \strng{fullhash}{78e825a3c051c43892f860e21527cfa3}
      \strng{bibnamehash}{37798d3144a8e42a41d5eb16f42b9e1e}
      \strng{authorbibnamehash}{37798d3144a8e42a41d5eb16f42b9e1e}
      \strng{authornamehash}{37798d3144a8e42a41d5eb16f42b9e1e}
      \strng{authorfullhash}{78e825a3c051c43892f860e21527cfa3}
      \field{labelalpha}{Nic+16}
      \field{sortinit}{N}
      \field{sortinithash}{f7242c3ed3dc50029fca1be76c497c7c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The ﬁrst is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google’s Knowledge Vault project as an example of such combination.}
      \field{issn}{0018-9219, 1558-2256}
      \field{journaltitle}{Proceedings of the IEEE}
      \field{month}{1}
      \field{number}{1}
      \field{title}{A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{104}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{11\bibrangedash 33}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1109/JPROC.2015.2483592
      \endverb
      \verb{file}
      \verb Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowledge Graphs.pdf:/Users/alexgoessmann/Zotero/storage/SYKDZ9EC/Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowledge Graphs.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/7358050/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/7358050/
      \endverb
    \endentry
    \entry{nikolic_survey_2022}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=792f1ec4ae9f91a2da9265fa20afae88}{%
           family={Nikolić},
           familyi={N\bibinitperiod},
           given={Goran\bibnamedelima S.},
           giveni={G\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=dd6aed843e8af934ad41ce82a5dbee4e}{%
           family={Dimitrijević},
           familyi={D\bibinitperiod},
           given={Bojan\bibnamedelima R.},
           giveni={B\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=9a7228f0144f45d43d9518124f3c8674}{%
           family={Nikolić},
           familyi={N\bibinitperiod},
           given={Tatjana\bibnamedelima R.},
           giveni={T\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=8391265dedbfb6653d4a3a4927f5e82a}{%
           family={Stojcev},
           familyi={S\bibinitperiod},
           given={Mile\bibnamedelima K.},
           giveni={M\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \strng{namehash}{78219851dbcbe8ce0837da8e8089c1c6}
      \strng{fullhash}{405a6ee83447c47de66d8fc7a7b6afc0}
      \strng{bibnamehash}{78219851dbcbe8ce0837da8e8089c1c6}
      \strng{authorbibnamehash}{78219851dbcbe8ce0837da8e8089c1c6}
      \strng{authornamehash}{78219851dbcbe8ce0837da8e8089c1c6}
      \strng{authorfullhash}{405a6ee83447c47de66d8fc7a7b6afc0}
      \field{labelalpha}{Nik+22}
      \field{sortinit}{N}
      \field{sortinithash}{f7242c3ed3dc50029fca1be76c497c7c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The CPU, GPU, and TPU are three different types of processing units. For the overall performance of the computer, the CPU is responsible. For delivering high-end graphics and video quality, the GPU is responsible. Along with the CPU, the GPU is a piece of additional hardware. TPU is used in the field of Artificial Intelligence, Machine Learning, and Deep Learning. Each of the three processing units has its own set of functions. This article may be of help to a reader with aim to understand the distinctions between the CPU, GPU, and TPU processing units.}
      \field{booktitle}{2022 57th {International} {Scientific} {Conference} on {Information}, {Communication} and {Energy} {Systems} and {Technologies} ({ICEST})}
      \field{month}{6}
      \field{shorttitle}{A {Survey} of {Three} {Types} of {Processing} {Units}}
      \field{title}{A {Survey} of {Three} {Types} of {Processing} {Units}: {CPU}, {GPU} and {TPU}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 6}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1109/ICEST55168.2022.9828625
      \endverb
      \verb{file}
      \verb IEEE Xplore Abstract Record:/Users/alexgoessmann/Zotero/storage/346ZFMIA/9828625.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9828625
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9828625
      \endverb
      \keyw{Central Processing Unit,Computer performance,CPU,Deep learning,GPU,Graphics processing units,Hardware,hardware accelerator,Quality assessment,TPU,Video recording}
    \endentry
    \entry{nickel_three-way_2011}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=9754396c9b0fc2f09bd396146fbf2ef3}{%
           family={Nickel},
           familyi={N\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=7b4bb47afe2e1fc9925f6178ea46b207}{%
           family={Tresp},
           familyi={T\bibinitperiod},
           given={Volker},
           giveni={V\bibinitperiod}}}%
        {{hash=9559fe65ed2c0877cf14a66fe1f8e9b3}{%
           family={Kriegel},
           familyi={K\bibinitperiod},
           given={Hans-Peter},
           giveni={H\bibinithyphendelim P\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Madison, WI, USA}%
      }
      \list{publisher}{1}{%
        {Omnipress}%
      }
      \strng{namehash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{fullhash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{bibnamehash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{authorbibnamehash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{authornamehash}{cbc1a834438f98968deed5b0ed65db6c}
      \strng{authorfullhash}{cbc1a834438f98968deed5b0ed65db6c}
      \field{labelalpha}{NTK11}
      \field{sortinit}{N}
      \field{sortinithash}{f7242c3ed3dc50029fca1be76c497c7c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.}
      \field{booktitle}{Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}}
      \field{isbn}{978-1-4503-0619-5}
      \field{month}{6}
      \field{series}{{ICML}'11}
      \field{title}{A three-way model for collective learning on multi-relational data}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2011}
      \field{urldateera}{ce}
      \field{pages}{809\bibrangedash 816}
      \range{pages}{8}
    \endentry
    \entry{orus_tensor_2019}{article}{}
      \name{author}{1}{}{%
        {{hash=220e5a12680385b58fbfac3e4405944d}{%
           family={Orús},
           familyi={O\bibinitperiod},
           given={Román},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{220e5a12680385b58fbfac3e4405944d}
      \strng{fullhash}{220e5a12680385b58fbfac3e4405944d}
      \strng{bibnamehash}{220e5a12680385b58fbfac3e4405944d}
      \strng{authorbibnamehash}{220e5a12680385b58fbfac3e4405944d}
      \strng{authornamehash}{220e5a12680385b58fbfac3e4405944d}
      \strng{authorfullhash}{220e5a12680385b58fbfac3e4405944d}
      \field{labelalpha}{Orú19}
      \field{sortinit}{O}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Originally developed in the context of condensed-matter physics and based on renormalization group ideas, tensor networks have been revived thanks to quantum information theory and the progress in understanding the role of entanglement in quantum many-body systems. Moreover, tensor network states have turned out to play a key role in other scientific disciplines. In this context, here I provide an overview of the basic concepts and key developments in the field. I briefly discuss the most important tensor network structures and algorithms, together with an outline of advances related to global and gauge symmetries, fermions, topological order, classification of phases, entanglement Hamiltonians, holografic duality, artificial intelligence, the 2D Hubbard model, 2D quantum antiferromagnets, conformal field theory, quantum chemistry, disordered systems and many-body localization.}
      \field{issn}{2522-5820}
      \field{journaltitle}{Nature Reviews Physics}
      \field{month}{9}
      \field{number}{9}
      \field{title}{Tensor networks for complex quantum systems}
      \field{urlday}{5}
      \field{urlmonth}{7}
      \field{urlyear}{2021}
      \field{volume}{1}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{538\bibrangedash 550}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1038/s42254-019-0086-7
      \endverb
      \verb{file}
      \verb Eingereichte Version:/Users/alexgoessmann/Zotero/storage/9FQ9Y6UB/Orús - 2019 - Tensor networks for complex quantum systems.pdf:application/pdf;Full Text PDF:/Users/alexgoessmann/Zotero/storage/SUTVREJ7/Orús - 2019 - Tensor networks for complex quantum systems.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/H88TAZGS/s42254-019-0086-7.html:text/html
      \endverb
      \keyw{Condensed-matter physics,Quantum information,Theoretical physics}
    \endentry
    \entry{oseledets_breaking_2009}{article}{}
      \name{author}{2}{}{%
        {{hash=51eaea8336c314e73559b6b8edb6a970}{%
           family={Oseledets},
           familyi={O\bibinitperiod},
           given={I.\bibnamedelimi V.},
           giveni={I\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=11f730529183e1d7182389f6608ab2f5}{%
           family={Tyrtyshnikov},
           familyi={T\bibinitperiod},
           given={E.\bibnamedelimi E.},
           giveni={E\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{fullhash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{bibnamehash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{authorbibnamehash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{authornamehash}{2d13c393e6278022f0d13835d8ad1fe5}
      \strng{authorfullhash}{2d13c393e6278022f0d13835d8ad1fe5}
      \field{labelalpha}{OT09}
      \field{sortinit}{O}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{For d-dimensional tensors with possibly large \$d{>}3\$, an hierarchical data structure, called the Tree-Tucker format, is presented as an alternative to the canonical decomposition. It has asymptotically the same (and often even smaller) number of representation parameters and viable stability properties. The approach involves a recursive construction described by a tree with the leafs corresponding to the Tucker decompositions of three-dimensional tensors, and is based on a sequence of SVDs for the recursively obtained unfolding matrices and on the auxiliary dimensions added to the initial “spatial” dimensions. It is shown how this format can be applied to the problem of multidimensional convolution. Convincing numerical examples are given.}
      \field{issn}{1064-8275}
      \field{journaltitle}{SIAM Journal on Scientific Computing}
      \field{month}{1}
      \field{note}{Publisher: Society for Industrial and Applied Mathematics}
      \field{number}{5}
      \field{title}{Breaking the {Curse} of {Dimensionality}, {Or} {How} to {Use} {SVD} in {Many} {Dimensions}}
      \field{urlday}{12}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{volume}{31}
      \field{year}{2009}
      \field{urldateera}{ce}
      \field{pages}{3744\bibrangedash 3759}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1137/090748330
      \endverb
      \verb{file}
      \verb Eingereichte Version:/Users/alexgoessmann/Zotero/storage/PAFGIT3R/Oseledets und Tyrtyshnikov - 2009 - Breaking the Curse of Dimensionality, Or How to Us.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/IADYGZMV/090748330.html:text/html
      \endverb
    \endentry
    \entry{paszke_pytorch_2019}{misc}{}
      \name{author}{21}{}{%
        {{hash=56bf0b340039cf8594436a624ff548a9}{%
           family={Paszke},
           familyi={P\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=4ba5062e5919c814aceec188d54c01f2}{%
           family={Gross},
           familyi={G\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
        {{hash=e5dfae4582081d649e3a0d5342050016}{%
           family={Massa},
           familyi={M\bibinitperiod},
           given={Francisco},
           giveni={F\bibinitperiod}}}%
        {{hash=b5815e1692fa2d0c1f44eecf509bd7c4}{%
           family={Lerer},
           familyi={L\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=b75383e6b48c8360c7a60031424c85cf}{%
           family={Bradbury},
           familyi={B\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=f897ed422c34d95af2e22778dfc2607e}{%
           family={Chanan},
           familyi={C\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=046269e070246feb6f394141db80ed87}{%
           family={Killeen},
           familyi={K\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=c40352c194e60a3ef458ee7e8685afb5}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Zeming},
           giveni={Z\bibinitperiod}}}%
        {{hash=6e45f49ec618e619efad90c8e8a61f0c}{%
           family={Gimelshein},
           familyi={G\bibinitperiod},
           given={Natalia},
           giveni={N\bibinitperiod}}}%
        {{hash=f65a80959d520337ae99a0798515036c}{%
           family={Antiga},
           familyi={A\bibinitperiod},
           given={Luca},
           giveni={L\bibinitperiod}}}%
        {{hash=954cf7680b6ce14813973eccdca3c4bc}{%
           family={Desmaison},
           familyi={D\bibinitperiod},
           given={Alban},
           giveni={A\bibinitperiod}}}%
        {{hash=048232cf7c525fbc0bc93052fe8cee03}{%
           family={Köpf},
           familyi={K\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
        {{hash=b9e701339e56fd0b171145b08288a1b7}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=42ac264897098b400e1367e5922c9b0d}{%
           family={DeVito},
           familyi={D\bibinitperiod},
           given={Zach},
           giveni={Z\bibinitperiod}}}%
        {{hash=d814afaa50b9e22ab92cc9f8f9a9e43a}{%
           family={Raison},
           familyi={R\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=3feeeebee8583ecc208f7fb3e0a55068}{%
           family={Tejani},
           familyi={T\bibinitperiod},
           given={Alykhan},
           giveni={A\bibinitperiod}}}%
        {{hash=e18536d5cb7543731fbf2ca1a4908732}{%
           family={Chilamkurthy},
           familyi={C\bibinitperiod},
           given={Sasank},
           giveni={S\bibinitperiod}}}%
        {{hash=0a0b028c6b85c46f368317d0c5bfe3a0}{%
           family={Steiner},
           familyi={S\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod}}}%
        {{hash=998a001f16bb57c079c1d5afb1cb02c8}{%
           family={Fang},
           familyi={F\bibinitperiod},
           given={Lu},
           giveni={L\bibinitperiod}}}%
        {{hash=3f19c633bbfb847db6a0e71d3659eacd}{%
           family={Bai},
           familyi={B\bibinitperiod},
           given={Junjie},
           giveni={J\bibinitperiod}}}%
        {{hash=8ef51a0906e47d2b4472c4e714ed598f}{%
           family={Chintala},
           familyi={C\bibinitperiod},
           given={Soumith},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{fullhash}{4842db6c92a33147f588935fdde44a69}
      \strng{bibnamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorbibnamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authornamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorfullhash}{4842db6c92a33147f588935fdde44a69}
      \field{labelalpha}{Pas+19}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.}
      \field{month}{12}
      \field{note}{arXiv:1912.01703 [cs]}
      \field{shorttitle}{{PyTorch}}
      \field{title}{{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1912.01703
      \endverb
      \verb{file}
      \verb Preprint PDF:/Users/alexgoessmann/Zotero/storage/Q7NZBUK6/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/NEDNHQYI/1912.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1912.01703
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1912.01703
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Mathematical Software}
    \endentry
    \entry{pearl_causality_2009}{book}{}
      \name{author}{1}{}{%
        {{hash=809f695b398afbb54b544c49e8d1bbbb}{%
           family={Pearl},
           familyi={P\bibinitperiod},
           given={Judea},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Cambridge New York, NY Port Melbourne New Delhi Singapore}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{fullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{bibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorbibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authornamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorfullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \field{extraname}{1}
      \field{labelalpha}{Pea09}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.}
      \field{edition}{2}
      \field{isbn}{978-0-521-89560-6}
      \field{month}{11}
      \field{shorttitle}{Causality}
      \field{title}{Causality: {Models}, {Reasoning} and {Inference}. {Ausgezeichnet}: {ACM} {Turing} {Award} for {Transforming} {Artificial} {Intelligence} 2011}
      \field{year}{2009}
    \endentry
    \entry{pearl_probabilistic_1988}{book}{}
      \name{author}{1}{}{%
        {{hash=809f695b398afbb54b544c49e8d1bbbb}{%
           family={Pearl},
           familyi={P\bibinitperiod},
           given={Judea},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {s.l.}%
      }
      \list{publisher}{1}{%
        {Morgan Kaufmann}%
      }
      \strng{namehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{fullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{bibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorbibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authornamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorfullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \field{extraname}{2}
      \field{labelalpha}{Pea88}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Probabilistic Reasoning in Intelligent Systems is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty-and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition-in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.}
      \field{isbn}{978-1-55860-479-7}
      \field{month}{9}
      \field{shorttitle}{Probabilistic {Reasoning} in {Intelligent} {Systems}}
      \field{title}{Probabilistic {Reasoning} in {Intelligent} {Systems}: {Networks} of {Plausible} {Inference}}
      \field{year}{1988}
    \endentry
    \entry{penrose_spinors_1987}{book}{}
      \name{author}{1}{}{%
        {{hash=f51dfd339a12125d0f82a2d93e66d9b7}{%
           family={Penrose},
           familyi={P\bibinitperiod},
           given={Roger},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{fullhash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{bibnamehash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{authorbibnamehash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{authornamehash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \strng{authorfullhash}{f51dfd339a12125d0f82a2d93e66d9b7}
      \field{labelalpha}{Pen87}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This volume introduces and systematically develops the calculus of 2-spinors. This is the first detailed exposition of this technique which leads not only to a deeper understanding of the structure of space-time, but also provides shortcuts to some very tedious calculations. Many results are given here for the first time.}
      \field{isbn}{978-0-521-33707-6}
      \field{month}{2}
      \field{shorttitle}{Spinors and {Space}-{Time}}
      \field{title}{Spinors and {Space}-{Time}: {Volume} 1, {Two}-{Spinor} {Calculus} and {Relativistic} {Fields}}
      \field{year}{1987}
    \endentry
    \entry{perez-garcia_matrix_2007}{article}{}
      \name{author}{4}{}{%
        {{hash=99ab157e40dc6cc9e7c8ca6aef64277a}{%
           family={Perez-Garcia},
           familyi={P\bibinithyphendelim G\bibinitperiod},
           given={D.},
           giveni={D\bibinitperiod}}}%
        {{hash=085ab1d28e21b822e7bebcfb7d68f99f}{%
           family={Verstraete},
           familyi={V\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=4bf6e0e0b644336c6c5e578a60aa21aa}{%
           family={Wolf},
           familyi={W\bibinitperiod},
           given={M.\bibnamedelimi M.},
           giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=e95cc0745e2fbc83da963e5d6ab9c104}{%
           family={Cirac},
           familyi={C\bibinitperiod},
           given={J.\bibnamedelimi I.},
           giveni={J\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \strng{namehash}{82a32fa9d2c9290465d1f60ba6e159a0}
      \strng{fullhash}{e51d433ca5c53cb17c9242ffea109fc3}
      \strng{bibnamehash}{82a32fa9d2c9290465d1f60ba6e159a0}
      \strng{authorbibnamehash}{82a32fa9d2c9290465d1f60ba6e159a0}
      \strng{authornamehash}{82a32fa9d2c9290465d1f60ba6e159a0}
      \strng{authorfullhash}{e51d433ca5c53cb17c9242ffea109fc3}
      \field{labelalpha}{Per+07}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This work gives a detailed investigation of matrix product state (MPS) representations for pure multipartite quantum states. We determine the freedom in representations with and without translation symmetry, derive respective canonical forms and provide efficient methods for obtaining them. Results on frustration free Hamiltonians and the generation of MPS are extended, and the use of the MPS-representation for classical simulations of quantum systems is discussed.}
      \field{issn}{1533-7146}
      \field{journaltitle}{Quantum Information \& Computation}
      \field{month}{7}
      \field{number}{5}
      \field{title}{Matrix product state representations}
      \field{volume}{7}
      \field{year}{2007}
      \field{pages}{401\bibrangedash 430}
      \range{pages}{30}
    \endentry
    \entry{puljak_tn4ml_2025}{misc}{}
      \name{author}{6}{}{%
        {{hash=5b0d2c5fe3ec56c4f02c712c175c0a1c}{%
           family={Puljak},
           familyi={P\bibinitperiod},
           given={Ema},
           giveni={E\bibinitperiod}}}%
        {{hash=3bf65995782a01f1090447d046b9c277}{%
           family={Sanchez-Ramirez},
           familyi={S\bibinithyphendelim R\bibinitperiod},
           given={Sergio},
           giveni={S\bibinitperiod}}}%
        {{hash=1fccb051da8e9ecc280dfe648edef35e}{%
           family={Masot-Llima},
           familyi={M\bibinithyphendelim L\bibinitperiod},
           given={Sergi},
           giveni={S\bibinitperiod}}}%
        {{hash=560618423ca0bbd53e34893a4fc5f464}{%
           family={Vallès-Muns},
           familyi={V\bibinithyphendelim M\bibinitperiod},
           given={Jofre},
           giveni={J\bibinitperiod}}}%
        {{hash=e27c89b3acba42f3f93dbcd9eeb3cf18}{%
           family={Garcia-Saez},
           familyi={G\bibinithyphendelim S\bibinitperiod},
           given={Artur},
           giveni={A\bibinitperiod}}}%
        {{hash=1c22a097d7d81eb61830498bc7f766dc}{%
           family={Pierini},
           familyi={P\bibinitperiod},
           given={Maurizio},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{3f9f538bee76744434e035cd84a448c6}
      \strng{fullhash}{89520fcc07c2ed28387c51d25409e780}
      \strng{bibnamehash}{3f9f538bee76744434e035cd84a448c6}
      \strng{authorbibnamehash}{3f9f538bee76744434e035cd84a448c6}
      \strng{authornamehash}{3f9f538bee76744434e035cd84a448c6}
      \strng{authorfullhash}{89520fcc07c2ed28387c51d25409e780}
      \field{labelalpha}{Pul+25}
      \field{sortinit}{P}
      \field{sortinithash}{8d51b3d5b78d75b54308d706b9bbe285}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Tensor Networks have emerged as a prominent alternative to neural networks for addressing Machine Learning challenges in foundational sciences, paving the way for their applications to real-life problems. This paper introduces tn4ml, a novel library designed to seamlessly integrate Tensor Networks into optimization pipelines for Machine Learning tasks. Inspired by existing Machine Learning frameworks, the library offers a user-friendly structure with modules for data embedding, objective function definition, and model training using diverse optimization strategies. We demonstrate its versatility through two examples: supervised learning on tabular data and unsupervised learning on an image dataset. Additionally, we analyze how customizing the parts of the Machine Learning pipeline for Tensor Networks influences performance metrics.}
      \field{month}{2}
      \field{note}{arXiv:2502.13090 [cs]}
      \field{shorttitle}{tn4ml}
      \field{title}{tn4ml: {Tensor} {Network} {Training} and {Customization} for {Machine} {Learning}}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2502.13090
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2502.13090
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2502.13090
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Mathematical Software,Quantum Physics}
    \endentry
    \entry{richardson_markov_2006}{article}{}
      \name{author}{2}{}{%
        {{hash=f96c5999895363061e2fe84b4cd057df}{%
           family={Richardson},
           familyi={R\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=926269a097c38456f5b32d7efb1d1a80}{%
           family={Domingos},
           familyi={D\bibinitperiod},
           given={Pedro},
           giveni={P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{fullhash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{bibnamehash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{authorbibnamehash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{authornamehash}{1021fc8259bc734b2f6116bc44be825e}
      \strng{authorfullhash}{1021fc8259bc734b2f6116bc44be825e}
      \field{labelalpha}{RD06}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a simple approach to combining ﬁrst-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a ﬁrst-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it speciﬁes a ground Markov network containing one feature for each possible grounding of a ﬁrst-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efﬁciently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.}
      \field{issn}{0885-6125, 1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{month}{2}
      \field{number}{1-2}
      \field{title}{Markov logic networks}
      \field{urlday}{14}
      \field{urlmonth}{1}
      \field{urlyear}{2023}
      \field{volume}{62}
      \field{year}{2006}
      \field{urldateera}{ce}
      \field{pages}{107\bibrangedash 136}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1007/s10994-006-5833-1
      \endverb
      \verb{file}
      \verb Richardson und Domingos - 2006 - Markov logic networks.pdf:/Users/alexgoessmann/Zotero/storage/AJNEGGHC/Richardson und Domingos - 2006 - Markov logic networks.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/s10994-006-5833-1
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/s10994-006-5833-1
      \endverb
    \endentry
    \entry{russell_artificial_2021}{book}{}
      \name{author}{2}{}{%
        {{hash=143fa183327d9fcd9de18eec99d6ca97}{%
           family={Russell},
           familyi={R\bibinitperiod},
           given={Stuart},
           giveni={S\bibinitperiod}}}%
        {{hash=5de798d5fa3c0236c0478134cd23f52a}{%
           family={Norvig},
           familyi={N\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Boston}%
      }
      \list{publisher}{1}{%
        {Pearson}%
      }
      \strng{namehash}{b280605b721b4ebcba5395298499f924}
      \strng{fullhash}{b280605b721b4ebcba5395298499f924}
      \strng{bibnamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authorbibnamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authornamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authorfullhash}{b280605b721b4ebcba5395298499f924}
      \field{labelalpha}{RN21}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Thelong-anticipated revision of ArtificialIntelligence: A Modern Approach explores the full breadth and depth of the field of artificialintelligence (AI). The 4th Edition brings readers up to date on the latest technologies,presents concepts in a more unified manner, and offers new or expanded coverageof machine learning, deep learning, transfer learning, multi agent systems,robotics, natural language processing, causality, probabilistic programming,privacy, fairness, and safe AI.}
      \field{edition}{4}
      \field{isbn}{978-1-292-40113-3}
      \field{month}{5}
      \field{shorttitle}{Artificial {Intelligence}}
      \field{title}{Artificial {Intelligence}: {A} {Modern} {Approach}, {Global} {Edition}: {A} {Modern} {Approach}, {Global} {Edition}}
      \field{year}{2021}
    \endentry
    \entry{rockafellar_convex_1997}{book}{}
      \name{author}{1}{}{%
        {{hash=b5c9101ddde9e15784b1c0d7ca7b7221}{%
           family={Rockafellar},
           familyi={R\bibinitperiod},
           given={Ralph\bibnamedelima Tyrell},
           giveni={R\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {Princeton}%
      }
      \list{publisher}{1}{%
        {Princeton University Press}%
      }
      \strng{namehash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{fullhash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{bibnamehash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{authorbibnamehash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{authornamehash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \strng{authorfullhash}{b5c9101ddde9e15784b1c0d7ca7b7221}
      \field{labelalpha}{Roc97}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Available for the first time in paperback, R. Tyrrell Rockafellar's classic study presents readers with a coherent branch of nonlinear mathematical analysis that is especially suited to the study of optimization problems. Rockafellar's theory differs from classical analysis in that differentiability assumptions are replaced by convexity assumptions. The topics treated in this volume include: systems of inequalities, the minimum or maximum of a convex function over a convex set, Lagrange multipliers, minimax theorems and duality, as well as basic results about the structure of convex sets and the continuity and differentiability of convex functions and saddle- functions. This book has firmly established a new and vital area not only for pure mathematics but also for applications to economics and engineering. A sound knowledge of linear algebra and introductory real analysis should provide readers with sufficient background for this book. There is also a guide for the reader who may be using the book as an introduction, indicating which parts are essential and which may be skipped on a first reading.}
      \field{edition}{reprint edition}
      \field{isbn}{978-0-691-01586-6}
      \field{month}{1}
      \field{title}{Convex {Analysis}}
      \field{year}{1997}
    \endentry
    \entry{robeva_duality_2019}{article}{}
      \name{author}{2}{}{%
        {{hash=fb455355b6019b9c510857c2cfba2bf8}{%
           family={Robeva},
           familyi={R\bibinitperiod},
           given={Elina},
           giveni={E\bibinitperiod}}}%
        {{hash=7f5aa105b6bbae27a02d4b92bd405efc}{%
           family={Seigal},
           familyi={S\bibinitperiod},
           given={Anna},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{5ac373fae89445f66b40cfee543db403}
      \strng{fullhash}{5ac373fae89445f66b40cfee543db403}
      \strng{bibnamehash}{5ac373fae89445f66b40cfee543db403}
      \strng{authorbibnamehash}{5ac373fae89445f66b40cfee543db403}
      \strng{authornamehash}{5ac373fae89445f66b40cfee543db403}
      \strng{authorfullhash}{5ac373fae89445f66b40cfee543db403}
      \field{labelalpha}{RS19}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this article we show the duality between tensor networks and undirected graphical models with discrete variables. We study tensor networks on hypergraphs, which we call tensor hypernetworks. We show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph. We translate various notions under duality. For example, marginalization in a graphical model is dual to contraction in the tensor network. Algorithms also translate under duality. We show that belief propagation corresponds to a known algorithm for tensor network contraction. This article is a reminder that the research areas of graphical models and tensor networks can benefit from interaction.}
      \field{issn}{2049-8772}
      \field{journaltitle}{Information and Inference: A Journal of the IMA}
      \field{month}{6}
      \field{number}{2}
      \field{title}{Duality of graphical models and tensor networks}
      \field{urlday}{6}
      \field{urlmonth}{7}
      \field{urlyear}{2021}
      \field{volume}{8}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{273\bibrangedash 288}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1093/imaiai/iay009
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/EFRFIZTN/Robeva und Seigal - 2019 - Duality of graphical models and tensor networks.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/5X57ZXMD/5041985.html:text/html
      \endverb
    \endentry
    \entry{rudolph_foundations_2011}{incollection}{}
      \name{author}{1}{}{%
        {{hash=4408b0bc9147789330e0059378241c8f}{%
           family={Rudolph},
           familyi={R\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \name{editor}{7}{}{%
        {{hash=a2be0ec850b04b1f5e9fe3aca6bf5226}{%
           family={Polleres},
           familyi={P\bibinitperiod},
           given={Axel},
           giveni={A\bibinitperiod}}}%
        {{hash=3eb1335a7c6a4acf8ca2e632ae7f6a09}{%
           family={d’Amato},
           familyi={d\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod}}}%
        {{hash=9282d63c7c5bda72aad9b39bc68fddeb}{%
           family={Arenas},
           familyi={A\bibinitperiod},
           given={Marcelo},
           giveni={M\bibinitperiod}}}%
        {{hash=1c08b5520041a41764ad0bc82b33746f}{%
           family={Handschuh},
           familyi={H\bibinitperiod},
           given={Siegfried},
           giveni={S\bibinitperiod}}}%
        {{hash=a3e7bd065215c1374025027954e59068}{%
           family={Kroner},
           familyi={K\bibinitperiod},
           given={Paula},
           giveni={P\bibinitperiod}}}%
        {{hash=100253a17b68cb3ad272498cd158b79f}{%
           family={Ossowski},
           familyi={O\bibinitperiod},
           given={Sascha},
           giveni={S\bibinitperiod}}}%
        {{hash=126c61ec2fb5ce6ac5e3bf5d81a405c9}{%
           family={Patel-Schneider},
           familyi={P\bibinithyphendelim S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{4408b0bc9147789330e0059378241c8f}
      \strng{fullhash}{4408b0bc9147789330e0059378241c8f}
      \strng{bibnamehash}{4408b0bc9147789330e0059378241c8f}
      \strng{authorbibnamehash}{4408b0bc9147789330e0059378241c8f}
      \strng{authornamehash}{4408b0bc9147789330e0059378241c8f}
      \strng{authorfullhash}{4408b0bc9147789330e0059378241c8f}
      \strng{editorbibnamehash}{1b6e04195267c58a21ad7803e856576f}
      \strng{editornamehash}{1b6e04195267c58a21ad7803e856576f}
      \strng{editorfullhash}{6e3175baa3af8695e784af0948525630}
      \field{labelalpha}{Rud11}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This chapter accompanies the foundational lecture on Description Logics (DLs) at the 7th Reasoning Web Summer School in Galway, Ireland, 2011. It introduces basic notions and facts about this family of logics which has significantly gained in importance over the recent years as these logics constitute the formal basis for today’s most expressive ontology languages, the OWL (Web Ontology Language) family.}
      \field{booktitle}{Reasoning {Web}. {Semantic} {Technologies} for the {Web} of {Data}: 7th {International} {Summer} {School} 2011, {Galway}, {Ireland}, {August} 23-27, 2011, {Tutorial} {Lectures}}
      \field{isbn}{978-3-642-23032-5}
      \field{title}{Foundations of {Description} {Logics}}
      \field{urlday}{14}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2011}
      \field{urldateera}{ce}
      \field{pages}{76\bibrangedash 136}
      \range{pages}{61}
      \verb{doi}
      \verb 10.1007/978-3-642-23032-5_2
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-642-23032-5_2
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-642-23032-5_2
      \endverb
    \endentry
    \entry{sander_large-scale_2025}{misc}{}
      \name{author}{9}{}{%
        {{hash=934a98c4d9d2154d39d3caac04c17ebf}{%
           family={Sander},
           familyi={S\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
        {{hash=8c66ced8a43a8802c4d608df4e037412}{%
           family={Fröhlich},
           familyi={F\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=4d2fe2daf40a829cbad79f3cf60017df}{%
           family={Eigel},
           familyi={E\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=878172bfd75f93a96d3646d7b66c3e48}{%
           family={Eisert},
           familyi={E\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod}}}%
        {{hash=aae598c66790875fb3f97e9e696f287f}{%
           family={Gelß},
           familyi={G\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
        {{hash=dd1462c8449f0d93707a2a574ae0fa01}{%
           family={Hintermüller},
           familyi={H\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=dd7853bf2542178e88a9b8779c5553e7}{%
           family={Milbradt},
           familyi={M\bibinitperiod},
           given={Richard\bibnamedelima M.},
           giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=942bfb89eecd82018713153f8561459b}{%
           family={Wille},
           familyi={W\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=a0392e761cae0a163d14dc6b48494423}{%
           family={Mendl},
           familyi={M\bibinitperiod},
           given={Christian\bibnamedelima B.},
           giveni={C\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{f53c1f6c2284fc307d3218c1bec37d8d}
      \strng{fullhash}{f7ed6a16ffddf61cd3e32c78f0f9613c}
      \strng{bibnamehash}{f53c1f6c2284fc307d3218c1bec37d8d}
      \strng{authorbibnamehash}{f53c1f6c2284fc307d3218c1bec37d8d}
      \strng{authornamehash}{f53c1f6c2284fc307d3218c1bec37d8d}
      \strng{authorfullhash}{f7ed6a16ffddf61cd3e32c78f0f9613c}
      \field{labelalpha}{San+25}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Understanding the precise interaction mechanisms between quantum systems and their environment is crucial for advancing stable quantum technologies, designing reliable experimental frameworks, and building accurate models of real-world phenomena. However, simulating open quantum systems, which feature complex non-unitary dynamics, poses significant computational challenges that require innovative methods to overcome. In this work, we introduce the tensor jump method (TJM), a scalable, embarrassingly parallel algorithm for stochastically simulating large-scale open quantum systems, specifically Markovian dynamics captured by Lindbladians. This method is built on three core principles where, in particular, we extend the Monte Carlo wave function (MCWF) method to matrix product states, use a dynamic time-dependent variational principle (TDVP) to significantly reduce errors during time evolution, and introduce what we call a sampling MPS to drastically reduce the dependence on the simulation's time step size. We demonstrate that this method scales more effectively than previous methods and ensures convergence to the Lindbladian solution independent of system size, which we show both rigorously and numerically. Finally, we provide evidence of its utility by simulating Lindbladian dynamics of XXX Heisenberg models up to a thousand spins using a consumer-grade CPU. This work represents a significant step forward in the simulation of large-scale open quantum systems, with the potential to enable discoveries across various domains of quantum physics, particularly those where the environment plays a fundamental role, and to both dequantize and facilitate the development of more stable quantum hardware.}
      \field{month}{1}
      \field{note}{arXiv:2501.17913 [quant-ph]}
      \field{title}{Large-scale stochastic simulation of open quantum systems}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2501.17913
      \endverb
      \verb{file}
      \verb Preprint PDF:/Users/alexgoessmann/Zotero/storage/FZI4UPFY/Sander et al. - 2025 - Large-scale stochastic simulation of open quantum systems.pdf:application/pdf;Snapshot:/Users/alexgoessmann/Zotero/storage/JGQC3S4N/2501.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2501.17913
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2501.17913
      \endverb
      \keyw{Quantum Physics,Condensed Matter - Other Condensed Matter}
    \endentry
    \entry{sarker_neuro-symbolic_2022}{article}{}
      \name{author}{4}{}{%
        {{hash=8058d6e1f797c6dfb26ccc11667983bf}{%
           family={Sarker},
           familyi={S\bibinitperiod},
           given={Md\bibnamedelima Kamruzzaman},
           giveni={M\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=b72846c7bce9dc4527c22588488d735a}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Lu},
           giveni={L\bibinitperiod}}}%
        {{hash=233e223f7d899ce0eecb0211d820a0dc}{%
           family={Eberhart},
           familyi={E\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
        {{hash=6c91a433a7cc7ab507b9bd279fa64cac}{%
           family={Hitzler},
           familyi={H\bibinitperiod},
           given={Pascal},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{b777d932502942548aa3fbaacb50e4f9}
      \strng{fullhash}{5365057c633ea9eb768b225849fcd208}
      \strng{bibnamehash}{b777d932502942548aa3fbaacb50e4f9}
      \strng{authorbibnamehash}{b777d932502942548aa3fbaacb50e4f9}
      \strng{authornamehash}{b777d932502942548aa3fbaacb50e4f9}
      \strng{authorfullhash}{5365057c633ea9eb768b225849fcd208}
      \field{labelalpha}{Sar+22}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Neuro-Symbolic Artificial Intelligence – the combination of symbolic methods with methods that are based on artificial neural networks – has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.}
      \field{issn}{18758452, 09217126}
      \field{journaltitle}{AI Communications}
      \field{month}{3}
      \field{number}{3}
      \field{shorttitle}{Neuro-symbolic artificial intelligence}
      \field{title}{Neuro-symbolic artificial intelligence: {Current} trends}
      \field{urlday}{17}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{34}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{197\bibrangedash 209}
      \range{pages}{13}
      \verb{doi}
      \verb 10.3233/AIC-210084
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/alexgoessmann/Zotero/storage/DT6NNMMP/Sarker et al. - 2022 - Neuro-symbolic artificial intelligence Current trends.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AIC-210084
      \endverb
      \verb{url}
      \verb https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AIC-210084
      \endverb
    \endentry
    \entry{sato_linear_2017}{article}{}
      \name{author}{1}{}{%
        {{hash=1a99e9f5aaf14b0a1e707546a6a0763b}{%
           family={Sato},
           familyi={S\bibinitperiod},
           given={Taisuke},
           giveni={T\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{fullhash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{bibnamehash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{authorbibnamehash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{authornamehash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \strng{authorfullhash}{1a99e9f5aaf14b0a1e707546a6a0763b}
      \field{labelalpha}{Sat17}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a fundamentally new approach to Datalog evaluation. Given a linear Datalog program DB written using N constants and binary predicates, we first translate if-and-only-if completions of clauses in DB into a set E q (DB) of matrix equations with a non-linear operation, where relations in M DB, the least Herbrand model of DB, are encoded as adjacency matrices. We then translate E q (DB) into another, but purely linear matrix equations Ẽ q (DB). It is proved that the least solution of Ẽ q (DB) in the sense of matrix ordering is converted to the least solution of E q (DB) and the latter gives M DB as a set of adjacency matrices. Hence, computing the least solution of Ẽ q (DB) is equivalent to computing M DB specified by DB. For a class of tail recursive programs and for some other types of programs, our approach achieves O(N 3) time complexity irrespective of the number of variables in a clause since only matrix operations costing O(N 3) or less are used. We conducted two experiments that compute the least Herbrand models of linear Datalog programs. The first experiment computes transitive closure of artificial data and real network data taken from the Koblenz Network Collection. The second one compared the proposed approach with the state-of-the-art symbolic systems including two Prolog systems and two ASP systems, in terms of computation time for a transitive closure program and the same generation program. In the experiment, it is observed that our linear algebraic approach runs 101 {\textasciitilde} 104 times faster than the symbolic systems when data is not sparse. Our approach is inspired by the emergence of big knowledge graphs and expected to contribute to the realization of rich and scalable logical inference for knowledge graphs.}
      \field{issn}{1471-0684, 1475-3081}
      \field{journaltitle}{Theory and Practice of Logic Programming}
      \field{month}{5}
      \field{note}{Publisher: Cambridge University Press}
      \field{number}{3}
      \field{title}{A linear algebraic approach to datalog evaluation}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{17}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{244\bibrangedash 265}
      \range{pages}{22}
      \verb{doi}
      \verb 10.1017/S1471068417000023
      \endverb
      \verb{file}
      \verb Eingereichte Version:/Users/alexgoessmann/Zotero/storage/C2AIFVTN/Sato - 2017 - A linear algebraic approach to datalog evaluation.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/linear-algebraic-approach-to-datalog-evaluation/CED3EEB903D9D8A16843CFCA5AC4D577
      \endverb
      \verb{url}
      \verb https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/linear-algebraic-approach-to-datalog-evaluation/CED3EEB903D9D8A16843CFCA5AC4D577
      \endverb
      \keyw{Datalog,least model,matrix,vector space}
    \endentry
    \entry{shalev-schwartz_shai_understanding_2014}{book}{}
      \name{author}{2}{}{%
        {{hash=750e22690c4fd3d72534c6f366e3c709}{%
           family={{Shalev-Schwartz, Shai}},
           familyi={S\bibinitperiod}}}%
        {{hash=481307faa6325b344d16f9db06d8db17}{%
           family={{Ben-David, Shai}},
           familyi={B\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{fullhash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{bibnamehash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{authorbibnamehash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{authornamehash}{c9722f58073756b2c8b41d61e5cff116}
      \strng{authorfullhash}{c9722f58073756b2c8b41d61e5cff116}
      \field{labelalpha}{SB14}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.}
      \field{isbn}{978-1-107-05713-5}
      \field{month}{7}
      \field{shorttitle}{Understanding {Machine} {Learning}}
      \field{title}{Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}}
      \field{year}{2014}
    \endentry
    \entry{serafini_learning_2016}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=cfd7b672fd7926ef1136b9790438416d}{%
           family={Serafini},
           familyi={S\bibinitperiod},
           given={Luciano},
           giveni={L\bibinitperiod}}}%
        {{hash=ad4b7cd82e81faa28e2e876ecd78447d}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod},
           prefix={d’Avila},
           prefixi={d\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer-Verlag}%
      }
      \strng{namehash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{fullhash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{bibnamehash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{authorbibnamehash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{authornamehash}{55b3fa6b8621a130ff936fc4ddd15383}
      \strng{authorfullhash}{55b3fa6b8621a130ff936fc4ddd15383}
      \field{labelalpha}{SG16}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The paper introduces real logic: a framework that seamlessly integrates logical deductive reasoning with efficient, data-driven relational learning. Real logic is based on full first order language. Terms are interpreted in n-dimensional feature vectors, while predicates are interpreted in fuzzy sets. In real logic it is possible to formally define the following two tasks: (i) learning from data in presence of logical constraints, and (ii) reasoning on formulas exploiting concrete data. We implement real logic in an deep learning architecture, called logic tensor networks, based on Google’s primitives. The paper concludes with experiments on a simple but representative example of knowledge completion.}
      \field{booktitle}{{AI}*{IA} 2016 {Advances} in {Artificial} {Intelligence}: {XVth} {International} {Conference} of the {Italian} {Association} for {Artificial} {Intelligence}, {Genova}, {Italy}, {November} 29 – {December} 1, 2016, {Proceedings}}
      \field{isbn}{978-3-319-49129-5}
      \field{month}{11}
      \field{title}{Learning and {Reasoning} with {Logic} {Tensor} {Networks}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{334\bibrangedash 348}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1007/978-3-319-49130-1_25
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-319-49130-1_25
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-319-49130-1_25
      \endverb
    \endentry
    \entry{suess_mpnum_2017}{article}{}
      \name{author}{2}{}{%
        {{hash=dd5a1a6450809c1ede59925d5098e64a}{%
           family={Suess},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=dffbdf4b843c5e9f9670ce5e734a6451}{%
           family={Holzäpfel},
           familyi={H\bibinitperiod},
           given={Milan},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{ea98a481cf153ce91c4da384c12be020}
      \strng{fullhash}{ea98a481cf153ce91c4da384c12be020}
      \strng{bibnamehash}{ea98a481cf153ce91c4da384c12be020}
      \strng{authorbibnamehash}{ea98a481cf153ce91c4da384c12be020}
      \strng{authornamehash}{ea98a481cf153ce91c4da384c12be020}
      \strng{authorfullhash}{ea98a481cf153ce91c4da384c12be020}
      \field{labelalpha}{SH17}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Suess et al., (2017). mpnum: A matrix product representation library for Python. Journal of Open Source Software, 2(20), 465, https://doi.org/10.21105/joss.00465}
      \field{issn}{2475-9066}
      \field{journaltitle}{Journal of Open Source Software}
      \field{month}{12}
      \field{number}{20}
      \field{shorttitle}{mpnum}
      \field{title}{mpnum: {A} matrix product representation library for {Python}}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{volume}{2}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{465}
      \range{pages}{1}
      \verb{doi}
      \verb 10.21105/joss.00465
      \endverb
      \verb{urlraw}
      \verb https://joss.theoj.org/papers/10.21105/joss.00465
      \endverb
      \verb{url}
      \verb https://joss.theoj.org/papers/10.21105/joss.00465
      \endverb
    \endentry
    \entry{shannon_mathematical_1948}{article}{}
      \name{author}{1}{}{%
        {{hash=22e2130b8d96f93daba648ff62af869b}{%
           family={Shannon},
           familyi={S\bibinitperiod},
           given={C.\bibnamedelimi E.},
           giveni={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{22e2130b8d96f93daba648ff62af869b}
      \strng{fullhash}{22e2130b8d96f93daba648ff62af869b}
      \strng{bibnamehash}{22e2130b8d96f93daba648ff62af869b}
      \strng{authorbibnamehash}{22e2130b8d96f93daba648ff62af869b}
      \strng{authornamehash}{22e2130b8d96f93daba648ff62af869b}
      \strng{authorfullhash}{22e2130b8d96f93daba648ff62af869b}
      \field{labelalpha}{Sha48}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1538-7305}
      \field{journaltitle}{Bell System Technical Journal}
      \field{note}{\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1948.tb01338.x}
      \field{number}{3}
      \field{title}{A {Mathematical} {Theory} of {Communication}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{27}
      \field{year}{1948}
      \field{urldateera}{ce}
      \field{pages}{379\bibrangedash 423}
      \range{pages}{45}
      \verb{doi}
      \verb 10.1002/j.1538-7305.1948.tb01338.x
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/6M2N6KGS/j.1538-7305.1948.tb01338.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x
      \endverb
      \verb{url}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x
      \endverb
    \endentry
    \entry{simonis_sudoku_2005}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=a0f56137e8c48bf456998afccf528847}{%
           family={Simonis},
           familyi={S\bibinitperiod},
           given={Helmut},
           giveni={H\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Citeseer Sitges, Spain}%
      }
      \strng{namehash}{a0f56137e8c48bf456998afccf528847}
      \strng{fullhash}{a0f56137e8c48bf456998afccf528847}
      \strng{bibnamehash}{a0f56137e8c48bf456998afccf528847}
      \strng{authorbibnamehash}{a0f56137e8c48bf456998afccf528847}
      \strng{authornamehash}{a0f56137e8c48bf456998afccf528847}
      \strng{authorfullhash}{a0f56137e8c48bf456998afccf528847}
      \field{labelalpha}{Sim05}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{{CP} {Workshop} on modeling and reformulating {Constraint} {Satisfaction} {Problems}}
      \field{title}{Sudoku as a constraint problem}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{volume}{12}
      \field{year}{2005}
      \field{urldateera}{ce}
      \field{pages}{13\bibrangedash 27}
      \range{pages}{15}
      \verb{file}
      \verb Available Version (via Google Scholar):/Users/alexgoessmann/Zotero/storage/ALDJFT8I/Simonis - 2005 - Sudoku as a constraint problem.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://ai.dmi.unibas.ch/_files/teaching/fs21/ai/material/ai26-simonis-cp2005ws.pdf
      \endverb
      \verb{url}
      \verb https://ai.dmi.unibas.ch/_files/teaching/fs21/ai/material/ai26-simonis-cp2005ws.pdf
      \endverb
    \endentry
    \entry{li_linear_2017}{incollection}{}
      \name{author}{3}{}{%
        {{hash=4585c2764c84ad855c78ef492089cd89}{%
           family={Sakama},
           familyi={S\bibinitperiod},
           given={Chiaki},
           giveni={C\bibinitperiod}}}%
        {{hash=1cc5d773bcbcfb4748990f19a2aaf2fe}{%
           family={Inoue},
           familyi={I\bibinitperiod},
           given={Katsumi},
           giveni={K\bibinitperiod}}}%
        {{hash=1a99e9f5aaf14b0a1e707546a6a0763b}{%
           family={Sato},
           familyi={S\bibinitperiod},
           given={Taisuke},
           giveni={T\bibinitperiod}}}%
      }
      \name{editor}{5}{}{%
        {{hash=56ba559f18e74cf681fe113aba3fa9af}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Gang},
           giveni={G\bibinitperiod}}}%
        {{hash=85670d97bd4b38184f6db06c023a19e6}{%
           family={Ge},
           familyi={G\bibinitperiod},
           given={Yong},
           giveni={Y\bibinitperiod}}}%
        {{hash=4b5e80777fd7e5d6b45a520f4dabe35e}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Zili},
           giveni={Z\bibinitperiod}}}%
        {{hash=59ead1ce694d996ebe61da34178792ee}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Zhi},
           giveni={Z\bibinitperiod}}}%
        {{hash=4f9200560583acf9c9918dc90828a228}{%
           family={Blumenstein},
           familyi={B\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{2fdcc809da537861747cf27964e41ec3}
      \strng{fullhash}{2fdcc809da537861747cf27964e41ec3}
      \strng{bibnamehash}{2fdcc809da537861747cf27964e41ec3}
      \strng{authorbibnamehash}{2fdcc809da537861747cf27964e41ec3}
      \strng{authornamehash}{2fdcc809da537861747cf27964e41ec3}
      \strng{authorfullhash}{2fdcc809da537861747cf27964e41ec3}
      \strng{editorbibnamehash}{4bc5fe7e313cd37e7052af92d5b78ecd}
      \strng{editornamehash}{4bc5fe7e313cd37e7052af92d5b78ecd}
      \strng{editorfullhash}{e71855149b8692c242bc88d7e7a460c5}
      \field{labelalpha}{SIS17}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper introduces a novel approach for computing logic programming semantics based on multilinear algebra. First, a propositional Herbrand base is represented in a vector space and if-then rules in a program are encoded in a matrix. Then we provide methods of computing the least model of a Horn logic program, minimal models of a disjunctive logic program, and stable models of a normal logic program by algebraic manipulation of higher-order tensors. The result of this paper exploits a new connection between linear algebraic computation and symbolic computation, which has potential to realize logical inference in huge scale of knowledge bases.}
      \field{booktitle}{Knowledge {Science}, {Engineering} and {Management}}
      \field{isbn}{978-3-319-63557-6 978-3-319-63558-3}
      \field{note}{Series Title: Lecture Notes in Computer Science}
      \field{title}{Linear {Algebraic} {Characterization} of {Logic} {Programs}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{10412}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{520\bibrangedash 533}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/978-3-319-63558-3_44
      \endverb
      \verb{file}
      \verb Sakama et al. - 2017 - Linear Algebraic Characterization of Logic Programs.pdf:/Users/alexgoessmann/Zotero/storage/5FT67GGU/Sakama et al. - 2017 - Linear Algebraic Characterization of Logic Programs.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/978-3-319-63558-3_44
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/978-3-319-63558-3_44
      \endverb
    \endentry
    \entry{de_silva_tensor_2008}{article}{}
      \name{author}{2}{}{%
        {{hash=8914ed3ba6a5221539f9df16b4db08ac}{%
           family={Silva},
           familyi={S\bibinitperiod},
           given={Vin},
           giveni={V\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=12750b991b9d711549fd3537dfdff320}{%
           family={Lim},
           familyi={L\bibinitperiod},
           given={Lek-Heng},
           giveni={L\bibinithyphendelim H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{b045c4cf654638530c16fbd93dc73779}
      \strng{fullhash}{b045c4cf654638530c16fbd93dc73779}
      \strng{bibnamehash}{b045c4cf654638530c16fbd93dc73779}
      \strng{authorbibnamehash}{b045c4cf654638530c16fbd93dc73779}
      \strng{authornamehash}{b045c4cf654638530c16fbd93dc73779}
      \strng{authorfullhash}{b045c4cf654638530c16fbd93dc73779}
      \field{labelalpha}{SL08}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{There has been continued interest in seeking a theorem describing optimal low-rank approximations to tensors of order 3 or higher, that parallels the Eckart–Young theorem for matrices. In this paper, we argue that the naive approach to this problem is doomed to failure because, unlike matrices, tensors of order 3 or higher can fail to have best rank-r approximations. The phenomenon is much more widespread than one might suspect: examples of this failure can be constructed over a wide range of dimensions, orders and ranks, regardless of the choice of norm (or even Br`egman divergence). Moreover, we show that in many instances these counterexamples have positive volume: they cannot be regarded as isolated phenomena. In one extreme case, we exhibit a tensor space in which no rank-3 tensor has an optimal rank-2 approximation. The notable exceptions to this misbehavior are rank-1 tensors and order-2 tensors (i.e. matrices).}
      \field{issn}{0895-4798, 1095-7162}
      \field{journaltitle}{SIAM Journal on Matrix Analysis and Applications}
      \field{month}{1}
      \field{number}{3}
      \field{title}{Tensor {Rank} and the {Ill}-{Posedness} of the {Best} {Low}-{Rank} {Approximation} {Problem}}
      \field{urlday}{17}
      \field{urlmonth}{9}
      \field{urlyear}{2019}
      \field{volume}{30}
      \field{year}{2008}
      \field{urldateera}{ce}
      \field{pages}{1084\bibrangedash 1127}
      \range{pages}{44}
      \verb{doi}
      \verb 10.1137/06066518X
      \endverb
      \verb{file}
      \verb de Silva and Lim - 2008 - Tensor Rank and the Ill-Posedness of the Best Low-.pdf:/Users/alexgoessmann/Zotero/storage/STN8KFRC/de Silva and Lim - 2008 - Tensor Rank and the Ill-Posedness of the Best Low-.pdf:application/pdf
      \endverb
    \endentry
    \entry{stoudenmire_supervised_2016}{incollection}{}
      \name{author}{2}{}{%
        {{hash=80e2277c9051dd68981bb4b55e98d0a8}{%
           family={Stoudenmire},
           familyi={S\bibinitperiod},
           given={Edwin},
           giveni={E\bibinitperiod}}}%
        {{hash=f17b3976b2fddd6a3cb0e96d85a6115b}{%
           family={Schwab},
           familyi={S\bibinitperiod},
           given={David\bibnamedelima J},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \name{editor}{5}{}{%
        {{hash=dc777b608117794a5ff6308cfbba8945}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={D.\bibnamedelimi D.},
           giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=0ef91066dee98e654f9aebd57da00647}{%
           family={Sugiyama},
           familyi={S\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=56e73897155d476124481b099f125669}{%
           family={Luxburg},
           familyi={L\bibinitperiod},
           given={U.\bibnamedelimi V.},
           giveni={U\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=e7e74de725116358b68a6e890c026145}{%
           family={Guyon},
           familyi={G\bibinitperiod},
           given={I.},
           giveni={I\bibinitperiod}}}%
        {{hash=36b98b7ab533936cf1b5716148de704f}{%
           family={Garnett},
           familyi={G\bibinitperiod},
           given={R.},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{314b92ac1c208ed2be766a17d5917007}
      \strng{fullhash}{314b92ac1c208ed2be766a17d5917007}
      \strng{bibnamehash}{314b92ac1c208ed2be766a17d5917007}
      \strng{authorbibnamehash}{314b92ac1c208ed2be766a17d5917007}
      \strng{authornamehash}{314b92ac1c208ed2be766a17d5917007}
      \strng{authorfullhash}{314b92ac1c208ed2be766a17d5917007}
      \strng{editorbibnamehash}{21d9daab5c7d1d638944b0bfa6ad4aef}
      \strng{editornamehash}{21d9daab5c7d1d638944b0bfa6ad4aef}
      \strng{editorfullhash}{070d5597ded4754ff563759b82b5aa4c}
      \field{labelalpha}{SS16}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems} 29}
      \field{title}{Supervised {Learning} with {Tensor} {Networks}}
      \field{urlday}{10}
      \field{urlmonth}{1}
      \field{urlyear}{2020}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{4799\bibrangedash 4807}
      \range{pages}{9}
      \verb{file}
      \verb NIPS Full Text PDF:/Users/alexgoessmann/Zotero/storage/4K5UAMAB/Stoudenmire und Schwab - 2016 - Supervised Learning with Tensor Networks.pdf:application/pdf;NIPS Full Text PDF:/Users/alexgoessmann/Zotero/storage/AET3HRDF/Stoudenmire und Schwab - 2016 - Supervised Learning with Tensor Networks.pdf:application/pdf;NIPS Snapshot:/Users/alexgoessmann/Zotero/storage/5CAEQ3Q8/6211-supervised-learning-with-tensor-networks.html:text/html;NIPS Snapshot:/Users/alexgoessmann/Zotero/storage/DM7LW8Q8/6211-supervised-learning-with-tensor-networks.html:text/html
      \endverb
    \endentry
    \entry{talagrand_upper_2014}{book}{}
      \name{author}{1}{}{%
        {{hash=c0aa8b0f36f087702a87b6002ab35850}{%
           family={Talagrand},
           familyi={T\bibinitperiod},
           given={Michel},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{fullhash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{bibnamehash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{authorbibnamehash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{authornamehash}{c0aa8b0f36f087702a87b6002ab35850}
      \strng{authorfullhash}{c0aa8b0f36f087702a87b6002ab35850}
      \field{labelalpha}{Tal14}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The book develops modern methods and in particular the "generic chaining" to bound stochastic processes. This methods allows in particular to get optimal bounds for Gaussian and Bernoulli processes. Applications are given to stable processes, infinitely divisible processes, matching theorems, the convergence of random Fourier series, of orthogonal series, and to functional analysis. The complete solution of a number of classical problems is given in complete detail, and an ambitious program for future research is laid out.}
      \field{isbn}{978-3-642-54074-5}
      \field{shorttitle}{Upper and {Lower} {Bounds} for {Stochastic} {Processes}}
      \field{title}{Upper and {Lower} {Bounds} for {Stochastic} {Processes}: {Modern} {Methods} and {Classical} {Problems}}
      \field{urlday}{5}
      \field{urlmonth}{5}
      \field{urlyear}{2020}
      \field{year}{2014}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-3-642-54075-2
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/YHG4TIAE/9783642540745.html:text/html
      \endverb
    \endentry
    \entry{tsilionis_tensor-based_2024}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=1f04abcb36ae84b02e7b05a4ba571aad}{%
           family={Tsilionis},
           familyi={T\bibinitperiod},
           given={Efthimis},
           giveni={E\bibinitperiod}}}%
        {{hash=7ead92ae0e88543d514f2cc4a7600590}{%
           family={Artikis},
           familyi={A\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=b4097f9e90befdf404349dd04af200e9}{%
           family={Paliouras},
           familyi={P\bibinitperiod},
           given={Georgios},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Jeju, South Korea}%
      }
      \list{publisher}{1}{%
        {International Joint Conferences on Artificial Intelligence Organization}%
      }
      \strng{namehash}{84978962e71cc42f91d64c35d1531605}
      \strng{fullhash}{84978962e71cc42f91d64c35d1531605}
      \strng{bibnamehash}{84978962e71cc42f91d64c35d1531605}
      \strng{authorbibnamehash}{84978962e71cc42f91d64c35d1531605}
      \strng{authornamehash}{84978962e71cc42f91d64c35d1531605}
      \strng{authorfullhash}{84978962e71cc42f91d64c35d1531605}
      \field{labelalpha}{TAP24}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a formalization of the Event Calculus (EC) in tensor spaces. The motivation for a tensorbased predicate calculus comes from the area of composite event recognition (CER). As a CER engine, we adopt a logic programming implementation of EC with optimizations for continuous narrative assimilation on data streams. We show how to evaluate EC rules algebraically and solve a linear equation to compute the corresponding models. We demonstrate the scalability of our approach with the use of large datasets from a real-world application domain, and show it outperforms significantly symbolic EC, in terms of processing time.}
      \field{booktitle}{Proceedings of the {Thirty}-{ThirdInternational} {Joint} {Conference} on {Artificial} {Intelligence}}
      \field{isbn}{978-1-956792-04-1}
      \field{month}{8}
      \field{title}{A {Tensor}-{Based} {Formalization} of the {Event} {Calculus}}
      \field{urlday}{24}
      \field{urlmonth}{9}
      \field{urlyear}{2024}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{3584\bibrangedash 3592}
      \range{pages}{9}
      \verb{doi}
      \verb 10.24963/ijcai.2024/397
      \endverb
      \verb{file}
      \verb PDF:/Users/alexgoessmann/Zotero/storage/3J9ZWKGW/Tsilionis et al. - 2024 - A Tensor-Based Formalization of the Event Calculus.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.ijcai.org/proceedings/2024/397
      \endverb
      \verb{url}
      \verb https://www.ijcai.org/proceedings/2024/397
      \endverb
    \endentry
    \entry{trouillon_complex_2017}{misc}{}
      \name{author}{2}{}{%
        {{hash=42de07a57ebce8948963bfa46f960d3e}{%
           family={Trouillon},
           familyi={T\bibinitperiod},
           given={Théo},
           giveni={T\bibinitperiod}}}%
        {{hash=9754396c9b0fc2f09bd396146fbf2ef3}{%
           family={Nickel},
           familyi={N\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{fullhash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{bibnamehash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{authorbibnamehash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{authornamehash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \strng{authorfullhash}{9285a5601bca345d14ce0b93fc5b5b4c}
      \field{labelalpha}{TN17}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Embeddings of knowledge graphs have received significant attention due to their excellent performance for tasks like link prediction and entity resolution. In this short paper, we are providing a comparison of two state-of-the-art knowledge graph embeddings for which their equivalence has recently been established, i.e., ComplEx and HolE [Nickel, Rosasco, and Poggio, 2016; Trouillon et al., 2016; Hayashi and Shimbo, 2017]. First, we briefly review both models and discuss how their scoring functions are equivalent. We then analyze the discrepancy of results reported in the original articles, and show experimentally that they are likely due to the use of different loss functions. In further experiments, we evaluate the ability of both models to embed symmetric and antisymmetric patterns. Finally, we discuss advantages and disadvantages of both models and under which conditions one would be preferable to the other.}
      \field{month}{7}
      \field{note}{arXiv:1707.01475 [cs, stat]}
      \field{shorttitle}{Complex and {Holographic} {Embeddings} of {Knowledge} {Graphs}}
      \field{title}{Complex and {Holographic} {Embeddings} of {Knowledge} {Graphs}: {A} {Comparison}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb Trouillon und Nickel - 2017 - Complex and Holographic Embeddings of Knowledge Graphs A Comparison.pdf:/Users/alexgoessmann/Zotero/storage/RR52API2/Trouillon und Nickel - 2017 - Complex and Holographic Embeddings of Knowledge Graphs A Comparison.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1707.01475
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1707.01475
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{towell_knowledge-based_1994}{article}{}
      \name{author}{2}{}{%
        {{hash=ba0f27f99c14c01e07ffa93313bc7e3c}{%
           family={Towell},
           familyi={T\bibinitperiod},
           given={Geoffrey\bibnamedelima G.},
           giveni={G\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=4df87652cd39798cafcfca148693289f}{%
           family={Shavlik},
           familyi={S\bibinitperiod},
           given={Jude\bibnamedelima W.},
           giveni={J\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \strng{namehash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{fullhash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{bibnamehash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{authorbibnamehash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{authornamehash}{be79b3a13ca69e2435d1a677ef475a69}
      \strng{authorfullhash}{be79b3a13ca69e2435d1a677ef475a69}
      \field{labelalpha}{TS94}
      \field{sortinit}{T}
      \field{sortinithash}{6f7aff9db9dcfeb7f95fd5bbd2f78df9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for accurately classifying examples not seen during training. The challenge of hybrid learning systems is to use the information provided by one source of information to offset information missing from the other source. By so doing, a hybrid learning system should learn more effectively than systems that use only one of the information sources. KBANN (Knowledge-Based Artificial Neural Networks) is a hybrid learning system built on top of connectionist learning techniques. It maps problem-specific “domain theories”, represented in propositional logic, into neural networks and then refines this reformulated knowledge using backpropagation. KBANN is evaluated by extensive empirical tests on two problems from molecular biology. Among other results, these tests show that the networks created by KBANN generalize better than a wide variety of learning systems, as well as several techniques proposed by biologists.}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{10}
      \field{number}{1}
      \field{title}{Knowledge-based artificial neural networks}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{70}
      \field{year}{1994}
      \field{urldateera}{ce}
      \field{pages}{119\bibrangedash 165}
      \range{pages}{47}
      \verb{doi}
      \verb 10.1016/0004-3702(94)90105-8
      \endverb
      \verb{file}
      \verb ScienceDirect Snapshot:/Users/alexgoessmann/Zotero/storage/LLDD5VDQ/0004370294901058.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/0004370294901058
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/0004370294901058
      \endverb
      \keyw{Machine learning,Computational biology,Connectionism,Explanation-based learning,Hybrid algorithms,Theory refinement}
    \endentry
    \entry{vershynin_high-dimensional_2018}{book}{}
      \name{author}{1}{}{%
        {{hash=711c5aa2fe76a4ff674a2c6e2c7a2d91}{%
           family={Vershynin},
           familyi={V\bibinitperiod},
           given={Roman},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {New York, NY}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{fullhash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{bibnamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authorbibnamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authornamehash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \strng{authorfullhash}{711c5aa2fe76a4ff674a2c6e2c7a2d91}
      \field{labelalpha}{Ver18}
      \field{sortinit}{V}
      \field{sortinithash}{75dd7385c90b2252c3ae853a80ca853b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.}
      \field{edition}{1st edition}
      \field{isbn}{978-1-108-41519-4}
      \field{month}{9}
      \field{shorttitle}{High-{Dimensional} {Probability}}
      \field{title}{High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}}
      \field{year}{2018}
    \endentry
    \entry{wainwright_high-dimensional_2019}{book}{}
      \name{author}{1}{}{%
        {{hash=7eb03239453694f59946ad6008e9f0ab}{%
           family={Wainwright},
           familyi={W\bibinitperiod},
           given={Martin\bibnamedelima J.},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{fullhash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{bibnamehash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{authorbibnamehash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{authornamehash}{7eb03239453694f59946ad6008e9f0ab}
      \strng{authorfullhash}{7eb03239453694f59946ad6008e9f0ab}
      \field{labelalpha}{Wai19}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.}
      \field{isbn}{978-1-108-49802-9}
      \field{series}{Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}}
      \field{shorttitle}{High-{Dimensional} {Statistics}}
      \field{title}{High-{Dimensional} {Statistics}: {A} {Non}-{Asymptotic} {Viewpoint}}
      \field{urlday}{23}
      \field{urlmonth}{11}
      \field{urlyear}{2020}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1017/9781108627771
      \endverb
      \verb{file}
      \verb Snapshot:/Users/alexgoessmann/Zotero/storage/65TBDVYU/8A91ECEEC38F46DAB53E9FF8757C7A4E.html:text/html
      \endverb
    \endentry
    \entry{wainwright_graphical_2008}{book}{}
      \name{author}{2}{}{%
        {{hash=7eb03239453694f59946ad6008e9f0ab}{%
           family={Wainwright},
           familyi={W\bibinitperiod},
           given={Martin\bibnamedelima J.},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=ce9a11a5b3beb20bfe74f41a05d92245}{%
           family={Jordan},
           familyi={J\bibinitperiod},
           given={Michael\bibnamedelima Irwin},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Now Publishers Inc}%
      }
      \strng{namehash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{fullhash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{bibnamehash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{authorbibnamehash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{authornamehash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \strng{authorfullhash}{f4ac6ed11fc67ede679abcea6822dbbb}
      \field{labelalpha}{WJ08}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances-including the key problems of computing marginals and modes of probability distributions-are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, Graphical Models, Exponential Families and Variational Inference develops general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. It describes how a wide variety of algorithms- among them sum-product, cluster variational methods, expectation-propagation, mean field methods, and max-product-can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.}
      \field{isbn}{978-1-60198-184-4}
      \field{title}{Graphical {Models}, {Exponential} {Families}, and {Variational} {Inference}}
      \field{year}{2008}
    \endentry
    \entry{wolfram_new_2002}{book}{}
      \name{author}{1}{}{%
        {{hash=fc7f119c141fbcc7810a4b3a86f599db}{%
           family={Wolfram},
           familyi={W\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Englisch}%
      }
      \list{location}{1}{%
        {Champaign (Ill.)}%
      }
      \list{publisher}{1}{%
        {Wolfram Media}%
      }
      \strng{namehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{fullhash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{bibnamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authorbibnamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authornamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authorfullhash}{fc7f119c141fbcc7810a4b3a86f599db}
      \field{extraname}{1}
      \field{labelalpha}{Wol02}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book promises to revolutionize science as we know it' - Daily Telegraph 'Stephen's magnum opus may be the book of the decade if not the century' - Arthur C Clarke Long-awaited work from one of the world's most respected scientists presents a series of dramatic discoveries never before made public. Starting with a collection of computer experiments, Wolfram shows how their unexpected results force a whole new way of looking at the universe. A seminal work of enormous importance. Includes over 950 illustrations. BBC documentary in development.'}
      \field{edition}{Illustrated Edition}
      \field{isbn}{978-1-57955-008-0}
      \field{month}{5}
      \field{title}{A {New} {Kind} of {Science}}
      \field{year}{2002}
    \endentry
    \entry{wolf_libxerusxerus_2024}{misc}{}
      \name{author}{1}{}{%
        {{hash=9c29d0496ec2ac633524176392651a27}{%
           family={Wolf},
           familyi={W\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {libxerus}%
      }
      \strng{namehash}{9c29d0496ec2ac633524176392651a27}
      \strng{fullhash}{9c29d0496ec2ac633524176392651a27}
      \strng{bibnamehash}{9c29d0496ec2ac633524176392651a27}
      \strng{authorbibnamehash}{9c29d0496ec2ac633524176392651a27}
      \strng{authornamehash}{9c29d0496ec2ac633524176392651a27}
      \strng{authorfullhash}{9c29d0496ec2ac633524176392651a27}
      \field{labelalpha}{Wol24}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A general purpose library for numerical calculations with higher order tensors, Tensor-Train Decompositions / Matrix Product States and other Tensor Networks}
      \field{month}{2}
      \field{note}{original-date: 2016-03-10T15:27:47Z}
      \field{title}{libxerus/xerus}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://github.com/libxerus/xerus
      \endverb
      \verb{url}
      \verb https://github.com/libxerus/xerus
      \endverb
    \endentry
    \entry{wolfram_statistical_1983}{article}{}
      \name{author}{1}{}{%
        {{hash=fc7f119c141fbcc7810a4b3a86f599db}{%
           family={Wolfram},
           familyi={W\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{fullhash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{bibnamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authorbibnamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authornamehash}{fc7f119c141fbcc7810a4b3a86f599db}
      \strng{authorfullhash}{fc7f119c141fbcc7810a4b3a86f599db}
      \field{extraname}{2}
      \field{labelalpha}{Wol83}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Cellular automata are used as simple mathematical models to investigate self-organization in statistical mechanics. A detailed analysis is given of "elementary" cellular automata consisting of a sequence of sites with values 0 or 1 on a line, with each site evolving deterministically in discrete time steps according to definite rules involving the values of its nearest neighbors. With simple initial configurations, the cellular automata either tend to homogeneous states, or generate self-similar patterns with fractal dimensions ≃ 1.59 or ≃ 1.69. With "random" initial configurations, the irreversible character of the cellular automaton evolution leads to several self-organization phenomena. Statistical properties of the structures generated are found to lie in two universality classes, independent of the details of the initial state or the cellular automaton rules. More complicated cellular automata are briefly considered, and connections with dynamical systems theory and the formal theory of computation are discussed.}
      \field{journaltitle}{Reviews of Modern Physics}
      \field{month}{7}
      \field{note}{Publisher: American Physical Society}
      \field{number}{3}
      \field{title}{Statistical mechanics of cellular automata}
      \field{urlday}{11}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{volume}{55}
      \field{year}{1983}
      \field{urldateera}{ce}
      \field{pages}{601\bibrangedash 644}
      \range{pages}{44}
      \verb{doi}
      \verb 10.1103/RevModPhys.55.601
      \endverb
      \verb{file}
      \verb APS Snapshot:/Users/alexgoessmann/Zotero/storage/9IELTEPH/RevModPhys.55.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://link.aps.org/doi/10.1103/RevModPhys.55.601
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/RevModPhys.55.601
      \endverb
    \endentry
    \entry{yang_embedding_2015}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=7b089ac99dfbed4f2eb0e3b33d72ddad}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Bishan},
           giveni={B\bibinitperiod}}}%
        {{hash=d4d56d3d370b3afd4ad2ebf985fbd3f8}{%
           family={Yih},
           familyi={Y\bibinitperiod},
           given={Wen-tau},
           giveni={W\bibinithyphendelim t\bibinitperiod}}}%
        {{hash=a2d92e419c3b04b620f2b0302581e68c}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Xiaodong},
           giveni={X\bibinitperiod}}}%
        {{hash=159bca5607e3797666654a2e3022978a}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Jianfeng},
           giveni={J\bibinitperiod}}}%
        {{hash=2f5fbdc5c3cf91f62a64663cd72397b3}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{8d6fbb7a7abaf78537651ae42485af2f}
      \strng{fullhash}{0804da74c4e7ba8755552124a76406cf}
      \strng{bibnamehash}{8d6fbb7a7abaf78537651ae42485af2f}
      \strng{authorbibnamehash}{8d6fbb7a7abaf78537651ae42485af2f}
      \strng{authornamehash}{8d6fbb7a7abaf78537651ae42485af2f}
      \strng{authorfullhash}{0804da74c4e7ba8755552124a76406cf}
      \field{labelalpha}{Yan+15}
      \field{sortinit}{Y}
      \field{sortinithash}{b8d711a035f7be9840c721c82920477e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a uniﬁed learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq {\textasciicircum} CityInCountrypb, cq ùñ N ationalitypa, cq. We ﬁnd that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-ofthe-art conﬁdence-based rule mining approach in mining Horn rules that involve compositional reasoning.}
      \field{month}{8}
      \field{note}{arXiv:1412.6575 [cs]}
      \field{title}{Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}}
      \field{urlday}{6}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2015}
      \field{urldateera}{ce}
      \verb{file}
      \verb Yang et al. - 2015 - Embedding Entities and Relations for Learning and Inference in Knowledge Bases.pdf:/Users/alexgoessmann/Zotero/storage/WWTTIZ78/Yang et al. - 2015 - Embedding Entities and Relations for Learning and Inference in Knowledge Bases.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1412.6575
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1412.6575
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{ziegler_lectures_2000}{incollection}{}
      \name{author}{1}{}{%
        {{hash=f78d48d2643d689692036d03a7958e8c}{%
           family={Ziegler},
           familyi={Z\bibinitperiod},
           given={Günter\bibnamedelima M.},
           giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=128f517dac6e5636f212c1bf83a7cd9a}{%
           family={Kalai},
           familyi={K\bibinitperiod},
           given={Gil},
           giveni={G\bibinitperiod}}}%
        {{hash=f78d48d2643d689692036d03a7958e8c}{%
           family={Ziegler},
           familyi={Z\bibinitperiod},
           given={Günter\bibnamedelima M.},
           giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Basel}%
      }
      \list{publisher}{1}{%
        {Birkhäuser}%
      }
      \strng{namehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{fullhash}{f78d48d2643d689692036d03a7958e8c}
      \strng{bibnamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authorbibnamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authornamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authorfullhash}{f78d48d2643d689692036d03a7958e8c}
      \strng{editorbibnamehash}{49c5058dd8625f53ab651acd3dd4c2a7}
      \strng{editornamehash}{49c5058dd8625f53ab651acd3dd4c2a7}
      \strng{editorfullhash}{49c5058dd8625f53ab651acd3dd4c2a7}
      \field{extraname}{1}
      \field{labelalpha}{Zie00}
      \field{sortinit}{Z}
      \field{sortinithash}{156173bd08b075d7295bc3e0f4735a04}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{These lectures on the combinatorics and geometry of 0/1-polytopes are meant as anintroductionandinvitation.Rather than heading for an extensive survey on 0/1-polytopes I present some interesting aspects of these objects; all of them are related to some quite recent work and progress.}
      \field{booktitle}{Polytopes — {Combinatorics} and {Computation}}
      \field{isbn}{978-3-0348-8438-9}
      \field{title}{Lectures on 0/1-{Polytopes}}
      \field{urlday}{4}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2000}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 41}
      \range{pages}{41}
      \verb{doi}
      \verb 10.1007/978-3-0348-8438-9_1
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-0348-8438-9_1
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-0348-8438-9_1
      \endverb
    \endentry
    \entry{ziegler_lectures_2013}{book}{}
      \name{author}{1}{}{%
        {{hash=f78d48d2643d689692036d03a7958e8c}{%
           family={Ziegler},
           familyi={Z\bibinitperiod},
           given={Günter\bibnamedelima M.},
           giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {English}%
      }
      \list{location}{1}{%
        {New York}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{fullhash}{f78d48d2643d689692036d03a7958e8c}
      \strng{bibnamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authorbibnamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authornamehash}{f78d48d2643d689692036d03a7958e8c}
      \strng{authorfullhash}{f78d48d2643d689692036d03a7958e8c}
      \field{extraname}{2}
      \field{labelalpha}{Zie13}
      \field{sortinit}{Z}
      \field{sortinithash}{156173bd08b075d7295bc3e0f4735a04}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Based on a graduate course given at the Technische Universität, Berlin, these lectures present a wealth of material on the modern theory of convex polytopes. The clear and straightforward exposition features many illustrations, and provides complete proofs for most theorems. The material requires only linear algebra as a prerequisite, but takes the reader quickly from the basics to topics of recent research, including a number of unanswered questions. The lectures introduce the basic facts about polytopes, with an emphasis on the methods that yield the results, discuss important examples and elegant constructions, and show the excitement of current work in the field. They will provide interesting and enjoyable reading for researchers as well as students.}
      \field{edition}{1995th edition}
      \field{isbn}{978-0-387-94365-7}
      \field{month}{10}
      \field{title}{Lectures on {Polytopes}}
      \field{year}{2013}
    \endentry
  \enddatalist
\endrefsection
\endinput

