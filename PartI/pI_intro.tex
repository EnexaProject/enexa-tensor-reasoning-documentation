\chapter{Introduction into \parref{par:one}}

Within the introduced factored representation of systems we will in \parref{par:one} present the probabilistic and logical approaches to artificial intelligence.
%Both paradigms have developed human interpretable tools into the representation of and reasoning on knowledge.
Both the probabilitic and the logic paradigm provide a human-understandable interface to machine learning.
\begin{itemize}
    \item \textbf{Probabilistic paradigm:} Models encode the uncertainty of states.
        They describe dependencies between variables, which receive a graphical representation.
    \item \textbf{Logical paradigm:} Models encode the sets of possible states.
        They are formulated in human interpretable logical syntax.
\end{itemize}
As we will describe in \parref{par:two}, they can be combined in one formalism providing efficient reasoning.
We will utilize that tensor network decompositions are in both useful tools of efficient calculus.


\sect{Representation of Factored Systems}

%\subsect{Comparing with probabilistic approaches }
\textbf{Probability} represents the uncertainty of states.
The categorical variables are called random variables and their joint distribution is represented by a probability tensor.
Humans interpret probabilities by Bayesian and frequentist approaches.
Reasoning based on Bayes Theorem has an intuitive interpretation in terms of evidence based update of prior distributions to posterior distributions.
However it is based on interpreting (large amounts) of numbers, which makes it hard for humans to assess the probabilistic reasoning process.

\textbf{Propositional Logics} explains relations between sets of worlds in a human understandable way.
Categorical variables have dimension $2$, where the first is interpreted as indicating a $\falsesymbol$ state and the second as a $\truesymbol$ state.
We mainly restrict to propositional logics, where there are finite sets of such variables called atomic formulas.
Using model-theoretic semantics it defines entailment of sets by other sets, which is understandable as a consequence relation.

\textbf{Tensors} unify both approaches since they are natural numerical structures to represent properties of states in factored systems.
The potential is then based in employing scalable multilinear algorithms to solve reasoning problems.
Further, algorithms formulated in tensor networks have a high parallelization potential, which is why they are of central interest in the development of AI-dedicated software and hardware.

The different areas have developed separated languages to describe similar objects.
Here we want to provide a rough comparison of those in a dictionary:

\begin{tabular}{|p{\fourcolumnwidth}|p{\fourcolumnwidth}|p{\fourcolumnwidth}|p{\fourcolumnwidth}|} % KEEP?
    \hline
    & \textbf{Probability} & \textbf{Logic} & \textbf{Tensors}   \\
    \hline
    \textit{Atomic Representation}        & Random Variable             & Atomic Formula               & Vector             \\
    \textit{Factored Representation}      & Joint Distribution          & Knowledge Base               & Tensor             \\
    %\textit{Categorical Variable} & Random Variable             & Atomic Formula               & Axis of the Tensor \\
    \hline
\end{tabular}

While probability theory lacks to provide an intuition about sets of events, propositional syntax has limited functionality to represent uncertainties.
Tensors on the other side can build a bridge by representing both functionalities and relying on probability theory and logics for respective interpretations.

\sect{Mechanisms of tensor network decompositions}

We investigate two mechanisms to identify tensor network decompositions of probability distributions:
\begin{itemize}
    \item \textbf{\IndependenceMechanism{}:} Conditional independence of random variables is a a concept of probability theory.
       The most prominent application of this approach is the motivation of graphical models, which we introduce in \charef{cha:probRepresentation} as tensor networks.
    \item \textbf{\ComputationMechanism{}:} When there are sufficient statistics providing probabilities, we construct tensor networks decompositions by computation of the statistics.
        Whenever the functions to be computed are compositions of functions of lower numbers of arguments, we utilize these representations to construct tensor network decompositions.
        Such decomposition schemes are provided by logical syntax as we will exploit in \charef{cha:logicalRepresentation}.
        In probability theory, we will make use of this approach in the efficient representation of sufficient statistics.
\end{itemize}
