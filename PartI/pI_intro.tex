\chapter{Introduction into Part I}

Within the introduced factored representation of systems we will in \parref{par:one} present the probabilistic and logical approaches to artificial intelligence.
%Both paradigms have developed human interpretable tools into the representation of and reasoning on knowledge.
Both the probabilitic and the logic paradigm provide a human-understandable interface to machine learning.
\begin{itemize}
    \item \textbf{Probability:} Models describe dependencies between variables, which receive a graphical representation.
    \item \textbf{Logics:} Models are formulated in human interpretable logical syntax.
\end{itemize}
As we will describe in \parref{par:two}, they can be combined in one formalism providing efficient reasoning.
We will utilize that tensor network decompositions are in both useful tools of efficient calculus.

%\subsect{Comparing with probabilistic approaches }
\textbf{Probability} represents the uncertainty of states.
The categorical variables are called random variables and their joint distribution is represented by a probability tensor.
Humans interpret probabilities by Bayesian and frequentist approaches.
Reasoning based on Bayes Theorem has an intuitive interpretation in terms of evidence based update of prior distributions to posterior distributions.
However it is based on interpreting (large amounts) of numbers, which makes it hard for humans to assess the probabilistic reasoning process.

\textbf{Logics} explains relations between sets of worlds in a human understandable way.
Categorical variables have dimension $2$, where the first is interpreted as indicating a $\falsesymbol$ state and the second as a $\truesymbol$ state.
We mainly restrict to propositional logics, where there are finite sets of such variables called atomic formulas.
Using model-theoretic semantics it defines entailment of sets by other sets, which is understandable as a consequence relation.

\textbf{Tensors} unify both approaches since they are natural numerical structures to represent properties of states in factored systems.
The potential is then based in employing scalable multilinear algorithms to solve reasoning problems.
Further, algorithms formulated in tensor networks have a high parallelization potential, which is why they are of central interest in the development of AI-dedicated software and hardware.

The different areas have developed separated languages to describe similar objects.
Here we want to provide a rough comparison of those in a dictionary.

\begin{tabular}{l|l|l|l}
    & \textbf{Probability Theory} & \textbf{Propositional Logic} & \textbf{Tensors}   \\
    \hline
    \textit{Atomic System}        & Random Variable             & Atomic Formula               & Vector             \\
    \textit{Factored System}      & Joint Distribution          & Knowledge Base               & Tensor             \\
    \textit{Categorical Variable} & Random Variable             & Atomic Formula               & Axis of the Tensor
\end{tabular}

While the probability theory lacks to provide an intuition about sets of events, propositional syntax has limited functionality to represent uncertainties.
Tensors on the other side can build a bridge by representing both functionalities and relying on probability theory and logics for respective interpretations.

