\section{Notation}\label{cha:TensorNetworks}

We here provide the fundamental definitions of tensors, which are essentiell for the content in Part~I and Part~II.
In Part~III we will further investigate the properties of tensors focusing on their contractions.

\subsection{Categorical Variables and Representations}

We will in this work investigate systems, which are described by a set of properties, each called a categorical variable. 
This is called an ontological commitment, since it defines what properties a system has.

\begin{definition}
	An atomic representation of a system is described by a categorical variables $\catvariable$ taking values $\catindex$ in a finite set 
		\[  [\catdim]\coloneqq \{0,\ldots, \catdim-1\} \]
	of cardinality $\catdim$.
\end{definition}

% Notation: Large and small literals
We notate variables by large literals and indices by small literals.


%The state of an atomic system is determined by the value of the associated categorical variable.

\begin{definition}
	A factored representation of a system is a set of categorical variables $\catvariableof{\atomenumerator}$, where $\atomenumeratorin$, taking values in $[\catdimof{\atomenumerator}]$.
\end{definition}

\subsection{Tensors}

\red{
We define Tensors here in an non-canonical way based on categorical variables assigned to its axis.
This allows us to define contractions without further specification of axes, based on comparisons of shared categorical variables.}

%% Index Set Abbreviation
In the following we will denote index sets as
	\[ [\catdimof{\atomenumerator}] \coloneqq \{0,\ldots,\catdimof{\atomenumerator-1} \} \, . \]

Tensors are multiway arrays and a generalization of vectors and matrices to higher orders.
We will first define Tensors by their coordinates.

\begin{definition}[Tensor]\label{def:tensor}
	Let there be numbers $\catdimof{\atomenumerator}\in\mathbb{N}$ for $\atomenumeratorin$ and categorical variables $\catvariableof{\atomenumerator}$ taking their values in $[\catdimof{\atomenumerator}]$.
	We call maps
	\begin{align*}
		\hypercoreat{\catvariables} : \bigtimes_{\atomenumeratorin} [\catdimof{\atomenumerator}] \rightarrow \rr
	\end{align*}
	tensor of order $\atomorder$ and leg dimensions $\catdimof{0},\ldots,\catdimof{\atomorder-1}$.
	Evaluations of these maps at indices $\catindices$ are denoted by
	\begin{align*}
		\hypercoreat{\indexedcatvariables} = \hypercoreat{\catvariables}(\catindices) \, .
	\end{align*}	
%	with coordinates denoted by $\hypercore_{\catindices}$ is called a tensor of order $\atomorder$ and legs with the dimensions $\catdimof{0},\ldots,\catdimof{\atomorder-1}$.
	Tensors $\hypercoreat{\catvariables}$ are elements of the space
	\begin{align*}
		\bigotimes_{\atomenumeratorin} \rr^{\catdimof{\atomenumerator}} \,  
	\end{align*}
	which is, with the operations of coordinatewise summation and scalar multiplication, a linear space called a tensor space.
\end{definition} 

We abbreviate lists $\catvariables$ of categorical variables by $\shortcatvariables$, that is denote $\hypercoreat{\catvariables}$ by $\hypercoreat{\shortcatvariables}$.
Occasionally, when the categorical variables of a tensor are clear from the context, we will omit the notation of the variables. %further abbreviate $\hypercoreat{\catvariables}$ by $\hypercore$.


\begin{example}[Trivial Tensor]\label{exa:trivialTensor}
	The trivial tensor is defined as the map 
		\[ \onesat{\shortcatvariables} : \facstates \rightarrow \{1\} \subset \rr \]
	with all coordinates being $1$, that is for all $\catindices\in\facstates$
		\[ \onesat{\indexedcatvariables} = 1 \, . \]
\end{example}


\subsection{One-hot encodings}

We are now ready to provide the link between tensors and states of systems with factored representations.
To this end, we define the one-hot encoding of a state, which is a bijection between the states and the basis elements of a tensor space.

\begin{definition}[One-hot representations of Atomic Systems]
	Given an atomic system described by the categorical variable $\catvariable$, we define for each $\catindex\in[\catdim]$ the basis vector $\onehotmapofat{\catindex}{\catvariable}$ by
	\begin{align}
		\onehotmapofat{\catindex}{\catvariable=\tilde{\catindex}} = \begin{cases}
			1 & \text{if} \quad \catindex=\tilde{\catindex} \\
			0 & \text{else} \, .
		\end{cases} 
	\end{align}
	The one-hot encoding of states $\catindex\in[\catdim]$ of the atomic system described by the categorical variable $\catvariable$ is the map
		\[ \onehotmap: [\catdim] \rightarrow \rr^\catdim \]
	which maps $\catindex \in [\catdim]$ to the basis vectors $\onehotmapofat{\catindex}{\catvariable}$.
\end{definition}

% Coordinatewise representation
The basis vectors $\onehotmapofat{\catindex}{\catvariable}$ are tensors of order $1$ and leg dimension $\catdim$ of the structure
\begin{align}
	\onehotmapofat{\catindex}{\catvariable} = \begin{bmatrix}
	0 & \cdots & 0 & 1 &  0 & \cdots & 0
	\end{bmatrix} \, ,
\end{align}
where the $1$ is at the $\catindex$th coordinate of the vector.

% Atomic -> Factored system
We have so far described one-hot representations of the states of a single categorical variable, which would suffice to encode the state of an atomic system.
In a factored system on the other side, we are dealing with multiple categorical variables.

\begin{definition}[One-hot representations of Factored Systems]
	Let there be a factored system defined by a tuple $(\catvariables)$ of variables taking values in $\facstates$.
	The one-hot encoding of its states is the tensor product of the one-hot encoding to each categorical variables, that is the map
		\[ \onehotmap : \facstates \rightarrow  \facspace \]
	defined by mapping $\catindices$ to
		\[ % \onehotmap(\catindices) =
		 \onehotmapofat{\catindices}{\catvariables}
		=: \bigotimes_{\atomenumeratorin} \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}} \, . \]
	We will call one-hot representations \emph{tensor representations} and depict them as
	\begin{center}
		\input{PartI/tikz_pics/overview/one_hot_tensorproduct.tex}
	\end{center}
\end{definition}


\begin{remark}[Flattening of Tensors]
	The use the tensor product to represent states of factored systems can be motivated by the reduction to atomic systems by enumeration of the states.
	We have this property reflected in the state encoding of factored systems, since the tensor space $\bigotimes_{\atomenumeratorin}\rr^{\catdimof{\atomenumerator}}$ is isomorphic to the vector spaces $\rr^{\prod_{\atomenumeratorin}\catdimof{\atomenumerator}}$.
	This operation is called flattening (or unfolding) of tensors with many axes to tensors of less axes.
\end{remark}



\subsection{Contractions}



\subsubsection{Tensor Product}



\begin{definition}[Tensor Product]\label{def:tensorProduct}
	Let there be two tensor
	\begin{align*}
		\hypercoreat{\catvariables} : \facstates \rightarrow \rr \quad \text{and} \quad  \sechypercoreat{\seccatvariables} : \secfacstates \rightarrow \rr \, 
	\end{align*}
	with different categorical variables assigned to its axes.
	Then there tensor product is the map
	\begin{align*}
		\left(\hypercore \otimes \sechypercore\right)\left[\catvariables,\seccatvariables \right] :  \left(\facstates\right) \times \left(\secfacstates\right) \rightarrow \rr
	\end{align*}
	defined for $\catindices\in\facstates$ and $\seccatindices\in\secfacstates$ as
	\begin{align*}
		\left(\hypercore \otimes \sechypercore\right)&\left[\indexedcatvariables,\indexedseccatvariables \right] \\
		&:=  \hypercoreat{\indexedcatvariables}\cdot \sechypercoreat{\indexedseccatvariables} \, .
	\end{align*}
\end{definition}


\subsection{Illustrations by Hypergraphs}

\begin{definition}\label{def:hypergraphs}
	A hypergraph is a pair $\graph=(\nodes,\edges)$ of a set of nodes $\nodes$ and a set of edges $\edges$, where each hyperedge $\edge\in\edges$ is a subset of the nodes $\nodes$.
	A directed hypergraph is a pair $\graph=(\nodes,\edges)$, such that each hyperedge $\edge\in\edges$ is the tuple of two disjoint sets $\incomingnodes,\outgoingnodes\subset\nodes$, that is
		\[ \edge = (\incomingnodes,\outgoingnodes)  \, . \]
\end{definition}


%% Diagrammatic representation
We will use a diagrammatic illustration of tensors, where tensors are represented by block nodes and each axis as a leg decorated with a categorical variable $\catvariableof{\atomenumerator}$ representing the choice of an element in the set $[\catdimof{\atomenumerator}]$ as shown in Figure~\ref{fig:tensors}b).

%% Hyperedge view
Another useful depiction is by hypergraphs, which are generalizations of graphs allowing that edges being arbitrary nonempty subsets of the nodes, whereas canonical graphs assume pairs.
Along this depiction (see Figure~\ref{fig:tensors}a) each axis of the tensor is represented by a node representing the variable $\catvariableof{\atomenumerator}$ and the tensor $\hypercore$ is associated with the hyperedge $\edge$ connecting all variables.

The two representations are dual to each other in the sense that the nodes of the one are the hyperedges of the other graph.

\begin{figure}[h!]
	\begin{center}
		\input{PartI/tikz_pics/tensor_networks/hypercore.tex}
	\end{center}
	\caption{Depiction of Tensors 
	a) Decoration of hyperedges connecting the variables $\catvariableof{\atomenumerator}$ to each axis $\atomenumeratorin$.
	b) Blockwise notation with axis denoted by open legs represented by the variables $\catvariableof{\atomenumerator}$.
	}\label{fig:tensors}
\end{figure}


% Diagramatic representation of vectors
To depict vector calculus and its generalizations, we will apply the graphical notation (mainly version b) introduced in Chapter~\ref{cha:TensorNetworks}. 
Along this line, we represent vectors and their generalization to tensors by blocks with legs representing its indices.
The basis vectors being one-hot encodings of states are in this scheme represented by
	\begin{center}
		\input{PartI/tikz_pics/overview/one_hot_atomic.tex}
	\end{center}
where $\tilde{\catindex}$ is an indexed represented by an open leg. 
Assigning $\catindex$ to this index will retrieve the $\catindex$th coordinate (with value $1$), whereas all other assignments will retrieve the coordinate values $0$. 

Drawing on the interpretation of tensors by hyeredges we can continue with the definition of tensor networks.

\begin{definition}\label{def:tensorNetwork}
	Let $\graph=(\nodes,\edges)$ be a hypergraph with nodes decorated by categorical variables $\catvariableof{\node}$ with dimensions
		\[ \catdimof{\node} \in \nn \]	
	and hyperedges $\edge\in\edges$ decorated by core tensors
		\[ \hypercoreofat{\edge}{\catvariableof{\edge}} \in \bigotimes_{\node\in\edge}\rr^{\catdimof{\node}} \, , \]
	where we denote by $\catvariableof{\edge}$ the set of categorical variables $\catvariableof{\node}$ with $\node\in\edge$.
	Then we call the set 
		\[ \tnetofat{\graph}{\catvariableof{\nodes}} = \{\hypercoreofat{\edge}{\catvariableof{\edge}}  \, : \, \edge\in\edges\} \]
	the Tensor Network of the decorated hypergraph $\graph$.
\end{definition}


\begin{figure}
	\begin{center}
		\input{PartI/tikz_pics/tensor_networks/network.tex}
	\end{center}
	\caption{
	Example of a tensor network.
	a) Hypergraph with edges $\edge_0=\{\catvariableof{0},\catvariableof{1},\catvariableof{2}\}$, $\edge_1=\{\catvariableof{1},\catvariableof{2}\}$ and $\edge_2=\{\catvariableof{2},\catvariableof{3}\}$ decorated by tensor cores.
	b) Dual tensor network, depicting a contraction with leaving all variables open.
	}\label{fig:network}
\end{figure}

%%
Diagrammatic notation: Best to do version a) as used in the definition, highlighting that tensors have shared categorical variables with fixed dimensions.


\subsection{Generic Contractions}


Contractions of Tensor Networks $\extnet$ are operations to retrieve single tensors by summing products of tensors in a network over common indices.
We will define contractions formally by specifying just the indices not to be summed over.


%need specification of Tensor Network at hand and edges to be left over (as in numpy.einsum)
%	\[ \contractionof{\extnet}{[\randomxof{\atomenumerator_1},...\randomxof{\atomenumerator_d}]} \, . \]
When some of the variables are not appearing as leg variables, we define the contraction as being a tensor product with the trivial tensor $\ones$ carrying the legs of the missing variables.


\begin{definition}\label{def:contraction}
	Let $\tnetof{\graph}$ be a tensor network on a decorated hypergraph $\graph=(\nodes,\edges)$.
	For any subset $\secnodes\subset\nodes$ we define the contraction  to be the tensor 
	\begin{align}
		\contractionof{\tnetof{\graph}}{\catvariableof{\secnodes}} \in \bigotimes_{\node\in\secnodes} \rr^{\catdimof{\node}}
	\end{align}
	defined coordinatewise by the sum	
	\begin{align}
		\contractionof{\tnetof{\graph}}{\indexedcatvariableof{\node} \, : \, \node\in\secnodes} =
%		\contractionof{\tnetof{\graph}}{\secnodes}\left(\{\catindexof{\node} \, : \, \node\in\secnodes\}\right) = 
		\sum_{\{\catindexof{\node} \in [\catdimof{\node}] \, : \, \node \in \nodes/\secnodes\}}
		\left( \prod_{\edge\in\edges}\hypercoreofat{\edge}{\indexedcatvariableof{\node} \, : \, \node\in\edge} \right) \, .
		 %\left(\{\catindexof{\node} : \node\in\edge\}\right) \, . 
	\end{align}
\end{definition}

\begin{figure}
	\begin{center}
		\input{PartI/tikz_pics/tensor_networks/contraction.tex}
	\end{center}
	\caption{
		Example of a tensor network contraction of all but the variables $\catvariableof{1},\catvariableof{3}$.
		Contraction of variables can always be depicted by closing the open legs with trivial tensors $\ones$ performing index sums.
	}\label{fig:contraction}
\end{figure}



%%
Diagrammatic notation: Best to do version b), since this is easiest to see how tensors combine to new tensors by contractions.

\begin{remark}[Alternative Notations]
	Contractions can also denoted by the Einstein summations of the indices along connected edges, understood as scalar product in each subspace.
%	This notation is used in $\mathrm{numpy.einsum}$.
	This is as in Definition~\ref{def:contraction}, just omitting the sums.
	We found it useful in this work to do the diagrammatic representation instead, since it offers a better possibility to depict hierarchical arrangements of shared variables.
\end{remark}


% Example
\begin{example}
	Tensor products are examples of contractions, where the hypergraph consists of two disjoint edges, which union is left open.
\end{example}


%% Diagrammatic representation of Matrix Vector
\begin{example}
	Matrix vector contractions are special cases of Tensor Contractions.
	We use the visualization as in Figure~\ref{fig:tensors}b where vectors are blocks with single categorical variables and matrices with two indices and matrix vector products by
	\begin{center}
		\input{PartI/tikz_pics/overview/one_hot_matrix.tex}
	\end{center}
	Here the index $j$ is represented by a closed edge, which means that it is eliminated by a sum.
\end{example}


%% Hadamard Product 
\begin{example}
	A node appearing in arbitrary many hyperedges denotes a Hadamard product of the axis of the respective decorating tensors.
	To give an example, let $V^k\in\rr^p$ be vectors for $\atomenumeratorin$. Their hadamard product is the vector
		\[ V^1\circ V^2 \circ \ldots \circ V^{\atomorder-1} \in \rr^p \]
	defined by
		\[ \left( V^1\circ V^2 \circ \ldots \circ V^{\atomorder-1} \right)_\catindex = \prod_{\atomenumeratorin} V^\atomenumerator_\catindex \, . \]
	In a contraction diagram the Hadamard product is depicted by 
	\begin{center}
		\input{PartI/tikz_pics/tensor_networks/hadamard.tex}
	\end{center}
\end{example}



\subsubsection{Decompositions}

\begin{definition}\label{def:tnDecomposition}
	Let $\extensor$ be a tensor in $\extensorspace$.
	A Tensor Network Decomposition of a tensor $\hypercoreat{\catvariablesinset{\nodes}}$ is a Tensor Network $\tnetof{\graph}$ to a hypergraph containing  such that
		\[ \hypercoreat{\catvariablesinset{\nodes}}= \contractionof{\tnetof{\graph}}{\catvariablesinset{\nodes}} \, . \]
\end{definition}



\subsection{Properties of Tensors}

%% Binary
We will often encounter situations, where the coordinates of tensors are binary valued, i.e. the tensor is a map to the set $[2]$.

\begin{definition}\label{def:binaryTensor} % CALL BOOLEAN INSTEAD?
	We call a tensor binary, when $\imageof{T}\subset[2]$, i.e. all coordinates are either $0$ or $1$.
\end{definition}

%% Directionality
Directionality represents constraints on the structure of tensors:
Summing over outgoing trivializes the tensor.

\begin{definition}\label{def:directedTensor}
	A Tensor 
		\[ \hypercoreat{\catvariablesinset{\nodes}} \in \bigotimes_{\nodein}\rr^{\catdimof{\node}} \]
	is said to be directed with incoming variables $\innodes$ and outgoing variables $\outnodes$, where $\nodes=\innodes\dot{\cup}\outnodes$, when
		\[ \contractionof{\{\hypercore\}}{\catvariablesinset{\outnodes}} =  \onesat{\catvariablesinset{\innodes}} \]
	where $\onesat{\catvariablesinset{\innodes}}$ denoted the trivial tensor in  $\bigotimes_{\node\in\innodes}\rr^{\catdimof{\node}}$ which coordinates are all $1$.
\end{definition}

While by default all legs are outgoing, we can change the direction by normation.

\begin{definition}\label{def:normation}
	A tensor $\hypercoreat{\catvariablesinset{\nodes}}$ is said to be normable on $\innodes\subset\nodes$, if for any $\atomlegindexof{\innodes}\in\bigtimes_{\node\in\innodes}[\catdimof{\node}]$ we have
		\[ \contractionof{\{\hypercore\} \cup\{\onehotmapof{\atomlegindexof{\innodes}}\}}{\varnothing} > 0 \, . \]
	The normation of a on $\innodes\subset\nodes$ normable tensor is the tensor
	\begin{align*}
		\normationofwrt{\{\hypercore\}}{\outnodes}{\innodes} = 
		\sum_{\atomlegindexof{\innodes}\in\bigtimes_{\node\in\innodes}[\catdimof{\node}]} 
		\onehotmapof{\atomlegindexof{\innodes}} \otimes \frac{
		\contractionof{\{\hypercore\}\cup\{\onehotmapof{\atomlegindexof{\innodes}}\}}{\outnodes}
		}{
		\contractionof{\{\hypercore\}\cup\{\onehotmapof{\atomlegindexof{\innodes}}\}}{\varnothing}
		} 
	\end{align*}
	where $\outnodes = \nodes / \innodes$.
%	
%	A tensor network $\extnet$ on variables $\nodes$ can be normed on $\secnodes$, if the coordinates of no slice with respect to $\secnodes$ sum to $0$.
%	Then we define the normed tensor
%		\[ \normationofwrt{\extnet}{\outnodes}{\innodes} 
%		\in \left( \bigotimes_{\node\in\innodes} \rr^{\catdimof{\node}} \right) \otimes \left( \bigotimes_{\node\in\outnodes} \rr^{\catdimof{\node}} \right) \]
%	by
%	 \begin{align*}
%	 	\normationofwrt{\extnet}{\outnodes}{\innodes} 
%		= \sum_{\atomlegindexof{\innodes}\in\bigtimes_{\node\in\innodes}[\catdimof{\node}]} 
%		\onehotmapof{\atomlegindexof{\innodes}} \otimes \frac{
%		\contractionof{\extnet\cup\{\onehotmapof{\atomlegindexof{\innodes}}\}}{\outnodes}
%		}{
%		\contractionof{\extnet\cup\{\onehotmapof{\atomlegindexof{\innodes}}\}}{\varnothing}
%		} \, . 
%	 \end{align*}
\end{definition}

We will investigate the contractions of directed tensors in Part~III, where we show in Theorem~\ref{the:normationDirected} that normations are directed tensors.


%% Diagrammatic notation
In our graphical tensor notation, we depict directed tensors by directed hyperedges (a), which are decorated by directed tensors (b), for example:
%\red{Draw incoming and outgoing example.}
	\begin{center}
		\input{PartI/tikz_pics/tensor_networks/directed_core.tex}
	\end{center}



\subsection{Encoding schemes for functions}

Tensors are defined here as real-valued functions on the state set of a system described by categorical variables.
We provide further schemes to represent functions in order to perform sparse calculus and to handle more generic functions.


\subsubsection{Real-valued functions}


\begin{example}[Uncertainty about States]\label{exa:onehotUncertainty}
	The uncertainty about the state of a categorical variable $\catvariable$ can be expressed in vectors.
	For example let there be real numbers $\probof{\catvariable=\catindex} \in [0,1]$ for $\catindex\in[\catdim]$ with $\sum_{\catindex\in[\catdim]}\probof{\catvariable=\catindex}=1$ with the interpretation that $\probof{\catvariable=\catindex}$ is the probability of a system being in state $\catindex$. 
	We can represent this uncertain state simply by a vector 
		\[ \probof{\catvariable}\in\rr^{\catdim} \]
	defined as the sum of one-hot representations weighted by $\probof{\catvariable=\catindex}$
	\[ \sum_{\catindex\in[\catdim]} \probof{\catvariable=\catindex} \cdot \onehotmapofat{\catindex}{\catvariable} =
		\begin{bmatrix}
		\probof{\catvariable=0} & \probof{\catvariable=1} & \cdots & \probof{\catvariable=\catdim-1}
		\end{bmatrix} \, . 
	\]
\end{example}



\subsubsection{Relational encodings}

We have already observed in Example~\ref{exa:atomicFunction}, that any function of a categorical variable has a representation as a linear function acting on the one-hot encoding of the variable.
Let us now show how we can encode maps between factored systems.
The scheme is described in more generality and detail (encoding of subsets and relations) in Chapter~\ref{cha:tensorEncodings}, see Definition~\ref{def:functionRelationEncoding}.

\begin{definition}[Relation encoding of maps between Factored Systems]\label{def:functionRepresentation}
	Let $\exfunction$ be a function
		\[ \exfunction : \facstates \rightarrow  \secfacstates \]
	which maps the states of a factored system to variables $\catvariables$ to the states of another factored system with variables $\seccatvariables$.
	Then the tensor representation of $\exfunction$ is a tensor
		\[ \rencodingofat{\exformula}{\catvariables,\seccatvariables} \in   \left(\facspace\right) \otimes \left(\secfacspace\right)  \]
	defined by
		\[ \rencodingofat{\exformula}{\catvariables,\seccatvariables}= \sum_{\catindices\in\facstates}  
		  \onehotmapofat{\catindices}{\catvariables} \otimes \onehotmapofat{\exfunction(\catindices)}{\seccatvariables} \, . \]
\end{definition}

% Notation with image categorical variable
When the categorical variables of the image factored system to a map $\exfunction$ are not specified otherwise, we will denote them by $\catvariableof{\exfunction}$.


\subsubsection{Tensor-valued functions}


%% TO DETAILLED HERE -> Part III?
\begin{definition}[Selection encoding of Maps between Factored Systems]\label{def:selectionEncoding}
	Given a tensor space $\parspace$ described by categorical variables $\selvariables$ and a tensor-valued function
		\[ \exfunction : \facstates \rightarrow \parspace \]
	the selection encoding of $\exfunction$ is a tensor
		\[ \sencodingofat{\exfunction}{\shortcatvariables,\shortselvariables} \in \left(\facspace\right) \otimes \left(\parspace\right) \]
	defined by the basis decomposition
		\[ \sencodingofat{\exfunction}{\shortcatvariables,\shortselvariables} = \sum_{\catindices\in\facstates} \onehotmapofat{\catindices}{\shortcatvariables} \otimes \exfunction(\catindices)[\shortselvariables] \, .  \]
\end{definition}

%%
We call these tensor representation of maps selection encodings, since the coordinate of a function $\exfunction$ to be processed is selected by another argument to $\sencodingof{\exfunction}$.

%\begin{example}[Vector valued functions]\label{exa:atomicFunction} %% CONFUSIN, since already needs selection variables?
%	When using a one-hot representation of the state of a categorical variable, any real valued function has a representation by a real valued matrix acting on the one-hot encoding. 
%	Let there be a vector valued function
%		\[ \exformula : [\catdim] \rightarrow \rr^p \]
%	which maps $\catindex\in[\catdim]$ to the vector
%		\[ \exformula(\catindex)[\selvariable] \in \rr^p \, , \]
%	where we introduced the variable $\selvariable\in[p]$ selecting a coordinate of the image vector.
%	The 
%		\[ \exformula(\catindex)[\selvariable] = 
%		\contractionof{\{\onehotmapof{\catindex}[\catvariable] , \,\concore_{\exformula}[\catvariable,\selvariable]\}}{\selvariable}  \]
%	where $\concore_{\exformula} \in \rr^{\catdim \times p} $ is the matrix defined by the function evaluation vectors of $\exformula$ as
%		\[ \concore_{\exformula}[\catvariable,\selvariable] = \begin{bmatrix}
%			-- & \exformula(0) & -- \\
%			-- & \exformula(1) & -- \\
%			& \vdots &  \\
%			-- & \exformula(\catdim-1) & -- 
%		\end{bmatrix} \, . 
%		\]
%	This can easily be verified, since matrix multiplication with basis vectors amounts to selection of rows (when the basis vector is acting from the left) or columns (when the basis vector is acting from the right).
%	Thus, linear transforms (matrices) acting on the one-hot representation are sufficient to represent any vector valued function of the states of a categorical variable.
%\end{example} 


%% 
We will provide more detail to the tensor representation of functions in Chapter~\ref{cha:tensorEncodings}. %where we show that domain encodings coincide with selection encodings.





