\documentclass[aps,onecolumn,nofootinbib,pra]{article}

\usepackage{../../article_compilation/spec_files/arxiv}
\input{../../article_compilation/spec_files/standard_preamble.tex}

\input{../../macros/organization_macros.tex}
\input{../../macros/general_macros.tex}
\input{../../macros/tc_macros.tex}
\input{../../macros/tikz_macros.tex}

\begin{document}
    \title{Characterization of \ComputationActivationNetworks{} by sufficient statistics}

    \maketitle
    \date{\today}
    \tableofcontents


    \section{Foundations}

    \subsection{Information Theory [Cover, Thomas - Section 2.10]}

    Consider two variables $Z$ and $X$ with a joint distribution $\probat{Z,X}$, and a function $T$ on the states of $X$.
    We augment this joint distribution by a variable $\headvariableof{T}$, which is the head variable to the function $T$
    \begin{align*}
        \probat{Z,X,\headvariableof{T}} = \contractionof{\probat{Z,X},\bencodingofat{T}{\headvariableof{T},X}}{Z,X,\headvariableof{T}}
    \end{align*}
    Then we have
    \begin{align*}
        \condindependent{\headvariableof{T}}{Z}{X}
    \end{align*}
    since
    \begin{align*}
        \condprobof{\headvariableof{T}}{Z,X} = \bencodingofat{T}{\headvariableof{T},X} \otimes \onesat{Z} \, .
    \end{align*}
    Thus, the variables are a Markov Chain $Z\rightarrow X\rightarrow Y$.

    \begin{definition}
        We call $T$ sufficient statistic of $Z$, if and only if
        \begin{align*}
            I(Z;X) = I(Z;T(X)) \, .
        \end{align*}
    \end{definition}

    \begin{lemma}
        \label{lem:doubleDetSstat}
        If there is a function $Q$ such that
        \begin{align*}
            \probat{Z,X} = \contractionof{\probat{X},\bencodingofat{Q}{Z,X}}{Z,X} \, ,
        \end{align*}
        and $T$ is sufficient for $Z$, then there is a function $R$ such that
        \begin{align*}
            Q = R \circ T \, .
        \end{align*}
    \end{lemma}
    \begin{proof}
%    (See proof of Theorem 2.19 in the Report)
        Since $Z$ has a deterministic dependence on X we have $\sentropyof{Z|X} = 0$ and by the sufficient statistic assumption (using that $I(X;\headvariableof{T}) = H(\headvariableof{T})-H(X|\headvariableof{T})$) we have
        \begin{align*}
            \sentropyof{Z|\headvariableof{T}} = \sentropyof{Z|X} = 0 \, .
        \end{align*}
        Now, $\sentropyof{Z|\headvariableof{T}}$ is equal to the existence of a function $R$ mapping the states of $Y$ to $Z$, such that for any state $y$
        \begin{align*}
            \condprobat{Z}{\headvariableof{T}=y} = \onehotmapofat{R(y)}{Z} \, .
        \end{align*}
        Since $Y$ itself is computable by $X$ with the function $T$, and $Z$ with $Q$, we have
        \begin{align*}
            Q = R \circ T \, . & \qedhere
        \end{align*}
    \end{proof}

    This lemma is applied when characterizing sufficient statistics for $Z = \probat{X}$.

    \subsection{Mathematical Statistic [Hogg - Chapter 2]}

    In mathematical statistic, sufficient statistics are used to characterize parameter estimation problems, i.e. where $Z$ is a parameter variable $\Theta$ of a parametrized family.
    The joint distribution of $\Theta$ and $X$ is constructed by drawing the parameter variable $\Theta$ first with outcome $\theta$ and then drawing $X$ from $\probof{\theta}$.


    \section{The Computation Mechanism of Tensor Network Decompositions}

    Sufficient statistics imply tensor network decompositions of joint distributions using basis encodings of them.
    The basis encoding of the sufficient statistics computes the sufficient statistic in the basis calculus scheme.
    We thus call this decomposition mechanism the computation mechanism.

    \begin{theorem}[Factorization Theorem of Fisher and Neyman]
        \label{the:factorizationFisherNeyman}
        Let $\probtensor$ be a joint distribution of variables $Z,X$ with values $\mathrm{val}(Z), \,\mathrm{val}(X)$ and let $T(X)$ be a statistic.
        The following are equivalent:
        \begin{itemize}
            \item[i)] The Data Processing Inequality holds straight, i.e.
            \begin{align*}
                I(Z;X) = I(Z;\headvariableof{T}) \, .
            \end{align*}
            \item[ii)] $Z\rightarrow \headvariableof{T}\rightarrow X$ is a Markov Chain, i.e.
            \begin{align*}
                \condindependent{Z}{X}{\headvariableof{T}}
            \end{align*}
            \item[iii)] There are functions $g : \imageof{T} \times \mathrm{val}(Z) \rightarrow \rr$ and $h: \mathrm{val}(X)\rightarrow\rr$ such that for any $(x,z)\in\mathrm{val}(Z)\times \mathrm{val}(X)$
            \begin{align*}
                \probat{Z=z,X=x} = g(T(x),z) \cdot h(x) \, .
            \end{align*}
        \end{itemize}
    \end{theorem}
    \begin{proof}
        $i) \Leftrightarrow ii)$:
        We have always
        \begin{align*}
            I(Z;X) = I(Z;X,\headvariableof{T}) = I(Z;\headvariableof{T}) + I(Z;X|\headvariableof{T})
        \end{align*}
        and thus if and only if $i)$ holds
        \begin{align*}
            I(Z;X|\headvariableof{T}) = 0 \, .
        \end{align*}
        Using the KL-divergence characterization of the mutual information, this is equal to
        \begin{align*}
            \condprobat{Z,X}{\headvariableof{T}} = \contractionof{\condprobat{Z}{\headvariableof{T}},\condprobat{X}{\headvariableof{T}} }{Z,X,\headvariableof{T}} \, .
        \end{align*}
        This is equivalent to the conditional independence statement $ii)$. \\

        $ii) \Rightarrow iii)$:
        For all $z\in\mathrm{val}(Z)$ and $x\in\mathrm{val}(X)$ we have
        \begin{align*}
            \condprobat{Z=z}{X=x}
            &= \condprobat{Z=z}{X=x,\headvariableof{T}=T(x)} \\
            &= \condprobat{Z=z}{\headvariableof{T}=T(x)}
        \end{align*}
        Here we used that $\headvariableof{T}$ has a deterministic dependence on $X$ and $ii)$.
        There is thus a function $g$ such that for all $z\in\mathrm{val}(Z)$ and $x\in\mathrm{val}(X)$
        \begin{align*}
            g(T(x),z) = \condprobat{Z=z}{X=x} \, .
        \end{align*}
        We further define a function $h(x)=\probat{X=x}$ and get
        \begin{align*}
            \probat{Z=z,X=x}
            &= \probat{X=x} \cdot \condprobat{Z=z}{X=x} \\
            &= g(T(x),z) \cdot h(x) \, .
        \end{align*}

        $iii) \Rightarrow ii)$:
        Using $iii)$ we have for all supported $(x,z)\in\mathrm{val}(Z)\times \mathrm{val}(X)$
        \begin{align*}
            \condprobat{Z=z}{X=x}
            &= \frac{\probat{Z=z,X=x}}{\probat{X=x}} \\
            &= \frac{g(T(x),z) \cdot h(x) }{\int g(T(x),z) \cdot h(x) \, dz} \\
            &= \frac{g(T(x),z)}{\int g(T(x),z)  \, dz} \\
            &= \frac{
                \left(\int_{\tilde{x}:T(x)=T(\tilde{x})} \, h(x) \, dx \right) \cdot g(T(x),z)
            }{
                \left(\int_{\{\tilde{x}:T(x)=T(\tilde{x})\}} \, h(x) \, dx \right) \cdot \int g(T(x),z)  \, dz
            } \\
            &= \frac{\probat{Z=z,\headvariableof{T}=T(x)}}{\probat{\headvariableof{T}=T(x)}} \\
            &= \condprobat{Z=z}{\headvariableof{T}=T(x)}
        \end{align*}

        We have at almost all $y\in\mathrm{val}(\headvariableof{T})$, $z\in\mathrm{val}(Z)$ and $x\in\mathrm{val}(X)$ that $y=T(x)$ and
        \begin{align*}
            \condprobat{Z=z}{X=x,\headvariableof{T}=y}
            = \condprobat{Z=z}{X=x}
        \end{align*}
        and with the above at thus at almost all such pairs
        \begin{align*}
            \condprobat{Z=z}{X=x,\headvariableof{T}=y} = \condprobat{Z=z}{\headvariableof{T}=y} \, .
        \end{align*}
        This is equivalent to $ii)$.
    \end{proof}

    \theref{the:factorizationFisherNeyman} thus states, that whenever a sufficient statistic $T$ of $X$ exists for a variable $Z$, then the joint distribution of $X$ and $Z$ decomposes as sketched in \figref{fig:comDecSufStat}.

    \begin{figure}[t]
        \begin{center}
            \input{./tikz_pics/computation_decomposition.tikz}
        \end{center}
        \caption{Sketch of the computation decomposition of a joint distribution of $X,Z$ given a sufficient statistic $T$.
        This decomposition follows from the Fisher-Neyman factorization \theref{the:factorizationFisherNeyman}.}\label{fig:comDecSufStat}
    \end{figure}


    \section{Sufficient Statistic for Parametrized Families}

    Sufficient statistics are treated in mathematical statistics and in information theory.
    We here choose a definition of information theory and apply a factorization theorem of mathematical statistics to relate with \ComputationActivationNetworks{}.
    The distribution of a canonical parameter is now drawn from a (possibly continuous) random variable $\Theta$, which takes values $\theta\in\Gamma$ with probability
    \begin{align*}
        \secprobat{\Theta=\theta} \, .
    \end{align*}

    \begin{definition}[Sufficient statistics for Parameters]
        Let $\{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}$ be a family of probability distributions and
        \begin{align*}
            \sstat\defcols\facstates\rightarrow\bigtimes_{\selindexin}[\seldimof{\selindex}]
        \end{align*}
        be a function.
        We say that $\sstat$ is sufficient for $\Theta$, if for any distribution $\secprobat{\Theta}$ of $\Theta$, when drawing $\shortcatvariables$ from $\probofat{\theta}{\shortcatvariables}$ with probability $\secprobat{\Theta=\theta}$, we have that
        \begin{align*}
            \condindependent{\Theta}{\shortcatvariables}{\sstatat{\shortcatvariables}} \, .
        \end{align*}
    \end{definition}

    We can characterize \ComputationActivationNetworks{} with arbitrary base measures based on sufficient statistics.

    \begin{theorem}[Characterization of \ComputationActivationNetworks{}]
        Let $\{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}$ be a family of probability distributions with a sufficient statistic $\sstat$.
        Then there is a non-negative (possibly non-Boolean) base measure $\basemeasurewith$ and a map
        \begin{align*}
            h : \Gamma \rightarrow \bigotimes_{\selindexin} \rr^{\seldimof{\selindex}}
        \end{align*}
        such that for all $\theta\in\Gamma$
        \begin{align*}
            \probofat{\theta}{\shortcatvariables}
            = \normalizationof{h(\Gamma)[\headvariables],\bencsstatwith,\basemeasurewith}{\shortcatvariables} \, .
        \end{align*}
        We further have that for a set $\{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}$ $\sstat$ is a sufficient statistic, if and only if there is a non-negative (possibly non-Boolean) base measure $\basemeasurewith$ with
        \begin{align*}
            \{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}
            \subset \realizabledistsof{\sstat,\maxgraph,\basemeasure} \, .
        \end{align*}
    \end{theorem}
    \begin{proof}
        By the Fisher-Neyman Factorization \theref{the:factorizationFisherNeyman} we have that $\sstat$ is a sufficient statistic if and only if there are real-valued functions $g$ on $\left(\bigtimes_{\selindexin}[\seldimof{\selindex}]\right)\times \Gamma$ and $h$ on $\facstates$ such that
        \begin{align}
            \label{eq:conFactorizationSufStat}
            \probofat{\theta}{\indexedshortcatvariables}
            = g(\sstatat{\shortcatindices},\Gamma) \cdot h(\shortcatindices) \, .
        \end{align}
        We define a base measure by the coordinate encoding of $h$ by
        \begin{align*}
            \basemeasureat{\shortcatvariables} = \sum_{\shortcatindicesin} h(\shortcatindices) \onehotmapofat{\shortcatindices}{\shortcatvariables}
        \end{align*}
        and for each $\theta\in\Gamma$ an activation tensor
        \begin{align*}
            \acttensorofat{\theta}{\headvariables} = \sum_{\shortheadindices} g(\shortheadindices,\theta) \onehotmapofat{\shortheadindices}{\headvariables} \, .
        \end{align*}
        With this we have for any $\theta\in\Gamma$
        \begin{align*}
            \contraction{h(\Gamma)[\headvariables],\bencsstatwith,\basemeasurewith} = 1
        \end{align*}
        and thus for any $\shortcatindicesin$ applying basis calculus
        \begin{align*}
            \normalizationof{h(\Gamma)[\headvariables],\bencsstatwith,\basemeasurewith}{\indexedshortcatvariables}
            &= h(\Gamma)[\headvariables=\sstatat{\shortcatindices}] \cdot \basemeasureat{\indexedshortcatvariables} \\
            &= g(\sstatat{\shortcatindices},\Gamma) \cdot h(\shortcatindices) \\
            &= \probofat{\theta}{\indexedshortcatvariables}
            \, .
        \end{align*}
        We therefore find for any $\probofat{\theta}{\shortcatvariables}$ a representation as a \ComputationActivationNetwork{} in $\realizabledistsof{\sstat,\maxgraph,\basemeasure}$ with the activation tensor $h(\Gamma)[\headvariables]$.

        To show the second claim, we are left to show that any set of \ComputationActivationNetworks{} in $\realizabledistsof{\sstat,\maxgraph,\basemeasure}$ has $\sstat$ as a sufficient statistic.
        Let us thus consider a parametric family
        \begin{align*}
            \{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}
            \subset \realizabledistsof{\sstat,\maxgraph,\basemeasure} \, .
        \end{align*}
        By this inclusion we find for any $\theta\in\Gamma$ an activation core $\actcoreofat{\theta}{\headvariables}$.
        We then construct functions $g$ and $h$ by
        \begin{align*}
            g(\shortheadindices,\Gamma) = \actcoreofat{\theta}{\indexedheadvariableof{[\seldim]}}
            \andspace
            h(\shortcatindices) = \basemeasureat{\indexedshortcatvariables}
        \end{align*}
        and notice that the equivalent condition \eqref{eq:conFactorizationSufStat} to $\sstat$ being a sufficient statistic is satisfied.
    \end{proof}


    \section{Sufficient Statistic for the Probability}

    % Compare with report version
    We here consider sufficient statistics for the parameter of a parametrized family, while in the report we considered sufficient statistics for the probability mass as a random variable.
    In both cases this results from the information theoretic viewpoint, that a function $T$ of $X$ is a sufficient statistic for a variable $Z$, if
    \begin{align*}
        \condindependent{Z}{X}{T(X)} \, .
    \end{align*}
    While we choose for $Z$ $\headvariableof{\theta}$ above, we now choose for $Z$ the variable $\headvariableof{\probtensor}$.
    This variable can be computed by contraction with
    \begin{align*}
        \bencodingofat{\probtensor}{\headvariableof{\probtensor},\shortcatvariables} \, .
    \end{align*}
    If $T$ is a sufficient statistic for $\headvariableof{\probtensor}$, we call it probability sufficient for $\probtensor$.

    \begin{theorem}[Theorem 2.19 in the report]
        If and only if a statistic $\sstat$ is probability sufficient for $\probwith$, then
        \begin{align*}
            \probwith \in \realizabledistsof{\sstat,\maxgraph,\trivbm} \, .
        \end{align*}
    \end{theorem}
    \begin{proof}
        By \lemref{lem:doubleDetSstat} we have a function $R$ such that for all $\shortcatindicesin$
        \begin{align*}
            \probat{\indexedshortcatvariables} = (R \circ \sstat)(\shortcatindices) \, .
        \end{align*}
        By basis calculus it follows that
        \begin{align*}
            \probwith = \contractionof{R(\indexinterpretationof{\sstat}[\headvariables]),\bencsstatwith}{\shortcatvariables}
        \end{align*}
        and thus
        \begin{align*}
            \probwith \in \realizabledistsof{\sstat,\maxgraph,\trivbm} \, . & \qedhere
        \end{align*}
    \end{proof}

    Note that by this theorem w can restrict ourselves to the \ComputationActivationNetworks{} with trivial base measure for the characterization of distributions with a probability sufficient statistic.


    \section{Minimal sufficient statistics}

    Minimal sufficient statistics are defined by existences of functions from any sufficient statistics.

    \begin{definition}
        A sufficient statistic $T$ of $Z$ is minimal, if and only if for any sufficient statistic $U$ of $Z$ there is a function $R$ such that $T=R\circ U$.
    \end{definition}

    Note that by construction, we can choose the same base measure $h$ when factorizing with respect to different sufficient statistics.
    The activation cores $g^{(U)}$ to an arbitrary sufficient statistic $U$ can thus be further decomposed by the basis encoding of $R$ and an activation core $g^{(T)}$ to a minimal sufficient statistic as
    \begin{align*}
        g^{(U)}[\headvariableof{U},Z] = \contractionof{\bencodingofat{R}{\headvariableof{T},\headvariableof{U}},g^{(T)}[\headvariableof{T},Z]}{\headvariableof{U},Z} \, .
    \end{align*}

    Minimal sufficient statistics thus provide the best embedding into a \ComputationActivationNetworks{}, by decomposing the activation tensor into refining \ComputationActivationNetwork{}.


    \section{Indicator Statistic HLN to families with sufficient statistics}

    \subsection{Indicator Statistics}

    \begin{definition} % Compare with report
        Given a statistic $\sstat$ we call the $\left(\prod_{\selindexin}\seldimof{\selindex}\right)$-dimensional statistic $I(\sstat)$ defined by selection variables $\selvariableof{\selindex}$ and slices
        \begin{align*}
            \sencodingofat{I(\sstat)}{\shortcatvariables,\selvariableof{0}=\secselindex_0,\ldots,\selvariableof{\seldim-1}=\secselindex_{\seldim-1}}
            = \contractionof{\left\{\indicatorofat{\sstatcoordinateof{\selindex} = \secselindex_{\selindex}}{\shortcatvariables} \wcols \selindexin \right\}}{\shortcatvariables}
        \end{align*}
        the indicator statistic to $\sstat$.
    \end{definition}

    We call this the indicator statistic, since each feature indexed by $\secselindex_{[\seldim]}$ is the indicator $\onesofat{\sstat=\secselindex_{[\seldim]}}{\shortcatvariables}$.
    We now show two technical lemmata, which will result in an embedding theorem of any maximal graph family of \ComputationActivationNetworks{} into the family of \HybridLogicNetworks{} with the indicator statistic.

    \begin{lemma}
        \label{lem:sencToIndBenc}
        For any statistic $\sstat$ the selection encoding of the indicator statistics coincides with the basis encoding of $\sstat$, i.e.
        \begin{align*}
            \contractionof{
                \sencodingofat{I(\sstat)}{\shortcatvariables,\selvariableof{[\seldim]}},\identityat{\selvariableof{[\seldim]},\headvariables}
            }{\shortcatvariables,\headvariables}
            = \bencsstatwith \, .
        \end{align*}
    \end{lemma}


    \begin{lemma}
        \label{lem:parStatSencToBenc}
        If $\sstat$ is a partition statistic (i.e. its features sum to the trivial feature $\onesat{\shortcatvariables}$), then for any $\hypercoreat{\selvariable}$
        \begin{align*}
            \contractionof{\sencsstatwith,\hypercoreat{\selvariable}}{\shortcatvariables}
            = \contractionof{\bencsstatwith \cup \{\actcoreofat{\selindex}{\headvariableof{\selindex}} \wcols \selindexin\}}{\shortcatvariables}
        \end{align*}
        where for $\selindexin$
        \begin{align*}
            \actcoreofat{\selindex}{\headvariableof{\selindex}}
            = \begin{bmatrix}
                  \hypercoreat{\selvariable=\selindex} \\
                  1
            \end{bmatrix} \, .
        \end{align*}
    \end{lemma}

    \begin{theorem}
        Any family of \ComputationActivationNetworks{} can be embedded into a family of \HybridLogicNetworks{} with respect to the indicator statistic of $\sstat$.
        In particular we have for any non-negative base measure
        \begin{align*}
            \realizabledistsof{\sstat,\maxgraph,\basemeasure} =
            \realizabledistsof{I(\sstat),\elgraph,\basemeasure} \, .
        \end{align*}
    \end{theorem}
    \begin{proof}
        To show $\realizabledistsof{\sstat,\maxgraph,\basemeasure} \subset \realizabledistsof{I(\sstat),\elgraph,\basemeasure}$ let $\acttensorat{\headvariables}$ be an arbitrary tensor.
        Using \lemref{lem:sencToIndBenc} and then \lemref{lem:parStatSencToBenc} on the indicator statistic we get
        \begin{align*}
            \contractionof{\acttensorat{\headvariables},\bencsstatwith}{\shortcatvariables}
            &= \contractionof{\acttensorat{\headvariables},\sencodingofat{I(\sstat)}{\shortcatvariables,\selvariableof{[\seldim]}},\identityat{\selvariableof{[\seldim]},\headvariables}}{\shortcatvariables}\\
            &= \contractionof{
                \actcoreat{\headvariableof{\bigtimes_{\selindexin}[\seldimof{\selindex}]}},\bencodingofat{I(\sstat)}{\actcoreat{\headvariableof{\bigtimes_{\selindexin}[\seldimof{\selindex}]}},\shortcatvariables}
            }{\shortcatvariables}
        \end{align*}
        where by $\actcoreat{\headvariableof{\bigtimes_{\selindexin}[\seldimof{\selindex}]}}$ we denote the elementary activation tensor constructed in \lemref{lem:parStatSencToBenc}.

        Conversely, to show $\realizabledistsof{I(\sstat),\elgraph,\basemeasure}\subset\realizabledistsof{\sstat,\maxgraph,\basemeasure}$ and let $\probtensor$ be an arbitrary elementary activation core to an element in $\realizabledistsof{I(\sstat),\elgraph,\basemeasure}$.
        Since $I(\sstat)$ is a partition statistic, we can choose an elementary parametrizing tensor $\actcoreat{\headvariableof{\bigtimes_{\selindexin}[\seldimof{\selindex}]}}$ such that the first coordinate of the leg vectors does not vanish.
        By multiplication with a scalar, we can choose an elementary parametrizing tensor of $\probtensor$ where all first coordinates are $1$.
        Now we can apply \lemref{lem:parStatSencToBenc} and \lemref{lem:sencToIndBenc} to get a corresponding parametrization in $\realizabledistsof{\sstat,\maxgraph,\basemeasure}$.
    \end{proof}

    As a consequence of this lemma we get together with the Neyman-Fisher factorization theorem:

    \begin{theorem}
        Given any family of distributions with a sufficient statistic $\sstat$.
        Then there is a base measure $\basemeasure$ such that the family is a subset of the \HybridLogicNetworks{} with statistic $I(\sstat)$ and the base measure $\basemeasure$.
    \end{theorem}
    \begin{proof}
        By Neyman-Fisher factorization get a representation of the family by \ComputationActivationNetworks{}.
        Then the above theorem embeds this family into \HybridLogicNetworks{} to the indicator statistic.
    \end{proof}

    \subsection{Average Indicator Statistic}

    We use the convention $1\cdot \lnof{0}=-\infty$ and $0\cdot\lnof{0}=0$.

    \begin{theorem}
        Given any by $\theta\in\Theta$ parametrized family of distributions with a sufficient statistic $\sstat$.
        Then the average of $I(\sstat)$ is sufficient for samples of arbitrary size.
    \end{theorem}
    \begin{proof}
        We use the representation of the family by \ComputationActivationNetworks{} with respect to $\sstat$ and a (possibly non-Boolean) base measure $\basemeasure$.
        In this parametrization, we choose for $\theta\in\Theta$ an activation tensor $\actcoreofat{\theta}{\headvariables}$ such that
        \begin{align*} % ! HERE NORMALIZED ACTIVATION
            \probofat{\theta}{\shortcatvariables} = \contractionof{\actcoreofat{\theta}{\headvariables},\bencsstatwith}{\shortcatvariables} \, .
        \end{align*}
        Let $\catvariableof{[\catorder]\times[n]}$ be a sample of length $n$.
        We then have for the likelihood for arbitrary $\theta$
        \begin{align*}
            \frac{1}{n} \cdot \lnof{\prod_{i\in[n]}\probofat{\theta}{\shortcatvariables=\catindexof{[\catorder],i}}} =
            \contraction{\lnof{\actcoreofat{\theta}{\headvariables}},\bencsstatwith,\empdistributionwith}
            + \frac{1}{n} \cdot \sum_{i\in[n]} \lnof{\basemeasureat{\shortcatvariables=\catindexof{[\catorder],i}}} \, .
        \end{align*}

        Now we notice that for any $\headindexof{[\seldim]}$ we have
        \begin{align*}
            \frac{1}{n} I(\sstat)[\shortcatvariables=\catindexof{[\catorder],i},\selvariableof{[\seldim]}=\headindexof{[\seldim]}]
            = \contractionof{\bencsstatwith,\empdistributionwith}{\headvariables=\headindexof{[\seldim]}} \, .
        \end{align*}

        The likelihood thus depends on the data only on the average of the indicator statistic.
        The latter is thus a sufficient statistic for samples of arbitrary size.
    \end{proof}

    Let us strengthen that the average of the indicator statistic is of finite dimension $2^{\seldim}$.
    Comparison with Pitman-Koopman-Darmois:
    \begin{itemize}
        \item State the existence of a finite dimensional sufficient statistic.
        \item Do not need to assume constant support in the parametrized family.
        \item Use \HybridLogicNetworks{} of indicator statistics instead of exponential families.
    \end{itemize}

    \subsection{Investigation Directions}

    \begin{itemize}
        \item In which cases is the average indicator statistic minimal?
        Hypothesis: If and only if the affine hull of the activation cores (chosen on the span of the one-hot encoded statistic image) is the span of the one-hot encoded statistic image.
        \item Is there a relation with partition statistic HLN being cube-like?
        \item Since HLNs these CANets are exactly maximum entropy distributions. Can we provide closed form activation tensors to max-entropy distributions?
    \end{itemize}

\end{document}