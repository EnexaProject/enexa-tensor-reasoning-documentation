\documentclass[aps,onecolumn,nofootinbib,pra]{article}

\usepackage{../../article_compilation/spec_files/arxiv}
\usepackage{amsmath,amsfonts,amssymb,amsthm,bbm,graphicx,enumerate,times}
\usepackage{mathtools}
\usepackage[usenames,dvipsnames]{color}
\usepackage{hyperref}
\hypersetup{
    breaklinks,
    colorlinks,
    linkcolor=gray,
    citecolor=gray,
    urlcolor=gray,
    pdftitle={},
    pdfauthor={Alex Goessmann}
}



\usepackage{tikz}
\usepackage{graphicx}
\usepackage{float}
\usepackage{comment}
\usepackage{csquotes}

\usepackage{listings}
\usepackage{verbatim}
\usepackage{etoolbox}
\usepackage{braket}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{bbm}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{lipsum}

\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\input{../../macros/organization_macros.tex}
\input{../../macros/general_macros.tex}
\input{../../macros/tc_macros.tex}
\input{../../macros/tikz_macros.tex}

\pretolerance=500
\tolerance=100
\emergencystretch=10pt

% Bibliography
\DeclareUnicodeCharacter{FB01}{fi}
\usepackage[round]{natbib}
\usepackage{wasysym}
\usepackage{adjustbox}


\newcommand{\red}[1]{\textcolor{red}{#1}}

\begin{document}
    \title{Representation of Discrete Maximum Entropy Distributions as Tensor Networks}

    \maketitle
    \date{\today}

    \begin{abstract}
        We here summarize results of the main report on maximum entropy distributions.
        The principle of maximum entropy serves as motivation for Computation Activation Networks.
        We then restrict to these distributions and study relative entropy minimization problems.

        In the report, first discussion on the mean polytope are in Chapter~3, and the general maximum entropy problem in Chapter~4.
        Chapter~8 contains the discussion on maximum entropy distributions in case of boolean statistics.
    \end{abstract}


    \section{Contents}

    We in this paper provide tensor network representations
    \begin{itemize}
        \item Representation of any distribution with a sufficient statistics: Generic activation tensors.
        \item Representation of distributions with maximum entropy, in case of positive realizability: Elementary activation tensors
        \item Representation of generic maximum entropy distributions: CP activation tensors.
    \end{itemize}

    Now, we want to characterize the CP rank of the activation tensors
    \begin{itemize}
        \item Depends on the face of the mean polytope, which contains the mean parameter
        \item We have thus a well-defined "CP rank" of faces
        \item Largest faces and vertices have always CP rank of 1, intermediate faces can have larger CP rank
    \end{itemize}

    For boolean statistics we further provide insights for boolean statistics (see Chapter~8.5):
    \begin{itemize}
        \item Example of independent statistics (see Exa.~8.28): Always elementary activation tensors (hypercubes)
        \item Example of partition statistics (see Exa.~8.30):
        \item Generic criterion for elementary activation: "Cube-like" polytopes (see Def.~8.29)
    \end{itemize}


    \section{Motivation}

    Consider a learning problem where we want to estimate a model based on observed data.
    The maximum entropy problem principle approaches this problem by designing statistics of the data, which means shall be reproduced in the model, and choosing the model reproducing the means of the statistic with least structure.
    The entropy of a distribution quantifies the degree of structureless in a distribution and is therefore maximized to solve the learning task.


    \section{The Maximum Entropy Problem}

    The mean parameter of a distribution $\probwith$ to a statistic $\sstat:\facstates\rightarrow\selstates$ is the vector $\meanparamwith\in\rr^\seldim$ with the coordinates
    \begin{align*}
        \meanparamat{\indexedselvariable} = \expectationof{\enumformula} = \contraction{\probwith,\enumformulaat{\shortcatvariables}} \, .
    \end{align*}

    We express the computation of the mean parameter in the contraction of the selection encoding $\sencsstatwith$ of $\sstat$
    \begin{align*}
        \meanparamwith = \contractionof{\probwith,\sencsstatwith}{\selvariable} \, .
    \end{align*}

    The maximum entropy problem given a mean parameter $\genmeanwith$ is
    \begin{align*}
        \max_{\probwith\in\bmrealprobof{\basemeasure}} \sentropyof{\probwith} \stspace \contractionof{\probwith,\sencsstatwith}{\selvariable} = \genmeanwith
    \end{align*}

    A quick argument shows, that maximum entropy distributions always have $\sstat$ as a sufficient statistics.

    \begin{theorem}
        \label{the:maxEntWithSufficientStatistic}
        Any maximum entropy distribution with respect to a moment constraint on $\sstat$ and a base measure $\basemeasure$ has the sufficient statistic $\sstat$.
    \end{theorem}
    \begin{proof}
        Let $\probwith$ be a feasible distribution for the maximum entropy problem, which does not have a sufficient statistic $\sstat$.
        Then we find $\shortcatindices,\tildeshortcatindices\in\facstates$ with $\shortcatindices\neq\tildeshortcatindices$, $\sstatat{\shortcatindices}=\sstatat{\tildeshortcatindices}$, $\basemeasureat{\indexedshortcatvariables}\neq0$, $\basemeasureat{\shortcatvariables=\tildeshortcatindices}$ and $\probat{\indexedshortcatvariables}\neq\probat{\shortcatvariables=\tildeshortcatindices}$.
        We then define a distribution $\secprobat{\shortcatvariables}$ coinciding with $\probwith$ except for the coordinates $\shortcatindices,\tildeshortcatindices$, where we set
        \begin{align*}
            \secprobat{\indexedshortcatvariables} = \secprobat{\shortcatvariables=\tildeshortcatindices} = \frac{\probat{\indexedshortcatvariables}+\secprobat{\shortcatvariables=\tildeshortcatindices}}{2}
        \end{align*}
        We notice that $\secprobat{\shortcatvariables}$ is also a feasible distribution with an larger entropy than $\probwith$.
        Therefore, a distribution which does not have the sufficient statistic $\sstat$ cannot be a maximum entropy distribution.
    \end{proof}

    This shows that any maximum entropy distribution is in $\realizabledistsof{\sstat,\maxgraph}$, where $\maxgraph$ is the maximal hypergraph $\maxgraph=([\seldim],\{[\seldim]\})$.
    We search for sparse representations of the corresponding activation tensors and investigate in which cases the maximum entropy distribution is also in $\realizabledistsof{\sstat,\graph}$ for sparser hypergraphs $\graph$.


    \section{Preparation}

    To prepare for the presentation of our main results we introduce
    \begin{itemize}
        \item \ComputationActivationNetworks{}: A tensor network architecture, which will be used to represent maximum entropy distributions
        \item Mean polytopes: Polytopes, which contain all realizable mean parameter vectors.
    \end{itemize}
    We will then show, that dependent on the position of the mean parameter in the mean polytope, we can characterize the corresponding maximum entropy distribution by a \ComputationActivationNetwork{}.

    \subsection{\ComputationActivationNetworks{}}

    Given a statistic $\sstat:\facstates\rightarrow\selstates$ we build its basis encoding tensor
    \begin{align*}
        \sstatccwith = \sum_{\shortcatindicesin} \onehotmapofat{\sstat(\shortcatindices)}{\headvariables} \otimes \onehotmapofat{\shortcatindices}{\shortcatvariables} \, .
    \end{align*}
    A computation network is any representation of $\sstatccwith$ as a tensor network.
    These can be constructed in the case statistics being a composition of connective functions.

    An activation tensor is $\hypercoreat{\headvariables}$ and the Computation Activation Network of $\sstat$ and $\hypercore$ the tensor
    \begin{align*}
        \probwith = \normalizationof{\sstatccwith,\hypercoreat{\headvariables}}{\shortcatvariables} \, .
    \end{align*}

    We are interested in decomposition formats of $\hypercoreat{\headvariables}$, where we use sets of tensor networks $\tnsetof{\graph}$ on a hypergraph $\graph$.
    The family of by $\sstat$ and a $\graph$ computable distributions are
    \begin{align*}
        \realizabledistsof{\sstat,\graph}
        = \left\{ \normalizationof{\bencodingofat{\sstat}{\sstatcatof{[\seldim]},\shortcatvariables},\hypercoreat{\headvariableof{\nodes}}
        }{\shortcatvariables}
        \wcols \hypercoreat{\headvariableof{\nodes}} \in \tnsetof{\graph} \right\} \, .
    \end{align*}

    \subsection{The mean polytope}

    The mean polytope is the set of mean parameters to any distribution.
    We define it
    \begin{align*}
        \genmeanset
        = \left\{\contractionof{\probtensor,\sencsstat,\basemeasure}{\selvariable}\wcols\probwith\in\bmrealprobof{\basemeasure} \right\} \, ,
    \end{align*}
    where we denote by $\bmrealprobof{\basemeasure}$ the set of all probability distributions representable with respect to $\basemeasure$.

    \begin{center}
        \input{../../PartI/tikz_pics/probability_reasoning/meanset_sketch_generic.tex}
    \end{center}

    %\subsection{Convex hull and faces}

    The mean polytope is the convex hull
    \begin{align*}
        \genmeanset
        = \convhullof{\sencsstatat{\indexedshortcatvariables,\selvariable}\wcols\shortcatindices\in\facstates\ncond\basemeasureat{\indexedshortcatvariables}=1} \, .
    \end{align*}
    It is thus a convex polytope, inherited by the convex polytope of distributions (the standard simplex).
    We can characterize the maximum entropy distribution based on the position of the mean parameter in the mean polytope.
    To be more precise, any polytope decomposes into effective interiors of its faces and we characterize the maximum entropy distribution depending on the face to the mean parameter.

    To be included:
    \begin{itemize}
        \item Faces of the mean polytope are itself mean polytopes with respect to refined base measures.
        \item Existence of vectors in the image of the statistic encoding corresponds with satisfiability of the corresponding formula.
    \end{itemize}


    \section{Tensor network representation of maximum entropy distributions}

    Given the mean polytope discussion we now characterize the tensor network representation of maximum entropy distributions.

    \subsection{Maximum entropy on the interior}

    A classical result states, that the maximum entropy distribution is in the exponential family $\expfamilyof{\sstat,\basemeasure}$.

    \begin{theorem} % TODO: Use the effective interior partition of the mean polytope
        \label{the:maxEntropyInterior}
        %If the only face $\genfacesetof{\facecondset}$ of $\genmeanset$ with $\meanparam\in\genfacesetof{\facecondset}$ is $\genmeanset$ itself
        If and only if $\genmean$ is in the effective interior of $\genmeanset$, then the unique solution of the maximum entropy problem is the distribution
        \begin{align*}
            \expdistofat{\sstat,\genmean,\basemeasure}{\shortcatvariables}\in\expfamilyof{\sstat,\basemeasure}
        \end{align*}
        with $\contractionof{\expdistofat{\sstat,\genmean,\basemeasure}{\shortcatvariables},\sencsstatwith}{\selvariable}=\genmeanat{\selvariable}$.
    \end{theorem}
    \begin{proof}
        By The~3.3 in [Wainwright and Jordan, 2008], since by assumption
        \begin{align*}
            \meanparamwith \in \sbinteriorof{\genmeanset}  \, ,
        \end{align*}
        there is a canonical parameter $\canparam$ with
        \begin{align*}
            \contractionof{\expdistofat{\sstat,\canparam,\basemeasure}{\shortcatvariables},\sencsstatwith}{\selvariable}=\meanparamat{\selvariable} \, .
        \end{align*}

        For any other feasible distribution $\secprobat{\shortcatvariables}$ we also have $\contractionof{\secprobat{\shortcatvariables},\sencsstatwith}{\selvariable}=\meanparamat{\selvariable}$ and thus
        \begin{align*}
            \centropyof{\secprobtensor}{\expdistof{(\sstat,\canparam,\basemeasure)}}
            &= -\contraction{\secprobtensor,\lnof{\expdistofat{(\sstat,\canparam,\basemeasure)}{\shortcatvariables}}} \\
            &= -\contraction{\secprobtensor,\contractionof{\sencsstatwith,\canparamwith}{\shortcatvariables}} + \cumfunctionof{\canparam} \\
            &= - \contraction{\canparam,\meanparam} + \cumfunctionof{\canparam} \\
            &= \sentropyof{\expdistof{(\sstat,\canparam,\basemeasure)}} \, .
        \end{align*}
        With the Gibbs inequality we have if $\secprobtensor\neq\expdistof{(\sstat,\canparam,\basemeasure)}$
        \begin{align*}
            \sentropyof{\expdistof{(\sstat,\estcanparam,\basemeasure)}} - \sentropyof{\secprobtensor}
            = \centropyof{\secprobtensor}{\expdistof{(\sstat,\estcanparam,\basemeasure)}} - \sentropyof{\secprobtensor} > 0 \, .
        \end{align*}
        Therefore, if $\secprobtensor$ does not coincide with$\expdistof{(\sstat,\estcanparam,\basemeasure)}$, it is not a maximum entropy distribution.
    \end{proof}

    Exponential families are in $\elrealizabledistsof{\sstat}$, if and only if $\normalizationof{\basemeasure}{\shortcatvariables}\in\elrealizabledistsof{\sstat}$.
    If $\normalizationof{\basemeasure}{\shortcatvariables}\in\elrealizabledistsof{\sstat}$ and $\meanparamwith \in \sbinteriorof{\genmeanset} $ we therefore have a sparse representation of the maximum entropy distribution with elementary activation tensors.

    \subsection{Mean parameter on faces}

    We always find a unique face of the polytope with the mean parameter being in the interior.
    Any distribution reproducing the mean parameter is realizable with respect to the face measure of that face.
    We conclude that the maximum entropy distribution of $\genmeanwith$ with respect to $\sstat,\basemeasure$ is also the maximum entropy distribution

    \begin{theorem}
        \label{the:tnRepresentationMaxEntropy}
        Given $\sstat$ and $\meanparamwith\in\genmeanset$, let $\facecondset$ be the smallest face of $\genmeanset$ such that
        \begin{align*}
            \meanparamat{\selvariable} \in \genfaceset \, .
        \end{align*}
        Then the corresponding maximum entropy distribution is in $\realizabledistsof{\sstat,\graph,\basemeasure}$ if and only if the face measure (see \defref{def:faceMeasure})
        \begin{align*}
            \kcoreofat{\facecondset}{\headvariables}
            = \sum_{\meanparam\in\genfacesetof{\facecondset}\cap\imageof{\sstatencoding}} \onehotmapofat{\meanparam}{\headvariables}
        \end{align*}
        is in $\tnsetof{\graph}$.
    \end{theorem}
    \begin{proof}
        By \theref{the:maxEntropyFace} the maximum entropy distribution is an element of the exponential family with by the face measure refined base measure $\secbasemeasure$.
        Let $\canparamwith$ be a canonical parameter such that
        \begin{align*}
            \contractionof{\sencsstatwith,\probofat{\sstat,\canparam,\secbasemeasure}{\shortcatvariables}}{\selvariable} = \meanparamwith \, ,
        \end{align*}
        that is $\probofat{\sstat,\canparam,\secbasemeasure}{\shortcatvariables}$ is the maximum entropy distribution.
        We apply \theref{the:faceMeasureCharacterization} to represent the face measure by
        \begin{align*}
            \basemeasureofat{\sstat,\facecondset}{\shortcatvariables} =
            \contractionof{\bencodingofat{\sstat}{\headvariables,\shortcatvariables},\kcoreofat{\facecondset}{\headvariables}}{\shortcatvariables}
        \end{align*}
        Then for the tensor
        \begin{align*}
            \hypercoreat{\headvariables}
            = \contractionof{
                \{\softactlegwith\wcols\selindexin\}
                \cup\{\kcoreofat{\facecondset}{\headvariables}\}}{\headvariables}
        \end{align*}
        we have
        \begin{align*}
            \probofat{\sstat,\canparam,\secbasemeasure}{\shortcatvariables}
            = \normalizationof{
                \bencodingofat{\sstat}{\headvariables,\shortcatvariables}, \hypercoreat{\headvariables}, \basemeasurewith
            }{\shortcatvariables} \, .
        \end{align*}
        Thus, the maximum entropy distribution is in $\realizabledistsof{\sstat,\graph,\basemeasure}$, if $\hypercore$ admits a tensor network decomposition with respect to $\graph$.
        Since the hard activation cores are elementary, this is the case when $\kcoreof{\facecondset}$ admits a tensor network decomposition with respect to $\graph$.
    \end{proof}

    \begin{figure}[t]
        \begin{center}
            \input{../../PartI/tikz_pics/probability_reasoning/max_entropy_actcore}
        \end{center}
        \caption{
            Tensor network decomposition of maximum entropy distributions to the constraint $\meanparamat{\selvariable}=\contractionof{\probtensor,\sencsstat}{\selvariable}$.
            Blue: Constraint activation cores $\hardactsymbolof{\selindex}$ in a $\cpformat$ decomposition, representing the face measure to the minimal face, such that $\meanparam\in\genfacesetof{\facecondset}$.
            Red: Probabilistic activation cores $\softactlegwith$ in an elementary decomposition, where each leg core is a scaled exponentials evaluated on the enumerated image $\imageof{\sstatcoordinateof{\selindex}}$.
        }\label{fig:maxEntropyActcore}
    \end{figure}


    \section{Characterization for boolean statistics}

    For boolean statistics $\hlnstat:\facstates\rightarrow\bigtimes_{\selindexin}[2]$ the mean polytope is a subset of the cube $[0,1]^\seldim$.
    In this case, any boolean vector in $\meansetof{\hlnstat,\basemeasure}$ is a vertex.
    It follows, that any distribution reproducing a mean parameter $\meanparamwith$ on the effective interior of $\meansetof{\hlnstat,\basemeasure}$ is positive with respect to $\basemeasure$.

    We apply the exponential distribution characterization of the maximum entropy distribution and get that the maximum entropy distribution is in $\elrealizabledistsof{\sstat}$, if and only if the face measure is in $\elrealizabledistsof{\sstat}$.
    This is exactly the case, when the face is an intersection of the mean polytope with a face of the cupe $[0,1]^\seldim$.

    The mean parameters, which can be realized by a distribution in $\elrealizabledistsof{\hlnstat}$ are those, which are on the effective interior of the intersection of the mean polytope with a face of the cube.


    \subsection{Example}

    \input{examples/nonel_hlnstat_maxent.tex}



\end{document}