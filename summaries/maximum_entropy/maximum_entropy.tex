\documentclass[aps,onecolumn,nofootinbib,pra]{article}

\usepackage{../../article_compilation/spec_files/arxiv}
\usepackage{amsmath,amsfonts,amssymb,amsthm,bbm,graphicx,enumerate,times}
\usepackage{mathtools}
\usepackage[usenames,dvipsnames]{color}
\usepackage{hyperref}
\hypersetup{
    breaklinks,
    colorlinks,
    linkcolor=gray,
    citecolor=gray,
    urlcolor=gray,
    pdftitle={},
    pdfauthor={Alex Goessmann}
}



\usepackage{tikz}
\usepackage{graphicx}
\usepackage{float}
\usepackage{comment}
\usepackage{csquotes}

\usepackage{listings}
\usepackage{verbatim}
\usepackage{etoolbox}
\usepackage{braket}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{bbm}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{lipsum}

\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\input{../../macros/organization_macros.tex}
\input{../../macros/general_macros.tex}
\input{../../macros/tc_macros.tex}
\input{../../macros/tikz_macros.tex}

\pretolerance=500
\tolerance=100
\emergencystretch=10pt

% Bibliography
\DeclareUnicodeCharacter{FB01}{fi}
\usepackage[round]{natbib}
\usepackage{wasysym}
\usepackage{adjustbox}


\newcommand{\red}[1]{\textcolor{red}{#1}}

\begin{document}
    \title{Representation of Discrete Maximum Entropy Distributions as Tensor Networks}

    \maketitle
    \date{\today}

    \begin{abstract}
        We here summarize results of the main report on maximum entropy distributions.
        The principle of maximum entropy serves as motivation for Computation Activation Networks.
        We then restrict to these distributions and study relative entropy minimization problems.

        In the report, first discussion on the mean polytope are in Chapter~3, and the general maximum entropy problem in Chapter~4.
        Chapter~8 contains the discussion on maximum entropy distributions in case of boolean statistics.
    \end{abstract}

    \tableofcontents


    \section{Contents}

    We in this paper provide tensor network representations
    \begin{itemize}
        \item Representation of any distribution with a sufficient statistics: Generic activation tensors.
        \item Representation of distributions with maximum entropy, in case of positive realizability: Elementary activation tensors
        \item Representation of generic maximum entropy distributions: CP activation tensors.
    \end{itemize}

    Now, we want to characterize the CP rank of the activation tensors
    \begin{itemize}
        \item Depends on the face of the mean polytope, which contains the mean parameter
        \item We have thus a well-defined "CP rank" of faces
        \item Largest faces and vertices have always CP rank of 1, intermediate faces can have larger CP rank
    \end{itemize}

    For boolean statistics we further provide insights for boolean statistics (see Chapter~8.5):
    \begin{itemize}
        \item Example of independent statistics (see Exa.~8.28): Always elementary activation tensors (hypercubes)
        \item Example of partition statistics (see Exa.~8.30):
        \item Generic criterion for elementary activation: "Cube-like" polytopes (see Def.~8.29)
    \end{itemize}


    \section{Motivation}

    Consider a learning problem where we want to estimate a model based on observed data.
    The maximum entropy problem principle approaches this problem by designing statistics of the data, which means shall be reproduced in the model, and choosing the model reproducing the means of the statistic with least structure.
    The entropy of a distribution quantifies the degree of structureless in a distribution and is therefore maximized to solve the learning task.


    \section{The Maximum Entropy Problem}

    The mean parameter of a distribution $\probwith$ to a statistic $\sstat:\facstates\rightarrow\selstates$ is the vector $\meanparamwith\in\rr^\seldim$ with the coordinates
    \begin{align*}
        \meanparamat{\indexedselvariable} = \expectationof{\enumformula} = \contraction{\probwith,\enumformulaat{\shortcatvariables}} \, .
    \end{align*}

    We express the computation of the mean parameter in the contraction of the selection encoding $\sencsstatwith$ of $\sstat$
    \begin{align*}
        \meanparamwith = \contractionof{\probwith,\sencsstatwith}{\selvariable} \, .
    \end{align*}

    The maximum entropy problem given a mean parameter $\genmeanwith$ is
    \begin{equation}\tag{$\mathrm{P}_{\sstat,\meanparam,\basemeasure}$}
        \max_{\probwith\in\bmrealprobof{\basemeasure}} \sentropyof{\probwith} \stspace \contractionof{\probwith,\sencsstatwith}{\selvariable} = \genmeanwith
    \end{equation}
    where $\bmrealprobof{\basemeasure}$ is the set of distributions, which are representable with respect to the base measure $\basemeasure$.

    A quick argument shows, that maximum entropy distributions always have $\sstat$ as a sufficient statistics.

    \begin{theorem}
        \label{the:maxEntWithSufficientStatistic}
        Any maximum entropy distribution with respect to a moment constraint on $\sstat$ and a base measure $\basemeasure$ has the sufficient statistic $\sstat$.
    \end{theorem}
    \begin{proof}
        Let $\probwith$ be a feasible distribution for the maximum entropy problem, which does not have a sufficient statistic $\sstat$.
        Then we find $\shortcatindices,\tildeshortcatindices\in\facstates$ with $\shortcatindices\neq\tildeshortcatindices$, $\sstatat{\shortcatindices}=\sstatat{\tildeshortcatindices}$, $\basemeasureat{\indexedshortcatvariables}\neq0$, $\basemeasureat{\shortcatvariables=\tildeshortcatindices}$ and $\probat{\indexedshortcatvariables}\neq\probat{\shortcatvariables=\tildeshortcatindices}$.
        We then define a distribution $\secprobat{\shortcatvariables}$ coinciding with $\probwith$ except for the coordinates $\shortcatindices,\tildeshortcatindices$, where we set
        \begin{align*}
            \secprobat{\indexedshortcatvariables} = \secprobat{\shortcatvariables=\tildeshortcatindices} = \frac{\probat{\indexedshortcatvariables}+\secprobat{\shortcatvariables=\tildeshortcatindices}}{2}
        \end{align*}
        We notice that $\secprobat{\shortcatvariables}$ is also a feasible distribution with an larger entropy than $\probwith$.
        Therefore, a distribution which does not have the sufficient statistic $\sstat$ cannot be a maximum entropy distribution.
    \end{proof}

    This shows that any maximum entropy distribution is in $\realizabledistsof{\sstat,\maxgraph}$, where $\maxgraph$ is the maximal hypergraph $\maxgraph=([\seldim],\{[\seldim]\})$.
    We search for sparse representations of the corresponding activation tensors and investigate in which cases the maximum entropy distribution is also in $\realizabledistsof{\sstat,\graph}$ for sparser hypergraphs $\graph$.

    %% OUTLOOK
    \subsection{Outlook}

    To prepare for the presentation of our main results we introduce
    \begin{itemize}
        \item \ComputationActivationNetworks{}: A tensor network architecture, which will be used to represent maximum entropy distributions
        \item Mean polytopes: Polytopes, which contain all realizable mean parameter vectors.
    \end{itemize}
    We will then show, that dependent on the position of the mean parameter in the mean polytope, we can characterize the corresponding maximum entropy distribution by a \ComputationActivationNetwork{}.

    \input{./sections/tensor_notation.tex}

    \input{./sections/mean_polytope.tex}

    \input{./sections/main_results.tex}

    \input{./sections/boolean_statistics.tex}


\end{document}