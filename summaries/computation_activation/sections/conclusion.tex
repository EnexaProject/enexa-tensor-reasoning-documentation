\section{Conclusion\& Outlook}

\begin{itemize}
    \item Conclusion: tensor representation close mathematical structures -> strength to do analysis? (Janina)
    \item Contraction algorithms (Alex)
    \item Max: Combine with foundation models / LLMs for agentic models (Max)
\end{itemize}


% CONTRACTION
This work has treated the representation of several models in tensor networks.
Model inference such as the computation of marginal distributions and the decision of entailment are formulated by tensor network contractions.
These contraction can become bottlenecks, which are known as
\begin{itemize}
    \item tree-widths of graphical models \cite{pearl_probabilistic_1988}
    \item intractability of generic logical reasoning \cite{russell_artificial_2021}
\end{itemize}
Approximation schemes can be derived based on variational inference \cite{wainwright_graphical_2008}, such as loopy message passing schemes and mean field methods.
Further frequenctly applied schemes are particle-based inference schemes such as Gibbs sampling.

Message-passing schemes appear in particular as belief propagation in probability theory and syntactial inference algorithms in logics.
We can understand them as approximation of (potentially intractible) contractions and will dedicate future work to study them in the \tnreason formalism.