\section{\HybridLogicNetworks{}}\label{sec:hyb_rep}
% from \charef{cha:networkRepresentation} (Chapter 8: Hybrid logic representation):
% \begin{itemize}
%     \item Unification of both frameworks by allowing for arbitrary elementary activation tensors
%     \item Entailment decision by hard parts (Theorem 8.12)
%     \item Optional: Mean parameter polytope, hard parameters by corresponding face measures
%     \item Optional: Extension by $\cpformat$ activation tensors
% \end{itemize}

% from \charef{cha:networkReasoning} (Chapter 9: Hybrid logic inference):
% \begin{itemize}
%     \item Optional (since a learning method): Alternating Moment Matching for maximum likelihood estimation
%     \item Optional: Structure learning
% \end{itemize}

Logical and probabilistic reasoning can be unified in a single tensor-network architecture called a Hybrid Logic Network (HLN).

In Markov logic networks, each state gets assigned a real-valued probability based on the basis encoding of a boolean statistic as a computation network and assigned probabilities of these features in terms of a positive elementary activation network.

\begin{definition}[\MarkovLogicNetwork]
    \label{def:mln} A
    \MarkovLogicNetworks{} is an element of and exponential family $\mlnexpfamily$ with sufficient statistic defined coordinatewise by propositional formulas $f_\ell$
    \begin{align*}
        \hlnstat \defcols\atomstates \rightarrow \bigtimes_{\selindexin}[2] \subset \rr^{p}, \quad \shortcatindices \mapsto (f_\ell[\shortcatvariables=\shortcatindices])_{\ell\in[d]}.
    \end{align*}
\end{definition}
That means, they have the form
\begin{align*}
    \mathbb{P}^{(\mathcal{F},\theta,\mathbb{I})}[{\shortcatvariables}]
    %&= \normalizationof{ \{\bencodingofat{f_{\selindex}}{\headvariableof{\selindex},\shortcatvariables} \wcols \selindexin\}\cup\{\softactlegwith \wcols \selindexin\}}{\shortcatvariables} \, \\
    &= \normalizationof{\hlnstatccwith,\alpha^\theta[Y_{[d]}]}{\shortcatvariables}
\end{align*}
with $\alpha^\theta[Y_{[d]}]=\bigotimes_{\selindexin} \alpha^{\ell,\theta}[Y_\ell]$ with
\begin{align*}
    \softactleg\left[\indexedheadvariableof{\selindex}\right]
    &= \expof{\canparamat{\indexedselvariable} \cdot \indexinterpretationofat{\selindex}{\headindexof{\selindex}} }
\end{align*}
% \begin{align*}
%     \mathbb{P}^{(\mathcal{F},\theta,\mathbb{I})}[{\shortcatvariables}]
%     = \normalizationof{\expof{\contractionof{\sigma^{\mathcal{F}}[{\shortcatvariables,\selvariable}],\canparamwith}{\shortcatvariables}},\mathbb{I}}{\shortcatvariables},
% \end{align*}
where $\theta[L]\in\mathbb{R}^{p}$, $I$ is an interpretation map.
The architecture is visualized in Figure~\ref{fig:mlnFactor}.
\begin{figure}[t]
    \begin{center}
        \input{../tikz_pics/hybrid_network_representation/factor.tex}
    \end{center}
    \caption{Factor of a \MarkovLogicNetwork{} to a formula $\enumformula$, represented as the contraction of a computation core $\enumformulacc$ and an activation core $\softactleg$.
    While the computation core $\enumformulacc$ prepares based on basis calculus a categorical variable representing the value of the statistic formula $\enumformula$ dependent on assignments to the distributed variables, the activation core multiplies an exponential weight to coordinates satisfying the formula.
    }
    \label{fig:mlnFactor}
\end{figure}

In Hard Logic Networks, on the other hand, states only get assigned true or false values $0/1$.
Basis encodings of propositional formulas can be interpreted as a CAN, where the basis encodings build the computation network and the boolean elementary activation network decides, which outcomes of the statistic are possible/ get assigned a true value.

\begin{definition}[\HardLogicNetwork{}]
    \label{def:hardLogicNetwork}
    Given a boolean statistic $\hlnstat:\bigtimes_{k\in[d]}[2]\to [2]^p$, a subset $\hardlegset\subset[\seldim]$ and a tuple $\headindexof{\hardlegset}\in\bigtimes_{\selindex\in\hardlegset}[2]$ the \HardLogicNetwork{} is the distribution
    \begin{align*}
        \probofat{\hlnstat,\hardparam}{\shortcatvariables}
        = \normalizationof{\hlnstatccwith,\hardacttensorwith}{\shortcatvariables}
    \end{align*}
    where $\hardacttensorwith=\bigotimes_{\selindexin}\hardactlegwith$ and for $\selindexin$
    \begin{align*}
        \hardactlegwith =
        \begin{cases}
            \onehotmapofat{\headindexof{\selindex}}{\headvariableof{\selindex}} & \ifspace \selindex\in\hardlegset  \\
            \onesat{\headvariableof{\selindex}} & \ifspace \selindex\notin\hardlegset \\
        \end{cases} \, ,
    \end{align*}
    provided that the normalization exists.
\end{definition}
Note that this distribution is uniform on its support.
Combining the \HardLogicNetwork{} and the \MarkovLogicNetwork{} leads to the representation of a probability density only over states, that fulfill an imposed hard logic, where the elementary activation tensors are allowed to take binary or real values.

\begin{definition}[\HybridLogicNetwork{}]
    \label{def:hybridLogicNetwork}
    Given a boolean statistic $\hlnstat$ we call any element of $\elrealizabledistsof{\hlnstat}$ a \HybridLogicNetwork{} (HLN).
    The extended canonical parameter set to $\hlnstat$ is the set
    \begin{align*}
        \hybridparamset\coloneqq
        \{\hardparam)\wcols \variableset\subset[\seldim]\ncond \headindexof{\variableset}\in\bigtimes_{\selindex\in\variableset}[2]\} \times \parspace \, .
    \end{align*}
    To each \HybridLogicNetwork{} $\hlnwith$ we find a tuple $\hybridparam$ consistent of a subset $\hardlegset\subset[\seldim]$, a tuple $\headindexof{\hardlegset}\in\bigtimes_{\selindex\in\hardlegset}[2]$ and $\canparamwithin$ such that
    \begin{align*}
        \hlnwith
        = \normalizationof{\hlnstatccwith,\paracttensorwith}{\shortcatvariables}
    \end{align*}
    where the activation core is
    \begin{align*}
        \paracttensorwith = \contractionof{\softacttensorwith,\hardacttensorwith}{\headvariables} \, .
    \end{align*}
\end{definition}

The architecture is demonstrated on an example in accounting logic.

\input{../examples/hln_accounting}

Also entailment can be checked for \HybridLogicNetwork{}. Assuming a positive probability of all models of the integrated \HardLogicNetwork{}, the entailment can be checked only considering the \HardLogicNetwork{}. Here a query is a formula to retrieve information from a given network.



\begin{theorem}[{~\cite[Theorem 8.12]{goessmann2025}}]
    Let $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)} [\catvariableof{[d]}]$ be a \HybridLogicNetwork{}. Given a query formula $g$, we have that $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)}[\catvariableof{[d]}]\models~g$ if and only if
    \begin{align*}
        f^{\mathcal{F},(A,y_A)}\models g,
    \end{align*}
    where the tuple $(A,y_A)$ denote the hard logic part of the network and
    \begin{align*}
        \hlnformula[\catvariableof{[d]}] =
        \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
        \land
        \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=0} \lnot\enumformulaat{\shortcatvariables}\right).
    \end{align*}
\end{theorem}

\begin{example}{Entailment for Accounting Logic}
    To check entailment of the query formula defined by
    \begin{align*}
        g[X_{A_1}=0,X_{A_2}=1,X_F=1] = 1
    \end{align*}
    and is set to zero otherwise, entailment can be checked by contracting
    \begin{align*}
        \hlnformula[\catvariableof{A_1},\catvariableof{A_2},\catvariableof{F}] = \catvariableof{A1} \oplus \catvariableof{A2} \otimes \mathbb{I}
    \end{align*}
    with $g$ arriving at
    \begin{align*}
        &\langle  \hlnformula[\catvariableof{A_1},\catvariableof{A_2},\catvariableof{F}], \lnot g[X_{A_1},X_{A_2},X_F]  \rangle \\
        &= \sum_{x_{A_1},x_{A_2},x_{F} \in \{0,1\}} \hspace{-2ex}\hlnformula[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f]  (1-g[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f])\\
        &= \sum_{\substack{x_{A_1},x_{A_2},x_{F} \in \{0,1\}\\ (x_{A_1},x_{A_2},x_{F})\neq (0,1,1)}} \hspace{-2ex}\hlnformula[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f] \\
        &\geq \hlnformula[X_{A_1}=1,X_{A_2}=0,X_F=0] > 0.
    \end{align*}
    Therefore, $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)}[\catvariableof{[d]}]$ does not entail $g$. In this case, this is due to the fact, that $g$ only assumes one model, despite the formula having multiple models.
    Doing the same calculations for
    \begin{align*}
        \tilde g[X_{A_1}=1,X_{A_2}=1,X_F=1] = 0
    \end{align*}
    and equal to $1$ everywhere else leads to the constraction being equal to zero. Therefore, $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)}[\catvariableof{[d]}]$ does entail $\tilde g$.
\end{example}