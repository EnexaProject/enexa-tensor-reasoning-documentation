 \section{\HybridLogicNetworks{}}\label{sec:hyb_rep}
    % from \charef{cha:networkRepresentation} (Chapter 8: Hybrid logic representation):
    % \begin{itemize}
    %     \item Unification of both frameworks by allowing for arbitrary elementary activation tensors
    %     \item Entailment decision by hard parts (Theorem 8.12)
    %     \item Optional: Mean parameter polytope, hard parameters by corresponding face measures
    %     \item Optional: Extension by $\cpformat$ activation tensors
    % \end{itemize}

    % from \charef{cha:networkReasoning} (Chapter 9: Hybrid logic inference):
    % \begin{itemize}
    %     \item Optional (since a learning method): Alternating Moment Matching for maximum likelihood estimation
    %     \item Optional: Structure learning
    % \end{itemize}

Logical and probabilistic reasoning can be unified in a single tensor-network architecture called a Hybrid Logic Network (HLN). 

In Markov logic networks, each state gets assigned a real-valued probability based on the basis encoding of a boolean statistic as a computation network and assigned probabilities of these features in terms of a positive elementary activation network. 

\begin{definition}[\MarkovLogicNetwork]
    \label{def:mln} A
    \MarkovLogicNetworks{} is an element of and exponential family $\mlnexpfamily$ with sufficient statistic defined coordinatewise by propositional formulas $f_\ell$
    \begin{align*}
        \hlnstat \defcols\atomstates \rightarrow \bigtimes_{\selindexin}[2] \subset \rr^{p}, \quad \shortcatindices \mapsto (f_\ell[\shortcatvariables=\shortcatindices])_{\ell\in[d]}.
    \end{align*}
\end{definition}
That means, they have the form
\begin{align*}
    \mathbb{P}^{(\mathcal{F},\theta,\mathbb{I})}[{\shortcatvariables}]
    %&= \normalizationof{ \{\bencodingofat{f_{\selindex}}{\headvariableof{\selindex},\shortcatvariables} \wcols \selindexin\}\cup\{\softactlegwith \wcols \selindexin\}}{\shortcatvariables} \, \\
    &= \normalizationof{\hlnstatccwith,\alpha^\theta[Y_{[d]}]}{\shortcatvariables}
\end{align*}
with $\alpha^\theta[Y_{[d]}]=\bigotimes_{\selindexin} \alpha^{\ell,\theta}[Y_\ell]$ with
\begin{align*}
    \softactleg\left[\indexedheadvariableof{\selindex}\right]
    &= \expof{\canparamat{\indexedselvariable} \cdot \indexinterpretationofat{\selindex}{\headindexof{\selindex}} }
\end{align*}
% \begin{align*}
%     \mathbb{P}^{(\mathcal{F},\theta,\mathbb{I})}[{\shortcatvariables}]
%     = \normalizationof{\expof{\contractionof{\sigma^{\mathcal{F}}[{\shortcatvariables,\selvariable}],\canparamwith}{\shortcatvariables}},\mathbb{I}}{\shortcatvariables},
% \end{align*}
where $\theta[L]\in\mathbb{R}^{p}$, $I$ is an interpretation map.
The architecture is visualized in Figure~\ref{fig:mlnFactor}.
\begin{figure}[t]
    \begin{center}
        \input{../tikz_pics/hybrid_network_representation/factor.tex}
    \end{center}
    \caption{Factor of a \MarkovLogicNetwork{} to a formula $\enumformula$, represented as the contraction of a computation core $\enumformulacc$ and an activation core $\softactleg$.
    While the computation core $\enumformulacc$ prepares based on basis calculus a categorical variable representing the value of the statistic formula $\enumformula$ dependent on assignments to the distributed variables, the activation core multiplies an exponential weight to coordinates satisfying the formula.
    }
    \label{fig:mlnFactor}
\end{figure}

In Hard Logic Networks, on the other hand, states only get assigned true or false values $0/1$.
Basis encodings of propositional formulas can be interpreted as a CAN, where the basis encodings build the computation network and the boolean elementary activation network decides, which outcomes of the statistic are possible/ get assigned a true value.

\begin{definition}[\HardLogicNetwork{}] 
    \label{def:hardLogicNetwork}
    Given a boolean statistic $\hlnstat:\bigtimes_{k\in[d]}[2]\to [2]^p$, a subset $\hardlegset\subset[\seldim]$ and a tuple $\headindexof{\hardlegset}\in\bigtimes_{\selindex\in\hardlegset}[2]$ the \HardLogicNetwork{} is the distribution
    \begin{align*}
        \probofat{\hlnstat,\hardparam}{\shortcatvariables}
        = \normalizationof{\hlnstatccwith,\hardacttensorwith}{\shortcatvariables}
    \end{align*}
    where $\hardacttensorwith=\bigotimes_{\selindexin}\hardactlegwith$ and for $\selindexin$
    \begin{align*}
        \hardactlegwith =
        \begin{cases}
            \onehotmapofat{\headindexof{\selindex}}{\headvariableof{\selindex}} & \ifspace \selindex\in\hardlegset  \\
            \onesat{\headvariableof{\selindex}} & \ifspace \selindex\notin\hardlegset \\
        \end{cases} \, ,
    \end{align*}
    provided that the normalization exists. 
\end{definition}
Note that this distribution is uniform on its support.
Combining the \HardLogicNetwork{} and the \MarkovLogicNetwork{} leads to the representation of a probability density only over states, that fulfill an imposed hard logic, where the elementary activation tensors are allowed to take binary or real values.

\begin{definition}[\HybridLogicNetwork{}]
    \label{def:hybridLogicNetwork}
    Given a boolean statistic $\hlnstat$ we call any element of $\elrealizabledistsof{\hlnstat}$ a \HybridLogicNetwork{} (HLN).
    The extended canonical parameter set to $\hlnstat$ is the set
    \begin{align*}
        \hybridparamset\coloneqq
        \{\hardparam)\wcols \variableset\subset[\seldim]\ncond \headindexof{\variableset}\in\bigtimes_{\selindex\in\variableset}[2]\} \times \parspace \, .
    \end{align*}
    To each \HybridLogicNetwork{} $\hlnwith$ we find a tuple $\hybridparam$ consistent of a subset $\hardlegset\subset[\seldim]$, a tuple $\headindexof{\hardlegset}\in\bigtimes_{\selindex\in\hardlegset}[2]$ and $\canparamwithin$ such that
    \begin{align*}
        \hlnwith
        = \normalizationof{\hlnstatccwith,\paracttensorwith}{\shortcatvariables}
    \end{align*}
    where the activation core is
    \begin{align*}
        \paracttensorwith = \contractionof{\softacttensorwith,\hardacttensorwith}{\headvariables} \, .
    \end{align*}
\end{definition}

The architecture is demonstrated on an example in accounting logic.

\begin{example}{\HybridLogicNetwork{} for Accounting Logic}

Let us consider a system of three variables $A1$ Account 1 is booked, $A2$ Account 2 is booked, $F$ a feature on an invoice.
We respect two rules
\begin{itemize}
    \item \textcolor{\concolor}{Exactly one account must be booked.}
    \item \textcolor{\probcolor}{If feature $\mathrm{F}$ is present on the invoice, the account $\mathrm{A1}$ is typically booked.}
\end{itemize}
We formalize this with the statistic
\begin{align*}
    \hlnstat = (\catvariableof{A1} \oplus \catvariableof{A2}, \catvariableof{F}\Rightarrow \catvariableof{A1})\, .
\end{align*}
While the first formula is a hard feature, the second is soft since prone to exceptions.
We parameterize the first output of the statistic with the hard parameters by setting the set of indices to be initialized with hard logic $A = \{0\}$ and the corresponding initialization $y_0 = 1$ meaning, that the first output of the statistic has to be true for the input to have positive probability.
% \begin{align*}
%     \variableset = \{0\} \quad, \quad \headindexof{\variableset} = 1.
% \end{align*}
Then "hard logic activation tensor", should be indifferent to the second part of the statistic, and only impose rules on the first part, leading to
\begin{align*}
    \textcolor{\concolor}{\kappa^{(A,y_A)}[Y_0,Y_1]} = 
        \onehotmapofat{\headindexof{0}}{\headvariableof{0}} \otimes
        \onesat{\headvariableof{1}}
    =\begin{bmatrix}
        0 \\
        1
    \end{bmatrix} \otimes \begin{bmatrix}
        1\\1
    \end{bmatrix}.
\end{align*}
Since the first feature is hard, the "soft logic activation tensor" should be invariant under the first coordinate of the canonical parameter and we set $\canparamat{\selvariable=0}=0$. We choose the soft parameters as $\theta[L] = [0,\theta[L=1]]^\intercal$ to achieve
\begin{align*}
    \textcolor{\probcolor}{\alpha^\theta[Y_0,Y_1]} &= \alpha^{0,0}\left[Y_0\right]\otimes \alpha^{1,\theta[L=1]}\left[Y_1\right] = \begin{bmatrix}
        1\\1
    \end{bmatrix}\otimes \begin{bmatrix}
        1\\ \expof{\canparamat{\selvariable=1}}
    \end{bmatrix}.
    % \\
    % \canparamat{\selvariable} 
    % &= \begin{bmatrix}
    %     0 \\
    %     \canparamat{\selvariable=1}
    % \end{bmatrix} \, .
\end{align*}
The activation tensor of the hybrid network then has the from
\begin{align*}
    \paracttensor[{\headvariableof{0},\headvariableof{1}}]
    = \begin{bmatrix}
        0 \\ 1
    \end{bmatrix}
    \otimes 
    \begin{bmatrix}
        1 \\ \expof{\canparamat{\selvariable=1}} 
    \end{bmatrix}.
\end{align*}

We get a tensor network representation of the \HybridLogicNetwork{} representing the toy accounting example, before normalization to a distribution
    \begin{center}
        \input{../tikz_pics/hybrid_network_representation/hybrid_accounting_example.tex}
    \end{center}
    % (see \figref{fig:hlnAccountingExample}) 
The resulting \HybridLogicNetwork{} is a tensor $\probofat{\hlnstat,\hybridparam}{\catvariableof{A1},\catvariableof{A2},\catvariableof{F}}$ of order $3$. With $Y_{F\Rightarrow A_1}=1$ for $F=0$ and any $A_1$ it has the coordinates
\begin{align*}
    &\probofat{\hlnstat,\hybridparam}{\catvariableof{A1},\catvariableof{A2},\catvariableof{F}=0}\\
    &= \left\langle \beta^{\oplus}[Y_{A_1\oplus A_2},X_{A_2},X_{A_1}]\otimes \begin{bmatrix}
        0&0\\1&1
    \end{bmatrix}[Y_{F\Rightarrow A_1},X_{A_1}], \begin{bmatrix}
        0\\1
    \end{bmatrix}[Y_{A_1\oplus A_2}]\otimes \begin{bmatrix}
        1\\\exp[\theta[L=1]]
    \end{bmatrix}[Y_{F\Rightarrow A_1}]\right\rangle
\end{align*}
where contraction along the $Y$-variables leads to
\begin{align*}
    \probofat{\hlnstat,\hybridparam}{\catvariableof{A1},\catvariableof{A2},\catvariableof{F}=0}
    &=\beta^{\oplus}[Y_{A_1\oplus A_2}=1,X_{A_2},X_{A_1}]\otimes \begin{bmatrix}
        \exp[\theta[L=1]]\\\exp[\theta[L=1]]
    \end{bmatrix}[X_{A_1}] \\
    &=
    \frac{1}{Z} \begin{bmatrix}
        0 & \exp[\canparamat{L=1}] \\
        \exp[\canparamat{L=1}] & 0 
    \end{bmatrix}
\end{align*}
for the normalization constant $Z= 1+3 \cdot \expof{\canparamat{L=1}}$ and
\begin{align*}
    \probofat{\hlnstat,\hybridparam}{\catvariableof{A1},\catvariableof{A2},\catvariableof{F}=1}
    =
    \frac{1}{Z} \begin{bmatrix}
        0 & 1 \\
        \exp[\canparamat{L=1}] & 0 
    \end{bmatrix} \, .
\end{align*}

% \begin{figure}[h!]
%     \caption{Tensor network representation of the \HybridLogicNetwork{} representing the toy accounting example, before normalization to a distribution.}
%     \label{fig:hlnAccountingExample}
% \end{figure}
    
\end{example}

Also entailment can be checked for \HybridLogicNetwork{}. Assuming a positive probability of all models of the integrated \HardLogicNetwork{}, the entailment can be checked only considering the \HardLogicNetwork{}. Here a query is a formula to retrieve information from a given network.



\begin{theorem}[{~\cite[Theorem 8.12]{goessmann2025}}]
    Let $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)} [\catvariableof{[d]}]$ be a \HybridLogicNetwork{}. Given a query formula $g$, we have that $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)}[\catvariableof{[d]}]\models~g$ if and only if
    \begin{align*}
        f^{\mathcal{F},(A,y_A)}\models g,
    \end{align*}
    where the tuple $(A,y_A)$ denote the hard logic part of the network and
    \begin{align*}
        \hlnformula[\catvariableof{[d]}] =
        \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
        \land
        \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=0} \lnot\enumformulaat{\shortcatvariables}\right).
    \end{align*}
\end{theorem}

\begin{example}{Entailment for Accounting Logic}
    To check entailment of the query formula defined by
    \begin{align*}
        g[X_{A_1}=0,X_{A_2}=1,X_F=1] = 1
    \end{align*}
    and is set to zero otherwise, entailment can be checked by contracting
    \begin{align*}
        \hlnformula[\catvariableof{A_1},\catvariableof{A_2},\catvariableof{F}] = \catvariableof{A1} \oplus \catvariableof{A2} \otimes \mathbb{I}
    \end{align*}
    with $g$ arriving at
    \begin{align*}
        &\langle  \hlnformula[\catvariableof{A_1},\catvariableof{A_2},\catvariableof{F}], \lnot g[X_{A_1},X_{A_2},X_F]  \rangle \\
        &= \sum_{x_{A_1},x_{A_2},x_{F} \in \{0,1\}} \hspace{-2ex}\hlnformula[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f]  (1-g[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f])\\
        &= \sum_{\substack{x_{A_1},x_{A_2},x_{F} \in \{0,1\}\\ (x_{A_1},x_{A_2},x_{F})\neq (0,1,1)}} \hspace{-2ex}\hlnformula[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f] \\
        &\geq \hlnformula[X_{A_1}=1,X_{A_2}=0,X_F=0] > 0.
    \end{align*}
    Therefore, $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)}[\catvariableof{[d]}]$ does not entail $g$. In this case, this is due to the fact, that $g$ only assumes one model, despite the formula having multiple models.
    Doing the same calculations for
     \begin{align*}
        \tilde g[X_{A_1}=1,X_{A_2}=1,X_F=1] = 0
    \end{align*}
    and equal to $1$ everywhere else leads to the constraction being equal to zero. Therefore, $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)}[\catvariableof{[d]}]$ does entail $\tilde g$.
\end{example}