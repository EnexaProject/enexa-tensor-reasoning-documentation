\section{Decomposition of Probability Distributions}\label{sec:prob_rep}

We here investigate tensor network decomposition mechanisms of probability distributions.
After introducing probability distributions as tensors we derive tensor network decompositions based on conditional independencies (applying the Hammersley-Clifford theorem \cite{clifford_markov_1971}) to motivate graphical models.
Further we present the \ComputationMechanism{}, in which the Fisher-Neyman Factorization Theorem is used to decompose distributions in the presence of sufficient statistics.



\subsection{Basic concepts}

Distributions $\probtensor$ over a discrete state space can be represented by tensors, where each entry corresponds to the probability of a corresponding state. The joint probability distribution for a set of categorical variables as in definition~\ref{def:atomic-factored-representation} is defined here.

\begin{definition}[Joint Probability Distribution]
    \label{def:probabilityDistribution} % From the axioms of Kolmogorov!
    Let there be for each $\catenumeratorin$ a categorical variable $\catvariableof{\catenumerator}$ taking values in $[\catdimof{\atomenumerator}]$.
    A joint probability distribution of these categorical variables is a function
    \begin{align*}
        \probwith \defcols \facstates \rightarrow \rr
    \end{align*}
    which is non-negative, that is for any $\shortcatindicesin$ it holds
    \begin{align*}
        \probat{\indexedshortcatvariables} \geq 0 \, ,
    \end{align*}
    and which is normalized, that is
    \begin{align*}
        \contraction{\probat{\shortcatvariables}} = 1 \, .
    \end{align*}
\end{definition}

\begin{example}{Coin toss}
    Consider two coin tosses \(\catvariableof{0},X_1\in\{0,1\}\) (1=heads). With $p \in [0,1]$ being the probability of heads. Then the probability for each toss and the joint probability have the form
    \begin{align*}
        \mathbb{P}[\catvariableof{0}] = \begin{bmatrix}
                                            \mathbb{P}[\catvariableof{0}=0]\\\mathbb{P}[X_1=1]
        \end{bmatrix} = \begin{bmatrix}
                            1-p\\p
        \end{bmatrix}, \qquad
        \mathbb{P}[X_{[2]}] = \begin{bmatrix}
        (1-p)
                                  ^2 & (1-p)p\\
                                  p(1-p) & p^2
        \end{bmatrix}
    \end{align*}
\end{example}
In most of the analysis, we assume that every state in the space is assigned a positive probability. We say, a distribution $\probwith$ is \emph{positive} if $\mathbb{P}[X_{[d]} = x_{[d]}]>0$ for every configuration $x_{[d]}$.
%
Note that in this tensor centered notation the calculation of marginal distributions reduces to contracting the open legs, not considered in the marginal with tensors $\ones$ only containing ones, see~\cite[Section 2.1.3]{goessmann2025}.
\begin{center}
    \input{../tikz_pics/probability_representation/marginalized_probability.tex}
\end{center}

The number of coordinates in a tensor representation of probability distributions is the product
\begin{align*}
    \prod_{\catenumeratorin}\catdimof{\catenumerator} \, ,
\end{align*}
and therefore scales exponentially in the number of coordinates.
To find efficient representation schemes of probability distributions by tensor networks, we need to exploit additional properties of the distribution, such as Markov chain properties or independence. We further explore one property based on sufficient statistics.


% \textcolor{gray}{One of them is by employing the chain rule.
% \begin{theorem}[Chain Rule]
%     \label{the:chainRule}
%     For any probability distribution $\probwith$ we have
%     \begin{align*}
%         \probat{\shortcatvariables}
%         = \contractionof{\{\margprobat{\catvariableof{0}}\} \cup
%         \left\{\condprobof{\catvariableof{\catenumerator}}{\catvariableof{0},\ldots,\catvariableof{\catenumerator-1}} \wcols \catenumeratorin \ncond \catenumerator\geq 1\right\}
%         }{\shortcatvariables} \, ,
%     \end{align*}
%     provided that all conditional probability distributions exist.
% \end{theorem}
% \textcolor{darkgray}{ graphical notation of chain rule/Markov chain?}
% In case of Markov chains, where each random variable only depends on the previous random variable, this leads to a efficient representation.
% In general, the chain rule does not lead to a complexity reduction as the distribution $\condprobof{\catvariableof{\catenumerator}}{\catvariableof{0},\ldots,\catvariableof{\catenumerator-1}}$ has the same dimensions as the original probability distribution $\probat{\shortcatvariables}$.
% Therefore, other decompositions can be considered.
% }
% \alex{Its important to note, that the chain rule for itself does not provide a more efficient representation of the distribution (since it is a generic decomposition). 
% To achieve efficient tensor network decomposition, one needs to combine that with conditional independence assumptions (as in the Markov Chain case), such that conditioned variables in the conditional probability distributions can be dropped.}

\subsection{The \IndependenceMechanism{}: Graphical Model Factorization}

Markov Networks on hypergraphs $\graph$ are exactly those \CompActNets{}, which are computable with respect to the identity and the graph $\graph$.
We here show how the sets of Markov Networks can be characterized by conditional independence assumptions.

%\subsection{Example of graphical models, reference to "the independence mechanism" (Hammersley-Clifford theorem)}

%\alex{We can present graphical models and independences as decomposition schemes of activation tensors.
%Usually in graphical models there are no computation cores, hence the "features" of the statistic are the same as the variables.
%In that case the activation tensor is the same as the joint distribution tensor, that is a contraction of tensors colored by subsets of the variables (building the edges of the graphical models).
%Plan for this section:
%    \begin{itemize}
%        \item Introduce independence and conditional independence, and relate with tensor network decompositions.
%        \item Apply the Hammersley-Clifford theorem to get "global" decompositions of activation tensors into graphical models.
%    \end{itemize}
%}
%
%\maxf{okay, I think we checked all points here now. Maybe add examples?}

%% Needed?

Recall \defref{def:realizableStatDistributions}.
Given a statistic $\sstat:\facstates \to \parspace$ and a hypergraph $\graph=(\nodes,\edges)$ on the image coordinates $\headvariables$, any by $\sstat$ computable and by $\graph$ activated \CompActNets{} has the form
\begin{align*}
    \probwith =  \normalizationof{\acttensorwith,\bencsstatwith}{\shortcatvariables}
\end{align*}
where $\acttensorwith$ is an arbitrary non-negative tensor.
For graphical models we take the \emph{identity statistic}
\[
    \identity\big(\shortcatindices\big)
    = \shortcatindices,
\]
so that the image coordinates coincide with the variables and there are no non-trivial computation cores.
The associated basis encoding is just the identity tensor
\begin{align*}
    \bencodingofat{\identity}{\headvariableof{[\catorder]},\shortcatvariables} = \identityat{\shortcatvariables,\headvariableof{[\catorder]}} \, .
\end{align*}
and therefore, for any activation tensor $\xi[Y[d]]$ we obtain
\begin{align*}
    \probwith
    &= \normalizationof{\acttensorwith,\bencodingofat{\identity}{\headvariableof{[\catorder]},\shortcatvariables}}{\shortcatvariables}
    & = \normalizationof{\acttensorat{\shortcatvariables}}{\shortcatvariables}
\end{align*}
In other words, in the graphical–model case the activation tensor coincides with the joint distribution tensor.
In this setting, structural properties of the distribution such as (conditional) independences can be read off as algebraic factorization patterns of the activation (and hence joint) tensor.

%% Independence

Independence leads to severe sparsifications of conditional probabilities and is therefore the key assumption to gain sparse decompositions of probability distributions.
Before showing such decomposition schemes, we first provide a coordinatewise definition of independent variables.

\begin{definition}[Independence]
    \label{def:independence}
    We say that $\exrandom$ is independent of $\secexrandom$ with respect to a distribution $\probat{\exrandom,\secexrandom}$, if for any values $\exrandindin$ and $\secexrandindin$ the distribution satisfies
    \begin{align*}
        \probat{\exrandom,\secexrandom} = \probat{\exrandom} \otimes \probat{\secexrandom} \, ,
    \end{align*}
%    where $\probat{\exrandom}$ and $\probat{\secexrandom}$ are the marginal prob
%
%    \begin{align*}
%        \probat{\indexedexrandom,\indexedsecexrandom}
%        = \margprobat{\indexedexrandom}\cdot\margprobat{\indexedsecexrandom} \, .
%    \end{align*}
    In this case we denote $\independent{\exrandom}{\secexrandom}$.
\end{definition}

%This can be transferred to a contraction equation of tensor networks:
%
%
%\begin{theorem}[Independence Criterion as a Contraction Equation]
%    \label{the:independenceProductCriterion}
%    The variable $\exrandom$ is independent from $\secexrandom$ with respect to a probability distribution $\probat{\exrandom,\secexrandom}$, if and only if
%    \begin{align*}
%        \probat{\exrandom,\secexrandom}
%        = \contractionof{\contractionof{\probat{\exrandom,\secexrandom}}{\exrandom},\contractionof{\probat{\exrandom,\secexrandom}}{\secexrandom}}{\exrandom,\secexrandom} \, .
%    \end{align*}
%\end{theorem}
%The proof of this theorem can be found in \cite[Theorem 2.12]{goessmann2025}.

%In the graphical–model \CompActNets with identity statistic, the joint distribution $\probat{\catvariableof{0},\catvariableof{1}}$ is represented by the activation tensor $\xi[\catvariableof{0},X_1]$.
%By \theref{the:independenceProductCriterion}, independence of $\catvariableof{0}$ and $X_1$ is equivalent to the factorization
%\begin{align*}
%   \probat{\catvariableof{0},\catvariableof{1}}
%   = \probat{\catvariableof{0}} \otimes \proabat{\catvariableof{1}} \, .
%\end{align*}
%which in activation form reads
%\[
%    \xi[\catvariableof{0},X_1] = \xi[\catvariableof{0}]\otimes \xi[X_1].
%\]
Thus, independence appears directly as a tensor–product decomposition of probability distribution. %the activation (and hence joint) tensor into two unary activation cores, already implying a substantial reduction in degrees of freedom.
%Two jointly distributed variables are by \theref{the:independenceProductCriterion} independent, if and only if their joint distribution $\probat{\exrandom,\secexrandom}$ is the tensor product of marginal probabilities.
Using tensor network diagrams we depict this property by
\begin{center}
    \input{../tikz_pics/probability_representation/independent_decomposition}
\end{center}
Let us notice, that the assumption of independence reduces the degrees of freedom from $\exranddim\cdot\secexranddim-1$ to $(\exranddim-1)+(\secexranddim-1)$.
The decomposition into marginal distributions furthermore exploits this reduced freedom and provides an efficient storage.
Having a joint distribution of multiple variables, which disjoint subsets are independent, we can iteratively apply the decomposition scheme.
As a result, we can reduce the scaling of the degrees of freedom from exponential to linear by the assumption of independence.

Independence is, as we observed, a strong assumption, which is often too restrictive.
Conditional independence instead is a less demanding assumption, which still implies efficient tensor network decompositions schemes.
We introduce conditional independence as independence of variables with respect to conditional distributions.


\begin{definition}[Conditional Independence]
    \label{def:condIndependence}
    Given a joint distribution of variables $\exrandom$, $\secexrandom$ and $\thirdexrandom$, such that $\margprobat{\thirdexrandom}$ is positive.
    We say that $\exrandom$ is independent of $\secexrandom$ conditioned on $\thirdexrandom$ if for any states $\exrandindin,\secexrandindin$ and $\thirdexrandindin$
    \begin{align*}
        \condprobof{\exrandom,\secexrandom}{\thirdexrandom}
        = \contractionof{
            \condprobof{\exrandom}{\thirdexrandom},\condprobof{\secexrandom}{\thirdexrandom}
        }{\exrandom,\secexrandom,\thirdexrandom} \, .
    \end{align*}
%    \begin{align*}
%        \condprobof{\indexedexrandom,\indexedsecexrandom}{\indexedthirdexrandom}
%        = \condprobof{\indexedexrandom}{\indexedthirdexrandom}
%        \cdot \condprobof{\indexedsecexrandom}{\indexedthirdexrandom}   \, .
%    \end{align*}
    In this case we denote $\condindependent{\exrandom}{\secexrandom}{\thirdexrandom}$.
\end{definition}

There are different ways to decompose a probability distribution into hopefully smaller parts, which can be represented more efficiently.

Conditional independence stated in \defref{def:condIndependence} has a close connection with independence stated in \defref{def:independence}.
To be more precise, $\exrandom$ is independent of $\secexrandom$ conditioned on $\thirdexrandom$, if and only if $\exrandom$ is independent of $\secexrandom$ with respect to any slice $\condprobof{\exrandom,\secexrandom}{\thirdexrandom=\thirdexrandind}$ of the conditional distribution $\condprobof{\exrandom,\secexrandom}{\thirdexrandom}$.
We further find a decomposition criterion for conditional independence.
Since conditional independence can be regarded as a property of conditional probabilities, this decomposition criterion also involves conditional probabilities.


%\begin{theorem}[Conditional Independence as a Contraction Equation]
%    \label{the:condIndependenceProductCriterion}
%    Given a distribution $\probtensor$ of variables $\exrandom$, $\secexrandom$ and $\thirdexrandom$, the variable $\exrandom$ is independent of $\secexrandom$ conditioned on $\thirdexrandom$, if and only if the equation
%    \begin{align*}
%        \condprobof{\exrandom,\secexrandom}{\thirdexrandom}
%        = \contractionof{
%            \condprobof{\exrandom}{\thirdexrandom},\condprobof{\secexrandom}{\thirdexrandom}
%        }{\exrandom,\secexrandom,\thirdexrandom}
%    \end{align*}
%    holds.
%\end{theorem}

We can further exploit conditional independence to ﬁnd tensor network decompositions of probabilities, as we show as the next corollary.

\begin{corollary}
    \label{cor:secCriterionCondIndepencence}
    %Let $\probat{\exrandom,\secexrandom,\thirdexrandom}$ be a distribution, such that
    If and only if $\exrandom$ is independent of $\secexrandom$ conditioned on $\thirdexrandom$ the probability distribution $\probtensor$ satisfies (see \figref{fig:condIndependenceDecomposition})
    \begin{align*}
        \probat{\exrandom,\secexrandom,\thirdexrandom}
        = \contractionof{\condprobof{\exrandom}{\thirdexrandom},\condprobof{\secexrandom}{\thirdexrandom},\margprobat{\thirdexrandom}}{\exrandom,\secexrandom,\thirdexrandom} \, .
    \end{align*}
\end{corollary}

\begin{figure}[t]
    \begin{center}
        \input{../tikz_pics/probability_representation/cond_independence_decomposition}
    \end{center}
    \caption{Diagrammatic visualization of the contraction equation in \corref{cor:secCriterionCondIndepencence}.
    Conditional independence of $\exrandom$ and $\secexrandom$ given $\thirdexrandom$ holds if the contraction on the right side is equal to the probability tensor on the left side.}
    \label{fig:condIndependenceDecomposition}
\end{figure}

%%% Repetition!
%By \theref{the:condIndependenceProductCriterion} and \corref{cor:secCriterionCondIndepencence}, the conditional independence $\catvariableof{0} \perp X_1 \mid X_2$ is equivalent to the factorization
%\[
%    \mathbb{P}[\catvariableof{0},X_1,X_2]
%    =
%    \bigl\langle
%    \mathbb{P}[\catvariableof{0}\mid X_2],\,\mathbb{P}[X_1\mid X_2],\,\mathbb{P}[X_2]
%    \bigr\rangle[\catvariableof{0},X_1,X_2],
%\]
%which is precisely the three–node graphical model with edges $(\catvariableof{0},X_1)$ and $(X_1,X_2)$ depicted in \figref{fig:condIndependenceDecomposition}.
%In the graphical–model CAN with identity statistic, the joint activation tensor $\xi[\catvariableof{0},X_1,X_2]$ thus decomposes into three smaller cores: a unary core on $X_2$ and two binary cores on $(\catvariableof{0},X_2)$ and $(X_1,X_2)$.
This conditional–independence pattern is the basic local building block that is generalized in Markov networks, which we define in the following.

% Place this below
It is known that probabilistic graphical models are dual to tensor networks \cite{robeva_duality_2019,glasser_expressive_2019}.
%By our hypergraph based definition of tensor networks, markov networks are equivalent to tensor networks of positive tensors.
We define graphical models based on hypergraphs, to establish a direct connection with tensor network decorating the hypergraph.
In a more canonical way, Markov Networks are instead defined by graphs, where instead of the edges the cliques are decorated by factor tensors (see for example \cite{koller_probabilistic_2009}).

\begin{definition}[Markov Network]
    \label{def:markovNetwork}
    Let $\tnetof{\graph}$ be a tensor network of non-negative tensors decorating a hypergraph $\graph$.
    Then the Markov Network $\probof{\graph}$ to $\tnetof{\graph}$ is the probability distribution of $\catvariableof{\node}$ defined by the tensor
    \begin{align*}
        \probofat{\graph}{\nodevariables} = \frac{
            \contractionof{\{\hypercoreof{\edge} \wcols \edgein\}}{\nodevariables}
        }{
            \contraction{\{\hypercoreof{\edge} \wcols \edgein\}}
        } = \normalizationof{\tnetof{\graph}}{\nodevariables} \, .
    \end{align*}
    We call the denominator
    \begin{align*}
        \partitionfunctionof{\tnetof{\graph}} = \contraction{\{\hypercoreof{\edge} \wcols \edgein\}}
    \end{align*}
    the partition function of the tensor network $\tnetof{\graph}$.
\end{definition}

We can interpret the factors $\hypercorewith$ as activation cores placed on the hyperedges $\edge$ of the graph.
The global activation tensor (and hence the joint distribution) is obtained by contracting this activation network and normalizing by its partition function.

We call $\probwith$ \emph{positive} if $\probat{\indexedshortcatvariables}> 0$ for all states $\shortcatindices$.
The marginalization of a Markov Network to $\tnetof{\graph}$ on subsets of variables $\catvariableof{\secnodes}$ is
\begin{align*}
    \probofat{\graph}{\catvariableof{\secnodes}}
    = \normalizationof{\tnetof{\graph}}{\catvariableof{\secnodes}} \, .
\end{align*}

%This can be derived from a commutativity of contractions, which established an equivalence of contractions with sequences of consecutive contractions.
%\begin{theorem}[Commutativity of Contractions] % NEEDED?
%    \label{the:splittingContractions}
%    Let $\tnetof{\graph}$ be a tensor network on a hypergraph $\graph=(\nodes,\edges)$.
%    Let us now split the $\graph$ into two graphs $\graph_1=(\nodesone,\edges_1)$ and $\graph_2=(\nodestwo,\edges_2)$, such that $\edges_1\dot{\cup}\edges_2=\edges$, $\nodesone\cup\nodestwo=\nodes$ and all nodes in $\nodestwo$ are contained in an hyperedge of $\edges_2$.
%    We then have for any $\secnodes\subset\nodes$
%    \begin{align*}
%        \contractionof{\tnetof{\graph}}{\catvariableof{\secnodes}}
%        = \contractionof{
%            \tnetofat{\graph_1}{\catvariableof{\nodesone}}
%            \cup \{\contractionof{\tnetof{\graph_2}}{\catvariableof{\nodestwo\cap(\nodesone\cup\secnodes)}}\}
%        }{\catvariableof{\secnodes}}   \, .
%    \end{align*}
%\end{theorem}

Further, the distribution of $\catvariableof{\secnodes}$ conditioned on $\catvariableof{\thirdnodes}$, where $\secnodes,\thirdnodes$ are disjoint subsets of $\nodes$, is
\begin{align*}
    \probtensor^{\graph}\left[\catvariableof{\secnodes}|\catvariableof{\thirdnodes}\right]
    = \normalizationofwrt{\tnetof{\graph}}{\catvariableof{\secnodes}}{\catvariableof{\thirdnodes}} \, .
\end{align*}

While we have directly defined Markov Networks as decomposed probability distributions, we now want to derive assumptions on a distribution assuring that such decompositions exist.
As we will see, the sets of conditional independencies encoded by a hypergraph are captured by its seperation properties, as we define next.

\begin{definition}[Separation of Hypergraph]
    A path in a hypergraph is a sequence of nodes $\node_{\atomenumerator}$ for $\atomenumeratorin$, such that for any $\atomenumerator\in[\atomorder-1]$ we find a hyperedge $\edgein$ such that $(\node_{\atomenumerator}, \node_{\atomenumerator+1})\subset \edge$.
    Given disjoint subsets $\nodesa$, $\nodesb$, $\nodesc$ of nodes in a hypergraph $\graph$ we say that $\nodesc$ separates $\nodesa$ and $\nodesb$ with respect to $\graph$, when any path starting at a node in $\nodesa$ and ending in a node in $\nodesb$ contains a node in $\nodesc$.
\end{definition}

To characterize Markov Networks in terms of conditional independencies we need to further define the property of clique-capturing.
This property of clique-capturing established a correspondence of hyperedges with maximal cliques in the more canonical graph-based definition of Markov Networks \cite{koller_probabilistic_2009}.

\begin{definition}[Clique-Capturing Hypergraph]
    \label{def:ccHypergraph}
    We call a hypergraph $\graph$ clique-capturing, when each subset $\secnodes\subset\nodes$ is contained in a hyperedge, if for any $a,b\in\secnodes$ there is a hyperedge $\edgein$ with $a,b\in\secnodes$.
\end{definition}

Let us now show a characterization of Markov Networks in terms of conditional independencies.

% Characterization
\begin{theorem}[Hammersley-Clifford Factorization Theorem]
    \label{the:factorizationHammersleyClifford}
    Given a clique-capturing hypergraph $\graph$, the set of positive Markov Networks on the hypergraph coincides with the set of positive probability distributions, such that for each disjoint subsets of variables $\nodesa$, $\nodesb$, $\nodesc$ we have $\catvariableof{\nodesa}$ is independent of $\catvariableof{\nodesb}$ conditioned on $\catvariableof{\nodesc}$, when $\nodesc$ separates $\nodesa$ and $\nodesb$ in the hypergraph. % called d-separation
\end{theorem}

\input{../examples/coin_toss_hc.tex}


Equivalently, \theref{the:factorizationHammersleyClifford} states that for any strictly positive joint distribution $\mathbb{P}[X_V]$ whose conditional independencies are exactly those encoded by a clique–capturing hypergraph $G=(V,E)$, there exist non–negative activation cores $\tau_e[X_e]$ such that
\[
    P[X_V]
    =
    \frac{1}{Z}\,
    \contractionof{}{}
    \bigl\langle \{\tau_e : e \in E(G)\} \bigr\rangle[X_V],
\]
for a suitable normalizing constant $Z>0$.
Thus, the conditional–independence structure of $\mathbb{P}$ determines a global tensor–network decomposition of its activation (and hence joint) tensor.
We refer to this correspondence between independence structure and tensor–network factorization as the \emph{independence mechanism}, in analogy to the computation mechanism provided by sufficient statistics in Section 3.1.

\subsection{The \ComputationMechanism{}: Factorization in presense of Sufficient Statistics}
%\subsection{Sufficient statistics leading to tensor network decompositions ("the computation mechanism")}

\begin{definition}
    Let $\probat{\catvariable,\thirdcatvariable}$ be a joint distribution of the $\catdim$-dimensional variable $\catvariable$ and the $\thirdcatdim$-dimensional variable $\thirdcatvariable$ and let
    \begin{align*}
        \sstat \defcols [\catdim] \rightarrow [\headdim]
    \end{align*}
    be a statistic.
    We are interested in the distribution $\probat{\catvariable,\thirdcatvariable,\headvariableof{\sstat}}=\contractionof{\probat{\catvariable,\thirdcatvariable},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\catvariable,\thirdcatvariable,\headvariableof{\sstat}}$.
    We say that $\sstat$ is a sufficient statistic for $\thirdcatvariable$ if and only if $\catvariable$ is independent of $\thirdcatvariable$ conditioned on $\headvariableof{\sstat}$.
\end{definition}

Note that the independence is true if and only if
\begin{align*}
    \condprobat{\catvariable}{\thirdcatvariable,\headvariableof{\sstat}}
    =   \condprobat{\catvariable}{\headvariableof{\sstat}} \otimes \onesat{\thirdcatvariable} \, .
\end{align*}

\input{../examples/sufstat_probability}

\exaref{exa:sufStatProb} hints at a connection between sufficient statistics and decompositions into \CompActNets{}.
More generally, such decompositions are provided by the Fisher-Neyman Factorization Theorem.

\begin{theorem}[Fisher-Neyman Factorization Theorem]
    \label{the:factorizationFisherNeyman}
    Let $\probtensor$ be a joint distribution of variables $Z,X$ with values $\mathrm{val}(Z), \,\mathrm{val}(X)$ and let $T(X)$ be a statistic.
    Then $\sstat$ is a sufficient statistic for $\thirdcatvariable$ if and only if there are tensors $\basemeasureat{\catvariable}$ and $\acttensorat{\headvariableof{\sstat},\thirdcatvariable}$ such that
    \begin{align*}
        \probat{\catvariable,\thirdcatvariable}
        = \contractionof{
            \acttensorat{\headvariableof{\sstat},\thirdcatvariable}\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable},\basemeasureat{\catvariable}
        }{\catvariable,\thirdcatvariable} \, .
    \end{align*}
\end{theorem}

The Fisher-Neyman Theorem is the fundamental motivation for the \CompActNets{} Architecture:
\begin{itemize}
    \item The tensors in a decomposition of $\acttensorwith$ are called \emph{activation cores}.
    \item The tensors in a decomposition of $\bencodingofat{\sstat}{\headvariables,\shortcatvariables}$ are called \emph{computation cores}.
\end{itemize}

\input{../examples/coin_toss_ft}

\subsection{Exponential families in case of elementary activation tensors}


%\alex{Further motivation: The Pitman-Koopman-Darmois theorem, stating that finite sufficient statistics (for the likelihood of a sequence of independent data) are only possible in exponential families, given that the support is constant.}

% Universal properties
A classical theorem by Pitman-Koopman-Darmois (see \cite{pitman_sufficient_1936}) states, that whenever a family with constant support and a finite sufficient statistic for arbitrary large data sets is in an exponential family.
We now restrict the activation cores to specific elementary tensors, which correspond with further assumptions on the dependence of $\probtensor$ and $\sstat$ made by exponential families.
For a discussion of further universal properties of exponential families, such that the existence of priors and entropy maximizers, see \cite{murphy_probabilistic_2022}.

\begin{definition}[Exponential Family]
    \alex{from \theref{the:expFamilyTensorRep}}
    Given any base measure $\basemeasure$ and a sufficient statistic $\sstat$ we enumerate for each coordinate $\selindexin$ the image $\imageof{\sstatcoordinateof{\selindex}}$ by a variable $\headvariableof{\selindex}$ taking values in $[\cardof{\imageof{\sstatcoordinateof{\selindex}}}]$ (see for more details on this scheme \charef{cha:basisCalculus}), given an interpretation map
    \begin{align*}
        \indexinterpretationof{\selindex} \defcols
        [\cardof{\imageof{\sstatcoordinateof{\selindex}}}] \rightarrow \imageof{\sstatcoordinateof{\selindex}} \, .
    \end{align*}
%    Since $\imageof{\sstatcoordinateof{\selindex}}$
    For any canonical parameter vector $\canparamwithin$ we build the activation cores $\softactlegwith$ for each coordinate $\headindexof{\selindex}\in[\cardof{\imageof{\sstatcoordinateof{\selindex}}}]$ by
    \begin{align*}
        \softactleg\left[\indexedheadvariableof{\selindex}\right]
        = \expof{\canparamat{\indexedselvariable} \cdot \indexinterpretationofat{\selindex}{\headindexof{\selindex}} } \,
    \end{align*}
    and have
    \begin{align*}
        \expdistwith =
        \normalizationof{\{\basemeasurewith\} \cup \{\bencodingofat{\sstatcoordinateof{\selindex}}{\headvariableof{\selindex},\shortcatvariables} \wcols \selindexin\}\cup\{\softactlegwith \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
    We then call the set
    \begin{align*}
        \expfamily=\{\expdist\wcols\canparamwithin\}
    \end{align*}
    the exponential family with respect to the statistic $\sstat$ and the base measure $\basemeasure$.
\end{definition}

\input{../examples/two_boolean_cascade}

\subsection{Contractions to compute marginal probabilities}

\alex{The most central inference operation on probability distributions is the computation of marginals, which is just the contraction leaving only the marginalized variable open.
Conditional probabilities are marginal proabilities of the distribution with added boolean tensors marking the evidence (e.g. one-hot encodings to states of some variables).
With this formalism we can answer queries like "What is the probability of booking this account when having observed this feature on the invoice?".
}

Having described how independence structures induce sparse tensor–network factorizations of the activation (and hence joint) tensor via the Hammersley-Clifford theorem (\theref{the:factorizationHammersleyClifford}) in the previous section, we now turn to inference.
In this section we explain how standard inference queries, namely marginals and conditional probabilities, are realized as tensor contractions of the resulting decomposed network.

In the graphical–model CAN with identity statistic, the joint distribution is represented by the activation tensor $\xi[X_V] = \mathbb{P}[X_V]$ on the variable set $V$.
For any subset $A \subseteq V$, the marginal distribution on $X_A$ is obtained by contracting all non–query legs with trivial tensors $I[X_v]$:
\[
    \mathbb{P}[X_A]
    =
    \bigl\langle
    \mathbb{P}[X_V],
    \{ I[X_v] : v \in V \setminus A \}
    \bigr\rangle[X_A].
\]
In words, we close all legs corresponding to variables outside $A$ with identity tensors and read off the resulting tensor on the remaining open legs $X_A$.
In the Markov/activation network representation of Section~3.3, computing a marginal thus corresponds to closing all non–query nodes and edges and evaluating the resulting contracted network on the query variables.

We can interpret conditionals as normalized evidence slices. Let $A,C \subseteq V$ and fix evidence $X_C = x_C$.
Using the one–hot encodings $\epsilon_{x_c}[X_c]$ from \defref{def:onehotenc}, we define the unnormalized slice
\begin{equation*}
    \mathbb{\tilde{P}}[X_A \mid X_C = x_C]:=
    \bigl\langle
    \mathbb{P}[X_V],
    \{\epsilon_{x_c}[X_c] : c \in C\},
    \{ I[X_v] : v \in V \setminus (A \cup C) \}
    \bigr\rangle[X_A].
\end{equation*}
The corresponding conditional distribution is obtained by normalization,
\begin{equation}
    \mathbb{P}[X_A \mid X_C = x_C] = \frac{\mathbb{\tilde{P}}[X_A \mid X_C = x_C]}{\mathbb{\tilde{P}}[X_A \mid X_C = x_C][\emptyset]}
    = \frac{\mathbb{\tilde{P}}[X_A \mid X_C = x_C]}
    {\displaystyle \sum_{x_A} \mathbb{\tilde{P}}[X_A = x_A \mid X_C = x_C]}.
\end{equation}


Thus conditional probabilities are obtained by contracting the activation/joint tensor with one–hot evidence tensors and renormalizing the resulting slice.
In a Computation–Activation Network, inference with evidence therefore corresponds to contracting the activation network with one–hot tensors encoding observed variables and reading out the resulting activation on the query legs.

\maxf{Accounting example here?}
