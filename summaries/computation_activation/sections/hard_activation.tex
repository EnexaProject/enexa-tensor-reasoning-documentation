\section{Decompositions based on Propositional Syntax}\label{sec:log_rep}

A tensor-based representation of propositional logic is developed by encoding boolean variables into vectors, defining formulas as boolean tensors, and showing how logical connectives and normal forms can be expressed as tensor contractions.

Here propositional logic is based on propositional formulas. Define
\begin{itemize}
    \item a \emph{propositional formula} as a boolean-valued tensor
    \begin{align*}
        \formulaat{\shortcatvariables} \defcols \atomstates \rightarrow \ozset \subset \rr \, ,
    \end{align*}
    \item a \emph{model} of a propositional formula as a state $\shortcatindices \in \atomstates$, which fulfills
    $% \begin{align*}
    \formulaat{\indexedshortcatvariables}=1 \, ,
    $% \end{align*}
    where we associate $\text{True}\leftrightarrow 1$ and $\text{False}\leftrightarrow 0$,
    \item and the propositional formula to be \emph{satisfiable}, if there is a model.
\end{itemize}

\begin{example}
    \label{ex:propform}
    The propositional formula defined for $d=3$ and $x_{[3]} = (x_1,x_2,x_3)\in \atomstates = \{0,1\}\times \{0,1\} \times \{0,1\}$ by
    \begin{align*}
        \formulaat{\shortcatvariables = x_{[3]}} = x_1 \wedge (x_2 \vee x_3)
    \end{align*}
    is satisfiable, since $\formulaat{\shortcatvariables = (1,1,1)} = 1$, $\formulaat{\shortcatvariables = (1,0,1)} = 1$, and $\formulaat{\shortcatvariables = (1,1,0)} = 1$ and therefore $x=(1,1,1)$, $x=(1,0,1)$, and $x=(1,1,0)$ are models of $\formulaat{\shortcatvariables}$.
\end{example}

Representing booleans by elements in $\{0,1\}$ leads to the problem, that negation is an affine transformation and can not be represented by multilinear tensors~\cite[Section 4.1.1]{goessmann2025}. Therefore, instead of using this \emph{coordinate calculus} an approach based on \emph{basis calculus} is employed, which is explained in this section.

This representation of propositional formulas with respect to basis encoding also leads to \ComputationActivationNetworks{}, which were also used to describe probability distributions in the last section. In this way the soft and hard logic can be combined in one framework.

\subsection{Propositional Semantics by Boolean Tensors}

To be able to express different kinds of connectives and finally any propositional formula by multi-linear tensors, booleans are encoded by one-hot encodings %vectors as
% \begin{align*}
%     \text{True}\mapsto 
%     1 \mapsto \begin{pmatrix}
%         0\\1
%     \end{pmatrix}\eqqcolon \tbasisat{Y},\\
%     \text{False}\mapsto 
%     0 \mapsto \begin{pmatrix}
%         1\\0
%     \end{pmatrix}\eqqcolon \fbasisat{Y}.
% \end{align*}
% This is the one-hot encoding of boolean states 
as defined in Definition~\ref{def:onehotenc}.

% \begin{definition}

%     \label{def:formulas}
%     A \emph{propositional formula} $\formulaat{\shortcatvariables}$ depending on $\atomorder$ atoms $\catvariableof{\atomenumerator}$ is a boolean-valued tensor
%     \begin{align*}
%         \formulaat{\shortcatvariables} \defcols \atomstates \rightarrow \ozset \subset \rr \, .
%     \end{align*}
%     We call a state $\shortcatindices \in \atomstates$ a \emph{model} of a propositional formula $\formula$, if
%     \begin{align*}
%         \formulaat{\indexedshortcatvariables}=1 \, .
%     \end{align*}
%     If there is a model to a propositional formula, we say the formula is \emph{satisfiable}.
% \end{definition}

There are different ways to represent propositional formulas based on tensor decompositions and basis encoding representation tensors.

\paragraph{CP decomposition}
Since the tensor $\formulaat{\shortcatvariables}$ is equal to one at index $x_{[d]}$ if and only if $x_{[d]}$ is a model of $\formula$, i.e. fulfills the formula, A propositional formula can be written as the sum over the one-hot encodings of its models.
\begin{center}
    \input{../tikz_pics/logic_representation/formula_direct.tex}
\end{center}
This decomposition corresponds to the CP decomposition of a tensor.
\begin{example}
    For the formula described in Example~\ref{ex:propform}, we have
    \begin{align*}
        \formulaat{X_{[3]}} &= \left(\tbasisat{X_1} \otimes \tbasisat{X_2} \otimes \tbasisat{X_3}\right) + (\tbasisat{X_1} \otimes \fbasisat{X_2} \otimes \tbasisat{X_3}) + (\tbasisat{X_1} \otimes \tbasisat{X_2} \otimes \fbasisat{X_3}),
    \end{align*}
    where $\tbasisat{Y} = (0,1)^T$ and $\fbasisat{Y} = (1,0)^T$. Then for the model $x_{[3]} = (1,0,1)$ it holds
    \begin{align*}
        \formulaat{X_{[3]}=x_{[3]}} = &\left(\tbasisat{X_1=1} \otimes \tbasisat{X_2=0} \otimes \tbasisat{X_3=1}\right) + (\tbasisat{X_1=1} \otimes \fbasisat{X_2=0} \otimes \tbasisat{X_3=1}) \\
        &+ (\tbasisat{X_1=1} \otimes \tbasisat{X_2=0} \otimes \fbasisat{X_3=1})\\
        = & 1\cdot 0 \cdot 1 + 1\cdot 1 \cdot 1 + 1 \cdot 0 \cdot 0 = 1.
    \end{align*}
\end{example}

\paragraph{Basis encoding}
Propositional formulas $\formula$ can be expressed in terms of a tensor describing the mapping and its negation by
\begin{align}
    \label{eq:basisencboolean}
    \bencodingofat{\formula}{\formulavar = y,\shortcatvariables = x_{[d]}}
    = \begin{cases}
          1, & \formulaat{\shortcatvariables = x_{[d]}} = y\\
          0, & \text{else}
    \end{cases}.
\end{align}
This basis encoding $\bencodingofat{\formula}{\formulavar,\shortcatvariables} \in \{0,1\}^{2\times 2^d}$ then has the form
\begin{align}
    \label{eq:basisencnegsum}
    \bencodingofat{\formula}{\formulavar,\shortcatvariables} = \formulaat{\shortcatvariables} \otimes \tbasisat{\formulavar} + \lnot\formulaat{\shortcatvariables} \otimes \fbasisat{\formulavar}.
\end{align}
Then the propositional formula and its negation can be represented by that tensor by
\begin{align*}
    \formulaat{\shortcatvariables}
    = \contractionof{\bencodingofat{\formula}{\formulavar,\shortcatvariables},\tbasisat{\formulavar}}{\shortcatvariables}
\end{align*}
and
\begin{align*}
    \lnot\formulaat{\shortcatvariables}
    = \contractionof{\bencodingofat{\formula}{\formulavar,\shortcatvariables},\fbasisat{\formulavar}}{\shortcatvariables} \, .
\end{align*}
% which are \ComputationActivationNetworks{} to the statistic $\{\formula\}$ and the hard activation tensor $\tbasisat{\formulavar}$, respectively $\fbasisat{\formulavar}$.


%As in \cite[Section 4.2.2]{goessmann2025},
In our graphical notation the basis encoding has the following form.
\begin{center}
    \input{../tikz_pics/logic_representation/formula_bencoding.tex}
\end{center}
We further provide a more detailled example in coordinate sensitive notation in the foolowing.

\input{../examples/bencoding_neg_con.tex}

%\input{../examples/old_bencoding_coordinate_visualization.tex}

\paragraph{Tensor network decomposition}
If the propositional formula allows for a decomposition into smaller propositional formulas, the representation can be reduced by employing a tensor network structure as described in the next subsection.

\paragraph{Model counts} At each index the propositional formula either carry a $1$ or $0$ encoding if the indexed value is true or false. This implies, that contracting the tensor with trivial $1$-tensors applied to each open leg leaving only one leg open results in a vector counting the number of models of the formula for each possible index of the open leg. Furthermore, contracting the whole tensor yields the number of models of the given formula. This is consistent with a probabalistic interpretation disscussed in later.

\subsection{Propositional Syntax leading to Tensor-Network Decompositions}

\begin{definition}
    Define an $r$-ary connective as a function $\circ: \times_{\ell\in[r]} [2] \to [2]$.
\end{definition}




If $\formulaat{\shortcatvariables}$ is defined as the $r$-ary composition of propositional formulas $\formula_\ell, \ell\in [r]$, i.e.
\begin{align*}
    \formulaat{\shortcatvariables = x_{[d]}} = \circ\left( \formula_0\left[\shortcatvariables=x_{[d]}\right], \dots, \formula_{r-1}\left[\shortcatvariables=x_{[d]}\right]\right),
\end{align*}
then the basis encoding can be written as the contraction of the basis encoding of the $r$-ary composition and the basis encodings of the individual propositional formulas $f_\ell, \ell\in[r]$. For the composition of two propositional formulas $\formulaat{\shortcatvariables}$ and $h\left[\shortcatvariables\right]$ the composition by some binary connective is pictured below.
\begin{center}
    \input{../tikz_pics/logic_representation/unary_binarry_composition}
\end{center}
The figure can be found in~\cite[Figure 4.2 b)]{goessmann2025}, where hyperedges made up by more than two nodes represent the unaltered connection between nodes.

\subsection{Contractions to decide entailment}

As in~\cite[Definition 5.1]{goessmann2025}, we define the entailment of two propositional formulas as follows.
\begin{definition}[Entailment of propositional formulas]
    \label{def:logicalEntailment}
    Given two propositional formulas $\kb$ and $\exformula$ we say that $\kb$ entails $\exformula$, denoted by $\kb\models\exformula$, if any model of $\kb$ is also a model of $\exformula$, that is
    \begin{align*}
        \uniquantwrtof{\shortatomindicesin}{\imppremhead{\kbat{\indexedshortcatvariables}=1}{\formulaat{\indexedshortcatvariables}=1}} \, .
    \end{align*}
    If $\kb\models\lnot\exformula$ holds, we say that $\kb$ contradicts $\exformula$.
\end{definition}
This property of interest, since $\kb$ can be chosen as a knowledge base, which can be interpreted in different ways: as the support of probability distributions,

This is the central operation of "logical inference", i.e. deduce true statements from known statements.
One can in some cases understand this as "making the knowledge base more accessible": Adding deduced statements to a knowledge base does not change the knowledge base as a tensor, but one can interpret it in an easier way.


% \begin{example}{Sudoku rules} In case we aim to solve a Sudoku, the initial board and Sudoku rules can be written in terms of a propositional formula $\kb$.
% To encode the Sudoku each a cell in row $\ell$ and column $k$ for $\ell,k=1,\dots,9$ can be represented by a variable r$\ell$c$k=1,\dots,9$. 
% Each Sudoku rule, e.g. that no number can be repeated in a row, can then be represented in a tensor network by a tensor connecting all variables in one row of size $9^9$.
% Technically the tensor-network contracts to the one-hot encoded unique solution, but neither is this obvious for a human seeing the initial board nor can the contraction be performed with reasonable demand. 
% Therefore, small entailment steps are done iteratively by finding and adding entailed (atomic) formulas concerning single coordinates of the solution.
% Entailments of the form
% \begin{align*}
%     \kb \models r3c5 = 7
% \end{align*}
% need to be decided, which would answer the question “Given all Sudoku rules and the initial clues, is it logically necessary that the cell in row 3, col 5 must be 7?”. Since the contraction of the whole network is infeasible, the hope is to be able to decide this by just contracting locally, e.g. checking with the initial board, if the $7$ could be somewhere else in the box in the upper box row and middle box column.
% One uses thereby the "monotonousity of entailment", i.e. when entailment holds on efficiently contractable subsets of the knowledge base then also on the full knowledge base.
% \end{example}




In the tensor network representation, these entailments can be decided by contracting the whole representing tensor with the statement, that needs to be checked.

% \alex{Maybe we use directly a contraction based definition of entailment (Theorem 5.2 in the report):}

\begin{theorem}[{Entailment in propositional logic~\cite[Theorem 5.2]{goessmann2025}}]
    For two propositional formulas $\kb$, $\exformula$ we have
    \begin{align*}
        \kb\models\exformula \iff \contraction{\kb,\lnot\exformula}=0 \, .
    \end{align*}
\end{theorem}

\input{../examples/sudoku.tex}

\subsection{Efficient Representation of Knowledge Bases}

When having multiple formulas: Represent with multiple $\tbasis$ activation cores~\cite[Theorem 14.36]{goessmann2025}.

We now investigate the representaion of knowledge bases, which are conjunctions
\begin{align*}
    \kbwith
    = \bigwedge_{\selindexin} \formulaofat{\catvariableof{\selindex}}{\shortcatvariables}
\end{align*}

\begin{lemma}[Computation Network Symmetries]
    \label{lem:comNetSymmetries}
    We have
    \begin{align*}
        \contractionof{\tbasisat{\headvariable},\bencodingofat{\land}{\headvariable,\shortcatvariables}}{\shortcatvariables}
        =
        \bigotimes_{\catenumeratorin} \tbasisat{\catvariableof{\catenumerator}}
    \end{align*}
    and
    \begin{align*}
        \contractionof{\tbasisat{\headvariable},\bencodingofat{\lnot}{\headvariable,\catvariable}}{\catvariable}
        =
        \fbasisat{\catvariable} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    Follows directly from the definitions of the basis encodings and the connectives.
\end{proof}

We use this to decompose knowledge bases into their individual formulas as follows.

\begin{theorem}
    \begin{align*}
        \kbwith
        = \contractionof{\{\formulaofat{\catvariableof{\selindex}}{\shortcatvariables} \wcols \selindexin\}}{\shortcatvariables}
    \end{align*}
\end{theorem}
\begin{proof}
    With \lemref{lem:comNetSymmetries} we have
    \begin{align*}
        \kbwith
        &= \contractionof{\{\tbasisat{\headvariableof{\land}},\bencodingofat{\land}{\headvariableof{\land},\headvariables}\}
            \cup \{\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables}\wcols \selindexin\}}{\shortcatvariables} \\
        &= \contractionof{
            \bigcup_{\selindexin} \{\tbasisat{\headvariableof{\selindex}},\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables}\wcols \selindexin\}}{\shortcatvariables} \\
        &= \contractionof{\{\formulaofat{\catvariableof{\selindex}}{\shortcatvariables} \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
\end{proof}



\begin{example}
    \alex{(see the notebook: \url{https://colab.research.google.com/drive/1p2wp61fFMu0otnfFhKoNsLiCNfWpuEsn?usp=sharing})}
    For the example described in~\cite[Figure 4.2]{goessmann2025} with the propositional formula $\formulaat{X_{[3]} = (a,b,c)}={(a \land b) \lor \lnot c}$, we can write the formula in terms of a \ComputationActivationNetwork{} with activation tensor $\tbasis$ and computation network decomposed by the basis encodings. First, it is written with one activation vector. Second, we see that it can also be interpreted with multiple features.
    \begin{center}
        \input{../tikz_pics/logic_representation/old_decomposition_example}
    \end{center}
\end{example}

% BAD NOTATION!
%Noting that for example for $x_a=x_b=\epsilon_1=[0,1]^\intercal$
%\begin{center}
%    \input{../tikz_pics/logic_representation/and_decomposition}
%\end{center}
%while for all other vectors $x_a,x_b$, all parts of the equations amount to $0$. This yields that a knowledge base consisting of multiple formulas connected by a $\land$ has an efficient representation by decomposing the the tensor into its individual formulas.
%

\subsection{Message-passing for Entailment}

% Infeasible constractions
Since contracting the whole tensor is often infeasible and for instance for the Sudoku example would correspond to solving the whole problem, local contractions can be considered to decide in some cases.
Here a local contraction describes the calculation of contractions along few closely connected legs in the tensor network. Now, if the local contraction of any legs leads to a zero-tensor in the network decomposition, the whole contraction amounts to zero, and the knowledge base entails $f$.

\begin{theorem}[Monotonocity of Propositional Logics]
    If $\seckb\subset\kb$ and $\seckb\models\formula$ then also $\kb\models\formula$.
\end{theorem}
\begin{proof}
    Since $\seckb\models\formula$ it holds that $\contraction{\seckb,\lnot\formula}=0$ and thus  $\contractionof{\seckb,\lnot\formula}{\shortcatvariables}=\zerosat{\shortcatvariables}$.
    Denoting by $\kb/\seckb$ the conjunctions of formulas in $\kb$ not in $\seckb$, we have
    \begin{align*}
        \contraction{\kbwith,\lnot\formulawith}%{\shortcatvariables}
        &= \contraction{\kb/\seckb[\shortcatvariables],\seckb,\lnot\formulawith} \\
        &= \contraction{\kb/\seckb[\shortcatvariables],\contractionof{\seckb[\shortcatvariables],\lnot\formula[\shortcatvariables]}{\shortcatvariables}} \\
        &= \contraction{\kb/\seckb[\shortcatvariables],\zerosat{\shortcatvariables}} \\
        &= 0 \, .
    \end{align*}
\end{proof}

To decide entailment, we can therefore investigate entailment on smaller parts of the knowledge base.
This is sound by the above theorem, but not complete, since it can happen that no smaller part of the knowledge base entails the formula, but the whole knowledge base does.

We can futhermore add entailed formulas to the knowledge base without the latter, as we show next.

\begin{theorem}
    If $\kb\models\formula$ then
    \begin{align*}
        \kbwith
        = \contractionof{\kb,\formula}{\shortcatvariables} \, .
    \end{align*}
\end{theorem}

This motivates an message-passing approach to
Such inference methods are nown e.g. as Constraint Propagation.

\begin{example}{Sudoku continued}
    For the Sudoku example, a local contraction of the left- and right-hand-side of~\eqref{eq:sudokukb} means that only direct connections via the basic Sudoku rules and the known board are checked.
    If the local contraction amounts to zero, the direct Sudoku rules already imply, that $X_{r0,r1,c0,c1,n}$ must be true/is the only possible choice.
    In most cases the local contraction will not lead to $0$.
    Then no statement can be made.
\end{example}

% By~\cite[Theorem 5.2]{goessmann2025} the following statements hold:
% \begin{align*}
%     \kb \text{ entails } \exformula \text{ i.e. } \kb\models\exformula &\iff \contraction{\kb,\lnot\exformula} = 0 \text{ and }\\
%     \kb \text{ contradicts } \exformula \text{ i.e. } \kb\models\lnot\exformula &\iff \contraction{\kb,\exformula} = 0.
% \end{align*}
% Additionally, a tensor can be defined, which can encode entailment and contradiction properties simultaneously, see~\cite[Theorem 5.3]{goessmann2025}.
